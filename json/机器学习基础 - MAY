{"title": "机器学习基础 - MAY ", "index": "机器学习,python", "content": "机器学习本质包含了数学原理推导与实际应用技巧\n推论事情的方法：演绎法和归纳法。根据经验进行推论，就像人成长一样。\n基础:\n\n机器学习的目的是：归纳(Induction), 从详细事实到一般推论\n\n找出有效的预测模型\n\n一开始都是从一个简单的模型开始\n藉由不断喂入训练数据，修改模型\n不断提升预测绩效\n\n\n\n机器学习的步骤：\n\n使用者的行为\n收集资料\n数据转换与清洗\n建立模型\n验证模型 （建立模型 和 验证模型 之间反复训练与验证）\n部署模型\n\n机器学习需要什么？ 算法，数据，程序，评估，应用。\n应用的方面：数据挖掘，图像识别，语音和自然语言，统计学习，计算机视觉。\n虚拟环境\n\n通过virtualenv来创建虚拟环境\n通过anaconda来创建虚拟环境\n\nvirtualenv\nvirtualenv就是用来为每一个项目创建一套“独立隔离”的Python运行环境的工具\npip install virtualenv\n创建虚拟环境: virtualenv -p /usr/bin/python2.7 --no-site-packages venvs启动虚拟环境: source venvs/bin/activate退出虚拟环境: deactivate删除虚拟环境: rm -r venvs\nvirtualenv -p /usr/local/bin/python --no-site-packages learn\nsource learn/bin/activate\ndeactivate\nrm -r learn\n可以一次性通过别的机器上或虚拟环境里，将文件里罗列的第三方库安装起来：pip install -r requirements.txt\nanaconda\n安装anaconda：anaconda download\n# 查看帮助\nconda -h \n# 基于python3.6版本创建一个名字为python36的环境\nconda create --name python36 python=3.6 \n# 激活此环境\nsource activate python36  \n# 再来检查python版本，显示是 3.6\npython -V  \n# 退出当前环境\nsource deactivate python36 \n# 删除该环境\nconda remove -n python36 --all\n# 或者 \nconda env remove  -n python36\n\n# 查看所以安装的环境\nconda info -e\nscikit-learn\nscikit-learn官网\n机器学习地图：\n\n\n一定量的样本\nclassification\nclustering\nregression\ndimensionality reduction\n\n机器学习问题分类\n\n监督式学习回归分析：连续性数值，使用一组已知对应值的数据产生的模型，预测新数据的对应值。分类问题：类别标签，根据已知标签的训练数据集，产生一个新模型，用以预测测试数据集的标签。\n非监督式学习降低维度：产生一有最大变异数的字段线性组合,可用来降低原本问题的维度与复杂度分群问题：物以类聚（近朱者赤，近墨者黑）\n\n利用正确的答案的数据来进行学习，就可以称之为监督式学习。通过既有的答案来得到新的理论，调整一些演算的过程，建立模型。\n同样或者类似的数据放在一起，透过放在一起的数据，分析学习，找到需要知道的答案，称之为非监督式学习。\n回归分析\n\n线性回归是研究单一因变量与一个或上一个自变量之间的关系\n线性回归有两个主要用处：   预测指的是用已观察的变量来预测因变量   因果分析则是将自变量当作因变量发生的原因\n\n线性回归\n数学模型：\ny = ax + b # 简单线性回归\ny = ax^2 + bx + c # 二项式线性回归\n最小平方估计法：找出残差平方和最小的一条线\n\n残差计算公式\n残差平方和计算公式\n\n绘制资料：\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\ndf = pd.read_csv('Data/salary.csv', index_col=0)\nX = df[['year']]\nY = df['salary'].values\n\nplt.scatter(X, Y, color='blue')\nplt.xlabel('year')\nplt.ylabel('salary')\n\nplt.show()\n\n绘制回归线：\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\ndf = pd.read_csv('Data/salary.csv', index_col=0)\nX = df[['year']]\nY = df['salary'].values\n\nplt.scatter(X, Y, color='blue')\nplt.xlabel('year')\nplt.ylabel('salary')\n\n# 使用scikit-learn进行预测\nregr = LinearRegression()\nregr.fit(X, Y)\n\n# 将回归线绘制在图上\nprint('Coefficients:', regr.coef_) # 涨幅\nprint('Intercept:', regr.intercept_)\n\nplt.plot(X, regr.predict(X), color='green', linewidth=3)\n\nplt.show()\n\n二次项线性回归：\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndf = pd.read_csv('Data/salary.csv', index_col=0)\nX = df[['year']]\nY = df['salary'].values\n\n# 使用scikit-learn进行预测\npoly_reg = PolynomialFeatures(degree=2)  # 二次项\nX_ = poly_reg.fit_transform(X)\n\n\nregr = LinearRegression()\nregr.fit(X_, Y)\n\nX2 = X.sort_values(['year'])\nX2_ = poly_reg.fit_transform(X2)\n\nplt.scatter(X, Y, color='blue')\nplt.plot(X2, regr.predict(X2_), color='green', linewidth=3)\nplt.xlabel('year')\nplt.ylabel('salary')\n\n# 将回归线绘制在图上\nprint('Coefficients:', regr.coef_)\nprint('Intercept:', regr.intercept_)\n\nplt.show()\n\n回归模型评估\n验证线性关系是显著的。验证方法通过“假设”目的：自变量是否有能力去影响自变量。\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport statsmodels.api as sm\n\ndf = pd.read_csv('Data/house-prices.csv')\n\n# 建立Dummy Variable\ns = pd.get_dummies(df['Brick']) # 根据字段中的值，建立新的字段，并新的字段的值为0或1 # 必须去掉一个字段，去掉的这个字段通过其它一个字段生成。（如果同时存在，会产生共线性问题）\nt = pd.get_dummies(df['Neighborhood']) # 必须去掉一个字段，去掉的这个字段通过其它二个字段生成。\n\nhouse = pd.concat([df, s, t], axis=1)\n\ndel house['No']\ndel house['West']\ndel house['Brick']\ndel house['Neighborhood']\ndel house['Home']\n\nX = house[['SqFt', 'Bedrooms', 'Bathrooms', 'Offers', 'Yes', 'East', 'North']]\nY = house['Price'].values\n\nX2 = sm.add_constant(X)\nest = sm.OLS(Y, X2)\nest2 = est.fit()\nprint(est2.summary()) # 回归模型评估数据\n\n\n假设显著性标准是0.01\n推翻假设的标准是p值 < 0.01 (假设不成立，可以推导出二者变量是密切联系)\n\nt = 2.658, P(>t)=0.009, P(0.09) < 0.01是不成立的，假设也不成立\n验证二者关系显著\n\n\nR-squared: 可作为自变量预测因变量准确度的指标。 值越大越准确，0.5以上可以作为指标。AIC: 鼓励数据拟合的优良性但是尽量避免出现过度拟合的情况。所以优先考虑的模型应该是AIC值最小的那一个.\n分析房天下的上海徐汇区数据\nimport pandas as pd\nimport time\nfrom sklearn.linear_model import LinearRegression\nfrom matplotlib import pyplot as plt\nimport statsmodels.api as sm\n\ndf = pd.read_excel('Data/house_price_regression.xlsx')\n\n# 处理数据\nnow_year = time.localtime(time.time()).tm_year\ndf['age'] = df['age'].map(lambda e: now_year - int(e.strip().strip('建筑年代：')) )\ndf[['room', 'living_room']] = df['layout'].str.extract(r'(\\d+)室(\\d+)厅') # 抽取字段, 房间和厅\ndf['room'] = df['room'].astype(int)\ndf['living_room'] = df['living_room'].astype(int)\ndf['total_floor'] = df['floor_info'].str.extract(r'共(\\d+)层')\ndf['total_floor'] = df['total_floor'].astype(int)\ndf['floor'] = df['floor_info'].str.extract(r'^(.)层')\ndf['direction'] = df['direction'].map(lambda e: e.strip())\n\ndel df['layout']\ndel df['floor_info']\ndel df['title']\ndel df['url']\n\n# 将values处理成字段\ndf = pd.concat([df, pd.get_dummies(df['direction']), pd.get_dummies(df['floor'])], axis=1)\n\ndel df['direction']\ndel df['floor']\ndel df['南北向']\ndel df['低']\n\n\n# 绘制散布图\n# 房价 与 平米\ndf[['price', 'area']].plot(kind='scatter', x='area', y='price', figsize=[10, 5])\n\n\n# 绘制线性模型\nx = df[['area']]\ny = df['price']\nregr = LinearRegression()\nregr.fit(x, y)\n\nprint('Coefficent: {}'.format(regr.coef_))\nprint('Intercept: {}'.format(regr.intercept_))\n\nplt.scatter(x, y, color='blue')\nplt.plot(x, regr.predict(x), linewidth=2, color='red')\nplt.xlabel('area')\nplt.ylabel('price')\n\n# 多元回归预测\ndf_col = list(df.columns)\ndel df_col[2]\nx = df[df_col]\ny = df['price']\nregr = LinearRegression()\nregr.fit(x, y)\nprint(x.info())\n\n# 评估回归模型\nx2 = sm.add_constant(x)\nest = sm.OLS(y, x2)\nest2 = est.fit()\nprint(est2.summary())\n\nplt.show()\n\n资料分类\n监督式学习\n分类问题：根据已知标签的训练数据集，产生一个新模型，用以预测测试数据集的标签\n决策树：\n\n用于计算一个系统中的失序现象，也就是计算该系统混乱的程度。\n决策树的目的行为上的预测和实质的分类\n\n单一变量的计算：\nEntropy = -p * log * p - q * log * q\n\n多变量的计算：\n\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}