{"title": "机器学习——K近邻算法 - 个人文章 ", "index": "python", "content": "机器学习——K近邻算法\n概述\nk近邻是一种基本分类与回归方法.\n\n输入:特征向量\n输出:实例的类别(可取多类)\n核心思想:如果一个样本在特征空间中的k个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性.\n优点:计算精度高、对异常值不敏感、无数据输入假定\n缺点:计算复杂度高、空间复杂度高\n适用范围:数值型和标称型\n\n算法流程\n\n收集数据\n准备数据：距离计算所需要的数值，最好是结构化的数据格式\n分析数据：可以适用任何方法\n训练算法：此步骤不适用于KNN\n测试算法：计算错误率\n使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输入数据分别属于哪个分类，最后应用对计算出的分类执行后续的处理\n\nkNN算法\n构造数据集\nimport numpy as np\n\ndef create_data_set():\n    \"\"\"构造数据集\"\"\"\n    group = np.array([[1.0, 1.1], [1.0, 1.0], [0, 0], [0, 0.1]])\n    label = ['A', 'A', 'B', 'B']\n    return group, label\n生成数据集\ngroup, label = create_data_set()\n实现kNN算法伪代码\n对未知类别属性的数据集种的每个点依次执行以下步骤： \n1. 计算已知类别属性的数据集中的每个点与当前点之间的距离 \n2. 按照距离递增次序排序 \n3. 选取与当前点距离最小的k个点 \n4. 确定前k个点所在类别的出现频率 \n5. 返回前k个点出现频率最高的类别作为当前点的预测分类  ​\nimport operator\n\ndef classify0(inX, data_set, label, k):\n    \"\"\"\n    KNN算法\n    :param inX: 用于分类的输入向量\n    :param data_set: 训练样本集\n    :param label: 训练标签向量\n    :param k: 选取最近邻居的数量\n    :return: k个邻居里频率最高的分类\n    \"\"\"\n\n    \"\"\"距离计算\"\"\"\n    # 获得样本量\n    data_set_size = data_set.shape[0]\n    # tile：在行方向重复inX,dataSetSize次，在列方向上重复inX,1次\n    diff_mat = np.tile(inX, (data_set_size, 1)) - data_set\n    # 离原点的距离，相见后平方\n    sq_diff_mat = diff_mat ** 2\n    # x轴和y轴差的平方和\n    sq_distances = sq_diff_mat.sum(axis=1)\n    # 然后开方\n    distances = sq_distances ** 0.5\n    # argsort函数返回的是数组值从小到大的索引值\n    sorted_distance_index = distances.argsort()\n    class_count = {}\n    \"\"\"选择距离最小的点\"\"\"\n    for i in range(k):\n        # 返回距离最近的第i个样本所对应的标签\n        vote_label = label[sorted_distance_index[i]]\n        # print(voteIlabel)\n        # print(classCount.get(voteIlabel, 0))\n        # 这里的0是设置默认值为0,而代替None。而代码是给出现情况增加次数，出现一次+1\n        class_count[vote_label] = class_count.get(vote_label, 0) + 1\n        # print(classCount)\n    \"\"\"排序\"\"\"\n    # 导入运算符模块的itemgetter方法，按照第二个元素的次序对元组进行排序，此处的排序为逆序。\n    sorted_class_count = sorted(class_count.items(), key=operator.itemgetter(1), reverse=True)\n    # 返回频率最大的Label\n    return sorted_class_count[0][0]\n添加算法测试代码\nclassify0([0, 0], group, label, k=3)\n约会网站示例\n加载并解析数据\n# 将文本记录转换为numpy的解析程序\ndef file2matrix(filename):\n    fr = open(filename)\n    # readlines把文件所有内容读取出来，组成一个列表，其中一行为一个元素\n    array_of_lines = fr.readlines()\n    number_of_lines = len(array_of_lines)\n    # 返回一个用1000行每行3个0填充的数组，形成特征矩阵\n    return_mat = np.zeros((number_of_lines, 3))\n    class_label_vector = []\n    for index, line in enumerate(array_of_lines):\n        # 去除每行前后的空格\n        line = line.strip()\n        # 根据\\t把每行分隔成由四个元素组成的列表\n        list_from_line = line.split('\\t')\n        # 选取前3个元素，将它们按顺序存储到特征矩阵中\n        return_mat[index, :] = list_from_line[0: 3]\n        # 将列表的最后一个元素储存到class_label_vector中去，储存的元素值为整型\n        class_label_vector.append(int(list_from_line[-1]))\n    return return_mat, class_label_vector\n获得解析数据\ndating_data_mat, dating_labels = file2matrix('datingTestSet2.txt')\n使用Matplotlib创建散点图\nimport matplotlib.pyplot as plt\n\n# 创建Figure实例\nfig = plt.figure()\n# 添加一个子图，返回Axes实例\nax = fig.add_subplot(111)  # 选取最近邻居的数量\n# 生成散点图，x轴使用dating_data_mat第二列数据，y轴使用dating_data_mat的第三列数据\n# ax.scatter(x=dating_data_mat[:, 1], y=dating_data_mat[:, 2])\n# 个性化标记散点图，形状(s)和颜色(c)\nax.scatter(x=dating_data_mat[:, 1], y=dating_data_mat[:, 2], s=15.0 * np.array(dating_labels), c=np.array(dating_labels))\nplt.show()\n归一化特征值\n<html><center>newValue=(oldValue−min)/(max−min)</center></html>\ndef auto_num(data_set):\n    \"\"\"\n    归一化特征值\n    :param data_set: 数据集\n    :return 归一化后的数据集， 列的差值范围， 列的最小值\n    \"\"\"\n    # 列的最小值\n    min_val = data_set.min()\n    # 列的最大值\n    max_val = data_set.max()\n    # 列的差值范围\n    range_s = max_val - min_val\n    # 构造返回矩阵\n    norm_data_set = np.zeros(shape=np.shape(data_set))\n    # m = data_set.shape[0]\n    # oldValue - min\n    norm_data_set = data_set - np.tile(min_val, (data_set.shape[0], 1))\n    # (oldValue - min) / (max - min)\n    norm_data_set = norm_data_set / np.tile(range_s, (data_set.shape[0], 1))\n    return norm_data_set, range_s, min_val\n归一化测试\nnormalize_data_set, ranges, min_val = auto_num(dating_data_mat)\nprint(normalize_data_set)\n测试算法\ndef dating_class_test():\n    # 选择测试数据量\n    ho_ratio = 0.10\n    # 解析数据\n    dating_data_mat, dating_labels = file2matrix('datingTestSet2.txt')\n    # 归一化数据\n    norm_mat, range_s, min_val = auto_num(dating_data_mat)\n    # 拆分10%数据作为测试数据\n    m = norm_mat.shape[0]  # 总数据量\n    num_test_vec = int(m * ho_ratio)  # 测试数据量\n    # 错误样本计数\n    error_count = 0.0\n    # 对测试数据进行分类，并对比检验结果正确率\n    for i in range(num_test_vec):\n        classifier_result = classify0(  # classifier_result : k个邻居里频率最高的分类\n            norm_mat[i, :],  # 用于分类的输入向量(测试数据, : 表示一行内所有元素)\n            norm_mat[num_test_vec: m, :],  # 训练样本集(从测试的数据开始到总数据量结束)\n            dating_labels[num_test_vec:m],  # 训练标签向量(从测试的数据开始到总数据量结束)\n            3  # 选取最近邻居的数量\n        )\n        print('the classifier came back with: %d, the real answer is: %d' % (classifier_result, dating_labels[i]))\n        if classifier_result != dating_labels[i]:\n            error_count += 1.0\n    print('the total error rate is: %f' % (error_count / float(num_test_vec)))\n执行测试\ndating_class_test()\n使用算法\ndef classify_person():\n    \"\"\"\n    根据输入指标，通过分类器进行预测喜欢程度\n    :return:\n    \"\"\"\n    result_list = ['not at all', 'in small doses', 'in large doses']\n    percent_tats = float(input('percentage of time spent playing vedio games?'))\n    ff_miles = float(input('frequent flier miles earned per year?'))\n    ice_cream = float(input('liters of ice cream consumed per year?'))\n    dating_data, dating_labels = file2matrix('datingTestSet2.txt')\n    normalize_matrix, ranges, min_val = auto_num(dating_data)\n    # 将输入指标，归一化后代入分类器进行预测\n    in_arr = np.array([ff_miles, percent_tats, ice_cream])\n    print(in_arr, min_val, ranges, (in_arr-min_val)/ranges)\n    print(ranges)\n    classifier_result = classify0((in_arr-min_val)/ranges, normalize_matrix, dating_labels, 3)\n    print(\"You will probably like this person: \", result_list[classifier_result - 1])\n执行函数\nclassify_person()\n输出\npercentage of time spent playing vedio games?20\nfrequent flier miles earned per year?299\nliters of ice cream consumed per year?1\nYou will probably like this person:  in large doses\nsklearn中实现\nfrom sklearn import neighbors\n\ndef knn_classify_person():\n    \"\"\"\n        根据输入指标，通过分类器进行预测喜欢程度\n    :return:\n    \"\"\"\n    result_list = np.array(['not at all', 'in small doses', 'in large doses'])\n    percent_tats = float(input('percentage of time spent playing vedio games?'))\n    ff_miles = float(input('frequent flier miles earned per year?'))\n    ice_cream = float(input('liters of ice cream consumed per year?'))\n    dating_data, dating_labels = file2matrix('datingTestSet2.txt')\n    normalize_matrix, ranges, min_val = auto_num(dating_data)\n    # 将输入指标，归一化后代入分类器进行预测\n    in_arr = np.array([ff_miles, percent_tats, ice_cream])\n    # 声明k为3的knn算法,n_neighbors即是邻居数量，默认值为5\n    knn = neighbors.KNeighborsClassifier(n_neighbors=3)\n    # 训练算法\n    knn.fit(normalize_matrix, dating_labels)\n    # 预测\n    classifier_result = knn.predict([(in_arr - min_val) / ranges])\n    print(\"You will probably like this person: \", result_list[classifier_result - 1][0])\n\n\n# 执行函数\nknn_classify_person()\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}