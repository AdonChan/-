{"title": "scrapy爬取豆瓣Top250电影 - EmptyColor ", "index": "网页爬虫,python", "content": "这次我们爬取的内容\n\n准备步骤\n找到html格式网页中需要爬取的数据的xpath   例如我们需要爬取图片的url   这里用的是xPath Checker不会用的同学请百度\n\n2.然后我们开始建立工程 打开cmd 然后在你想要建立工程的目录下面 输入 scrapy startproject douban就会自动建立一个工程 然后去根目录建立一个run.py 去spiders这个目录里建立一个douban_spiders.py（注意这里的主爬虫文件和项目名称不能相同 不然会报错）\n源码\n# run.py\nfrom scrapy import cmdline\ncmdline.execute(\"scrapy crawl douban\".split())\n# douban_spiders.py\n#coding:utf-8  \nimport scrapy  \nfrom douban.items import DoubanItem  \n   \nfrom scrapy.crawler import CrawlerProcess  \n   \nclass doubanSpider(scrapy.Spider):  \n    name = 'douban'  \n    allowed_domains = [\"douban.com\"]  \n    start_urls = [\"https://movie.douban.com/top250\"]  \n       \n    def parse(self, response):  \n        item = DoubanItem()  \n        item['image_urls'] = response.xpath('//div[@class=\"pic\"]//img//@src').extract()#提取图片链接  \n        # print 'image_urls',item['image_urls']  \n        item['title'] = response.xpath('//div[@class=\"hd\"]/a/span[1]/text()').extract()#提取电影标题 \n        # print 'title',item['title']  \n        item['quote'] = response.xpath('//p[@class=\"quote\"]/span/text()').extract()#提取简介\n        # print 'quote',item['quote']  \n        item['level'] = response.xpath('//em/text()').extract()#提取排名\n        # print 'level',item['level']  \n        yield item    \n        new_url= \"https://movie.douban.com/top250\" + response.xpath('//span[@class=\"next\"]/link/@href').extract_first()#翻页  \n        # print 'new_url',new_url  \n        if new_url:  \n            yield scrapy.Request(new_url,callback=self.parse)  \n\n# items.py\nimport scrapy\n\nclass DoubanItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    image_urls = scrapy.Field()\n    title = scrapy.Field()\n    quote = scrapy.Field()\n    level = scrapy.Field()\n# pipelines.py\nimport os  \nimport urllib \n\nfrom douban import settings  \n\nclass DoubanPipeline(object):\n    def process_item(self, item, spider):  \n        i = 0\n        dir_path = '%s/%s'%(settings.IMAGES_STORE,spider.name)#存储路径  \n        print 'dir_path',dir_path  \n        if not os.path.exists(dir_path):  \n            os.makedirs(dir_path)  \n        for image_url in item['image_urls']:  \n            file_name = \"Top\" + item['level'][i] + ' ' +item['title'][i] + '('+item['quote'][i]+ \").jpg\"#图片名称  \n            i = i + 1\n            # print 'filename',file_name  \n            file_path = '%s/%s'%(dir_path,file_name)  \n            # print 'file_path',file_path  \n            if os.path.exists(file_name):  \n                continue  \n            with open(file_path,'wb') as file_writer:  \n                conn = urllib.urlopen(image_url)#下载图片  \n                file_writer.write(conn.read())  \n            file_writer.close()  \n        return item  \n\n# setting.py\nBOT_NAME = 'douban'\n\nSPIDER_MODULES = ['douban.spiders']\nNEWSPIDER_MODULE = 'douban.spiders'\n\nITEM_PIPELINES = {  \n   'douban.pipelines.DoubanPipeline': 1,  \n}  \nIMAGES_STORE='E:'  \nDOWNLOAD_DELAY = 0.25\n\nUSER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_3) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.54 Safari/536.5'  \n最终爬取的结果\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "3"}