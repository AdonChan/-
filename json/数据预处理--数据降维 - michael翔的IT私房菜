{"title": "数据预处理--数据降维 - michael翔的IT私房菜 ", "index": "数据挖掘,数据分析,python", "content": "数据规约产生更小但保持数据完整性的新数据集。在规约后的数据集上进行数据分析和挖掘将更有效率。\n机器学习领域中所谓的降维就是指采用某种映射方法，将原高维空间中的数据点映射到低维度的空间中。降维的本质是学习一个映射函数 f : x->y，其中x是原始数据点的表达，目前最多使用向量表达形式。 y是数据点映射后的低维向量表达，通常y的维度小于x的维度（当然提高维度也是可以的）。f可能是显式的或隐式的、线性的或非线性的。\n目前大部分降维算法处理向量表达的数据，也有一些降维算法处理高阶张量表达的数据。之所以使用降维后的数据表示是因为在原始的高维空间中，包含有冗余信息以及噪音信息，在实际应用例如图像识别中造成了误差，降低了准确率；而通过降维,我们希望减少冗余信息所造成的误差,提高识别（或其他应用）的精度。又或者希望通过降维算法来寻找数据内部的本质结构特征。\n在很多算法中，降维算法成为了数据预处理的一部分，如PCA。事实上，有一些算法如果没有降维预处理，其实是很难得到很好的效果的。1\n主要是介绍了PCA,还有其他降维算法：LDA（Linear Discriminant Analysis）2,LLE (Locally Linear Embedding) 局部线性嵌入3,拉普拉斯特征映射4。\n主成分分析--PCA\n主成分分析也称为卡尔胡宁-勒夫变换（Karhunen-Loeve Transform），是一种用于探索高维数据结构的技术。PCA通常用于高维数据集的探索与可视化。还可以用于数据压缩，数据预处理等。PCA可以把可能具有相关性的高维变量合成线性无关的低维变量，称为主成分（ principal components）。新的低维数据集会经可能的保留原始数据的变量。\nPCA将数据投射到一个低维子空间实现降维。例如，二维数据集降维就是把点投射成一条线，数据集的每个样本都可以用一个值表示，不需要两个值。三维数据集可以降成二维，就是把变量映射成一个平面。一般情况下，n 维数据集可以通过映射降成k 维子空间。5\n在Python中，主成分的函数位于Scikit-Learn下：sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False)\n参数说明：\n\n\nn_components\n\n意义：PCA算法中所要保留的主成分个数n，也即保留下来的特征个数。\n类型：int或者string，缺省时默认为None，所有成分保留。赋值为int，比如n_components=1，将把原始数据降到一个维度。赋值为string，比如n_components='mle'，将自动选取特征个数n，使得满足所要求的方差百分比。\n\n\n\ncopy\n\n类型：bool,True或者False，缺省时默认为True\n意义：表示是否在运行算法时，将原始数据复制一份。如果为True，则运行PCA算法后，原始数据的值不会有任何改变。因为是在原始数据的副本上进行运算的。\n\n\n\nwhiten\n\n类型：bool，缺省时默认为False\n意义：白化，是的每个特征具有相同的方差。\n\n\n\n栗子\nfrom sklearn.decomposition import PCA\nimport numpy as np\nimport pandas as pd\n\ndata=np.random.randn(10,4)\n\npca=PCA()\npca.fit(data)\npca.components_  #返回模型的各个特征向量\npca.explained_variance_ratio_  #返回各个成为各自的方差百分比（贡献率）\n通过计算累计贡献率，可以确定找到一个合适的n值，比如累计达到97%时，是前3的值，那么下一步去降维时，确定n_components=3。那么，这3维数据占了原始数据95%以上的信息。6\n下面，再重新建立PCA模型。\npca=PCA(3)\npca.fit(data)\nlow_d=pca.transform(data)  #用这个方法来降低维度\npd.DataFrame(low_d).to_excel('result.xlsx')  #保存结果\npca.inverse_transform(low_d)  #必要时，可以用这个函数来复原数据。\n\n\n\n机器学习降维算法一：PCA(主成分分析算法) ↩\n\n\n 机器学习降维算法二：LDA（Linear Discriminant Analysis） ↩\n\n\n机器学习降维算法三：LLE (Locally Linear Embedding) 局部线性嵌入 ↩\n\n\n机器学习降维算法四：Laplacian Eigenmaps 拉普拉斯特征映射 ↩\n\n\n7-dimensionality-reduction-with-pca ↩\n\n\n书-Python数据分析与挖掘实战 ↩\n\n\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "11"}