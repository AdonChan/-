{"title": "Scrapy入门程序点评 - 一起学习python网络爬虫 ", "index": "编程,scrapy,网页爬虫,python", "content": "\n1，引言\n在《Scrapy的架构初探》一文，我基于爬虫开发的经验对Scrapy官网文章作了点评和解读，事件驱动的异步处理架构、极强的模块化等是个绝好的框架，接着我细读了官网的《Scrapy at a glance》，更加强了我的感受：就是他了——开源Python网络爬虫项目需要一个爬虫框架，我不想重复发明轮子，只想专注于爬虫里面的提取器的生成和使用，也就是Scrapy中的Spider部分。\n本文大部分内容摘抄自Scrapy官网的《Scrapy at a glance》，看到Scrapy巧妙之处则加了点评。\n2，Scrapy的Spider例子\n在Scrapy的框架中，Spider与GooSeeker开源爬虫的提取器类似，核心特征是\n\nSpider通常针对一个特定网站\nSpider里面存了爬行入口URLs集合\nScrapy的引擎顺序拿Spider中的入口URL，构造Request对象，启动消息循环\nSpider提供接口方法，把抓取下来的内容进行输出\n\n对GooSeeker的MS谋数台和DS打数机比较了解的读者，可以把Spider想象成：MS谋数台上定义的一组抓取规则 + 会员中心的爬虫罗盘\n下面我们从官网拷贝一个例子：\nclass StackOverflowSpider(scrapy.Spider):\n    name = 'stackoverflow'\n    start_urls = ['http://stackoverflow.com/questions?sort=votes']\n\n    def parse(self, response):\n        for href in response.css('.question-summary h3 a::attr(href)'):\n            full_url = response.urljoin(href.extract())\n            yield scrapy.Request(full_url, callback=self.parse_question)\n\n    def parse_question(self, response):\n        yield {\n            'title': response.css('h1 a::text').extract()[0],\n            'votes': response.css('.question .vote-count-post::text').extract()[0],\n            'body': response.css('.question .post-text').extract()[0],\n            'tags': response.css('.question .post-tag::text').extract(),\n            'link': response.url,\n        }\n\n看这个例子需要注意以下几点\n\nstart_urls存储入口网址列表，本例只有一个网址\nparse()函数是爬到了网页后执行的，是由引擎回调的\n本来到parse()就完成了，但是这个例子展示了一个两级抓取的案例，在parse()里面构造了下一级抓取的任务，生成Request对象，并登记一个回调函数\nparse_question()是第二级的解析网页的函数，返回了一个JSON对象\n事件驱动模式显而易见，可以构造好多Request，丢给引擎即可，不用阻塞式等待\n\n官网文章还总结了其他很多功能特性，总之，Scrapy是一个十分完善和强大的框架。\n3，接下来的工作\n至此，Scrapy框架已经明确选定了，接下来，我们将进一步研读Scrapy的文档，研究怎样把GooSeeker的gsExtractor封装成Scrapy需要的Spider。\n4，文档修改历史\n2016-06-13：V1.0，首次发布\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "4"}