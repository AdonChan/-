{"title": "Python scrapy框架用21行代码写出一个爬虫 - 个人文章 ", "index": "python爬虫,python3.x,网页爬虫,scrapy,python", "content": "开发环境:Pycharm 2017.1(目前最新)开发框架: Scrapy 1.3.3(目前最新)\n目标\n爬取线报网站,并把内容保存到items.json里\n页面分析\n根据上图我们可以发现内容都在类为post这个div里下面放出post的代码\n<div class=\"post\">\n<!-- baidu_tc block_begin: {\"action\": \"DELETE\"} -->\n<div class=\"date\"><span>04月</span><span class=\"f\">07日</span></div><!-- baidu_tc block_end -->\n<h2><a href=\"http://www.abckg.com/193.html\" title=\"4月7日 淘金币淘里程领取京东签到\" rel=\"bookmark\" target=\"_blank\">4月7日 淘金币淘里程领取京东签到</a><span>已结束</span></h2>\n<h6>发布日期: 2017-04-07 | 分类: <a href=\"http://www.abckg.com/xunibi\">虚拟币</a>   |  浏览:125177\n</h6><div class=\"intro\"><p>淘金币一键领取 http://021.tw/t/ https://www.chaidu.com/App/Web/Taobao-Coin/ 【电脑端30金币】 https://taojinbi.taobao.com/inde ... auto_take=true 【手机端30金币】 http://h5.m.taobao...</p></div></div>\n\n实现方法\n1.定义items\nclass DemoItem(scrapy.Item):\n    id = scrapy.Field()\n    title = scrapy.Field()\n    href = scrapy.Field()\n    content = scrapy.Field()\n\n2.新建一个爬虫名为test\n# -*- coding: utf-8 -*-\nimport scrapy\nfrom demo.items import DemoItem\nfrom scrapy.http import Request\n\nclass TestSpider(scrapy.Spider):\n    #定义爬虫的名字和需要爬取的网址\n    name = \"test\"\n    allowed_domains = [\"www.abckg.com\"]\n    start_urls = ['http://www.abckg.com/']\n\n    def parse(self, response):\n        for resp in response.css('.post'):\n            #实例化item\n            item = DemoItem()\n            #把获取到的内容保存到item内\n            item['href'] = resp.css('h2 a::attr(href)').extract()\n            item['title'] = resp.css('h2 a::text').extract()\n            item['content'] = resp.css('.intro p::text').extract()\n            yield item\n            \n        #下面是多页面的爬取方法\n        urls = response.css('.pageinfo a::attr(href)').extract()\n        for url in urls:\n            yield Request(url, callback=self.parse)\n        categorys = response.css('.menu li a::attr(href)').extract()\n        for ct in categorys:\n            yield Request(ct, callback=self.parse)\n3.修改settings.py,添加以下代码\nFEED_EXPORT_ENCODING = 'utf-8'\n运行\n打开cmd输入\nscrapy crawl test -o items.json\n\n\n已知bug\n如果多次运行该爬虫,不会覆盖原有的内容,而是追加数据(好像是scrapy的bug)\n可拓展内容\n1.定时运行爬虫,当检查到网站更新时获取新数据并发邮件通知2.检测数据是否重复\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "7"}