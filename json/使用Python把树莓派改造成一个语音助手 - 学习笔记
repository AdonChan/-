{"title": "使用Python把树莓派改造成一个语音助手 - 学习笔记 ", "index": "api,语音合成,语音,python,raspberry-pi", "content": "语音助手已经不是什么新事物了。就在两三年前，语音助手的使用体验还不是那么好，尝尝鲜后也就没用过了。但最近发现不管是微软的Cortana、苹果的Siri，还是一些不怎么有名气的，例如MIUI的小爱同学等，使用体验真的改善了很多，确确实实能带来一些方便了。\n随着各种云服务、API的面世，语音方面的云服务可以说是十分健全了。你是否也想过自己动手搭建一个语音助手系统呢？本文将总结使用Python把树莓派（3代b型）改造成一个简易语音助手的基本流程。\n概述\n这次要做的说白了，就是把各种云服务、API串起来，并不涉及任何核心技术、算法的实现，望知悉。\n这次将要使用到的服务包括：\n\n谷歌Cloud Speech API\n图灵机器人\n科大讯飞 语音合成WebAPI\n\n为了实现这个语音助手系统，需要完成的工作每一个都不难，但数量稍多了些。以下是涉及到的一些博客：\n\n使用Google云计算引擎实现科学上网\n在Windows命令行、Linux终端使用代理\n树莓派学习手记——使用Python录音\n在Python中使用谷歌Cloud Speech API将语音转换为文字（另一种方案）\n使用Python与图灵机器人聊天\n在Python中使用科大讯飞Web API进行语音合成\n\n后文在介绍各部分的具体实现时，只附上代码和进行一些必要的说明，详细内容还需要参考相应博客。\n各部分的实现\n由于整个项目用到的服务比较多，而且各部分的分工很明显，所以选择各部分分别用一个python程序来实现，最后再用一个程序整合在一起的方式。\n录音\n参考：树莓派学习手记——使用Python录音\n笔者采用了“按住按钮进行录音”的操作方式，如下图所示接线。如果你手头上没有按钮或觉得这么做不方便，可以修改代码改成“按回车键开始/结束录音”之类的操作方式。\n\n另外，树莓派的板载3.5mm耳机接口是不带语音输入功能的，所以你需要另外购买USB声卡。\n* 文件 rec.py\nimport RPi.GPIO as GPIO\nimport pyaudio\nimport wave\nimport os\nimport sys\n\ndef rec_fun():\n    # 隐藏错误消息，因为会有一堆ALSA和JACK错误消息，但其实能正常录音\n    os.close(sys.stderr.fileno())\n    \n    BUTT = 26    # 开始录音的按钮：一边接GPIO26，一边接地\n    GPIO.setmode(GPIO.BCM)\n    # 设GPIO26脚为输入脚，电平拉高，也就是说26脚一旦读到低电平，说明按了按钮\n    GPIO.setup(BUTT, GPIO.IN, pull_up_down = GPIO.PUD_UP)\n\n    # wav文件是由若干个CHUNK组成的，CHUNK我们就理解成数据包或者数据片段。\n    CHUNK = 512 \n    FORMAT = pyaudio.paInt16  # pyaudio.paInt16表示我们使用量化位数 16位来进行录音\n    RATE = 44100  # 采样率 44.1k，每秒采样44100个点。\n    WAVE_OUTPUT_FILENAME = \"/home/pi/chat/command.wav\"\n    print('请按住按钮开始录音...')\n    GPIO.wait_for_edge(BUTT, GPIO.FALLING)\n\n    # To use PyAudio, first instantiate PyAudio using pyaudio.PyAudio(), which sets up the portaudio system.\n    p = pyaudio.PyAudio()\n    stream = p.open(format = FORMAT,\n                    channels = 1,    # cloud speecAPI只支持单声道\n                    rate = RATE,\n                    input = True,\n                    frames_per_buffer = CHUNK)\n    print(\"录音中...\")\n\n    frames = []\n    # 按住按钮录音，放开时结束\n    while GPIO.input(BUTT) == 0:\n        data = stream.read(CHUNK)\n        frames.append(data)\n    print(\"录音完成，输出文件：\" + WAVE_OUTPUT_FILENAME + '\\n')\n    stream.stop_stream()\n    stream.close()\n    p.terminate()\n\n    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n    wf.setnchannels(1)\n    wf.setsampwidth(p.get_sample_size(FORMAT))    # Returns the size (in bytes) for the specified sample format.\n    wf.setframerate(RATE)\n    wf.writeframes(b''.join(frames))\n    wf.close()\n    \n    return\n\n# 可以直接运行rec.py进行测试，同时保证该文件import时不会自动运行\nif __name__ == '__main__':\n    rec_fun()\n语音识别\n参考：\n使用Google云计算引擎实现科学上网\n在Windows命令行、Linux终端使用代理\n在Python中使用谷歌Cloud Speech API将语音转换为文字（另一种方案）\n由于某些原因，笔者选择了使用谷歌Cloud Speech API进行语音识别。既然要用谷歌的服务，自然就涉及到了科学上网、代理、谷歌云平台的使用，如果不想这么折腾，完全可以用国内的讯飞、百度来实现。\n另外，API KEY之类的字符串在这里删除了，还请麻烦修改代码加上你自己申请的API KEY。\n* 文件 speech_api.py\nimport json\nimport urllib.request\nimport base64\n\ndef wav_to_text():\n    api_url = \"https://speech.googleapis.com/v1beta1/speech:syncrecognize?key=替换成你的API密钥\"\n    print('语音文件编码中...')\n    audio_file = open('/home/pi/chat/command.wav', 'rb')\n    audio_b64str = (base64.b64encode(audio_file.read())).decode()\n    audio_file.close()\n\n    voice = {\n      \"config\":\n      {\n        \"languageCode\": \"cmn-Hans-CN\"\n      },\n\n      \"audio\":\n      {\n        \"content\": audio_b64str\n      }\n    }\n    voice = json.dumps(voice).encode('utf8')\n    print('编码完成。正在上传语音...')\n    req = urllib.request.Request(api_url, data=voice, headers={'content-type': 'application/json'})\n    response = urllib.request.urlopen(req)\n    response_str = response.read().decode('utf8')\n    response_dic = json.loads(response_str)\n\n    if ('results' not in response_dic.keys()):\n        print('您录制的文件似乎没有声音，请检查麦克风。')\n        return\n    \n    transcript = response_dic['results'][0]['alternatives'][0]['transcript']\n    confidence = response_dic['results'][0]['alternatives'][0]['confidence']\n    result_dic = {'text':transcript ,'confidence':confidence}\n    print('识别完成。以字典格式输出：')\n    print(result_dic)\n    \n    return result_dic\n\nif __name__ == '__main__':\n    wav_to_text()\n获取文字回答\n参考：使用Python与图灵机器人聊天\n这个获取回答的程序有些粗糙，只能获得普通的文字回答。实际上图灵机器人回复的内容中包括了文字、问题类型甚至情感等信息，还有很多修改的空间。\n* 文件 turing.py\nimport json\nimport urllib.request\n\ndef chat(question):\n    api_url = \"http://openapi.tuling123.com/openapi/api/v2\"\n    text_input = question['text']\n    req = {\n        \"perception\":\n        {\n            \"inputText\":\n            {\n                \"text\": text_input\n            },\n\n            \"selfInfo\":\n            {\n                \"location\":\n                {\n                    \"city\": \"上海\",\n                    \"province\": \"上海\",\n                    \"street\": \"文汇路\"\n                }\n            }\n        },\n\n        \"userInfo\": \n        {\n            \"apiKey\": \"替换成你的APIKEY\",\n            \"userId\": \"用户参数\"\n        }\n    }\n    # 将字典格式的req转为utf8编码的字符串\n    req = json.dumps(req).encode('utf8')\n    \n    print('\\n' + '正在调用图灵机器人API...')\n    http_post = urllib.request.Request(api_url, data=req, headers={'content-type': 'application/json'})\n    response = urllib.request.urlopen(http_post)\n    \n    print('得到回答，输出为字典格式：')\n    response_str = response.read().decode('utf8')\n    response_dic = json.loads(response_str)\n    intent_code = response_dic['intent']['code']\n    \n    # 返回网页类的输出方式\n    if(intent_code == 10023):\n        results_url = response_dic['results'][0]['values']['url']\n        results_text = response_dic['results'][1]['values']['text']\n        answer = {\"code\": intent_code, \"text\": results_text, \"url\":results_url}\n        print(answer)\n        return(answer)\n    # 一般的输出方式\n    else:\n        results_text = response_dic['results'][0]['values']['text']\n        answer = {\"code\": intent_code, \"text\": results_text}\n        print(answer)\n        return(answer)\n    \nif __name__ == '__main__':\n    eg_question = {'text': '今天是几号', 'confidence': 0.9}\n    chat(eg_question)\n读出回答（语音合成）\n参考：在Python中使用科大讯飞Web API进行语音合成\n笔者在使用讯飞Web API时，该服务才开放不到一周，难免以后该API会有所变动，如有问题建议查阅官方文档。\n* 文件 tts.py\nimport base64\nimport json\nimport time\nimport hashlib\nimport urllib.request\nimport urllib.parse\nimport os\n\ndef speak(text_content):\n    # API请求地址、API KEY、APP ID等参数，提前填好备用\n    api_url = \"http://api.xfyun.cn/v1/service/v1/tts\"\n    API_KEY = \"替换成你的APIKEY\"\n    APP_ID = \"替换成你的APPID\"\n    AUE = \"lame\"\n\n    # 构造输出音频配置参数\n    Param = {\n        \"auf\": \"audio/L16;rate=16000\",    #音频采样率\n        \"aue\": AUE,    #音频编码，raw(生成wav)或lame(生成mp3)\n        \"voice_name\": \"xiaoyan\",\n        \"speed\": \"50\",    #语速[0,100]\n        \"volume\": \"10\",    #音量[0,100]\n        \"pitch\": \"50\",    #音高[0,100]\n        \"engine_type\": \"aisound\"    #引擎类型。aisound（普通效果），intp65（中文），intp65_en（英文）\n    }\n    # 配置参数编码为base64字符串，过程：字典→明文字符串→utf8编码→base64(bytes)→base64字符串\n    Param_str = json.dumps(Param)    #得到明文字符串\n    Param_utf8 = Param_str.encode('utf8')    #得到utf8编码(bytes类型)\n    Param_b64 = base64.b64encode(Param_utf8)    #得到base64编码(bytes类型)\n    Param_b64str = Param_b64.decode('utf8')    #得到base64字符串\n\n    # 构造HTTP请求的头部\n    time_now = str(int(time.time()))\n    checksum = (API_KEY + time_now + Param_b64str).encode('utf8')\n    checksum_md5 = hashlib.md5(checksum).hexdigest()\n    header = {\n        \"X-Appid\": APP_ID,\n        \"X-CurTime\": time_now,\n        \"X-Param\": Param_b64str,\n        \"X-CheckSum\": checksum_md5\n    }\n\n    # 构造HTTP请求Body\n    body = {\n        \"text\": text_content\n    }\n    body_urlencode = urllib.parse.urlencode(body)\n    body_utf8 = body_urlencode.encode('utf8')\n\n    # 发送HTTP POST请求\n    print('\\n' + \"正在调用科大讯飞语音合成API...\")\n    req = urllib.request.Request(api_url, data=body_utf8, headers=header)\n    response = urllib.request.urlopen(req)\n\n    # 读取结果\n    response_head = response.headers['Content-Type']\n    if(response_head == \"audio/mpeg\"):\n        out_file = open('/home/pi/chat/answer.mp3', 'wb')\n        data = response.read() # a `bytes` object\n        out_file.write(data)\n        out_file.close()\n        print('得到结果，输出文件: /home/pi/chat/answer.mp3')\n    else:\n        print(response.read().decode('utf8'))\n\n    # 播放音频\n    print(\"播放音频中...\")\n    print(\"以下均为mplayer的输出内容\\n\")\n    os.system(\"mplayer -ao alsa:device=hw=1.0 /home/pi/chat/answer.mp3\")\n    \n    return\n    \nif __name__ == '__main__':\n    eg_text_content = \"苟利国家生死以，岂因祸福避趋之\"\n    speak(eg_text_content)\n整合&测试\n现在，你的项目文件夹中应该有这些python代码文件：\n\n接下来我们只需要将他们整合在一起运行。\n* 文件 combine.py\n# 这些import进来的模块是同目录下的py文件\nimport rec    # rec.py负责录制wav音频\nimport speech_api    # speech_api.py负责wav转文字\nimport turing    # turing.py负责获得图灵机器人的文字回答\nimport tts    # tts.py负责读出回答\n\nrec.rec_fun()    # 录制音频\nrecognize_result = speech_api.wav_to_text()    # 识别语音，返回值是字典格式，包含文字结果和信心\nturing_answer = turing.chat(recognize_result)    # 得到图灵的回答，返回值仍是字典格式\ntts.speak(turing_answer['text'])\n如果一切顺利的话，实际运行效果如下：\nhttp://v.youku.com/v_show/id_...\n小结\n语音助手这边的工作算是告一段落了，结果小结却不知道怎么写了。不管怎么说，很开心最后能得到实际的结果，做的过程中也有一些脑洞想要继续扩展，过段时间应该还会继续！\n做这个项目的过程中，项目外的收获或许比这个项目本身还要多。这段时间从很多博客、论坛得到了数不尽的帮助，国内的、国外的、中文的、英文的、日文的都有，深深地感受到了互联网共享精神的力量，这也是促使我开始写这些文章的原因。那么，最后还是说一句：感谢你阅读文章！\n\n                ", "mainLikeNum": ["6 "], "mainBookmarkNum": "1"}