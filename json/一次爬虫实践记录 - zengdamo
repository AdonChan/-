{"title": "一次爬虫实践记录 - zengdamo ", "index": "python,网页爬虫", "content": "前言说明：\n公司在2017年3月的时候开发过一个「数据中心」，是将有赞的用户和订单信息通过API拉取到自己开发的网站上，再结合我们自己和用户沟通的信息，组成一个简单的用户管理中心。数据中心虽然简单，但对我们意义非常大，它的出现标志着我们想要定位一个用户的时候告别了“办公室吼一吼”的纯人肉方法。\n不过随着时间的推移，数据中心跟不上我们的业务发展，我们想在近期将其重新做一遍，进行一系列大的功能升级。别的都好说，唯一的问题是在过去一年半中，我们在数据中心添加了大量的信息，比如同学的微信号、昵称、他家长的信息、他的订单备注信息等等。随着远良的离职，后来的人已经很难从数据库中找出这些数据。在和伟锋探讨CRM的时候，我突然想到了可以用爬虫的手段，来将数据中心的数据全部爬取出来。于是便有了下面的代码，经过断断续续两三天的研究，我终于搞定了代码，顺利爬取了我们所有超过1万条的用户数据。\n这里做一个技术记录，中间涉及到的知识点包括：\n\n如何通过Network中的authorization，以及requests.session()维持登陆状态，以爬取不同的需要登陆以后才能查看的网页。这里面有个坑，就是我先用了session的方法来试图解决问题，但是怎么试都总是失败，反复查询后发现发现数据中心是用node.js来写的，而这样的网页用的是token来验证身份，session()那套行不通。最后我在头信息中发现authorization，直接粘到headers的信息才解决问题的；\n查看网页源代码，如果你需要的信息不在源代码中，那就说明要找的内容在JS文件中，在开发者模式下找到Network中的XHR或者JS，里面一般都会有某个JS文件包含你要的信息（header包含头信息，preview包含要爬取的信息，最上面的request URL则是要爬取内容所需要的网址信息）。\n复习了json的使用方法。另外，不管是python还是json，爬取的关键都是找到循环点，因为循环点意味着有规律循环的开始；\n复习了在python中打开、写入csv文件的方式；\n复习了在python中连接数据库的知识；Python3 MySQL 数据库连接 - PyMySQL 驱动\n\n学习了try...except的用法：将可能出现bug的代码写入try的部分，然后在except那里写入报错类型和报错的提示。报错提示可以根据需要自定义。这样的好处是程序出现bug的时候不会报错终止，而是会给出报错提示以后继续后面的运行，直到结束。（Python）异常处理try...except、raise\n\n复习列表构造的知识；\n简单学习了SQL的基本操作语句 SQL基本语法\n\n\nimport requests\nimport json\nimport csv\nimport pymysql\nimport time\n\n\n# 从数据库中获取所有的用户ID\n\nyz_uids = []\n\ndb = pymysql.connect('192.168.31.24','root','root','danci_tddc') # 连接数据库\ncursor = db.cursor() # 用cursor 方法获取操作游标\nsql = \"SELECT * FROM td_crm_customers\" # 写sql语句\ncursor.execute(sql) # 执行sql语句\ndata = cursor.fetchall() # 获取数据\nfor row in data:\n    yz_uid = row[0]\n    yz_uids.append(yz_uid)\n\ndb.close() # 关闭数据库连接\n\n\n\nlogin_url = 'http://data.testdaily.cn/#!/login'\n\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36',\n    'Referer': 'http://data.testdaily.cn/',\n    'Cookie': 'Hm_lvt_fc5a4042b4f7e4c87111dce89bb04bea=1539932447,1540895924',\n    'Authorization': 'Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJfaWQiOiI1OGI3ZWU4ZmI2NmVmZjEwMWM5NGVjODgiLCJ1c2VybmFtZSI6Inplbmd5aWNoYW8iLCJleHAiOjE1NDIxODA5OTksImlhdCI6MTU0MTU3NjE5OX0.dl7o4lnPZnfw7e606sVOrW4dYCKOmQJzSsMBHCFPAc4'\n}\n\n# 打开存储的csv文档\nf = open('/Users/damo/Desktop/4.csv', 'w+')\nwriter = csv.writer(f)\nwriter.writerow(['user_id', 'wechat', 'nickname', 'majia', 'phone', 'address', 'name', 'tag', 'parentInfo', 'remark', 'update_time','trade_history'])\n\n# 获取单个页面的数据\ndef get_info(url):\n\n    try:\n        # 读取客户详情页面并获取json数据\n        res = requests.get(url,headers = headers)\n        json_data = json.loads(res.text)\n\n        user_id = json_data['user_id']\n        wechat = json_data['wechat']\n        if 'nickname' in json_data.keys():\n            nickname = json_data['nickname']\n        else:\n            nickname = ''\n        majia = json_data['tdAlias']\n        phone = json_data['mobile']\n        address = json_data['address']\n        name = json_data['name']\n        tag = json_data['tags']\n        if 'parentsInfo' in json_data.keys():\n            parentInfo = json_data['parentsInfo']\n        else:\n            parentInfo = ''\n        if 'remark' in json_data.keys():\n            remark = json_data['remark']\n        else:\n            remark = ''\n        update_time = json_data['update_time']\n        trade_history = json_data['trades']\n\n        writer.writerow([user_id,wechat,nickname,majia,phone,address,name,tag,parentInfo,remark,update_time,trade_history]) # 将数据写入csv文件\n    except TypeError:\n        print(url + '有问题')\n\nif __name__ == '__main__':\n    urls = ['http://data.testdaily.cn/api/customers/{}'.format(i) for i in yz_uids] # 构造列表表达式\n    for url in urls:\n        get_info(url)\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}