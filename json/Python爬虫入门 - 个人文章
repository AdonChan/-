{"title": "Python爬虫入门 - 个人文章 ", "index": "python", "content": "什么是爬虫？\n一段自动抓取互联网信息的程序，从互联网上抓取对于我们有价值的信息\nPython四种基本数据结构\n列表\n**列表中的每个元素都是可变的;列表的元素都是有序的，也就是说每个元素都有对应的位置;列表可以容纳所有的对象;**\nlist = ['波波', '90', '超哥', '小明']\nprint(list[0])\nprint(list(2:))\n# result\n波波\n['超哥', '小明'] # 如果为切片返回的也是列表的数据结构\n字典\nuser_info = {\n  'name': '小明',\n  'age': '23',\n  'sex': 'male'\n}\n元组\n**在爬虫中元组和集合很少用到，这里只做简单的介绍;元组: 类似于列表，但是元组的元素是不能修改只能查看的**\n# 元组\ntuple = (1,2,3)\n\n集合\n集合：类似数学中的集合，每个集合中的元素是无序的，不可以有重复的对象，因此可以通过集合把重复的数据去除!\n# 集合\nlist = [1,1,2,2,3,4,5] \nset = set(list)\n# result {1,2,3,4,5}\nPython文件操作\n# 打开文件\nopen(name,[, mode[,buffering]])\n\nf = open('/Users/GreetingText/PycharmProjects/demo/hello.txt')\n\n# 读写文件\n\nf = open('/Users/GreetingText/PycharmProjects/demo/hello.txt', 'w')\nf.write('Hello World')\n\nf = open('/Users/GreetingText/PycharmProjects/demo/hello.txt', 'r')\ncontent = f.read()\nprint(content)\n# result Hello World\n\n# 关闭文件\nf.close()\n爬虫原理\n\n多页面爬虫流程\n\n如何安装Python环境？\nMac 系统自带Python 2.7，安装 新版本请前往官网下载，安装成功之后，在命令行输入python3 如图：\n\n工欲善其事，必先利其器\n推荐PyCharm\nPyCharm破解方法拿走不谢！\n推荐两个第三方库\n\nBeautiful Soup  中文文档\n\nScrapy 中文文档\n\n\nQuickDemo\n安装Scrapy并创建项目\npip install scrapy\nscrapy startproject QuickDemo\ncd QuickDemo\n在spiders目录下创建test_spilder.py文件\n\n具体代码(需要事先安装BeautifulSoup库)\n# -*- coding:utf-8 -*-\nimport scrapy\nfrom bs4 import BeautifulSoup\n\n\nclass tsSpride(scrapy.Spider):\n    name = 'test' # 爬虫的唯一名字，在项目中爬虫名字一定不能重复\n\n    # start_requests() 必须返回一个迭代的Request\n    def start_requests(self):\n        # 待爬取的URL列表\n        urls = ['http://www.jianshu.com/',]\n        # 模拟浏览器\n        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36'}\n        for url in urls:\n            yield scrapy.Request(url=url, headers=headers, callback=self.parse)\n\n    # 处理每个请求的下载响应\n    def parse(self, response):\n        soup = BeautifulSoup(response.body, 'html.parser')\n        titles = soup.find_all('a', 'title')\n        for title in titles:\n            print(title.string)\n\n        try:\n            file = open(r'/Users/GreetingText/QuickDemo/jianshu.txt', 'w')\n            # 将爬取到的文章题目写入txt中\n            for title in titles:\n                file.write(title.string + '\\n')\n        finally:\n            if file:\n                # 关闭文件（很重要）\n                file.close()\n在命令行输入\nscrapy crawl test\n爬取数据成功如图：\n\n而且项目里面也生成了一个jianshu.txt文件\n\n打开jianshu.txt如图:\n\n以下是参考链接\n本文参考文章\nBeautifulSoup官网\nScrapy官网\nwindows安装Python3\nMac安装Python3\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}