{"title": "记录一次用Python写爬虫的心得 - Jack Qiu的专栏 ", "index": "网页爬虫,python", "content": "现在网络爬虫有很多方式可以写，比如Node.js或者Go, 甚至PHP都行，我之所以选择Python的原因是因为教程多，可以系统学习，因为光懂得使用Html选择器来爬去页面是不够的，我还要想学习一些爬虫过程中常见的坑，以及一些注意事项，比如修改浏览器的Header之类的小技巧。\n前前后后弄了一个星期，看书+写代码，我写出了一个基本能用的python爬虫小代码，github地址：https://github.com/qiujumper/...\n代码注释都很详细了，其实只要直接阅读源码即可。\n这个爬虫的目的很简单，爬去某个房产网站的楼盘名字+价格+1张图片的下载（单纯测试文件下载功能），以备之后分析房价走势而用，为了不给对方服务器增加太多压力，我只选择了爬取3个页面。\n我这里说说几个需要注意的知识点吧：\n#记得修改发送的Headers听说默认发送过去的都是带有python信息的头，很容易被对方网站检查出是一个爬虫机器人，导致IP被封，所以最好让自己的爬虫程序像人类一点，但是这个代码只能起到一般的隐瞒，真的有网站想防止爬虫，你也是骗不过的，上代码：\nheaders = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit 537.36 (KHTML, like Gecko) Chrome\",\n                \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"},\n#html的选择器，我采用pyquery而不是beautifulsoup很多书都推荐beautifulsoup,但是作为一个习惯了jquery的人来说，beautifulsoup的语法实在是有点拗口，而且貌似还不支持:first-child等高级复杂的css选择器模式，或者支持，但是我没找到，也不是很仔细看文档。\n然后我网上找了一下资料，发现很多人推荐pyquery这个库，自己下来用了一下，发现真的很舒服，所以果断采用了。\n#爬虫思路思路其实很简单：1.找到某个房产的列表页，分析第二第三页的URL结构；2.获取每一个列表页的所有列表条目信息的URL，存入python的set()集合中，之所以用set，是为了去掉重复的URL信息。3.通过获取的房子的URL，进入详情页，再爬去有价值的字段信息，比如图片文字之类的。4.目前我只进行简单的print数据而已，没有把获取的数据存为本地的json或者CSV格式，这个之后做吧，to be done.\n下面是全部代码代码：\n#获取页面对象\nfrom urllib.request import urlopen\nfrom urllib.request import urlretrieve\nfrom pyquery import PyQuery as pq\n#修改请求头模块,模拟真人访问\nimport requests\nimport time\n#引入系统对象\nimport os\n\n#你自己的配置文件，请将config-sample.py重命名为config.py,然后填写对应的值即可\nimport config\n\n#定义链接集合，以免链接重复\npages = set()\nsession = requests.Session()\nbaseUrl = 'http://pic1.ajkimg.com'\ndownLoadDir = 'images'\n\n#获取所有列表页连接\ndef getAllPages():\n    pageList = []\n    i = 1\n    while(i < 2):\n        newLink = 'http://sh.fang.anjuke.com/loupan/all/p' + str(i) + '/'\n        pageList.append(newLink)\n        i = i + 1\n    return pageList\n\ndef getAbsoluteURL(baseUrl, source):\n    if source.startswith(\"http://www.\"):\n        url = \"http://\"+source[11:] \n    elif source.startswith(\"http://\"):\n        url = source\n    elif source.startswith(\"www.\"):\n        url = \"http://\"+source[4:] \n    else:\n        url = baseUrl+\"/\"+source \n    if baseUrl not in url:\n        return None \n    return url\n\n#这个函数内部的路径按照自己的真实情况来写，方便之后的数据导入\ndef getDownloadPath(baseUrl, absoluteUrl, downloadDirectory): \n    path = absoluteUrl.replace(\"www.\", \"\")\n    path = path.replace(baseUrl, \"\")\n    path = downloadDirectory+path\n    directory = os.path.dirname(path)\n    if not os.path.exists(directory): \n        os.makedirs(directory)\n    return path\n\n#获取当前页面的所有连接\ndef getItemLinks(url):\n    global pages;\n    #先判断是否能获取页面\n    try:\n        req = session.get(url, headers = config.value['headers'])\n    #这个判断只能判定是不是404或者500的错误，如果DNS没法解析，是无法判定的\n    except IOError as e:\n        print('can not reach the page. ')\n        print(e)\n    \n    else: \n        h = pq(req.text)\n        #获取第一页的所有房子模块\n        houseItems = h('.item-mod')\n        #从模块中提取我们需要的信息，比如详情页的URL,价格，略缩图等\n        #我倾向只获取详情页的URL，然后在详情页中获取更多的信息\n        for houseItem in houseItems.items():\n            houseUrl = houseItem.find('.items-name').attr('href')\n            #print(houseUrl)\n            pages.add(houseUrl)\n        \n#获取详情页的各种字段，这里可以让用户自己编辑\ndef getItemDetails(url):\n    #先判断是否能获取页面\n    try:\n        req = session.get(url, headers = config.value['headers'])\n    #这个判断只能判定是不是404或者500的错误，如果DNS没法解析，是无法判定的\n    except IOError as e:\n        print('can not reach the page. ')\n        print(e)\n    else:\n        time.sleep(1)\n        h = pq(req.text)\n\n        #get title\n        housePrice = h('h1').text() if h('h1') != None else 'none'\n\n        #get price\n        housePrice = h('.sp-price').text() if h('.sp-price') != None else 'none'\n\n        #get image url\n        houseImage = h('.con a:first-child img').attr('src')\n        houseImageUrl = getAbsoluteURL(baseUrl, houseImage)\n        if houseImageUrl != None:\n            urlretrieve(houseImageUrl, getDownloadPath(baseUrl, houseImageUrl, downLoadDir))     \n        # if bsObj.find('em',{'class','sp-price'}) == None:\n        #     housePrice = 'None'\n        # else:\n        #     housePrice = bsObj.find('em',{'class','sp-price'}).text;\n        # if bsObj.select('.con a:first-child .item img')== None:\n        #     houseThumbnail = 'None'\n        # else:\n        #     houseThumbnail = bsObj.select('.con a:first-child .item img');\n\n        \n\n\n#start to run the code\nallPages = getAllPages()\n\nfor i in allPages:\n    getItemLinks(i)\n#此时pages 应该充满了很多url的内容\nfor i in pages:\n    getItemDetails(i)\n#print(pages)\n\n有问题欢迎和我讨论，这段代码还可以继续完善。\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "1"}