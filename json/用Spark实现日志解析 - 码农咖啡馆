{"title": "用Spark实现日志解析 - 码农咖啡馆 ", "index": "python,scala,日志分析,apache-spark", "content": "当下数据矿工们谈论最多的，莫过于这「星火」了。Spark这名字，总让我联想起高中背过的单词书，从而印象不佳，哈哈。\n今天也凑了把热闹，把一个日志解析的模块改成Spark实现，算是体验之旅吧。\n刚开始我是用看起来很像Swift的Scala写的：SparkLogExtract.scala\n然后我希望为这个程序增加参数传入的功能，然后我谷歌大法了，然后就没有然后了。\n总体感觉Scala是一个不够可爱的姑娘，表现在：\n\n\n语法似乎还没进入稳定状态，像列表扩展、正则匹配这样基础的API都能搜出一大堆眼花缭乱的做法。\n没有break和continue的日子里我真的很想他们……\nSBT对天朝子民真的很慢！\n虽然提供了Shell，可提交还是要编译的好不。\n长得不好看，函数没有「return」真的充满了违和感（咦，我怎么会用违和感这种高级词汇？）\n\n当然要换回熟悉的Python！\n接下来就爽快多了，除了map的函数传参遇到了困难（最后我恶心地用一个lambda调用普通函数搞定了），其他一帆风顺，运行速度和Scala一样快的（嘿嘿）。\n代码请见：spark_log_extract.py\n求赐星星！\n\n\n  来自：建造者说\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "1"}