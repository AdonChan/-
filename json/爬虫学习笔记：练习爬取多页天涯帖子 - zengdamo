{"title": "爬虫学习笔记：练习爬取多页天涯帖子 - zengdamo ", "index": "python,网页爬虫", "content": "今天练习了抓取多页天涯帖子，重点复习的知识包括\n\nsoup.find_all和soup.selcet两个筛选方式对应不同的参数；\n希望将获取到的多个内容组合在一起返回的时候，要用'zip()'的代码来实现；\n两层代码结构之间的关系如何构造；\n\n这里有一个疑问：有时候一个标签可以有多个属性，不知道soup.find_all()能不能接受不止一个属性以缩小查找的范围。\n# 引入库和请求头文件\nimport requests\nfrom bs4 import BeautifulSoup\nheaders = {\n    'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36'\n}\n\n# 定义主体函数。函数主体由四部分组成：\n#1.请求网址\n#2.解析网页，形成可查找的html格式\n#3.通过soup.select或者sopu.find_all查找需要的标签\n#4.通过zip()代码将多个内容合并在一起\n\ndef get_content(url):\n    res = requests.get(url,headers = headers)\n    res.encoding = 'utf-8'\n    soup = BeautifulSoup(res.text,'html.parser')\n    contents = soup.find_all('div','bbs-content')\n    authors = soup.find_all('a','js-vip-check')\n    for author,content in zip(authors,contents):\n        data = {\n        'author':author.get_text().strip(),\n        'content': content.get_text().strip()\n    }\n        print(data)\n# 在这里构造一个两层链接结构：帖子的1、2、3、4页和帖子的详情页的关系在这里实现\nif __name__ == '__main__':\n    urls = ['http://bbs.tianya.cn/post-develop-2271894-{}.shtml'.format(str(i))  for i in range (1,5)] # 要注意，这里的‘5’是不包括在范围里的\n    for url in urls:\n        get_content(url)\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}