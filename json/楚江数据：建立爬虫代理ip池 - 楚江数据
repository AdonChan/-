{"title": "楚江数据：建立爬虫代理ip池 - 楚江数据 ", "index": "数据采集,网络爬虫,python", "content": "在爬取网站信息的过程中，有些网站为了防止爬虫，可能会限制每个ip的访问速度或访问次数。对于限制访问速度的情况，我们可以通过time.sleep进行短暂休眠后再次爬取。对于限制ip访问次数的时候我们需要通过代理ip轮换去访问目标网址。所以建立并维护好一个有效的代理ip池也是爬虫的一个准备工作。网上提供免费代理ip的网址很多，下面我们以西刺网站为例来建立一个有效的代理ip池。项目流程：\n第一步：构造请求代理ip网站链接def get_url(url): # 国内高匿代理的链接 url_list = [] for i in range(1,100): url_new = url + str(i) url_list.append(url_new) return url_listget_url ：生成要爬取目标网址的链接第二步：获取网页内容def get_content(url): # 获取网页内容 user_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0' headers = {'User-Agent': user_agent} req = urllib.request.Request(url=url, headers=headers) res = urllib.request.urlopen(req) content = res.read() return content.decode('utf-8')get_content：接受的参数是传入的目标网站链接第三步：提取网页中ip地址和端口号信息def get_info(content): # 提取网页信息 / ip 端口 datas_ip = etree.HTML(content).xpath('//table[contains(@id,\"ip_list\")]/tr/td[2]/text()') datas_port = etree.HTML(content).xpath('//table[contains(@id,\"ip_list\")]/tr/td[3]/text()') with open(\"data.txt\", \"w\") as fd: for i in range(0,len(datas_ip)): out = u\"\" out += u\"\" + datas_ip[i] out += u\":\" + datas_port[i] fd.write(out + u\"n\") # 所有ip和端口号写入data文件get_info：接收从get_content函数传来的网页内容，并使用etree解析出ip和端口号，将端口号和ip写入data.第四步：验证代理ip的有效性def verif_ip(ip,port): # 验证ip有效性 user_agent ='Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.22 Safari/537.36 SE 2.X MetaSr 1.0' headers = {'User-Agent':user_agent} proxy = {'http':'http://%s:%s'%(ip,port)} print(proxy) proxy_handler = urllib.request.ProxyHandler(proxy) opener = urllib.request.build_opener(proxy_handler) urllib.request.install_opener(opener) test_url = \"https://www.baidu.com/\" req = urllib.request.Request(url=test_url,headers=headers) time.sleep(6) try: res = urllib.request.urlopen(req) time.sleep(3) content = res.read() if content: print('that is ok') with open(\"data2.txt\", \"a\") as fd: # 有效ip保存到data2文件夹 fd.write(ip + u\":\" + port) fd.write(\"n\") else: print('its not ok') except urllib.request.URLError as e: print(e.reason)verif_ip：使用ProxyHandler建立代理，使用代理ip访问某网址，查看是否得到响应。如数据有效，则保存到data2.txt文件最后：调用各个函数if name == '__main__': url = 'http://www.xicidaili.com/nn/' url_list = get_url(url) for i in url_list: print(i) content = get_content(i) time.sleep(3) get_info(content) with open(\"dali.txt\", \"r\") as fd: datas = fd.readlines() for data in datas: print(data.split(u\":\")[0]) # print('%d : %d'%(out[0],out[1])) verif_ip(data.split(u\":\")[0],data.split(u\":\")[1])得到爬取结果\n【楚江数据】团队提供网站、手机APP(包括电商、社交网站、公众号、论坛及客户提供的其他类型)爬虫定制开发和数据(文本、图片、电话邮箱、文档等)批量采集服务。不用学编程，不用写代码，我们直接交付采集好的数据或者编写好的程序。微信：p02721606官方网站http://www.chujiangdata.com\n转载知乎；\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "9"}