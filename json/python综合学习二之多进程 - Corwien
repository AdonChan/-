{"title": "python综合学习二之多进程 - Corwien ", "index": "python", "content": "本节讲学习Python的多进程。\n一、多进程和多线程比较\n多进程 Multiprocessing 和多线程 threading 类似, 他们都是在 python 中用来并行运算的. 不过既然有了 threading, 为什么 Python 还要出一个 multiprocessing 呢? 原因很简单, 就是用来弥补 threading 的一些劣势, 比如在 threading 教程中提到的GIL.\n使用 multiprocessing 也非常简单, 如果对 threading 有一定了解的朋友, 你们的享受时间就到了. 因为 python 把 multiprocessing 和 threading 的使用方法做的几乎差不多. 这样我们就更容易上手. 也更容易发挥你电脑多核系统的威力了!\n二、添加进程Process\nimport multiprocessing as mp\nimport threading as td\n\ndef job(a,d):\n    print('aaaaa')\n\nt1 = td.Thread(target=job,args=(1,2))\np1 = mp.Process(target=job,args=(1,2))\nt1.start()\np1.start()\nt1.join()\np1.join()\n从上面的使用对比代码可以看出，线程和进程的使用方法相似。\n使用\n在运用时需要添加上一个定义main函数的语句\nif __name__=='__main__':\n完整的应用代码：\n# -*- coding:utf-8 -*-\n\n\"\"\"\n@author: Corwien\n@file: process_test.py\n@time: 18/8/26 01:12\n\"\"\"\n\nimport multiprocessing as mp\n\ndef job(a, d):\n    print a, d\n\nif __name__ == '__main__':\n    p1 = mp.Process(target=job, args=(1, 2))\n    p1.start()\n    p1.join()\n\n运行环境要在terminal环境下，可能其他的编辑工具会出现运行结束后没有打印结果，在terminal中的运行后打印的结果为：\n➜  baseLearn python ./process/process_test.py\n1 2\n➜  baseLearn\n三、存储进程输出Queue\nQueue的功能是将每个核或线程的运算结果放在队里中， 等到每个线程或核运行完毕后再从队列中取出结果， 继续加载运算。原因很简单, 多线程调用的函数不能有返回值, 所以使用Queue存储多个线程运算的结果\nprocess_queue.py\n# -*- coding:utf-8 -*-\n\n\"\"\"\n@author: Corwien\n@file: process_queue.py\n@time: 18/8/26 01:12\n\"\"\"\n\nimport multiprocessing as mp\n\n# 定义一个被多线程调用的函数，q 就像一个队列，用来保存每次函数运行的结果\ndef job(q):\n    res = 0\n    for i in range(1000):\n        res += i + i**2 + i**3\n    q.put(res)   #queue\n\nif __name__ == '__main__':\n    q = mp.Queue()\n    p1 = mp.Process(target=job, args=(q,))\n    p2 = mp.Process(target=job, args=(q,))\n\n    # 分别启动、连接两个线程\n    p1.start()\n    p2.start()\n    p1.join()\n    p2.join()\n\n    # 上面是分两批处理的，所以这里分两批输出，将结果分别保存\n    res1 = q.get()\n    res2 = q.get()\n\n    print res1,res2\n\n打印输出结果：\n➜ python ./process/process_queue.py\n249833583000 249833583000\n四、进程池\n进程池就是我们将所要运行的东西，放到池子里，Python会自行解决多进程的问题。\n1、导入多进程模块\n首先import multiprocessing 和定义job()\nimport multiprocessing as mp\n\ndef job(x):\n    return x*x\n2、进程池Pool()和map()\n然后我们定义一个Pool\npool = mp.Pool()\n有了池子之后，就可以让池子对应某一个函数，我们向池子里丢数据，池子就会返回函数返回的值。 Pool和之前的Process的不同点是丢向Pool的函数有返回值，而Process的没有返回值。\n接下来用map()获取结果，在map()中需要放入函数和需要迭代运算的值，然后它会自动分配给CPU核，返回结果\nres = pool.map(job, range(10))\n让我们来运行一下\ndef multicore():\n    pool = mp.Pool()\n    res = pool.map(job, range(10))\n    print(res)\n    \nif __name__ == '__main__':\n    multicore()\n完成代码：\n# -*- coding:utf-8 -*-\n\n\"\"\"\n@author: Corwien\n@file: process_queue.py\n@time: 18/8/26 01:12\n\"\"\"\n\nimport multiprocessing as mp\n\ndef job(x):\n    return x*x  # 注意这里的函数有return返回值\n\ndef multicore():\n    pool = mp.Pool()\n    res = pool.map(job, range(10))\n    print(res)\n    \nif __name__ == '__main__':\n    multicore()\n执行结果：\n➜  baseLearn python ./process/process_pool.py\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n3、自定义核数量\n我们怎么知道Pool是否真的调用了多个核呢？我们可以把迭代次数增大些，然后打开CPU负载看下CPU运行情况\n打开CPU负载(Mac)：活动监视器 > CPU > CPU负载(单击一下即可)\nPool默认大小是CPU的核数，我们也可以通过在Pool中传入processes参数即可自定义需要的核数量\ndef multicore():\n    pool = mp.Pool(processes=3) # 定义CPU核数量为3\n    res = pool.map(job, range(10))\n    print(res)\n4、apply_async()\nPool除了map()外，还有可以返回结果的方式，那就是apply_async().\napply_async()中只能传递一个值，它只会放入一个核进行运算，但是传入值时要注意是可迭代的，所以在传入值后需要加逗号, 同时需要用get()方法获取返回值\ndef multicore():\n    pool = mp.Pool() \n    res = pool.map(job, range(10))\n    print(res)\n    res = pool.apply_async(job, (2,))\n    # 用get获得结果\n    print(res.get())\n运行结果；\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]  # map()\n4 # apply_async()\n总结\n\n\nPool默认调用是CPU的核数，传入processes参数可自定义CPU核数\n\nmap() 放入迭代参数，返回多个结果\n\napply_async()只能放入一组参数，并返回一个结果，如果想得到map()的效果需要通过迭代\n\n五、共享内存shared memory\n这节我们学习如何定义共享内存。只有用共享内存才能让CPU之间有交流。\nShared Value\n我们可以通过使用Value数据存储在一个共享的内存表中。\nimport multiprocessing as mp\n\nvalue1 = mp.Value('i', 0) \nvalue2 = mp.Value('d', 3.14)\n其中d和i参数用来设置数据类型的，d表示一个双精浮点类型 double，i表示一个带符号的整型。\n\n\nType code\nC Type\nPython Type\nMinimum size in bytes\n\n\n\n'b'\nsigned char\nint\n1\n\n\n'B'\nunsigned char\nint\n1\n\n\n'u'\nPy_UNICODE\nUnicode character\n2\n\n\n'h'\nsigned short\nint\n2\n\n\n'H'\nunsigned short\nint\n2\n\n\n'i'\nsigned int\nint\n2\n\n\n'I'\nunsigned int\nint\n2\n\n\n'l'\nsigned long\nint\n4\n\n\n'L'\nunsigned long\nint\n4\n\n\n'q'\nsigned long long\nint\n8\n\n\n'Q'\nunsigned long long\nint\n8\n\n\n'f'\nfloat\nfloat\n4\n\n\n'd'\ndouble\nfloat\n8\n\n\n\nShared Array\n在Python的 mutiprocessing 中，有还有一个Array类，可以和共享内存交互，来实现在进程之间共享数据。\narray = mp.Array('i', [1, 2, 3, 4])\n这里的Array和numpy中的不同，它只能是一维的，不能是多维的。同样和Value 一样，需要定义数据形式，否则会报错。 我们会在后一节举例说明这两种的使用方法.\n错误形式\narray = mp.Array('i', [[1, 2], [3, 4]]) # 2维list\n\n\"\"\"\nTypeError: an integer is required\n\"\"\"\n六、进程锁Lock\n不加进程锁\n让我们看看没有加进程锁时会产生什么样的结果。\n# -*- coding:utf-8 -*-\n\n\"\"\"\n@author: Corwien\n@file: process_no_lock.py\n@time: 18/8/26 09:22\n\"\"\"\n\nimport multiprocessing as mp\nimport time\n\ndef job(v, num):\n    for _ in range(5):\n        time.sleep(0.5) # 暂停0.5秒，让输出效果更明显\n        v.value += num  # v.value获取共享变量值\n        print(v.value)\n\ndef multicore():\n    v = mp.Value('i', 0)  # 定义共享变量\n    p1 = mp.Process(target=job, args=(v, 1))\n    p2 = mp.Process(target=job, args=(v, 4)) # 设定不同的number看如何抢夺内存\n    p1.start()\n    p2.start()\n    p1.join()\n    p2.join()\n\nif __name__ == '__main__':\n    multicore()\n在上面的代码中，我们定义了一个共享变量v，两个进程都可以对它进行操作。 在job()中我们想让v每隔0.1秒输出一次累加num的结果，但是在两个进程p1和p2 中设定了不同的累加值。所以接下来让我们来看下这两个进程是否会出现冲突。\n结果打印：\n➜  baseLearn python ./process/process_no_lock.py\n1\n5\n9\n9\n13\n13\n17\n17\n18\n18\n➜  baseLearn\n我们可以看到，进程1和进程2在相互抢着使用共享内存v。\n加进程锁\n为了解决上述不同进程抢共享资源的问题，我们可以用加进程锁来解决。\n首先需要定义一个进程锁\n l = mp.Lock() # 定义一个进程锁\n然后将进程锁的信息传入各个进程中\np1 = mp.Process(target=job, args=(v,1,l)) # 需要将Lock传入\np2 = mp.Process(target=job, args=(v,3,l)) \n在job()中设置进程锁的使用，保证运行时一个进程的对锁内内容的独占\ndef job(v, num, l):\n    l.acquire() # 锁住\n    for _ in range(5):\n        time.sleep(0.1) \n        v.value += num # v.value获取共享内存\n        print(v.value)\n    l.release() # 释放\n全部代码：\n# -*- coding:utf-8 -*-\n\n\"\"\"\n@author: Corwien\n@file: process_lock.py\n@time: 18/8/26 09:22\n\"\"\"\n\nimport multiprocessing as mp\nimport time\n\ndef job(v, num, l):\n    l.acquire() # 锁住\n    for _ in range(5):\n        time.sleep(0.5) # 暂停0.5秒，让输出效果更明显\n        v.value += num  # v.value获取共享变量值\n        print(v.value)\n    l.release() # 释放\n\ndef multicore():\n    l = mp.Lock() # 定义一个进程锁\n    v = mp.Value('i', 0)  # 定义共享变量\n    p1 = mp.Process(target=job, args=(v, 1, l)) # 需要将lock传入\n    p2 = mp.Process(target=job, args=(v, 4, l)) # 设定不同的number看如何抢夺内存\n    p1.start()\n    p2.start()\n    p1.join()\n    p2.join()\n\nif __name__ == '__main__':\n    multicore()\n\n\n运行一下，让我们看看是否还会出现抢占资源的情况：\n结果打印：\n➜  baseLearn python ./process/process_lock.py\n1\n2\n3\n4\n5\n9\n13\n17\n21\n25\n显然，进程锁保证了进程p1的完整运行，然后才进行了进程p2的运行\n\n                ", "mainLikeNum": ["7 "], "mainBookmarkNum": "6"}