{"title": "python 爬取微信文章（搜狗为入口） - 爬虫家 ", "index": "网页爬虫,微信,python", "content": "本人想搞个采集微信文章的网站，无奈实在从微信本生无法找到入口链接，网上翻看了大量的资料，发现大家的做法总体来说大同小异，都是以搜狗为入口。下文是笔者整理的一份python爬取微信文章的代码，有兴趣的欢迎阅读，本人小站：www.quzhuanpan.com喜欢的话可以去首页加官方微信或者qq。#coding:utf-8author = 'haoning'**#!/usr/bin/env pythonimport timeimport datetimeimport requests**import jsonimport sysreload(sys)sys.setdefaultencoding( \"utf-8\" )import reimport xml.etree.ElementTree as ETimport os#OPENID = 'oIWsFtyel13ZMva1qltQ3pfejlwU'OPENID = 'oIWsFtw_-W2DaHwRz1oGWzL-wF9M&ext'XML_LIST = []# get current time in millisecondscurrent_milli_time = lambda: int(round(time.time() * 1000))def get_json(pageIndex):\nglobal OPENID\nthe_headers = {\n'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36',\n'Referer': 'http://weixin.sogou.com/gzh?openid={0}'.format(OPENID),\n'Host': 'weixin.sogou.com'\n}\n\nurl = 'http://weixin.sogou.com/gzhjs?cb=sogou.weixin.gzhcb&openid={0}&page={1}&t={2}'.format(OPENID, pageIndex, current_milli_time()) #url\nprint(url)\n\nresponse = requests.get(url, headers = the_headers)\n# TO-DO; check if match the reg\nresponse_text = response.text\nprint response_text\njson_start = response_text.index('sogou.weixin.gzhcb(') + 19\njson_end = response_text.index(')') - 2\njson_str = response_text[json_start : json_end] #get json\n#print(json_str)\n# convert json_str to json object\njson_obj = json.loads(json_str) #get json obj\n# print json_obj['totalPages']\nreturn json_obj\n\ndef add_xml(jsonObj):\nglobal XML_LIST\nxmls = jsonObj['items'] #get item\n#print type(xmls)\nXML_LIST.extend(xmls) #用新列表扩展原来的列表\n**[#www.oksousou.com][2]**\n# ------------ Main ----------------print 'play it :) '# get total pagesdefault_json_obj = get_json(1)total_pages = 0total_items = 0if(default_json_obj):\n# add the default xmls\nadd_xml(default_json_obj)\n# get the rest items\ntotal_pages = default_json_obj['totalPages']\ntotal_items = default_json_obj['totalItems']\nprint total_pages\n# iterate all pages\nif(total_pages >= 2):\n    for pageIndex in range(2, total_pages + 1):\n        add_xml(get_json(pageIndex)) #extend\n        print 'load page ' + str(pageIndex)\n        print len(XML_LIST)\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "9"}