{"title": "【DL-CV】浅谈GoogLeNet（咕咕net） - 个人文章 ", "index": "python,人工智能,计算机视觉,深度学习,神经网络", "content": "咕了一个多月后终于重新变成人，今天我们就来谈谈  咕咕net（GoogLeNet） 的结构，在下次咕咕（大表哥2）之前挣扎一下。\n\nGoogLeNet初始的想法很简单，“大力出奇迹”，即通过增加网络的网络的尺寸（深度与宽度）来变强。这脑回路看上去没啥毛病，但是一用在原味版的cnn上问题就来了，尺寸的增加和全连接层的存在带来了巨量的参数，计算成本暴增的同时增加了过拟合的风险。为了解决这一麻烦贯彻“大力出奇迹”方针，新的网络结构被提了出来，而其中的精妙之处就是inception模块，用上该模块的GoogLeNet能更高效的利用计算资源，在相同的计算量下能提取到更多的特征，从而提升训练结果，下面就谈谈他。\nInception 模块\n\n从图片来看inception模块就用不同尺寸的卷积核同时对输入进行卷积操作，外加一个池化操作，最后把各自的结果汇聚在一起作为总输出（暗示他们都有相同的尺寸）。与传统cnn的串联结构不同，inception模块使用了并行结构并且引入了不同尺寸的卷积核。关于为什么这种改变是好的，下面是一些参考解释：\n\n直观感觉上，在多个尺度上同时进行卷积，能提取到不同尺度的特征，这是好的\n（最主要的优点）以往为了打破网络对称性和提高学习能力，传统的网络都使用了随机稀疏连接。但是，计算机软硬件对非均匀稀疏数据的计算效率是很差的。那么存不存在既能保持网络结构的稀疏性，又能利用密集矩阵的高计算性能的方法呢？答案就在这个inception里，其实现将稀疏矩阵聚类为较为密集的子矩阵来提高计算性能。\n\n再说下inception的一些设定：\n\n卷积核尺寸使用1，3，5是为了方便对齐，只需padding分别为0，1，2；步长都取1 就能获得相同尺寸的输出以叠加\n网络越到后面，特征越抽象，而且每个特征所涉及的感受野也更大了，因此随着层数的增加，3x3和5x5卷积（数量）的比例也要增加\n\n降维操作\n然而像上图一样直接投入使用，参数量和计算量还是很大的，为了进一步降低消耗，inception在 3x3 和 5x5 卷积前和池化后引进了 1x1 卷积进行数据降维（事先把数据深度下降），还能顺便增加网络深度。如下图红色区域。另：降维后还是需要经过激活函数\n至于降维操作是否会造成数据丢失？就结果来看来不必担心，别人已经测试过了\nGoogLeNet结构\n既然最核心的inception模块讲完了，那就直接展示GoogLeNet的结构了，其大部分都是各种inception模块叠加而成的。\n整个网络除了inception外最引人注目的就是中途露出的两个小尾巴了，那是两个辅助分类器。说实话这是GoogLeNet第二个精妙之处了。除了最终的分类结果外，中间节点的分类效果还是不错的，所以GoogLeNet干脆从中间拉了两条分类器出来，然他们按一个较小的权重（如0.3）加到最终的分类结果中，这样做好处有三：\n\n相当于做了模型整合\n给网络增加了反向传播的梯度信号，一定程度解决了深网络带来的梯度消失的问题\n而且还提供了额外的正则化\n喵啊喵啊\n\n当然辅助分类器只用于训练阶段，在测试阶段是要去掉的\n其他一些新奇之处就是网络的最后用了平均池化代替了全连接层，然而后面还是接了一个全连接层，这是方便其他人进行迁移学习的。\n靓文推荐\n以上介绍的就是最原始最开始的GoogLeNet，也叫GoogLeNet Incepetion V1，2014年提出的。在经过多年的改进后GoogLeNet也有几个延伸版本了如使用了BN的V2版本，借鉴了ResNet的V4版本，这里也不再细讲，只推荐几篇我认为比较好的靓文\n\nGoogLeNet各层Inception的详细设定\nGoogLeNet及其延伸版本\n更多自寻\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}