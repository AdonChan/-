{"title": "小白爬虫scrapy第三篇 - python爬虫 ", "index": "python", "content": "在你的spiders目录下创建自己第一个爬虫项目,我我这儿命名为AiquerSpider.py然后编辑文件\n# !/usr/bin/python\n# -*- coding: UTF-8 -*-\nimport scrapy\nfrom scrapy.http import Request\n\n以上是我所需要导入使用的包,你也可以自定义别的包.接下来咱们创建类:\n# !/usr/bin/python\n# -*- coding: UTF-8 -*-\nimport scrapy\nfrom scrapy.http import Request\n\nclass AiquerSpider(scrapy.Spider):\n    # name 定义爬虫名称\n    name = ''\n    # allowed_domains定义访问域\n    allowed_domains = []\n    # bash_url定义要爬取的网站\n    bash_url = ''\n\n    # 这个方法是必须有不然你的爬虫跑不起来(同等java中的main方法)\n    def parse(self, response):\n        \n        pass\n在写代码之前呢咱们要去做点大事,具体看下面,嘿嘿!咱们要首先定义集合就是items.py中用来存放的数据咱们看看网页吧,在具体说需要哪些东西.上面呢我们需要网站地址\\用户名称\\视频图片\\视频地址下载视频的话我这儿就不做讲解了我们就获取这几个参数为例子首先,我们需要爱奇艺网站用户地址做分析\nhttp://www.iqiyi.com/u/141242...http://www.iqiyi.com/u/用户ID 这一段是找到用户网站首页/v 这个是该用户下的视频这样我们就了解到了如何去手动指定用户并且爬取他下面的视频了废话不多说,先上items的代码\n# -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# http://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\n\nclass AiquerItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    # 视频名称\n    video_name = scrapy.Field()\n    # 视频时间\n    video_time = scrapy.Field()\n    # 视频图片路径\n    video_imgurl = scrapy.Field()\n    # 视频路径\n    video_url = scrapy.Field()\n    pass\n我们的items就写完了再回到咱们的爬虫上面,具体解释都在里面的注释中了\n# !/usr/bin/python\n# -*- coding: UTF-8 -*-\n# 这里是自己导入的包\nimport scrapy\nfrom scrapy.http import Request\nfrom AiQuer.items import AiquerItem\n# 定义类\nclass AiquerSpider(scrapy.Spider):\n    # http://www.iqiyi.com/u/1412422046/v?page=1&video_type=1\n    # name 定义爬虫名称\n    name = 'AiquerSpider'\n    # allowed_domains定义访问域\n    allowed_domains = ['iqiyi.com']\n    # bash_url定义要爬取的网站\n    bash_url = 'http://www.iqiyi.com/u/'\n    # 做拼接地址的结尾\n    bashurl = '/v?page=1'\n    user_id = None\n\n    # 用来获取输入的用户编号返回拼接地址\n    def start(self):\n        self.user_id = self.validateIsNull(input(u'请输入用户编号:'))\n        if self.user_id:\n            url = self.bash_url + self.user_id + self.bashurl\n            return url\n\n\n    def start_requests(self):\n        # 首先获取用户首页地址\n        url = self.start()\n        # Request函数第一个是地址,第二个是调用的方法\n        yield Request(url, self.max_page)\n        #yield Request('http://www.iqiyi.com/u/1412422046/v?page=2', self.parse)\n\n    # 非空验证\n    def validateIsNull(self, user_id):\n        if user_id.strip() == '':\n            return None\n        else:\n            return user_id\n\n    # 获取最大页数\n    def max_page(self, response):\n        max_num = int(response.xpath('//div//a[last()-1]/text()').extract()[0])\n        for i in range(1, max_num + 1):\n            url = self.bash_url + self.user_id + '/v?page=' + str(i) + '&video_type=1'\n            # print(url)\n            yield Request(url, self.parse)\n\n    # 获取页面需要的数据\n    def parse(self, response):\n        item = AiquerItem()\n        # 注释代码块用来做测试的,小伙伴可以拿出来一个一个测试\n        '''\n        names = response.xpath('//ul/li//div//p//a/@title').extract()\n        times = response.xpath('//div//span[@class=\"mod-listTitle_right\"]/text()').extract()\n        imgurls = response.xpath('//div[@class=\"site-piclist_pic\"]//a//img/@src').extract()\n        urls = response.xpath('//div[@class=\"site-piclist_pic\"]//a/@href').extract()\n        print(names)\n        print(times)\n        print(imgurls)\n        print(urls)\n        '''\n        # 通过xpath去寻找HTML页面中指定数据封装在items类中\n        item['video_name'] = response.xpath('//ul/li//div//p//a/@title').extract()\n        item['video_time'] = response.xpath('//div//span[@class=\"mod-listTitle_right\"]/text()').extract()\n        item['video_imgurl'] = response.xpath('//div[@class=\"site-piclist_pic\"]//a//img/@src').extract()\n        item['video_url'] = response.xpath('//div[@class=\"site-piclist_pic\"]//a/@href').extract()\n        return item\n\n有同学肯定会问('//div[@class=\"site-piclist_pic\"]//a/@href')这是啥玩意Xpath:\nXPath即为XML路径语言（XML Path Language），它是一种用来确定XML文档中某部分位置的语言。 \nXPath基于XML的树状结构，提供在数据结构树中找寻节点的能力。 \n起初XPath的提出的初衷是将其作为一个通用的、介于XPointer与XSL间的语法模型。\n在这篇中是要有点XPath的基础的可以先去看看这个教程配合这个教程一起写提高XPath熟练度.教程链接:XPath教程接下来就是如何保存这些数据了介绍两种一个是直接保存为文件,另外一个是保存到数据库请看下篇\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}