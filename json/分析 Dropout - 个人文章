{"title": "分析 Dropout - 个人文章 ", "index": "python", "content": "作者：chen_h微信号 & QQ：862251340微信公众号：coderpai简书地址：https://www.jianshu.com/p/ba9...\n\n这篇教程是翻译Paolo Galeone写的Dropout分析教程，作者已经授权翻译，这是原文。\n过拟合一直是深度神经网络（DNN）所要面临的一个问题：模型只是在训练数据上学习分类，使其适应训练样本，而不是去学习一个能够对通用数据进行分类的完全决策边界。这些年，提出了很多的方案去解决过拟合问题。其中一种方法就是Dropout，由于这种方法非常简单，但是在实际使用中又具有很好的效果，所以被广泛使用。\nDropout\n\nDropout 背后的思想其实就是把DNN当做一个集成模型来训练，之后取所有值的平均值，而不只是训练单个DNN。\nDNN网络将Dropout率设置为 p，也就是说，一个神经元被保留的概率是 1-p。当一个神经元被丢弃时，无论输入或者相关的参数是什么，它的输出值就会被设置为0。\n丢弃的神经元在训练阶段，对BP算法的前向和后向阶段都没有贡献。因为这个原因，所以每一次训练，它都像是在训练一个新的网络。\n你可以查看这篇论文，进行更详细的了解。\n简而言之：Dropout 可以在实际工作中发挥很好的效果，因为它能防止神经网络在训练过程中产生共适应。\n现在，我们对Dropout有了一个直观的概念，接下来让我们深入的分析它。\nDropout是如何工作的？\n正如前面所说的，Dropout 以概率 p 来丢弃神经元， 并且让别的神经元以概率 q = 1 - p，进行保留。\n每一个神经元都有相同的概率被丢弃和保留。 也就是说：\n给定\n\nh(x) = xW + b 是一个线性转换方程，其中输入 x 是一个 di 维度的数据，输出数据是一个 dh 维度的。\na(h) 是一个激活函数。\n\n我们只将 Dropout 作用在模型的训练阶段，即我们可以把模型的激活函数修改为：\n\n其中，D = （X1, ..., Xdn） 是一个 dh 维度的向量，Xi 是一个伯努利变量。\n*注：Probability density function 是概率密度函数，针对连续型随机变量而言，一般写法是一个函数，如 f(x)=e^(-x)，积分得到∫f(x)dx=1。Probability mass function 是概率质量函数，是针对离散型随机变量而言。一般写法是写成对应每一个特定取值的概率，如P{x=xi}=1/15。*\n伯努利随机变量具有以下概率质量分布：\n\n其中，k 是可能的输出结果。\n很明显，这个随机变量完美的模拟了单个神经元上面的 Dropout 过程。实际上，神经元以概率 p = P(k=1) 丢弃，以 p = P(k=0) 保留。\n比如，在第 i 个神经元上，Dropout 的应用如下所示：\n\n其中，P(Xi = 0) = p 。\n因为，在训练阶段，一个神经元被保留的概率是 q 。但是在测试阶段，我们必须去模拟训练阶段的集成网络模型。\n为此，作者建议在测试阶段将神经元的激活值乘以因子 q 再输出。以便在训练阶段集成模型，在测试阶段只要输出单个模型的值即可。从而得到下式：\n\nInverted Dropout\n我们稍微将 Dropout 方法改进一下，使得我们只需要在训练阶段缩放激活函数的输出值，而不用在测试阶段改变什么。这个改进的 Dropout 方法就被称之为 Inverted Dropout 。\n比例因子将修改为是保留概率的倒数，即：\n\n因此，我们最终可以把模型修改为：\n\n在各种深度学习框架的实现中，我们都是用 Inverted Dropout 来代替 Dropout，因为这种方式有助于模型的完整性，我们只需要修改一个参数（保留/丢弃概率），而整个模型都不用修改。\n对一层神经元进行 Dropout 处理\n假设第 h 层有 n 个神经元，那么在一次循环中，神经网络可以被看做是 n 次的伯努利实验的集成，每个神经元被保留的概率是 p 。\n因此，第 h 层一共被保留的神经元个数如下：\n\n由于每个神经元都是用伯努利随机变量进行建模的，并且所有这些随机变量是独立同分布的，所以所有被丢弃的神经元的总数也是一个随机量，称为二项式：\n\n其中，在 n 个试验中，获得保留 k 个的概率质量分布为：\n\n这个式子也很好解释，如下：\n\n我们现在可以利用这个分布来分析丢弃指定神经元的概率。\n当我们使用 Dropout 时，我们需要先定义一个固定的 Dropout 概率 p，即我们期望从网络中丢弃多少比例的神经元。\n举个例子，如果我们的神经元数量是 n = 1024，p = 0.5，那么我们希望有 512 个神经元被丢弃。让我们来验证一下：\n\n因此，丢弃的神经元个数是 np = 512 的概率是 0.025 。\nPython 代码可以帮助我们可视化结果，比如我们把 n 值固定，然后改变 p 的值，那么可以得到下图：\n\n正如我们在上图中看到的，不管 p 值如何改变，平均丢弃的神经元个数都是 np 。也就是：\n\n而且，我们可以注意到，值的分布是关于 p = 0.5 对称的。而且，p 随着离 0.5 越远，np 的值越来越大。\n在训练阶段，我们需要把缩放因子 p 添加到网络中，因为我们期望在训练阶段只保留百分之 1-p 的神经元。相反，在测试阶段，我们需要开启所有的神经元。\nDropout 和其他正则化\nDropout 方法通常和 L2 范数或者其他参数约束技术（比如Max Norm）一起使用。规范化有助于使模型参数的值不是很大，而且这种方法参数值的变化过程不会很大。\n简而言之，例如，L2 归一化是损失函数的一个附加项，其中 λ∈[0,1] 是被称为正则化的超参数，F(W;x) 是模型，E 是真实值y和预测值y^的误差函数。\n\n对于这个附加项，我们很容易理解。当通过梯度下降进行反向传播时，这可以减少更新量。如果 η 是学习率，则参数 w∈W 的更新量为：\n\n相反，单独使用 Dropout 方法不能防止参数值在训练阶段变得过大。而且，Inverted Dropout 方法还会导致更新步骤变得更大，正如下面所描述的。\nInverted Dropout 和其他正则化\n由于 Dropout 方法不会阻止参数过大，而且参数之间也不会互相牵制。所以我们要使用 L2 正则化来改变这个情况，或者其他的正则化方法。\n加入明确的缩放因子，前面的方程就变为：\n\n观察上式，很容易发现，当使用 Inverted Dropout 方法时，学习率被缩放到 q 的因子，由于 q 的取值范围是 [0,1]，那么 η 和 q 之间的比率可以在以下之间变化：\n\n因此，从现在开始，我们称 q 是一个 boosting 因子，因为它提高了学习率。此外，我们称 r(q) 为有效学习率。\n因此，有效学习率相对于所选择的学习率有更好的表示性。由于这个原因，限制参数值的规范化方法可以帮助简化学习率的选择过程。\n总结\n\nDropout 方法存在两种形式：直接的和 Inverted。\n在单个神经元上面，Dropout 方法可以使用伯努利随机变量。\n在一层神经元上面，Dropout 方法可以使用伯努利随机变量。\n我们精确的丢弃 np 个神经元是不太可能的，但是在一个拥有 n 个神经元的网络层上面，平均丢弃的神经元就是 np 个。\nInverted Dropout 方法可以产生有效学习率。\nInverted Dropout 方法应该和别的规范化参数的技术一起使用，从而帮助简化学习率的选择过程。\nDropout 方法有助于防止深度神经网路的过拟合。\n\n\n作者：chen_h微信号 & QQ：862251340简书地址：https://www.jianshu.com/p/ba9...\nCoderPai 是一个专注于算法实战的平台，从基础的算法到人工智能算法都有设计。如果你对算法实战感兴趣，请快快关注我们吧。加入AI实战微信群，AI实战QQ群，ACM算法微信群，ACM算法QQ群。长按或者扫描如下二维码，关注 “CoderPai” 微信号（coderpai）\n\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}