{"title": "TensorFlow学习笔记（7）：TensorBoard——Tensor与Graph可视化 - 数据实验室 ", "index": "tensorflow,python", "content": "前言\n本文基于TensorFlow官网How-Tos的Visualizing Learning和Graph Visualization写成。\nTensorBoard是TensorFlow自带的一个可视化工具。本文在学习笔记（4）的基础上修改少量代码，以探索TensorBoard的使用方法。\n代码\n# -*- coding=utf-8 -*-\n# @author: 陈水平\n# @date: 2017-02-09\n# @description: implement a softmax regression model upon MNIST handwritten digits\n# @ref: http://yann.lecun.com/exdb/mnist/\n\nimport gzip\nimport struct\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf\n\n# MNIST data is stored in binary format, \n# and we transform them into numpy ndarray objects by the following two utility functions\ndef read_image(file_name):\n    with gzip.open(file_name, 'rb') as f:\n        buf = f.read()\n        index = 0\n        magic, images, rows, columns = struct.unpack_from('>IIII' , buf , index)\n        index += struct.calcsize('>IIII')\n\n        image_size = '>' + str(images*rows*columns) + 'B'\n        ims = struct.unpack_from(image_size, buf, index)\n        \n        im_array = np.array(ims).reshape(images, rows, columns)\n        return im_array\n\ndef read_label(file_name):\n    with gzip.open(file_name, 'rb') as f:\n        buf = f.read()\n        index = 0\n        magic, labels = struct.unpack_from('>II', buf, index)\n        index += struct.calcsize('>II')\n        \n        label_size = '>' + str(labels) + 'B'\n        labels = struct.unpack_from(label_size, buf, index)\n\n        label_array = np.array(labels)\n        return label_array\n\nprint \"Start processing MNIST handwritten digits data...\"\ntrain_x_data = read_image(\"MNIST_data/train-images-idx3-ubyte.gz\")\ntrain_x_data = train_x_data.reshape(train_x_data.shape[0], -1).astype(np.float32)\ntrain_y_data = read_label(\"MNIST_data/train-labels-idx1-ubyte.gz\")\ntest_x_data = read_image(\"MNIST_data/t10k-images-idx3-ubyte.gz\")\ntest_x_data = test_x_data.reshape(test_x_data.shape[0], -1).astype(np.float32)\ntest_y_data = read_label(\"MNIST_data/t10k-labels-idx1-ubyte.gz\")\n\ntrain_x_minmax = train_x_data / 255.0\ntest_x_minmax = test_x_data / 255.0\n\n# Of course you can also use the utility function to read in MNIST provided by tensorflow\n# from tensorflow.examples.tutorials.mnist import input_data\n# mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\n# train_x_minmax = mnist.train.images\n# train_y_data = mnist.train.labels\n# test_x_minmax = mnist.test.images\n# test_y_data = mnist.test.labels\n\n# We evaluate the softmax regression model by sklearn first\neval_sklearn = False\nif eval_sklearn:\n    print \"Start evaluating softmax regression model by sklearn...\"\n    reg = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\")\n    reg.fit(train_x_minmax, train_y_data)\n    np.savetxt('coef_softmax_sklearn.txt', reg.coef_, fmt='%.6f')  # Save coefficients to a text file\n    test_y_predict = reg.predict(test_x_minmax)\n    print \"Accuracy of test set: %f\" % accuracy_score(test_y_data, test_y_predict)\n\neval_tensorflow = True\nbatch_gradient = False\n\ndef variable_summaries(var):\n    with tf.name_scope('summaries'):\n        mean = tf.reduce_mean(var)\n        tf.summary.scalar('mean', mean)\n        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n        tf.summary.scalar('stddev', stddev)\n        tf.summary.scalar('max', tf.reduce_max(var))\n        tf.summary.scalar('min', tf.reduce_min(var))\n        tf.summary.histogram('histogram', var)\nif eval_tensorflow:\n    print \"Start evaluating softmax regression model by tensorflow...\"\n    # reformat y into one-hot encoding style\n    lb = preprocessing.LabelBinarizer()\n    lb.fit(train_y_data)\n    train_y_data_trans = lb.transform(train_y_data)\n    test_y_data_trans = lb.transform(test_y_data)\n\n    x = tf.placeholder(tf.float32, [None, 784])\n    with tf.name_scope('weights'):\n        W = tf.Variable(tf.zeros([784, 10]))\n        variable_summaries(W)\n    with tf.name_scope('biases'):\n        b = tf.Variable(tf.zeros([10]))\n        variable_summaries(b)\n    with tf.name_scope('Wx_plus_b'):\n        V = tf.matmul(x, W) + b\n        tf.summary.histogram('pre_activations', V)\n    with tf.name_scope('softmax'):\n        y = tf.nn.softmax(V)\n        tf.summary.histogram('activations', y)\n\n    y_ = tf.placeholder(tf.float32, [None, 10])\n\n    with tf.name_scope('cross_entropy'):\n        loss = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n        tf.summary.scalar('cross_entropy', loss)\n\n    with tf.name_scope('train'):\n        optimizer = tf.train.GradientDescentOptimizer(0.5)\n        train = optimizer.minimize(loss)\n    \n    with tf.name_scope('evaluate'):\n        with tf.name_scope('correct_prediction'):\n            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n        with tf.name_scope('accuracy'):\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n            tf.summary.scalar('accuracy', accuracy)\n\n    init = tf.global_variables_initializer()\n\n    sess = tf.Session()\n    sess.run(init)\n\n    merged = tf.summary.merge_all()\n    train_writer = tf.summary.FileWriter('log/train', sess.graph)\n    test_writer = tf.summary.FileWriter('log/test')\n    \n    if batch_gradient:\n        for step in range(300):\n            sess.run(train, feed_dict={x: train_x_minmax, y_: train_y_data_trans})\n            if step % 10 == 0:\n                print \"Batch Gradient Descent processing step %d\" % step\n        print \"Finally we got the estimated results, take such a long time...\"\n    else:\n        for step in range(1000):\n            if step % 10 == 0:\n                summary, acc = sess.run([merged, accuracy], feed_dict={x: test_x_minmax, y_: test_y_data_trans})\n                test_writer.add_summary(summary, step)\n                print \"Stochastic Gradient Descent processing step %d accuracy=%.2f\" % (step, acc)\n            else:\n                sample_index = np.random.choice(train_x_minmax.shape[0], 100)\n                batch_xs = train_x_minmax[sample_index, :]\n                batch_ys = train_y_data_trans[sample_index, :]\n                summary, _ = sess.run([merged, train], feed_dict={x: batch_xs, y_: batch_ys})\n                train_writer.add_summary(summary, step)\n\n    np.savetxt('coef_softmax_tf.txt', np.transpose(sess.run(W)), fmt='%.6f')  # Save coefficients to a text file\n    print \"Accuracy of test set: %f\" % sess.run(accuracy, feed_dict={x: test_x_minmax, y_: test_y_data_trans})\n思考\n主要修改点有：\n\nSummary：所有需要在TensorBoard上展示的统计结果。\ntf.name_scope()：为Graph中的Tensor添加层级，TensorBoard会按照代码指定的层级进行展示，初始状态下只绘制最高层级的效果，点击后可展开层级看到下一层的细节。\ntf.summary.scalar()：添加标量统计结果。\ntf.summary.histogram()：添加任意shape的Tensor，统计这个Tensor的取值分布。\ntf.summary.merge_all()：添加一个操作，代表执行所有summary操作，这样可以避免人工执行每一个summary op。\ntf.summary.FileWrite：用于将Summary写入磁盘，需要制定存储路径logdir，如果传递了Graph对象，则在Graph Visualization会显示Tensor Shape Information。执行summary op后，将返回结果传递给add_summary()方法即可。\n\n效果\nVisualizing Learning\nScalar\n\nHistogram\n首先是Distribution，显示取值范围：\n\n更细节的取值概率信息在Historgram里，如下：\n\nGraph Visualization\n\n双击train后，可查看下一层级的详细信息：\n\n\n                ", "mainLikeNum": ["5 "], "mainBookmarkNum": "4"}