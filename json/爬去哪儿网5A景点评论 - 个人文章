{"title": "爬去哪儿网5A景点评论 - 个人文章 ", "index": "python,python爬虫", "content": "目的\n获得去哪儿网评论信息（包括评论和打分），保存到本地csv文件。\n前期准备工作（踩点）\n1、打开网站，在门票搜索框直接输入“5A”，发现可以获得5A景区列表。\n\n2、按下F12，让我们看一看，他们是如何组织这些景点信息的\n\n哈哈，可以看到他们把所有的景点信息都放在了一个json文件里面，也就说我们直接获取这个json文件就能直接得到5A景点的列表了。下面是获取json的url，参数一目了然有没有？！\nhttp://piao.qunar.com/ticket/list.json?keyword=5A&region=&from=mps_search_suggest&page=2\n3、接下来我们再研究一下每个景点的详情页\n\n一样的配方，一样的味道！每个景点所有的评论依然在一个json文件里面看到下面url里面的sightId了吗,经过验证这个就是景点的编号.\nhttp://piao.qunar.com/ticket/detailLight/sightCommentList.json?sightId=3076&index=2&page=2&pageSize=10&tagType=0\n现在我们大致已经有了思路.第一步,获得所有5A景点的信息用列表保存[景点名字,景点id],第二部,由景点id可以获得景点评论.一切看起来都是那么顺利.\n4、等等似乎有什么不对我们再仔细看看\n\n注意到左边json里面的sightid了吗?是\"1582294258\"明显不是我们刚才获取评论时用到的那个呀!倒是和地址栏里面的那个是一样的.好吧,看来看来他们对每个景点设置两个id,一个用于获取html文件(记为id)另一个是评论id(记为rid).看来需要研究一下怎么把id变为rid\n\n好吧,发现这个id隐藏在head标签下喽.\n上代码\n采完点,我们就可以写代码了\nimport pandas as pd\nimport requests as req\nimport time,json\n\n#获得景点id\ndef GetId():\n    sightlist = []#储存包含景点列表的json\n    for i in range(1,23):#先把所有景点列表的json都下载下来\n        url = 'http://piao.qunar.com/ticket/list.json?from=mpl_search_suggest_h&keyword=5a&page=' + str(i) + '&sort='\n        response = req.get(url)\n        r = json.loads(response.text)\n        sightlist.append(r)\n        response.close\n        #time.sleep(3)\n    sight = {}#景点仓库所有的景点及其对应的id都放在这里\n    for jsons in sightlist:#处理每个json，获得景点信息\n        lists = jsons['data']['sightList']\n        for each in lists:\n            #print(each['sightName'])\n            key = each['sightName']\n            sight[key] = each['sightId']\n    \n    return sight\n#垃圾查找函数，不会正则表达式，真是无奈\ndef search(s,e,r):\n    start = r.find(s)\n    l = len(s)\n    end = r.find(e,start+l,start+100)\n    rr = r[start+l:end]\n    return rr\n#获得评论id\ndef Getrid(id):\n    url = 'http://piao.qunar.com/ticket/detail_' + str(id) + '.html'\n    response = req.get(url)\n    text = response.text\n    rid = search('piao.qunar.com/ticket/detail_','.html',text)#这个方法是我百度的，有些时候用起来还真是方便\n    num = search('<li class=\"mp-commentstab-item mp-actived\" mp-role=\"tagItem\" data-type=\"0\">全部(',')</li>',text)\n    \n    dd = []\n    dd.append(rid)\n    dd.append(num)\n    return dd\n#获得一个景点的评论\ndef Getcom(rid,num,name):\n    comments = [['*********************************以下是' + name + '的评论*********************************','']]#初始化并给每个景点的评论加上表头\n    page = 1200 if int(num) > 1200 else int(num)#按照上级要求，每个景点1200条左右评论就行\n    for i in range(1,(page//10)+1):#逐页获取评论并保存\n        url = 'http://piao.qunar.com/ticket/detailLight/sightCommentList.json?sightId=' + str(rid) + '&index='+ str(i) + '&page=' + str(i) + '&pageSize=10&tagType=0'\n        response = req.get(url)\n        r = json.loads(response.text)\n        if 'commentList' in r['data'].keys():\n            for each in r['data']['commentList']:\n                data = []#【评论，评分】\n                data.append(each['content'])\n                data.append(each['score'])\n                if data[0] == \"用户未点评，系统默认好评。\":#去除无用的评论\n                    continue\n                comments.append(data)\n                print(data[0])\n                print(data[1])\n                \n    return comments\n\n#保存到本地文件\ndef save(l):\n    \n    head = [\"哪网5A景区的的评论\" , \"评分\" ]\n    df = pd.DataFrame (l , columns = head)\n    df.to_csv ('去哪网.csv', encoding = \"utf-8\")\n\n    \nif __name__ ==\"__main__\":\n    sightid = GetId()#获得景点 id\n    comment = []\n    for each in sightid:\n        print(each)\n        print(Getrid(sightid[each])[0])\n        print(Getrid(sightid[each])[1])\n        \n        co = Getcom(Getrid(sightid[each])[0],Getrid(sightid[each])[1],each)\n        comment.extend(co)\n            \n        \n    print(\"本次共爬取信息%d条数据\"%(len(comment)))\n    save(comment)\n    \n\n        \n               \n\n成果展示外加反省\n大概30万条吧,美滋滋.等我开心完就回来,就回来整理出现的问题.欢迎各位大佬批评指正.\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}