{"title": "Python网页信息采集：使用PhantomJS采集淘宝天猫商品内容 - 一起学习python网络爬虫 ", "index": "编程语言,网络爬虫,phantomjs,python", "content": "\n1，引言\n最近一直在看Scrapy 爬虫框架，并尝试使用Scrapy框架写一个可以实现网页信息采集的简单的小程序。尝试过程中遇到了很多小问题，希望大家多多指教。\n本文主要介绍如何使用Scrapy结合PhantomJS采集天猫商品内容，文中自定义了一个DOWNLOADER_MIDDLEWARES，用来采集需要加载js的动态网页内容。看了很多介绍DOWNLOADER_MIDDLEWARES资料，总结来说就是使用简单，但会阻塞框架，所以性能方面不佳。一些资料中提到了自定义DOWNLOADER_HANDLER或使用scrapyjs可以解决阻塞框架的问题，有兴趣的小伙伴可以去研究一下，这里就不多说了。\n2，具体实现\n2.1，环境需求\n需要执行以下步骤，准备Python开发和运行环境：\n\nPython--官网下载安装并部署好环境变量 （本文使用Python版本为3.5.1）\nlxml-- 官网库下载对应版本的.whl文件，然后命令行界面执行 \"pip install .whl文件路径\"\nScrapy--命令行界面执行 \"pip install Scrapy\"，详细请参考《Scrapy的第一次运行测试》\nselenium--命令行界面执行 \"pip install selenium\"\nPhantomJS -- 官网下载\n\n上述步骤展示了两种安装：1，安装下载到本地的wheel包；2，用Python安装管理器执行远程下载和安装。注：包的版本需要和python版本配套\n2.2，开发和测试过程\n首先找到需要采集的网页，这里简单找了一个天猫商品，网址https://world.tmall.com/item/526449276263.htm，页面如下：\n然后开始编写代码，以下代码默认都是在命令行界面执行\n1)，创建scrapy爬虫项目tmSpider\nE:\\python-3.5.1>scrapy startproject tmSpider\n\n2)，修改settings.py配置\n\n更改ROBOTSTXT_OBEY的值为False；\n关闭scrapy默认的下载器中间件；\n加入自定义DOWNLOADER_MIDDLEWARES。\n\n配置如下：\nDOWNLOADER_MIDDLEWARES = {\n    'tmSpider.middlewares.middleware.CustomMiddlewares': 543,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None\n}\n\n3)，在项目目录下创建middlewares文件夹,然后在文件夹下创建middleware.py文件，代码如下：\n# -*- coding: utf-8 -*-\n\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.http import HtmlResponse, Response\n\nimport tmSpider.middlewares.downloader as downloader\n\nclass CustomMiddlewares(object):\n    def process_request(self, request, spider):\n        url = str(request.url)\n        dl = downloader.CustomDownloader()\n        content = dl.VisitPersonPage(url)\n        return HtmlResponse(url, status = 200, body = content)\n    \n    def process_response(self, request, response, spider):\n        if len(response.body) == 100:\n            return IgnoreRequest(\"body length == 100\")\n        else:\n            return response\n\n4)，使用selenium和PhantomJS写一个网页内容下载器，同样在上一步创建好的middlewares文件夹中创建downloader.py文件，代码如下：\n# -*- coding: utf-8 -*-\nimport time\nfrom scrapy.exceptions import IgnoreRequest\nfrom scrapy.http import HtmlResponse, Response\nfrom selenium import webdriver\nimport selenium.webdriver.support.ui as ui \n\nclass CustomDownloader(object):\n    def __init__(self):\n        # use any browser you wish\n        cap = webdriver.DesiredCapabilities.PHANTOMJS\n        cap[\"phantomjs.page.settings.resourceTimeout\"] = 1000\n        cap[\"phantomjs.page.settings.loadImages\"] = True\n        cap[\"phantomjs.page.settings.disk-cache\"] = True\n        cap[\"phantomjs.page.customHeaders.Cookie\"] = 'SINAGLOBAL=3955422793326.2764.1451802953297; '\n        self.driver = webdriver.PhantomJS(executable_path='F:/phantomjs/bin/phantomjs.exe', desired_capabilities=cap)\n        wait = ui.WebDriverWait(self.driver,10)\n    \n    def VisitPersonPage(self, url):\n        print('正在加载网站.....')\n        self.driver.get(url)\n        time.sleep(1)\n        # 翻到底，详情加载\n        js=\"var q=document.documentElement.scrollTop=10000\"\n        self.driver.execute_script(js)\n        time.sleep(5)\n        content = self.driver.page_source.encode('gbk', 'ignore')\n        print('网页加载完毕.....')\n        return content\n\n    def __del__(self):\n        self.driver.quit()\n\n5) 创建爬虫模块\n在项目目录E:python-3.5.1tmSpider，执行如下代码：\nE:\\python-3.5.1\\tmSpider>scrapy genspider tmall 'tmall.com'\n\n执行后，项目目录E:python-3.5.1tmSpidertmSpiderspiders下会自动生成tmall.py程序文件。该程序中parse函数处理scrapy下载器返回的网页内容，采集网页信息的方法可以是：\n\n使用xpath或正则方式从response.body中采集所需字段，\n通过gooseeker api获取的内容提取器实现一站转换所有字段，而且不用手工编写转换用的xpath（如何获取内容提取器请参考python使用xslt提取网页数据）,代码如下：\n\n# -*- coding: utf-8 -*-\nimport time\nimport scrapy\n\nimport tmSpider.gooseeker.gsextractor as gsextractor\n\nclass TmallSpider(scrapy.Spider):\n    name = \"tmall\"\n    allowed_domains = [\"tmall.com\"]\n    start_urls = (\n        'https://world.tmall.com/item/526449276263.htm',\n    )\n    \n    # 获得当前时间戳\n    def getTime(self):\n        current_time = str(time.time())\n        m = current_time.find('.')\n        current_time = current_time[0:m]\n        return current_time\n\n    def parse(self, response):\n        html = response.body\n        print(\"----------------------------------------------------------------------------\")\n        extra=gsextractor.GsExtractor()\n        extra.setXsltFromAPI(\"31d24931e043e2d5364d03b8ff9cc77e\", \"淘宝天猫_商品详情30474\",\"tmall\",\"list\")\n\n        result = extra.extract(html)\n        print(str(result).encode('gbk', 'ignore').decode('gbk'))\n        #file_name = 'F:/temp/淘宝天猫_商品详情30474_' + self.getTime() + '.xml'\n        #open(file_name,\"wb\").write(result)\n6)，启动爬虫\n在E:python-3.5.1tmSpider项目目录下执行命令\nE:\\python-3.5.1\\simpleSpider>scrapy crawl tmall\n\n输出结果:\n提一下，上述命令只能一次启动一个爬虫，如果想同时启动多个呢？那就需要自定义一个爬虫启动模块了，在spiders下创建模块文件runcrawl.py，代码如下\n# -*- coding: utf-8 -*-\n\nimport scrapy\nfrom twisted.internet import reactor\nfrom scrapy.crawler import CrawlerRunner\n\nfrom tmall import TmallSpider\n...\nspider = TmallSpider(domain='tmall.com')\nrunner = CrawlerRunner()\nrunner.crawl(spider)\n...\nd = runner.join()\nd.addBoth(lambda _: reactor.stop())\nreactor.run()\n\n执行runcrawl.py文件，输出结果：\n3，展望\n以自定义DOWNLOADER_MIDDLEWARES调用PhantomJs的方式实现爬虫后，在阻塞框架的问题上纠结了很长的时间，一直在想解决的方式。后续会研究一下scrapyjs，splash等其他调用浏览器的方式看是否能有效的解决这个问题。\n4，相关文档\n1， Python即时网络爬虫：API说明\n5，集搜客GooSeeker开源代码下载源\n1， GooSeeker开源Python网络爬虫GitHub源\n6，文档修改历史\n1，2016-07-04：V1.0\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "27"}