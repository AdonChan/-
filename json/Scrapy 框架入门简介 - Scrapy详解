{"title": "Scrapy 框架入门简介 - Scrapy详解 ", "index": "scrapy,网页爬虫,python,yield", "content": "Scrapy 框架\nScrapy是用纯Python实现一个为了爬取网站数据、提取结构性数据而编写的应用框架，用途非常广泛。\n框架的力量，用户只需要定制开发几个模块就可以轻松的实现一个爬虫，用来抓取网页内容以及各种图片，非常之方便。\nScrapy 使用了 Twisted'twɪstɪd异步网络框架来处理网络通讯，可以加快我们的下载速度，不用自己去实现异步框架，并且包含了各种中间件接口，可以灵活的完成各种需求。\nScrapy架构图(绿线是数据流向)：\n\nScrapy Engine(引擎): 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。\nScheduler(调度器): 它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。\nDownloader（下载器）：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理，\nSpider（爬虫）：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器)，\nItem Pipeline(管道)：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方.\nDownloader Middlewares（下载中间件）：你可以当作是一个可以自定义扩展下载功能的组件。\nSpider Middlewares（Spider中间件）：你可以理解为是一个可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests）\nScrapy的运作流程\n代码写好，程序开始运行...\n1 引擎：Hi！Spider, 你要处理哪一个网站？\n\n2 Spider：老大要我处理xxxx.com。\n\n3 引擎：你把第一个需要处理的URL给我吧。\n\n4 Spider：给你，第一个URL是xxxxxxx.com。\n\n5 引擎：Hi！调度器，我这有request请求你帮我排序入队一下。\n\n6 调度器：好的，正在处理你等一下。\n\n7 引擎：Hi！调度器，把你处理好的request请求给我。\n\n8 调度器：给你，这是我处理好的request\n\n9 引擎：Hi！下载器，你按照老大的下载中间件的设置帮我下载一下这个request请求\n\n10 下载器：好的！给你，这是下载好的东西。（如果失败：sorry，这个request下载失败了。然后引擎告诉调度器，这个request下载失败了，你记录一下，我们待会儿再下载）\n\n11 引擎：Hi！Spider，这是下载好的东西，并且已经按照老大的下载中间件处理过了，你自己处理一下（注意！这儿responses默认是交给def parse()这个函数处理的）\n\n12 Spider：（处理完毕数据之后对于需要跟进的URL），Hi！引擎，我这里有两个结果，这个是我需要跟进的URL，还有这个是我获取到的Item数据。\n\n13 引擎：Hi ！管道 我这儿有个item你帮我处理一下！调度器！这是需要跟进URL你帮我处理下。然后从第四步开始循环，直到获取完老大需要全部信息。\n\n14 管道``调度器：好的，现在就做！\n\n注意！只有当调度器中不存在任何request了，整个程序才会停止，（也就是说，对于下载失败的URL，Scrapy也会重新下载。）\n\n制作 Scrapy 爬虫 一共需要4步：\n\n新建项目 (scrapy startproject xxx)：新建一个新的爬虫项目\n明确目标 （编写items.py）：明确你想要抓取的目标\n制作爬虫 （spiders/xxspider.py）：制作爬虫开始爬取网页\n存储内容 （pipelines.py）：设计管道存储爬取内容\n\n\nScrapy的安装介绍\nScrapy框架官方网址：http://doc.scrapy.org/en/latest\nScrapy中文维护站点：http://scrapy-chs.readthedocs...\nWindows 安装方式\nPython 2 / 3升级pip版本：pip install --upgrade pip通过pip 安装 Scrapy 框架pip install Scrapy\nUbuntu 需要9.10或以上版本安装方式\nPython 2 / 3安装非Python的依赖 sudo apt-get install python-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev通过pip 安装 Scrapy 框架 sudo pip install scrapy安装后，只要在命令终端输入 scrapy，提示类似以下结果，代表已经安装成功\n具体Scrapy安装流程参考：http://doc.scrapy.org/en/late... 里面有各个平台的安装方法\n\n入门案例\n学习目标\n\n创建一个Scrapy项目\n定义提取的结构化数据(Item)\n编写爬取网站的 Spider 并提取出结构化数据(Item)\n编写 Item Pipelines 来存储提取到的Item(即结构化数据)\n\n一. 新建项目(scrapy startproject)\n在开始爬取之前，必须创建一个新的Scrapy项目。进入自定义的项目目录中，运行下列命令：\nscrapy startproject mySpider\n\n其中， mySpider 为项目名称，可以看到将会创建一个 mySpider 文件夹，目录结构大致如下：\n下面来简单介绍一下各个主要文件的作用：\nscrapy.cfg ：项目的配置文件\nmySpider/ ：项目的Python模块，将会从这里引用代码\nmySpider/items.py ：项目的目标文件\nmySpider/pipelines.py ：项目的管道文件\nmySpider/settings.py ：项目的设置文件\nmySpider/spiders/ ：存储爬虫代码目录\n二、明确目标(mySpider/items.py)\n我们打算抓取：http://www.itcast.cn/channel/... 网站里的所有讲师的姓名、职称和个人信息。\n\n打开mySpider目录下的items.py\nItem 定义结构化数据字段，用来保存爬取到的数据，有点像Python中的dict，但是提供了一些额外的保护减少错误。\n可以通过创建一个 scrapy.Item 类， 并且定义类型为   scrapy.Field的类属性来定义一个Item（可以理解成类似于ORM的映射关系）。\n\n接下来，创建一个ItcastItem 类，和构建item模型（model）。\nimport scrapy\nclass ItcastItem(scrapy.Item):\n   name = scrapy.Field()\n   level = scrapy.Field()\n   info = scrapy.Field()\n\n\n\n三、制作爬虫 （spiders/itcastSpider.py）\n爬虫功能要分两步：\n1. 爬数据\n在当前目录下输入命令，将在mySpider/spider目录下创建一个名为itcast的爬虫，并指定爬取域的范围：scrapy genspider itcast \"itcast.cn\"打开 mySpider/spider目录里的 itcast.py，默认增加了下列代码:\nimport scrapy\n\nclass ItcastSpider(scrapy.Spider):\n    name = \"itcast\"\n    allowed_domains = [\"itcast.cn\"]\n    start_urls = (\n        'http://www.itcast.cn/',\n    )\n\n    def parse(self, response):\n        pass\n\n其实也可以由我们自行创建itcast.py并编写上面的代码，只不过使用命令可以免去编写固定代码的麻烦\n要建立一个Spider， 你必须用scrapy.Spider类创建一个子类，并确定了三个强制的属性 和 一个方法。\nname = \"\" ：这个爬虫的识别名称，必须是唯一的，在不同的爬虫必须定义不同的名字。\nallow_domains = [] 是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页，不存在的URL会被忽略。\nstart_urls = () ：爬取的URL元祖/列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成。\nparse(self, response) ：解析的方法，每个初始URL完成下载后将被调用，调用的时候传入从每一个URL传回的Response对象来作为唯一参数，主要作用如下：\n负责解析返回的网页数据(response.body)，提取结构化数据(生成item)生成需要下一页的URL请求。将start_urls的值修改为需要爬取的第一个url\nstart_urls = (\"http://www.itcast.cn/channel/teacher.shtml\",)\n\n修改parse()方法\ndef parse(self, response):\n    filename = \"teacher.html\"\n    open(filename, 'w').write(response.body)\n\n然后运行一下看看，在mySpider目录下执行：\nscrapy crawl itcast\n\n是的，就是 itcast，看上面代码，它是 ItcastSpider 类的 name 属性，也就是使用 scrapy genspider命令的唯一爬虫名。\n运行之后，如果打印的日志出现 [scrapy] INFO: Spider closed (finished)，代表执行完成。 之后当前文件夹中就出现了一个 teacher.html 文件，里面就是我们刚刚要爬取的网页的全部源代码信息。\n**注意，Python2.x默认编码环境是ASCII，当和取回的数据编码格式不一致时，可能会造成乱码； 我们可以指定保存内容的编码格式，一般情况下，我们可以在代码最上方添加：**\nimport sys\nreload(sys)\nsys.setdefaultencoding(\"utf-8\")\n\n这三行代码是Python2.x里解决中文编码的万能钥匙，经过这么多年的吐槽后Python3学乖了，默认编码是Unicode了...(祝大家早日拥抱Python3)2. 取数据\n爬取整个网页完毕，接下来的就是的取过程了，首先观察页面源码：\n<div class=\"li_txt\">\n    <h3>  xxx  </h3>\n    <h4> xxxxx </h4>\n    <p> xxxxxxxx </p>\n\n是不是一目了然？直接上XPath开始提取数据吧。\n我们之前在mySpider/items.py 里定义了一个ItcastItem类。 这里引入进来\n  from mySpider.items import ItcastItem\n\n然后将我们得到的数据封装到一个 ItcastItem 对象中，可以保存每个老师的属性：\nfrom mySpider.items import ItcastItem\n\ndef parse(self, response):\n    #open(\"teacher.html\",\"wb\").write(response.body).close()\n\n    # 存放老师信息的集合\n    items = []\n\n    for each in response.xpath(\"//div[@class='li_txt']\"):\n        # 将我们得到的数据封装到一个 `ItcastItem` 对象\n        item = ItcastItem()\n        #extract()方法返回的都是unicode字符串\n        name = each.xpath(\"h3/text()\").extract()\n        title = each.xpath(\"h4/text()\").extract()\n        info = each.xpath(\"p/text()\").extract()\n\n        #xpath返回的是包含一个元素的列表\n        item['name'] = name[0]\n        item['title'] = title[0]\n        item['info'] = info[0]\n\n        items.append(item)\n\n    # 直接返回最后数据\n    return items\n我们暂时先不处理管道，后面会详细介绍。\n保存数据\n\nscrapy保存信息的最简单的方法主要有四种，-o 输出指定格式的文件，，命令如下：\njson格式，默认为Unicode编码\nscrapy crawl itcast -o teachers.json\n\njson lines格式，默认为Unicode编码\nscrapy crawl itcast -o teachers.jsonl\n\ncsv 逗号表达式，可用Excel打开\nscrapy crawl itcast -o teachers.csv\n\nxml格式\nscrapy crawl itcast -o teachers.xml\n\n思考\n如果将代码改成下面形式，结果完全一样。\n请思考 yield 在这里的作用：\nfrom mySpider.items import ItcastItem\n\ndef parse(self, response):\n    #open(\"teacher.html\",\"wb\").write(response.body).close()\n\n    # 存放老师信息的集合\n    #items = []\n\n    for each in response.xpath(\"//div[@class='li_txt']\"):\n        # 将我们得到的数据封装到一个 `ItcastItem` 对象\n        item = ItcastItem()\n        #extract()方法返回的都是unicode字符串\n        name = each.xpath(\"h3/text()\").extract()\n        title = each.xpath(\"h4/text()\").extract()\n        info = each.xpath(\"p/text()\").extract()\n\n        #xpath返回的是包含一个元素的列表\n        item['name'] = name[0]\n        item['title'] = title[0]\n        item['info'] = info[0]\n\n        #items.append(item)\n\n        #将获取的数据交给pipelines\n        yield item\n\n    # 返回数据，不经过pipeline\n    #return items\n思考过后 下面给出        Python中yield的解释\n\n                ", "mainLikeNum": ["5 "], "mainBookmarkNum": "4"}