{"title": "python爬虫爬取百度网盘-怎么做一个百度网盘搜索引擎 - python+java+c专栏 ", "index": "网页爬虫,python", "content": "因为要做去转盘网，所以一定要爬取网盘资源，本来想自己写一个爬虫挺不容易的，不想分享出来，但最后还是决定了拿给大家一起看吧，毕竟有交流才有进步，有兴趣的朋友也可以看看我写的其他日志或者关注我，会发现去转盘网,的大部分技术现在可以说是公开状态，如有对你有帮助还是认真读读吧，下面是爬虫代码，我立马公开：\nps：不会python的孩子先去学学python，代码是python写的\n我附上点资料：点我 或者点我\nTell:很高心该博客得到大量的网友好评，为了回馈大家，本人今天又公开了百度图片爬虫代码，下面是链接，喜欢的可以看看地址：http://5912119.blog.51cto.com...\n\n#####coding: utf8\n\n\"\"\"\n\nauthor:haoning\n\ncreate time: 2015-8-15\n\n\"\"\"\n\nimport re #####正则表达式模块\n\nimport urllib2 #####获取URLs的组件\n\nimport time\n\nfrom Queue import Queue\n\nimport threading, errno, datetime\n\nimport json\n\nimport requests #Requests is an Apache2 Licensed HTTP library\n\nimport MySQLdb as mdb\n\n \n\nDB_HOST = '127.0.0.1'\n\nDB_USER = 'root'\n\nDB_PASS = ''\n\n\n#####以下是正则匹配规则\n\nre_start = re.compile(r'start=(\\d+)') #\\d 表示0-9 任意一个数字 后面有+号 说明这个0-9单个数位出现一到多次 比如21312314\n\nre_uid = re.compile(r'query_uk=(\\d+)') #查询编号\n\nre_urlid = re.compile(r'&urlid=(\\d+)') #url编号\n\n \n\nONEPAGE = 20 #一页数据量\n\nONESHAREPAGE = 20 #一页分享连接量\n\n \n\n#####缺少专辑列表\n\nURL_SHARE = 'http://yun.baidu.com/pcloud/feed/getsharelist?auth_type=1&start={start}&limit=20&query_uk={uk}&urlid={id}' #获得分享列表\n\n\"\"\"\n\n{\"feed_type\":\"share\",\"category\":6,\"public\":\"1\",\"shareid\":\"1541924625\",\"data_id\":\"2418757107690953697\",\"title\":\"\\u5723\\u8bde\\u58c1\\u7eb8\\u5927\\u6d3e\\u9001\",\"third\":0,\"clienttype\":0,\"filecount\":1,\"uk\":1798788396,\"username\":\"SONYcity03\",\"feed_time\":1418986714000,\"desc\":\"\",\"avatar_url\":\"http:\\/\\/himg.bdimg.com\\/sys\\/portrait\\/item\\/1b6bf333.jpg\",\"dir_cnt\":1,\"filelist\":[{\"server_filename\":\"\\u5723\\u8bde\\u58c1\\u7eb8\\u5927\\u6d3e\\u9001\",\"category\":6,\"isdir\":1,\"size\":1024,\"fs_id\":870907642649299,\"path\":\"%2F%E5%9C%A3%E8%AF%9E%E5%A3%81%E7%BA%B8%E5%A4%A7%E6%B4%BE%E9%80%81\",\"md5\":\"0\",\"sign\":\"1221d7d56438970225926ad552423ff6a5d3dd33\",\"time_stamp\":1439542024}],\"source_uid\":\"871590683\",\"source_id\":\"1541924625\",\"shorturl\":\"1dDndV6T\",\"vCnt\":34296,\"dCnt\":7527,\"tCnt\":5056,\"like_status\":0,\"like_count\":60,\"comment_count\":19},\n\npublic:公开分享\n\ntitle:文件名称\n\nuk:用户编号\n\n\"\"\"\n\nURL_FOLLOW = 'http://yun.baidu.com/pcloud/friend/getfollowlist?query_uk={uk}&limit=20&start={start}&urlid={id}' #获得订阅列表\n\n\"\"\"\n\n{\"type\":-1,\"follow_uname\":\"\\u597d\\u55e8\\u597d\\u55e8\\u554a\",\"avatar_url\":\"http:\\/\\/himg.bdimg.com\\/sys\\/portrait\\/item\\/979b832f.jpg\",\"intro\":\"\\u9700\\u8981\\u597d\\u8d44\\u6599\\u52a0994798392\",\"user_type\":0,\"is_vip\":0,\"follow_count\":2,\"fans_count\":2276,\"follow_time\":1415614418,\"pubshare_count\":36,\"follow_uk\":2603342172,\"album_count\":0},\n\nfollow_uname:订阅名称\n\nfans_count：粉丝数\n\n\"\"\"\n\nURL_FANS = 'http://yun.baidu.com/pcloud/friend/getfanslist?query_uk={uk}&limit=20&start={start}&urlid={id}' # 获取关注列表\n\n\"\"\"\n\n{\"type\":-1,\"fans_uname\":\"\\u62e8\\u52a8\\u795e\\u7684\\u5fc3\\u7eea\",\"avatar_url\":\"http:\\/\\/himg.bdimg.com\\/sys\\/portrait\\/item\\/d5119a2b.jpg\",\"intro\":\"\",\"user_type\":0,\"is_vip\":0,\"follow_count\":8,\"fans_count\":39,\"follow_time\":1439541512,\"pubshare_count\":15,\"fans_uk\":288332613,\"album_count\":0}\n\navatar_url：头像\n\nfans_uname：用户名\n\n\"\"\"\n\n \n\nQNUM = 1000\n\nhc_q = Queue(20) #请求队列\n\nhc_r = Queue(QNUM) #接收队列\n\nsuccess = 0\n\nfailed = 0\n\n \n\ndef req_worker(inx): #请求\n\n    s = requests.Session() #请求对象\n\n    while True:\n\n        req_item = hc_q.get() #获得请求项\n\n        \n\n        req_type = req_item[0] #请求类型，分享?订阅？粉丝？\n\n        url = req_item[1] #url\n\n        r = s.get(url) #通过url获得数据\n\n        hc_r.put((r.text, url)) #将获得数据文本和url放入接收队列\n\n        print \"req_worker#\", inx, url #inx 线程编号； url 分析了的 url\n\n        \n\ndef response_worker(): #处理工作\n\n    dbconn = mdb.connect(DB_HOST, DB_USER, DB_PASS, 'baiduyun', charset='utf8')\n\n    dbcurr = dbconn.cursor()\n\n    dbcurr.execute('SET NAMES utf8')\n\n    dbcurr.execute('set global wait_timeout=60000') #以上皆是数据库操作\n\n    while True:\n\n        \"\"\"\n\n        #正则备注\n\n        match() 决定 RE 是否在字符串刚开始的位置匹配\n\n        search() 扫描字符串，找到这个 RE 匹配的位置\n\n        findall() 找到 RE 匹配的所有子串，并把它们作为一个列表返回\n\n        finditer() 找到 RE 匹配的所有子串，并把它们作为一个迭代器返回\n\n                  百度页面链接：http://pan.baidu.com/share/link?shareid=3685432306&uk=1798788396&from=hotrec\n\n        uk 其实用户id值\n\n        \"\"\"\n\n        metadata, effective_url = hc_r.get() #获得metadata（也就是前面的r.text）和有效的url\n\n        #print \"response_worker:\", effective_url\n\n        try:\n\n            tnow = int(time.time()) #获得当前时间\n\n            id = re_urlid.findall(effective_url)[0] #获得re_urlid用户编号\n\n            start = re_start.findall(effective_url)[0] #获得start用户编号\n\n            if True:\n\n                if 'getfollowlist' in effective_url: #type = 1，也就是订阅类\n\n                    follows = json.loads(metadata) #以将文本数据转化成json数据格式返回\n\n                    uid = re_uid.findall(effective_url)[0] #获得re_uid，查询编号\n\n                    if \"total_count\" in follows.keys() and follows[\"total_count\"]>0 and str(start) == \"0\": #获得订阅数量\n\n                        for i in range((follows[\"total_count\"]-1)/ONEPAGE): #开始一页一页获取有用信息\n\n                            try:\n\n                                dbcurr.execute('INSERT INTO urlids(uk, start, limited, type, status) VALUES(%s, %s, %s, 1, 0)' % (uid, str(ONEPAGE*(i+1)), str(ONEPAGE)))\n\n                                #存储url编号，订阅中有用户编号，start表示从多少条数据开始获取，初始status=0为未分析状态\n\n                            except Exception as ex:\n\n                                print \"E1\", str(ex)\n\n                                pass\n\n                    \n\n                    if \"follow_list\" in follows.keys(): #如果订阅者也订阅了，即拥有follow_list\n\n                        for item in follows[\"follow_list\"]:\n\n                            try:\n\n                                dbcurr.execute('INSERT INTO user(userid, username, files, status, downloaded, lastaccess) VALUES(%s, \"%s\", 0, 0, 0, %s)' % (item['follow_uk'], item['follow_uname'], str(tnow)))\n\n                                #存储订阅这的用户编号，用户名，入库时间\n\n                            except Exception as ex:\n\n                                print \"E13\", str(ex)\n\n                                pass\n\n                    else:\n\n                        print \"delete 1\", uid, start\n\n                        dbcurr.execute('delete from urlids where uk=%s and type=1 and start>%s' % (uid, start))\n\n                elif 'getfanslist' in effective_url: #type = 2,也就是粉丝列表\n\n                    fans = json.loads(metadata)\n\n                    uid = re_uid.findall(effective_url)[0]\n\n                    if \"total_count\" in fans.keys() and fans[\"total_count\"]>0 and str(start) == \"0\":\n\n                        for i in range((fans[\"total_count\"]-1)/ONEPAGE):\n\n                            try:\n\n                                dbcurr.execute('INSERT INTO urlids(uk, start, limited, type, status) VALUES(%s, %s, %s, 2, 0)' % (uid, str(ONEPAGE*(i+1)), str(ONEPAGE)))\n\n                            except Exception as ex:\n\n                                print \"E2\", str(ex)\n\n                                pass\n\n                    \n\n                    if \"fans_list\" in fans.keys():\n\n                        for item in fans[\"fans_list\"]:\n\n                            try:\n\n                                dbcurr.execute('INSERT INTO user(userid, username, files, status, downloaded, lastaccess) VALUES(%s, \"%s\", 0, 0, 0, %s)' % (item['fans_uk'], item['fans_uname'], str(tnow)))\n\n                            except Exception as ex:\n\n                                print \"E23\", str(ex)\n\n                                pass\n\n                    else:\n\n                        print \"delete 2\", uid, start\n\n                        dbcurr.execute('delete from urlids where uk=%s and type=2 and start>%s' % (uid, start))\n\n                else: #type=0，也即是分享列表\n\n                    shares = json.loads(metadata)\n\n                    uid = re_uid.findall(effective_url)[0]\n\n                    if \"total_count\" in shares.keys() and shares[\"total_count\"]>0 and str(start) == \"0\":\n\n                        for i in range((shares[\"total_count\"]-1)/ONESHAREPAGE):\n\n                            try:\n\n                                dbcurr.execute('INSERT INTO urlids(uk, start, limited, type, status) VALUES(%s, %s, %s, 0, 0)' % (uid, str(ONESHAREPAGE*(i+1)), str(ONESHAREPAGE)))\n\n                            except Exception as ex:\n\n                                print \"E3\", str(ex)\n\n                                pass\n\n                    if \"records\" in shares.keys():\n\n                        for item in shares[\"records\"]:\n\n                            try:\n\n                                dbcurr.execute('INSERT INTO share(userid, filename, shareid, status) VALUES(%s, \"%s\", %s, 0)' % (uid, item['title'], item['shareid'])) #item['title']恰好是文件名称\n\n                                #返回的json信息：\n\n                            except Exception as ex:\n\n                                #print \"E33\", str(ex), item\n\n                                pass\n\n                    else:\n\n                        print \"delete 0\", uid, start\n\n                        dbcurr.execute('delete from urlids where uk=%s and type=0 and start>%s' % (uid, str(start)))\n\n                dbcurr.execute('delete from urlids where id=%s' % (id, ))\n\n                dbconn.commit()\n\n        except Exception as ex:\n\n            print \"E5\", str(ex), id\n\n    dbcurr.close()\n\n    dbconn.close() #关闭数据库\n\n    \n\ndef worker():\n\n    global success, failed\n\n    dbconn = mdb.connect(DB_HOST, DB_USER, DB_PASS, 'baiduyun', charset='utf8')\n\n    dbcurr = dbconn.cursor()\n\n    dbcurr.execute('SET NAMES utf8')\n\n    dbcurr.execute('set global wait_timeout=60000')\n\n    #以上是数据库相关设置\n\n    while True:\n\n \n\n        #dbcurr.execute('select * from urlids where status=0 order by type limit 1')\n\n        dbcurr.execute('select * from urlids where status=0 and type>0 limit 1') #type>0,为非分享列表\n\n        d = dbcurr.fetchall()\n\n        #每次取出一条数据出来\n\n        #print d\n\n        if d: #如果数据存在\n\n            id = d[0][0] #请求url编号\n\n            uk = d[0][1] #用户编号\n\n            start = d[0][2]\n\n            limit = d[0][3]\n\n            type = d[0][4] #哪种类型\n\n            dbcurr.execute('update urlids set status=1 where id=%s' % (str(id),)) #状态更新为1，已经访问过了\n\n            url = \"\"\n\n            if type == 0: #分享\n\n                url = URL_SHARE.format(uk=uk, start=start, id=id).encode('utf-8') #分享列表格式化\n\n                #query_uk uk 查询编号\n\n                #start\n\n                #urlid id url编号\n\n            elif  type == 1: #订阅\n\n                url = URL_FOLLOW.format(uk=uk, start=start, id=id).encode('utf-8') #订阅列表格式化\n\n            elif type == 2: #粉丝\n\n                url = URL_FANS.format(uk=uk, start=start, id=id).encode('utf-8') #关注列表格式化\n\n            if url:\n\n                hc_q.put((type, url)) #如果url存在，则放入请求队列，type表示从哪里获得数据\n\n                #通过以上的url就可以获得相应情况下的数据的json数据格式，如分享信息的，订阅信息的，粉丝信息的\n\n                \n\n            #print \"processed\", url\n\n        else: #否则从订阅者或者粉丝的引出人中获得信息来存储，这个过程是爬虫树的下一层扩展\n\n            dbcurr.execute('select * from user where status=0 limit 1000')\n\n            d = dbcurr.fetchall()\n\n            if d:\n\n                for item in d:\n\n                    try:\n\n                        dbcurr.execute('insert into urlids(uk, start, limited, type, status) values(\"%s\", 0, %s, 0, 0)' % (item[1], str(ONESHAREPAGE)))\n\n                        #uk 查询号，其实是用户编号\n\n                        #start 从第1条数据出发获取信息\n\n                        #\n\n                        dbcurr.execute('insert into urlids(uk, start, limited, type, status) values(\"%s\", 0, %s, 1, 0)' % (item[1], str(ONEPAGE)))\n\n                        dbcurr.execute('insert into urlids(uk, start, limited, type, status) values(\"%s\", 0, %s, 2, 0)' % (item[1], str(ONEPAGE)))\n\n                        dbcurr.execute('update user set status=1 where userid=%s' % (item[1],)) #做个标志，该条数据已经访问过了\n\n                        #跟新了分享，订阅，粉丝三部分数据\n\n                    except Exception as ex:\n\n                        print \"E6\", str(ex)\n\n            else:\n\n                time.sleep(1)\n\n                \n\n        dbconn.commit()\n\n    dbcurr.close()\n\n    dbconn.close()\n\n        \n\ndef main():\n\n    print 'starting at:',now()\n\n    for item in range(16):    \n\n        t = threading.Thread(target = req_worker, args = (item,))\n\n        t.setDaemon(True)\n\n        t.start() #请求线程开启，共开启16个线程\n\n    s = threading.Thread(target = worker, args = ())\n\n    s.setDaemon(True)\n\n    s.start() #worker线程开启\n\n    response_worker()  #response_worker开始工作\n\n    print 'all Done at:', now()  \n 本人建个qq群，欢迎大家一起交流技术， 群号：512245829 喜欢微博的朋友关注：转盘娱乐即可\n\n                ", "mainLikeNum": ["5 "], "mainBookmarkNum": "51"}