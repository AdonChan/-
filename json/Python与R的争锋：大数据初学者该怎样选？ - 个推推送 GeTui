{"title": "Python与R的争锋：大数据初学者该怎样选？ - 个推推送 GeTui ", "index": "r语言,python", "content": "\n作者：个推大数据工程师李树桓\n在当下，人工智能的浪潮席卷而来。从AlphaGo、无人驾驶技术、人脸识别、语音对话，到商城推荐系统，金融业的风控，量化运营、用户洞察、企业征信、智能投顾等，人工智能的应用广泛渗透到各行各业，也让数据科学家们供不应求。Python和R作为机器学习的主流语言，受到了越来越多的关注。数据学习领域的新兵们经常不清楚如何在二者之间做出抉择，本文就语言特性与使用场景为大家对比剖析。\n一．Python和R的概念与特性\nPython是一种面向对象、解释型免费开源高级语言。它功能强大，有活跃的社区支持和各式各样的类库，同时具备简洁、易读以及可扩展等优点，在近几年成为高人气的编程语言。\nPython的优势：\n1、Python的使用场景非常多，不仅和R一样可以用于统计分析，更广泛应用于系统编程、图形处理、文本处理、数据库编程、网络编程、Web编程、网络爬虫等，非常适合那些想深入钻研数据分析或者应用统计技术的程序员。\n2、目前主流的大数据和机器学习框架对Python都提供了很好的支持，比如Hadoop、Spark、Tensorflow；同时，Python也有着强大的社区支持，特别是近年来随着人工智能的兴起，越来越多的开发者活跃在Python的社区中。\n3、Python作为一种胶水语言，能够和其他语言连结在一起，比如你的统计分析部分可以用R语言写，然后封装为Python可以调用的扩展类库。\nR语言是一种用来进行数据探索、统计分析和作图的解释型语言，但更像一种数学计算的环境。它模块丰富，为数学计算提供了极为方便的编程方式，特别是针对矩阵的计算。\nR语言的优势：\n1、R语言拥有许多优雅直观的图表，常见的数据可视化的工具包有：\n· 交互式图表rCharts、Plotly，交互时序图dygraphs，交互树状图TreeMap\n· ggplot2-一个基于图形语法的绘图系统\n· lattice-R语言格子图形\n· rbokeh-针对Bokeh的R语言接口\n· RGL-使用了OpenGL的3D可视化\n· Shiny-用于创建交互式应用和可视化的框架\n· visNetwork-交互式网络可视化散点图时序图词云图\n2、拥有大量专门面向统计人员的实用功能和丰富的数学工具包。自带base一R的基础模块、mle一极大似然估计模块、ts一时间序列分析模块、mva一多元统计分析模块、survival一生存分析模块等，同时用户可以灵活使用数组和矩阵的操作运算符，及一系列连贯而又完整的数据分析中间工具。\n3、语言简洁上手快，不需要明确定义变量类型。比如下面简简单单三行代码，就能定义一元线性回归，是不是很酷炫：\nx <- 1:10y <- x+rnorm(10, 0, 1)fit <- lm(y ~ x)\n同时，R语言对向量化的支持程度高，通过向量化运算，数据在计算过程中前后不依赖，是一种高度并行计算的实现，也避免了许多循环结构的使用。\n当然了，相比于Python它也存在着一些劣势。比如内存管理问题，在大样本的回归中，如使用不当就会出现内存不足的情况，但目前spark也提供了对R的支持，开发者可以使用sparkR进行大数据的计算处理。\n二.Python和R在文本信息挖掘和时序分析方面的区别\nPython和R都有非常强大的代码库，Python有PyPi，R有CRAN。但两者方向不同，Python使用的范围更加广泛，涉及到方方面面；R更专注统计方面，但在数据量大时运行速度很慢。下面我针对数据分析中的两种使用场景来比较Python和R：\n1. 文本信息挖掘：\n文本信息挖掘的应用非常广泛，例如根据网购评价、社交网站的推文或者新闻进行情感极性分析等。这里我们用例子分析比较一下。\nPython有良好的程序包帮助我们进行分析。比如NLTK，以及专门针对中文的SnowNLP，包含了中文分词、词性标注、情感分析，文本分类、TextRank、TF-IDF等模块。\n在用Python做情感极性分析时，首先需要将句子分解为单词，这里我们可以使用Python中jieba分词，使用起来也非常简单：\nword=jieba.cut(m,cut_all=False)\n然后操作特征提取，可以利用NLTK中的stopwords先去除停用词。如果有需要，可以对文本进行向量化处理，这里我们可以采用Bag of Words，选择TF-IDF进行基于权重的向量转化，也可以使用Word2Vec进行基于相似度的转化。接下来，使用sklearn包中的pca进行降维：\npca=PCA(n_components=1)\nnewData=pca.fit_transform(data)\n除了pca，还可以选择使用互信息或者信息熵等其他方法。\n之后，我们进行分类算法模型训练和模型评估，可以使用朴素贝叶斯（NaiveBayes），决策树（Decision Tree）等NLTK 自带的机器学习方法。\n使用R进行情感极性分析\n首先需要对数据进行预处理，安装Rwordseg/rJava（其中有不少坑）两个包；\n进行数据清理清除掉没用的符号后，进行分词：Rwordseg中的segmentCN方法可以对中文进行分词。当然，也可以使用jiebaR；\n接下来构建单词-文档-标签数据集，去除停用词；\n创建文档-词项矩阵，可以选择TermDocumentMatrix，使用weightTfIdf方法得到tf-idf矩阵；\n最后用e1071包中的贝叶斯方法进行文本分类，或者可以用RTextTools包中的其他机器学习算法来完成分类，其中包含九种算法：BAGGING(ipred:bagging)：bagging集成分类\nBOOSTING (caTools:LogitBoost)：Logit Boosting 集成分类\nGLMNET(glmnet:glmnet)：基于最大似然的广义线性回归\nMAXENT(maxent:maxent)：最大熵模型\nNNET(nnet:nnet) ：神经网络\nRF(randomForest:randomForest)：随机森林\nSLDA(ipred:slda)：scaled 线性判别分析\nSVM(e1071:svm) ：支持向量机\nTREE (tree:tree)：递归分类树\n2.时序分析：\n时间序列分析是根据系统观察得到的时间序列数据，通过曲线拟合和参数估计来建立数学模型的理论和方法，通常用于金融领域、气象预测、市场分析领域等。R语言拥有许多程序包可用于处理规则和不规则时间序列，因而更有优势。\nPython进行时序分析的时常用ARIMA(p,d,q)模型，其中d指的是差分项，p和q分别代表自回归项和移动平均项。构建ARIMA模型使用最多的就是statsmodels模块，该模块可以用来进行时间序列的差分，建模和模型的检验。这里例举一个周期性预测的例子：\n下面是一组数据，代表美国某公交公司发布的五十年中每年的乘客相关数据（比如1950-2000）：\ndata = [9930, 9318, 9595, 9972, 6706, 5756, 8092, 9551, 8722, 9913, 10151, 7186, 5422, 5337, 10649, 10652, 9310, 11043, 6937, 5476, 8662, 8570, 8981, 8331, 8449, 5773, 5304, 8355, 9477, 9148, 9395, 10261, 7713, 6299, 9424,9795, 10069, 10602, 10427, 8095, 6707, 9767, 11136, 11812, 11006, 11528, 9329, 6818, 10719, 10683]\n1).首先，使用pandas进行处理和存储数据：\ndata=pd.Series(data)\n2).然后需要对数据进行平稳性检验，一般利用单位根检验，常用的方法有ADF、DFGLS、PP等等：\nPython中直接用ADF(data), DFGLS(data)就可以得出pvalue的结果\n3).序列平稳性是进行时间序列分析的前提条件,如果上一个步骤显示结果不平稳，就需要对时间序列做平稳性处理，一般用差分法最多：\ndiff1 = data.diff(2)\n其中diff（object）表示差分的阶数，这里我们使用2阶，当然你也可以用1阶、3阶、4阶等等\n4).进行白噪声检验：\nvalue=acorr_ljungbox(data,lags=1)\n5).现在，我们的ARIMA(p,d,q)中的d=2，接下来我们进行模型选择。第一步是计算出p和q，首先检查平稳时间序列的自相关图和偏自相关图，通过sm.graphics.tsa.plot_acf (data)和sm.graphics.tsa.plot_pacf(data)，然后通过系数情况进行模型选择，可供选择的有AR,MA,ARMA,ARIMA。\n6).模型训练：model=sm.tsa.ARMA(data,(p,d,q)).fit()，此处用ARMA模型计算出p和q，从而训练出模型。\n用R来构建时间序列模型\nR针对时间序列有各式各样的工具包，比如：\nlibrary(xts)，library(timeSeires)，library(zoo)—时间基础包\nlibrary(urca)--进行单位根检验\nlibrary(tseries)--arma模型\nlibrary(fUnitRoots)--进行单位根检验\nlibrary(FinTS)--调用其中的自回归检验函数\nlibrary(fGarch)--GARCH模型\nlibrary(nlme)--调用其中的gls函数\nlibrary(fArma)--进行拟合和检验\nlibrary(forecast)—arima建模\n下面我介绍一下R语言中forecast工具包里面两个很强大的工具：ets和auto.arima。用户什么都不需要做，这两个函数会自动挑选一个最恰当的算法去分析数据。比如用ets来处理：\nfit<-ets(train)\naccuracy(predict(fit,12),test)\n或者用auto.arima处理：\nfit<-auto.arima(train)\naccuracy(forecast(fit,h=12),test)\n除此之外，forecast包中有针对增长或者降低趋势并且存在季节性波动的时间序列算法Holt-Winters。Holt-Winters的思想是把数据分解成三个成分：平均水平（level），趋势（trend），周期性（seasonality）。R里面一个简单的函数stl就可以把原始数据进行分解。\n本文主要从各自优势及具体例子中分析了Python与R两种编程语言。不难看出，二者在“综合实力”上难分伯仲，具体选择哪一种深入学习，依然需要考虑自己实际期望解决的问题、应用的领域等等方面。最后欢迎大家就大数据编程语言相关问题与我沟通交流~\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}