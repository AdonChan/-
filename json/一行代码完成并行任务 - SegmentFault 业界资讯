{"title": "一行代码完成并行任务 - SegmentFault 业界资讯 ", "index": "python,并行", "content": "众所周知，Python的并行处理能力很不理想。我认为如果不考虑线程和GIL的标准参数（它们大多是合法的），其原因不是因为技术不到位，而是我们的使用方法不恰当。大多数关于Python线程和多进程的教材虽然都很出色，但是内容繁琐冗长。它们的确在开篇铺陈了许多有用信息，但往往都不会涉及真正能提高日常工作的部分。\n\n经典例子\n\nDDG上以“Python threading tutorial （Python线程教程）”为关键字的热门搜索结果表明：几乎每篇文章中给出的例子都是相同的类+队列。\n\n事实上，它们就是以下这段使用producer/Consumer来处理线程/多进程的代码示例：\n\n#Example.py\n'''\nStandard Producer/Consumer Threading Pattern\n'''\n\nimport time \nimport threading \nimport Queue \n\nclass Consumer(threading.Thread): \n    def __init__(self, queue): \n        threading.Thread.__init__(self)\n        self._queue = queue \n\n    def run(self):\n        while True: \n            # queue.get() blocks the current thread until \n            # an item is retrieved. \n            msg = self._queue.get() \n            # Checks if the current message is \n            # the \"Poison Pill\"\n            if isinstance(msg, str) and msg == 'quit':\n                # if so, exists the loop\n                break\n            # \"Processes\" (or in our case, prints) the queue item   \n            print \"I'm a thread, and I received %s!!\" % msg\n        # Always be friendly! \n        print 'Bye byes!'\n\ndef Producer():\n    # Queue is used to share items between\n    # the threads.\n    queue = Queue.Queue()\n\n    # Create an instance of the worker\n    worker = Consumer(queue)\n    # start calls the internal run() method to \n    # kick off the thread\n    worker.start() \n\n    # variable to keep track of when we started\n    start_time = time.time() \n    # While under 5 seconds.. \n    while time.time() - start_time < 5: \n        # \"Produce\" a piece of work and stick it in \n        # the queue for the Consumer to process\n        queue.put('something at %s' % time.time())\n        # Sleep a bit just to avoid an absurd number of messages\n        time.sleep(1)\n\n    # This the \"poison pill\" method of killing a thread. \n    queue.put('quit')\n    # wait for the thread to close down\n    worker.join()\n\nif __name__ == '__main__':\n    Producer()\n\n\n唔…….感觉有点像Java。\n\n我现在并不想说明使用Producer / Consume来解决线程/多进程的方法是错误的——因为它肯定正确，而且在很多情况下它是最佳方法。但我不认为这是平时写代码的最佳选择。\n\n它的问题所在（个人观点）\n\n首先，你需要创建一个样板式的铺垫类。然后，你再创建一个队列，通过其传递对象和监管队列的两端来完成任务。（如果你想实现数据的交换或存储，通常还涉及另一个队列的参与）。\n\nWorker越多，问题越多。\n\n接下来，你应该会创建一个worker类的pool来提高Python的速度。下面是IBM tutorial给出的较好的方法。这也是程序员们在利用多线程检索web页面时的常用方法。\n\n#Example2.py\n'''\nA more realistic thread pool example \n'''\n\nimport time \nimport threading \nimport Queue \nimport urllib2 \n\nclass Consumer(threading.Thread): \n    def __init__(self, queue): \n        threading.Thread.__init__(self)\n        self._queue = queue \n\n    def run(self):\n        while True: \n            content = self._queue.get() \n            if isinstance(content, str) and content == 'quit':\n                break\n            response = urllib2.urlopen(content)\n        print 'Bye byes!'\n\ndef Producer():\n    urls = [\n        'http://www.python.org', 'http://www.yahoo.com'\n        'http://www.scala.org', 'http://www.google.com'\n        # etc.. \n    ]\n    queue = Queue.Queue()\n    worker_threads = build_worker_pool(queue, 4)\n    start_time = time.time()\n\n    # Add the urls to process\n    for url in urls: \n        queue.put(url)  \n    # Add the poison pillv\n    for worker in worker_threads:\n        queue.put('quit')\n    for worker in worker_threads:\n        worker.join()\n\n    print 'Done! Time taken: {}'.format(time.time() - start_time)\n\ndef build_worker_pool(queue, size):\n    workers = []\n    for _ in range(size):\n        worker = Consumer(queue)\n        worker.start() \n        workers.append(worker)\n    return workers\n\nif __name__ == '__main__':\n    Producer()\n\n\n它的确能运行，但是这些代码多么复杂阿！它包括了初始化方法、线程跟踪列表以及和我一样容易在死锁问题上出错的人的噩梦——大量的join语句。而这些还仅仅只是繁琐的开始！\n\n我们目前为止都完成了什么？基本上什么都没有。上面的代码几乎一直都只是在进行传递。这是很基础的方法，很容易出错（该死，我刚才忘了在队列对象上还需要调用task_done()方法（但是我懒得修改了）），性价比很低。还好，我们还有更好的方法。\n\n介绍：Map\n\nMap是一个很棒的小功能，同时它也是Python并行代码快速运行的关键。给不熟悉的人讲解一下吧，map是从函数语言Lisp来的。map函数能够按序映射出另一个函数。例如\n\nurls = ['http://www.yahoo.com', 'http://www.reddit.com']\nresults = map(urllib2.urlopen, urls)\n\n\n这里调用urlopen方法来把调用结果全部按序返回并存储到一个列表里。就像：\n\nresults = []\nfor url in urls: \n    results.append(urllib2.urlopen(url))\n\n\nMap按序处理这些迭代。调用这个函数，它就会返回给我们一个按序存储着结果的简易列表。\n\n为什么它这么厉害呢？因为只要有了合适的库，map能使并行运行得十分流畅！\n\n\n\n有两个能够支持通过map函数来完成并行的库：一个是multiprocessing，另一个是鲜为人知但功能强大的子文件：multiprocessing.dummy。\n\n题外话：这个是什么？你从来没听说过dummy多进程库？我也是最近才知道的。它在多进程的说明文档里面仅仅只被提到了一句。而且那一句就是大概让你知道有这么个东西的存在。我敢说，这样几近抛售的做法造成的后果是不堪设想的！\n\nDummy就是多进程模块的克隆文件。唯一不同的是，多进程模块使用的是进程，而dummy则使用线程（当然，它有所有Python常见的限制）。也就是说，数据由一个传递给另一个。这能够使得数据轻松的在这两个之间进行前进和回跃，特别是对于探索性程序来说十分有用，因为你不用确定框架调用到底是IO 还是CPU模式。\n\n准备开始\n\n要做到通过map函数来完成并行，你应该先导入装有它们的模块：\n\nfrom multiprocessing import Pool\nfrom multiprocessing.dummy import Pool as ThreadPool\n\n\n再初始化:\n\npool = ThreadPool()\n\n\n这简单的一句就能代替我们的build_worker_pool 函数在example2.py中的所有工作。换句话说，它创建了许多有效的worker，启动它们来为接下来的工作做准备，以及把它们存储在不同的位置，方便使用。\n\nPool对象需要一些参数，但最重要的是：进程。它决定pool中的worker数量。如果你不填的话，它就会默认为你电脑的内核数值。\n\n如果你在CPU模式下使用多进程pool，通常内核数越大速度就越快（还有很多其它因素）。但是，当进行线程或者处理网络绑定之类的工作时，情况会比较复杂所以应该使用pool的准确大小。\n\npool = ThreadPool(4) # Sets the pool size to 4\n\n\n如果你运行过多线程，多线程间的切换将会浪费许多时间，所以你最好耐心调试出最适合的任务数。\n\n我们现在已经创建了pool对象，马上就能有简单的并行程序了，所以让我们重新写example2.py中的url opener吧！\n\nimport urllib2 \nfrom multiprocessing.dummy import Pool as ThreadPool \n\nurls = [\n    'http://www.python.org', \n    'http://www.python.org/about/',\n    'http://www.onlamp.com/pub/a/python/2003/04/17/metaclasses.html',\n    'http://www.python.org/doc/',\n    'http://www.python.org/download/',\n    'http://www.python.org/getit/',\n    'http://www.python.org/community/',\n    'https://wiki.python.org/moin/',\n    'http://planet.python.org/',\n    'https://wiki.python.org/moin/LocalUserGroups',\n    'http://www.python.org/psf/',\n    'http://docs.python.org/devguide/',\n    'http://www.python.org/community/awards/'\n    # etc.. \n    ]\n\n# Make the Pool of workers\npool = ThreadPool(4) \n# Open the urls in their own threads\n# and return the results\nresults = pool.map(urllib2.urlopen, urls)\n#close the pool and wait for the work to finish \npool.close() \npool.join()\n\n\n看吧！这次的代码仅用了4行就完成了所有的工作。其中3句还是简单的固定写法。调用map就能完成我们前面例子中40行的内容！为了更形象地表明两种方法的差异，我还分别给它们运行的时间计时。\n\n# results = [] \n# for url in urls:\n#   result = urllib2.urlopen(url)\n#   results.append(result)\n\n# # ------- VERSUS ------- # \n\n# # ------- 4 Pool ------- # \n# pool = ThreadPool(4) \n# results = pool.map(urllib2.urlopen, urls)\n\n# # ------- 8 Pool ------- # \n\n# pool = ThreadPool(8) \n# results = pool.map(urllib2.urlopen, urls)\n\n# # ------- 13 Pool ------- # \n\n# pool = ThreadPool(13) \n# results = pool.map(urllib2.urlopen, urls)\n\n\n结果：\n\n#                       Single thread:  14.4 Seconds \n#                              4 Pool:   3.1 Seconds\n#                              8 Pool:   1.4 Seconds\n#                             13 Pool:   1.3 Seconds\n\n\n相当出色！并且也表明了为什么要细心调试pool的大小。在这里，只要大于9，就能使其运行速度加快。\n\n实例2：\n\n生成成千上万的缩略图\n\n我们在CPU模式下来完成吧！我工作中就经常需要处理大量的图像文件夹。其任务之一就是创建缩略图。这在并行任务中已经有很成熟的方法了。\n\n基础的单线程创建\n\nimport os \nimport PIL \n\nfrom multiprocessing import Pool \nfrom PIL import Image\n\nSIZE = (75,75)\nSAVE_DIRECTORY = 'thumbs'\n\ndef get_image_paths(folder):\n    return (os.path.join(folder, f) \n            for f in os.listdir(folder) \n            if 'jpeg' in f)\n\ndef create_thumbnail(filename): \n    im = Image.open(filename)\n    im.thumbnail(SIZE, Image.ANTIALIAS)\n    base, fname = os.path.split(filename) \n    save_path = os.path.join(base, SAVE_DIRECTORY, fname)\n    im.save(save_path)\n\nif __name__ == '__main__':\n    folder = os.path.abspath(\n        '11_18_2013_R000_IQM_Big_Sur_Mon__e10d1958e7b766c3e840')\n    os.mkdir(os.path.join(folder, SAVE_DIRECTORY))\n\n    images = get_image_paths(folder)\n\n    for image in images:\n             create_thumbnail(Image)\n\n\n对于一个例子来说，这是有点难，但本质上，这就是向程序传递一个文件夹，然后将其中的所有图片抓取出来，并最终在它们各自的目录下创建和储存缩略图。\n\n我的电脑处理大约6000张图片用了27.9秒。\n\n如果我们用并行调用map来代替for循环的话：\n\nimport os \nimport PIL \n\nfrom multiprocessing import Pool \nfrom PIL import Image\n\nSIZE = (75,75)\nSAVE_DIRECTORY = 'thumbs'\n\ndef get_image_paths(folder):\n    return (os.path.join(folder, f) \n            for f in os.listdir(folder) \n            if 'jpeg' in f)\n\ndef create_thumbnail(filename): \n    im = Image.open(filename)\n    im.thumbnail(SIZE, Image.ANTIALIAS)\n    base, fname = os.path.split(filename) \n    save_path = os.path.join(base, SAVE_DIRECTORY, fname)\n    im.save(save_path)\n\nif __name__ == '__main__':\n    folder = os.path.abspath(\n        '11_18_2013_R000_IQM_Big_Sur_Mon__e10d1958e7b766c3e840')\n    os.mkdir(os.path.join(folder, SAVE_DIRECTORY))\n\n    images = get_image_paths(folder)\n\n    pool = Pool()\n        pool.map(create_thumbnail,images)\n        pool.close()\n        pool.join()\n\n\n5.6秒！\n\n对于只改变了几行代码而言，这是大大地提升了运行速度。这个方法还能更快，只要你将cpu 和 io的任务分别用它们的进程和线程来运行——但也常造成死锁。总之，综合考虑到 map这个实用的功能，以及人为线程管理的缺失，我觉得这是一个美观，可靠还容易debug的方法。\n\n好了，文章结束了。一行完成并行任务。\n\n\n原文：Parallelism in one line\n转载自：伯乐在线 - colleen__chen\n\n                ", "mainLikeNum": ["3 "], "mainBookmarkNum": "9"}