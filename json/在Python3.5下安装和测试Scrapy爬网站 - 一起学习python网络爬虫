{"title": "在Python3.5下安装和测试Scrapy爬网站 - 一起学习python网络爬虫 ", "index": "网络爬虫,编程语言,python,scrapy,数据采集", "content": "\n1. 引言\nScrapy框架结构清晰，基于twisted的异步架构可以充分利用计算机资源，是爬虫做大的必备基础。本文将讲解如何快速安装此框架并使用起来。\n2. 安装Twisted\n2.1 同安装Lxml库\n(参考《为编写网络爬虫程序安装Python3.5》3.1节)一样，通过下载对应版本的.whl文件先安装twisted库，下载地址: http://www.lfd.uci.edu/~gohlk...\n\n2.2 安装twisted\n打开命令提示符窗口，输入命令：\npip install E:\\demo\\Twisted-16.4.1-cp35-cp35m-win_amd64.whl(下载好的twisted模块的whl文件路径)\n\n\n3. 安装scrapy\ntwisted库安装成功后，安装scrapy就简单了，在命令提示符窗口直接输入命令： pip install scrapy 回车\n\n安装关联模块pypiwin32，在命令提示符窗口直接输入命令： pip install pypiwin32 回车\n\n4. Scrapy测试，敲一个基于Scrapy框架的爬虫程序\n新建一个Scrapy爬虫项目fourth（因为这是继Python3.5安装的第四篇教程，有兴趣的话请从头看起）：在任意目录按住shift+右键->选择在此处打开命令提示符窗口（这里默认为E:demo），然后输入命令：\nE:\\demo>scrapy startproject fourth\n\n\n该命令将会创建包含下列内容的fourth目录:\nfourth/\n    scrapy.cfg\n    fourth/\n            __init__.py\n            items.py\n            pipelines.py\n            settings.py\n            spiders/\n                __init__.py\n                ...\n\n修改项目配置文件settings.py，有些网站会在根目录下放置一个名字为robots.txt的文件，里面声明了此网站希望爬虫遵守的规范，Scrapy默认遵守这个文件制定的规范，即ROBOTSTXT_OBEY默认值为True。在这里需要修改ROBOTSTXT_OBEY的值，找到项目目录（这里为：E:demofourthfourth）下文件settings.py，更改ROBOTSTXT_OBEY的值为False\n引入Gooseeker最新规则提取器模块gooseeker.py(下载地址: https://github.com/FullerHua/gooseeker/tree/master/core)，拷贝到项目目录下，这里为E:demofourthgooseeker.py\n创建爬虫模块，进入项目目录E:demofourth下，在此处打开命提示符窗口输入命令：\nE:\\demo\\fourth>scrapy genspider anjuke 'anjuke.com'\n\n\n该命令将会在项目目录E:demofourthfourthspiders下创建模块文件anjuke.py，以记事本打开然后添加代码，主要代码：\n# -*- coding: utf-8 -*-\n# Scrapy spider 模块\n# 采集安居客房源信息\n# 采集结果保存在anjuke-result.xml中\nimport os\nimport time\nimport scrapy\nfrom gooseeker import GsExtractor\n\nclass AnjukeSpider(scrapy.Spider):\n        name = \"anjuke\"\n        allowed_domains = [\"'anjuke.com'\"]\n        start_urls = (\n        'http://bj.zu.anjuke.com/fangyuan/p1',\n        )\n\n        def parse(self, response):\n        print(\"----------------------------------------------------------------------------\")\n            # 引用提取器\n        bbsExtra = GsExtractor()\n            # 设置xslt抓取规则\n            bbsExtra.setXsltFromAPI(\"31d24931e043e2d5364d03b8ff9cc77e\", \"安居客_房源\")\n            # 调用extract方法提取所需内容\n        result = bbsExtra.extractHTML(response.body)\n        \n            # 打印采集结果\n            print(str(result).encode('gbk','ignore').decode('gbk'))\n            # 保存采集结果\n            file_path = os.getcwd() + \"/anjuke-result.xml\"\n            open(file_path,\"wb\").write(result)\n        # 打印结果存放路径\n            print(\"采集结果文件：\" + file_path)\n\n启动爬虫，进入项目目录E:demofourth下，在此处打开命提示符窗口输入命令：\nE:\\demo\\fourth>scrapy crawl anjuke\n\n\n注：网站若发现抓取时报重定向错误了，尝试修改user-agent后，再启动爬虫爬取数据。操作步骤如下：1、在爬虫项目目录（这里为E:demofourthfourth）下创建模块文件middlewares.py，以记事本打开后，添加如下代码：\n#-*-coding:utf-8-*-\n# 随机更换user agent\nimport random\nfrom scrapy.downloadermiddlewares.useragent import UserAgentMiddleware\n\nclass RotateUserAgentMiddleware(UserAgentMiddleware):\n    def __init__(self, user_agent=''):\n        self.user_agent = user_agent\n\n    def process_request(self, request, spider):\n        ua = random.choice(self.user_agent_list)\n        if ua:\n            request.headers.setdefault('User-Agent', ua)\n\n    user_agent_list = [\\\n        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1\"\\\n        \"Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11\",\\\n        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6\",\\\n        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6\",\\\n        \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1\",\\\n        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5\",\\\n        \"Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5\",\\\n        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\\\n        \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\\\n        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_0) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3\",\\\n        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\",\\\n        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3\",\\\n        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\\\n        \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\\\n        \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3\",\\\n        \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3\",\\\n        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\",\\\n        \"Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24\"\n]\n2、修改项目配置文件settings.py，加上如下代码：\nDOWNLOADER_MIDDLEWARES = {  \n            'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware':None,  \n            'fourth.middlewares.RotateUserAgentMiddleware':400,  \n}\n\n查看保存结果文件,进入Scrapy爬虫项目目录，这里为E:demofourth，找到名称为anjuke-result.xml的文件夹然后打开\n\n5. 总结\n安装pypiwin32时碰到了一次超时断开，再次输入命令重新安装才成功，若重复安装都失败可以尝试连接vpn再安装。下一篇《Python爬虫实战:单页采集》将讲解如何爬取微博数据(单页)，同时整合Python爬虫程序以Gooseeker规则提取器为接口制作一个通用的采集器，欢迎有兴趣的小伙伴一起交流进步。\n6. 集搜客GooSeeker开源代码下载源\nGooSeeker开源Python网络爬虫GitHub源\n7. 修改记录\n2017.03.02 补充报重定向错误解决方案\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "7"}