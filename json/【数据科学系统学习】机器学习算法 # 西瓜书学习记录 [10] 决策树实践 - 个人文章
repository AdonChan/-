{"title": "【数据科学系统学习】机器学习算法 # 西瓜书学习记录 [10] 决策树实践 - 个人文章 ", "index": "python,机器学习", "content": "本篇内容为《机器学习实战》第 3 章决策树部分程序清单。所用代码为 python3。\n\n决策树优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。缺点：可能会产生过度匹配问题。适用数据类型：数值型和标称型\n在构造决策树时，我们需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征。完成测试之后，原始数据集就被划分为几个数据子集。这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，则无需进一步对数据集进行分割。如果数据子集内的数据不属于同一类型，则需要重复划分数据子集的过程。划分数据子集的算法和划分原始数据集的方法相同，直到所有具有相同类型的数据均在一个数据子集内。\n创建分支的伪代码函数createBranch()如下所示：\n检测数据集中的每个子项是否属于同一分类：\n    If so return 类标签\n    Else\n        寻找划分数据集的最好特征\n        划分数据集\n        创建分支节点\n            for 每个划分的子集\n                调整函数createBranch()并增加返回结果到分支节点中\n        return 分支节点\n\n下面我们采用量化的方法来判定如何划分数据，我们以下图所示的数据集为例：\n\n程序清单 3-1 计算给定数据集的香农熵\n'''\nCreated on Sep 16, 2018\n\n@author: yufei\n'''\n\n# coding=utf-8\n\n\"\"\"\n计算给定数据的香农熵\n\"\"\"\nfrom math import log\ndef calcShannonEnt(dataSet):\n    numEntries = len(dataSet)\n    labelCounts = {}\n\n    # 为所有可能的分类创建字典\n    for featVec in dataSet:\n        currentLabel = featVec[-1]\n        if currentLabel not in labelCounts.keys():\n            labelCounts[currentLabel] = 0\n        labelCounts[currentLabel] += 1\n    shannonEnt = 0.0\n\n    # 以 2 为底求对数\n    for key in labelCounts:\n        prob = float(labelCounts[key])/numEntries\n        shannonEnt -= prob * log(prob, 2)\n    return shannonEnt\n\n\"\"\"\n得到数据集\n\"\"\"\ndef createDataSet():\n    dataSet = [[1, 1, 'yes'],\n               [1, 1, 'yes'],\n               [1, 0, 'no'],\n               [0, 1, 'no'],\n               [0, 1, 'no'],]\n    labels = ['no surfacing', 'flippers']\n    return dataSet, labels\n在 python 提示符下，执行代码并得到结果：\n>>> import trees\n>>> myDat, labels = trees.createDataSet()\n>>> myDat\n[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n>>> trees.calcShannonEnt(myDat)\n0.9709505944546686\n\n程序清单 3-2 按照给定特征划分数据集\n# 参数：待划分的数据集、划分数据集的特征、需要返回的特征的值\ndef splitDataSet(dataSet, axis, value):\n    # 为了不修改原始数据集，创建一个新的列表对象\n    retDataSet = []\n    for featVec in dataSet:\n        # 将符合特征的数据抽取出来\n        # 当我们按照某个特征划分数据集时，就需要将所有符合要求的元素抽取出来\n        if featVec[axis] == value:\n            reducedFeatVec = featVec[:axis]\n            reducedFeatVec.extend(featVec[axis+1:])\n            retDataSet.append(reducedFeatVec)\n    return retDataSet\n测试函数splitDataSet()，在 python 提示符下，执行代码并得到结果：\n>>> myDat, labels = trees.createDataSet()\n>>> myDat\n[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n>>> trees.splitDataSet(myDat, 0, 0)\n[[1, 'no'], [1, 'no']]\n\n程序清单 3-3 选择最好的数据集划分方式\n\"\"\"\n函数功能：选择特征，划分数据集，计算得出最好的划分数据集的特征\n\n数据集需满足：\n1、数据是一种由列表元素组成的列表，且所有的列表元素都要具有相同的数据长度\n2、数据的最后一列或每个实例的最后一个元素是当前实例的类别标签\n\"\"\"\ndef chooseBestFeatureToSplit(dataSet):\n    # 判定当前数据集包含多少特征属性\n    numFeatures = len(dataSet[0]) - 1\n    # 计算整个数据集的原始香农熵，即最初的无序度量值\n    baseEntropy = calcShannonEnt(dataSet)\n\n    bestInfoGain = 0.0\n    bestFeatures = -1\n\n    # 遍历数据集中的所有特征\n    for i in range(numFeatures):\n        # 创建唯一的分类标签列表，将数据集中所有第 i 个特征值写入这个 list 中\n        featList = [example[i] for example in dataSet]\n        # 从列表中创建集合来得到列表中唯一元素值\n        uniqueVals = set(featList)\n        newEntropy = 0.0\n\n        # 遍历当前特征中的所有唯一属性值，对每个唯一属性值划分一次数据集，计算数据集的新熵值\n        # 即计算每种划分方式的信息熵\n        for value in uniqueVals:\n            subDataSet = splitDataSet(dataSet, i, value)\n            prob = len(subDataSet) / float(len(dataSet))\n            newEntropy += prob * calcShannonEnt(subDataSet)\n        # 计算信息增益\n        infoGain = baseEntropy - newEntropy\n        # 比较所有特征中的信息增益，返回最好特征划分的索引值\n        if(infoGain > bestInfoGain):\n            bestInfoGain = infoGain\n            bestFeatures = i\n    return bestFeatures\n\n在 python 提示符下，执行代码并得到结果：\n>>> myDat, labels = trees.createDataSet()\n>>> trees.chooseBestFeatureToSplit(myDat)\n0\n>>> myDat\n[[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n\n代码运行结果告诉我们，第 0 个特征是最好的用于划分数据集的特征。也就是说第一个特征是 1 的放在一个组，第一个特征是 0 的放在另一个组。因为这个数据集比较简单，我们直接观察可以看到第一种划分更好地处理了相关数据。\n下面我们会介绍如何将上述实现的函数功能放在一起，构建决策树。\n\n程序清单 3-4 创建树的函数代码\n\"\"\"\n使用分类名称的列表，创建数据字典\n返回出现次数最多的分类名称\n\"\"\"\nimport operator\ndef majorityCnt(classList):\n    classCount = {}\n    for vote in classList:\n        if vote in classList:\n            classCount[vote] = 0\n        classCount[vote] += 1\n    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)\n    return sortedClassCount[0][0]\n\n# 参数：数据集，标签列表\ndef createTree(dataSet, labels):\n    # 创建名为 classList 的列表变量，包含了数据集的所有类标签\n    classList = [example[-1] for example in dataSet]\n    # 递归函数的第一个停止条件：所有类标签完全相同，则直接返回该类标签\n    if classList.count(classList[0]) == len(classList):\n        return classList[0]\n    # 递归函数的第二个停止条件：使用完所有特征，仍然不能将数据集划分成仅包含唯一类别的分组\n    # 由于无法简单地返回唯一的类标签，这里遍历完所有特征时使用 majorityCnt 函数返回出现次数最多的类别\n    if len(dataSet[0]) == 1:\n        return majorityCnt(classList)\n\n    # 当前数据集选取的最好特征存储在变量 bestFeat 中，得到列表包含的所有属性值\n    bestFeat = chooseBestFeatureToSplit(dataSet)\n    bestFeatLabel = labels[bestFeat]\n    # 字典变量 myTree 存储了树的所有信息\n    myTree = {bestFeatLabel:{}}\n    del(labels[bestFeat])\n\n    featValues = [example[bestFeat] for example in dataSet]\n    uniqueVals = set(featValues)\n    # 遍历当前选择特征包含的所有属性值\n    for value in uniqueVals:\n        # 复制类标签，将其存储在新列表变量 subLabels 中\n        # 在python语言中，函数参数是列表类型时，参数是按照引用方式传递的\n        # 为了保证每次调用函数 createTree 时不改变原始列表的内容\n        subLabels = labels[:]\n        # 在每个数据集划分上递归的调用函数 createTree（）\n        # 得到的返回值被插入字典变量 myTree 中\n        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)\n    return myTree\n\n在 python 提示符下，执行代码并得到结果：\n>>> myDat, labels = trees.createDataSet()\n>>> myTree = trees.createTree(myDat, labels)\n>>> myTree\n{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n\n最后得到的变量myTree包含了很多代表树结构信息的嵌套字典。这棵树包含了 3 个叶子节点以及 2 个判断节点，形状如下图所示：\n\n\n不足之处，欢迎指正。\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "1"}