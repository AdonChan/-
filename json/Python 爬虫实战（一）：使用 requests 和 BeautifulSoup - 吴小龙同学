{"title": "Python 爬虫实战（一）：使用 requests 和 BeautifulSoup - 吴小龙同学 ", "index": "python", "content": "Python 基础\n我之前写的《Python 3 极简教程.pdf》，适合有点编程基础的快速入门，通过该系列文章学习，能够独立完成接口的编写，写写小东西没问题。\nrequests\nrequests，Python HTTP 请求库，相当于 Android 的 Retrofit，它的功能包括 Keep-Alive 和连接池、Cookie 持久化、内容自动解压、HTTP 代理、SSL 认证、连接超时、Session 等很多特性，同时兼容 Python2 和 Python3，GitHub：https://github.com/requests/r... 。\n安装\nMac：\npip3 install requests\nWindows：\npip install requests\n发送请求\nHTTP 请求方法有 get、post、put、delete。\nimport requests\n\n# get 请求\nresponse = requests.get('http://127.0.0.1:1024/developer/api/v1.0/all')\n\n# post 请求\nresponse = requests.post('http://127.0.0.1:1024/developer/api/v1.0/insert')\n\n# put 请求\nresponse = requests.put('http://127.0.0.1:1024/developer/api/v1.0/update')\n\n# delete 请求\nresponse = requests.delete('http://127.0.0.1:1024/developer/api/v1.0/delete')\n请求返回 Response 对象，Response 对象是对 HTTP 协议中服务端返回给浏览器的响应数据的封装，响应的中的主要元素包括：状态码、原因短语、响应首部、响应 URL、响应 encoding、响应体等等。\n# 状态码\nprint(response.status_code)\n\n# 响应 URL\nprint(response.url)\n\n# 响应短语\nprint(response.reason)\n\n# 响应内容\nprint(response.json())\n定制请求头\n请求添加 HTTP 头部 Headers，只要传递一个 dict 给 headers 关键字参数就可以了。\nheader = {'Application-Id': '19869a66c6',\n          'Content-Type': 'application/json'\n          }\nresponse = requests.get('http://127.0.0.1:1024/developer/api/v1.0/all/', headers=header)\n构建查询参数\n想为 URL 的查询字符串(query string)传递某种数据，比如：http://127.0.0.1:1024/developer/api/v1.0/all?key1=value1&key2=value2 ，Requests 允许你使用 params 关键字参数，以一个字符串字典来提供这些参数。\npayload = {'key1': 'value1', 'key2': 'value2'}\nresponse = requests.get(\"http://127.0.0.1:1024/developer/api/v1.0/all\", params=payload)\n还可以将 list 作为值传入：\npayload = {'key1': 'value1', 'key2': ['value2', 'value3']}\nresponse = requests.get(\"http://127.0.0.1:1024/developer/api/v1.0/all\", params=payload)\n\n# 响应 URL\nprint(response.url)# 打印：http://127.0.0.1:1024/developer/api/v1.0/all?key1=value1&key2=value2&key2=value3\npost 请求数据\n如果服务器要求发送的数据是表单数据，则可以指定关键字参数 data。\npayload = {'key1': 'value1', 'key2': 'value2'}\nresponse = requests.post(\"http://127.0.0.1:1024/developer/api/v1.0/insert\", data=payload)\n如果要求传递 json 格式字符串参数，则可以使用 json 关键字参数，参数的值都可以字典的形式传过去。\nobj = {\n    \"article_title\": \"小公务员之死2\"\n}\n# response = requests.post('http://127.0.0.1:1024/developer/api/v1.0/insert', json=obj)\n响应内容\nRequests 会自动解码来自服务器的内容。大多数 unicode 字符集都能被无缝地解码。请求发出后，Requests 会基于 HTTP 头部对响应的编码作出有根据的推测。\n# 响应内容\n# 返回是 是 str 类型内容\n# print(response.text())\n# 返回是 JSON 响应内容\nprint(response.json())\n# 返回是二进制响应内容\n# print(response.content())\n# 原始响应内容，初始请求中设置了 stream=True\n# response = requests.get('http://127.0.0.1:1024/developer/api/v1.0/all', stream=True)\n# print(response.raw())\n超时\n如果没有显式指定了 timeout 值，requests 是不会自动进行超时处理的。如果遇到服务器没有响应的情况时，整个应用程序一直处于阻塞状态而没法处理其他请求。\nresponse = requests.get('http://127.0.0.1:1024/developer/api/v1.0/all', timeout=5)  # 单位秒数\n代理设置\n如果频繁访问一个网站，很容易被服务器屏蔽掉，requests 完美支持代理。\n# 代理\nproxies = {\n    'http': 'http://127.0.0.1:1024',\n    'https': 'http://127.0.0.1:4000',\n}\nresponse = requests.get('http://127.0.0.1:1024/developer/api/v1.0/all', proxies=proxies)\nBeautifulSoup\nBeautifulSoup，Python Html 解析库，相当于 Java 的 jsoup。\n安装\nBeautifulSoup 3 目前已经停止开发，直接使用BeautifulSoup 4。\nMac：\npip3 install beautifulsoup4\nWindows：\npip install beautifulsoup4\n安装解析器\n我用的是 html5lib，纯 Python 实现的。\nMac：\npip3 install html5lib\nWindows：\npip install html5lib\n简单使用\nBeautifulSoup 将复杂 HTML 文档转换成一个复杂的树形结构，每个节点都是 Python 对象。\n解析\nfrom bs4 import BeautifulSoup\n\ndef get_html_data():\n    html_doc = \"\"\"\n    <html>\n    <head>\n    <title>WuXiaolong</title>\n    </head>\n    <body>\n    <p>分享 Android 技术，也关注 Python 等热门技术。</p>\n    <p>写博客的初衷：总结经验，记录自己的成长。</p>\n    <p>你必须足够的努力，才能看起来毫不费力！专注！精致！\n    </p>\n    <p class=\"Blog\"><a href=\"http://wuxiaolong.me/\">WuXiaolong's blog</a></p>\n    <p class=\"WeChat\"><a href=\"https://open.weixin.qq.com/qr/code?username=MrWuXiaolong\">公众号：吴小龙同学</a> </p>\n    <p class=\"GitHub\"><a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">GitHub</a></p>\n    </body>\n    </html>   \n    \"\"\"\n    soup = BeautifulSoup(html_doc, \"html5lib\")\ntag\ntag = soup.head\nprint(tag)  # <head><title>WuXiaolong</title></head>\nprint(tag.name)  # head\nprint(tag.title)  # <title>WuXiaolong</title>\nprint(soup.p)  # <p>分享 Android 技术，也关注 Python 等热门技术。</p>\nprint(soup.a['href'])  # 输出 a 标签的 href 属性：http://wuxiaolong.me/\n注意：tag 如果多个匹配，返回第一个，比如这里的 p 标签。\n查找\nprint(soup.find('p'))  # <p>分享 Android 技术，也关注 Python 等热门技术。</p>\nfind 默认也是返回第一个匹配的标签，没找到匹配的节点则返回 None。如果我想指定查找，比如这里的公众号，可以指定标签的如 class 属性值：\n# 因为 class 是 Python 关键字，所以这里指定为 class_。\nprint(soup.find('p', class_=\"WeChat\"))\n# <p class=\"WeChat\"><a href=\"https://open.weixin.qq.com/qr/code?username=MrWuXiaolong\">公众号</a> </p>\n查找所有的 P 标签：\nfor p in soup.find_all('p'):\n    print(p.string) \n实战\n前段时间，有用户反馈，我的个人 APP 挂了，虽然这个 APP 我已经不再维护，但是我也得起码保证它能正常运行。大部分人都知道这个 APP 数据是爬来的（详见：《手把手教你做个人app》），数据爬来的好处之一就是不用自己管数据，弊端是别人网站挂了或网站的 HTML 节点变了，我这边就解析不到，就没数据。这次用户反馈，我在想要不要把他们网站数据直接爬虫了，正好自学 Python，练练手，嗯说干就干，本来是想着先用 Python 爬虫，MySQL 插入本地数据库，然后 Flask 自己写接口，用 Android 的 Retrofit 调，再用 bmob sdk 插入 bmob……哎，费劲，感觉行不通，后来我得知 bmob 提供了 RESTful，解决大问题，我可以直接 Python 爬虫插入就好了，这里我演示的是插入本地数据库，如果用 bmob，是调 bmob 提供的 RESTful 插数据。\n网站选定\n我选的演示网站：https://meiriyiwen.com/random ，大家可以发现，每次请求的文章都不一样，正好利用这点，我只要定时去请求，解析自己需要的数据，插入数据库就 OK 了。\n创建数据库\n我直接用 NaviCat Premium 创建的，当然也可以用命令行。\n创建表\n创建表 article，用的 pymysql，表需要 id，article_title，article_author，article_content 字段，代码如下，只需要调一次就好了。\nimport pymysql\n\n\ndef create_table():\n    # 建立连接\n    db = pymysql.connect(host='localhost',\n                         user='root',\n                         password='root',\n                         db='python3learn')\n    # 创建名为 article 数据库语句\n    sql = '''create table if not exists article (\n    id int NOT NULL AUTO_INCREMENT, \n    article_title text,\n    article_author text,\n    article_content text,\n    PRIMARY KEY (`id`)\n    )'''\n    # 使用 cursor() 方法创建一个游标对象 cursor\n    cursor = db.cursor()\n    try:\n        # 执行 sql 语句\n        cursor.execute(sql)\n        # 提交事务\n        db.commit()\n        print('create table success')\n    except BaseException as e:  # 如果发生错误则回滚\n        db.rollback()\n        print(e)\n\n    finally:\n        # 关闭游标连接\n        cursor.close()\n        # 关闭数据库连接\n        db.close()\n\n\nif __name__ == '__main__':\n    create_table()\n\n解析网站\n首先需要 requests 请求网站，然后 BeautifulSoup 解析自己需要的节点。\nimport requests\nfrom bs4 import BeautifulSoup\n\n\ndef get_html_data():\n    # get 请求\n    response = requests.get('https://meiriyiwen.com/random')\n\n    soup = BeautifulSoup(response.content, \"html5lib\")\n    article = soup.find(\"div\", id='article_show')\n    article_title = article.h1.string\n    print('article_title=%s' % article_title)\n    article_author = article.find('p', class_=\"article_author\").string\n    print('article_author=%s' % article.find('p', class_=\"article_author\").string)\n    article_contents = article.find('div', class_=\"article_text\").find_all('p')\n    article_content = ''\n    for content in article_contents:\n        article_content = article_content + str(content)\n        print('article_content=%s' % article_content)\n插入数据库\n这里做了一个筛选，默认这个网站的文章标题是唯一的，插入数据时，如果有了同样的标题就不插入。\nimport pymysql\n\n\ndef insert_table(article_title, article_author, article_content):\n    # 建立连接\n    db = pymysql.connect(host='localhost',\n                         user='root',\n                         password='root',\n                         db='python3learn',\n                         charset=\"utf8\")\n    # 插入数据\n    query_sql = 'select * from article where article_title=%s'\n    sql = 'insert into article (article_title,article_author,article_content) values (%s, %s, %s)'\n    # 使用 cursor() 方法创建一个游标对象 cursor\n    cursor = db.cursor()\n    try:\n        query_value = (article_title,)\n        # 执行 sql 语句\n        cursor.execute(query_sql, query_value)\n        results = cursor.fetchall()\n        if len(results) == 0:\n            value = (article_title, article_author, article_content)\n            cursor.execute(sql, value)\n            # 提交事务\n            db.commit()\n            print('--------------《%s》 insert table success-------------' % article_title)\n            return True\n        else:\n            print('--------------《%s》 已经存在-------------' % article_title)\n            return False\n\n    except BaseException as e:  # 如果发生错误则回滚\n        db.rollback()\n        print(e)\n\n    finally:  # 关闭游标连接\n        cursor.close()\n        # 关闭数据库连接\n        db.close()\n定时设置\n做了一个定时，过段时间就去爬一次。\nimport sched\nimport time\n\n\n# 初始化 sched 模块的 scheduler 类\n# 第一个参数是一个可以返回时间戳的函数，第二个参数可以在定时未到达之前阻塞。\nschedule = sched.scheduler(time.time, time.sleep)\n\n\n# 被周期性调度触发的函数\ndef print_time(inc):\n    # to do something\n    print('to do something')\n    schedule.enter(inc, 0, print_time, (inc,))\n\n\n# 默认参数 60 s\ndef start(inc=60):\n    # enter四个参数分别为：间隔事件、优先级（用于同时间到达的两个事件同时执行时定序）、被调用触发的函数，\n    # 给该触发函数的参数（tuple形式）\n    schedule.enter(0, 0, print_time, (inc,))\n    schedule.run()\n\n\nif __name__ == '__main__':\n    # 5 s 输出一次\n    start(5)\n完整代码\nimport pymysql\nimport requests\nfrom bs4 import BeautifulSoup\nimport sched\nimport time\n\n\ndef create_table():\n    # 建立连接\n    db = pymysql.connect(host='localhost',\n                         user='root',\n                         password='root',\n                         db='python3learn')\n    # 创建名为 article 数据库语句\n    sql = '''create table if not exists article (\n    id int NOT NULL AUTO_INCREMENT, \n    article_title text,\n    article_author text,\n    article_content text,\n    PRIMARY KEY (`id`)\n    )'''\n    # 使用 cursor() 方法创建一个游标对象 cursor\n    cursor = db.cursor()\n    try:\n        # 执行 sql 语句\n        cursor.execute(sql)\n        # 提交事务\n        db.commit()\n        print('create table success')\n    except BaseException as e:  # 如果发生错误则回滚\n        db.rollback()\n        print(e)\n\n    finally:\n        # 关闭游标连接\n        cursor.close()\n        # 关闭数据库连接\n        db.close()\n\n\ndef insert_table(article_title, article_author, article_content):\n    # 建立连接\n    db = pymysql.connect(host='localhost',\n                         user='root',\n                         password='root',\n                         db='python3learn',\n                         charset=\"utf8\")\n    # 插入数据\n    query_sql = 'select * from article where article_title=%s'\n    sql = 'insert into article (article_title,article_author,article_content) values (%s, %s, %s)'\n    # 使用 cursor() 方法创建一个游标对象 cursor\n    cursor = db.cursor()\n    try:\n        query_value = (article_title,)\n        # 执行 sql 语句\n        cursor.execute(query_sql, query_value)\n        results = cursor.fetchall()\n        if len(results) == 0:\n            value = (article_title, article_author, article_content)\n            cursor.execute(sql, value)\n            # 提交事务\n            db.commit()\n            print('--------------《%s》 insert table success-------------' % article_title)\n            return True\n        else:\n            print('--------------《%s》 已经存在-------------' % article_title)\n            return False\n\n    except BaseException as e:  # 如果发生错误则回滚\n        db.rollback()\n        print(e)\n\n    finally:  # 关闭游标连接\n        cursor.close()\n        # 关闭数据库连接\n        db.close()\n\n\ndef get_html_data():\n    # get 请求\n    response = requests.get('https://meiriyiwen.com/random')\n\n    soup = BeautifulSoup(response.content, \"html5lib\")\n    article = soup.find(\"div\", id='article_show')\n    article_title = article.h1.string\n    print('article_title=%s' % article_title)\n    article_author = article.find('p', class_=\"article_author\").string\n    print('article_author=%s' % article.find('p', class_=\"article_author\").string)\n    article_contents = article.find('div', class_=\"article_text\").find_all('p')\n    article_content = ''\n    for content in article_contents:\n        article_content = article_content + str(content)\n        print('article_content=%s' % article_content)\n\n    # 插入数据库\n    insert_table(article_title, article_author, article_content)\n\n\n# 初始化 sched 模块的 scheduler 类\n# 第一个参数是一个可以返回时间戳的函数，第二个参数可以在定时未到达之前阻塞。\nschedule = sched.scheduler(time.time, time.sleep)\n\n\n# 被周期性调度触发的函数\ndef print_time(inc):\n    get_html_data()\n    schedule.enter(inc, 0, print_time, (inc,))\n\n\n# 默认参数 60 s\ndef start(inc=60):\n    # enter四个参数分别为：间隔事件、优先级（用于同时间到达的两个事件同时执行时定序）、被调用触发的函数，\n    # 给该触发函数的参数（tuple形式）\n    schedule.enter(0, 0, print_time, (inc,))\n    schedule.run()\n\n\nif __name__ == '__main__':\n    start(60*5)\n\n问题：这只是对一篇文章爬虫，如果是那种文章列表，点击是文章详情，这种如何爬虫解析？首先肯定要拿到列表，再循环一个个解析文章详情插入数据库？还没有想好该如何做更好，留给后面的课题吧。\n最后\n虽然我学 Python 纯属业余爱好，但是也要学以致用，不然这些知识很快就忘记了，期待下篇 Python 方面的文章。\n参考\n快速上手 — Requests 2.18.1 文档\n爬虫入门系列（二）：优雅的HTTP库requests\nBeautiful Soup 4.2.0 文档\n爬虫入门系列（四）：HTML文本解析库BeautifulSoup\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "3"}