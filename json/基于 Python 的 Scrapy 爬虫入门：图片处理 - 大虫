{"title": "基于 Python 的 Scrapy 爬虫入门：图片处理 - 大虫 ", "index": "python,scrapy", "content": "目录\n\n基于 Python 的 Scrapy 爬虫入门：环境搭建\n基于 Python 的 Scrapy 爬虫入门：页面提取\n基于 Python 的 Scrapy 爬虫入门：图片处理\n\n\n上篇文章中讲解了如何从网站页面抓取所需要的数据，很幸运范例中所需的数据是通过 Ajax 请求返回的 JSON 结构化数据，处理起来很简单，图片内容也只取了一个链接，对于我最初的目标“把这些图集添加到自己的博客站点中”这已经够了，说白了就是“盗链”嘛，如果有一天网站做了防盗链措施，那这些抄来的图集就都作废了，保险的方法就是把图片也下载到本地，再把图片链接替换为本地图片。\n下载图片\n下载图片有两种方式，一种是通过 Requests 模块发送 get 请求下载，另一种是使用 Scrapy 的 ImagesPipeline 图片管道类，这里主要讲后者。\n安装 Scrapy 时并没有安装图像处理依赖包 Pillow，需手动安装否则运行爬虫出错。\n首先在 settings.py 中设置图片的存储路径：\nIMAGES_STORE = 'D:/'\n图片处理相关的选项还有：\n# 图片最小高度和宽度设置，可以过滤太小的图片\nIMAGES_MIN_HEIGHT = 110\nIMAGES_MIN_WIDTH = 110\n\n# 生成缩略图选项\nIMAGES_THUMBS = {\n    'small': (50, 50),\n    'big': (270, 270),\n}\n更多选项请参考：https://doc.scrapy.org/en/lat...\n之前已经存在提取内容的 TuchongPipeline 类，如果使用 ImagePipeline 可以将提取内容的操作都合并过来，但是为了更好的说明图片管道的作用，我们再单独创建一个 ImagePipeline 类，加到 pipelines.py 文件中，同时重载函数 get_media_requests：\nclass PhotoGalleryPipeline(object):\n    ...\n\nclass PhotoPipeline(ImagesPipeline):\n    def get_media_requests(self, item, info):\n        for (id, url) in item['images'].items():\n            yield scrapy.Request(url)\n上篇文章中我们把图片的URL保存在了 item['images'] 中，它是一个字典类型的数组，形如：[{img_id: img_url}, ...]，此函数中需要把 img_url 取出并构建为 scrapy.Request 请求对象并返回，每一个请求都将触发一次下载图片的操作。\n到 settings.py 中注册 PhotoPipeline，并把优先级设的比提取内容的管道要高一些，保证图片下载优先于内容处理，目的是如果有图片下载未成功，通过触发 DropItem 异常可以中断这一个 Item 的处理，防止不完整的数据进入下一管道：\nITEM_PIPELINES = {\n    'Toutiao.pipelines.PhotoGalleryPipeline': 300,\n    'Toutiao.pipelines.PhotoPipeline': 200,\n}\n执行爬虫 scrapy crawl photo ，如无错误，在设定的存储目录中会出现一个 full 目录，里面是下载后的图片。\n文件名处理\n下载的文件名是以图片URL通过 sha1 编码得到的字符，类似 0a79c461a4062ac383dc4fade7bc09f1384a3910.jpg 不是太友好，可以通过重载 file_path 函数自定义文件名，比如可以这样保留原文件名：\n...\n    def file_path(self, request, response=None, info=None):\n        file_name = request.url.split('/')[-1]\n        return 'full/%s' % (file_name)\n...\n上面这样处理难免会有重名的文件被覆盖，但参数 request 中没有过多的信息，不便于对图片分类，因此可以改为重载 item_completed 函数，在下载完成后对图片进行分类操作。\n函数 item_completed 的定义：\ndef item_completed(self, results, item, info)\n参数中包含 item ，有我们抓取的所有信息，参数 results 为下载图片的结果数组，包含下载后的路径以及是否成功下载，内容如下：\n[(True,\n  {'checksum': '2b00042f7481c7b056c4b410d28f33cf',\n   'path': 'full/0a79c461a4062ac383dc4fade7bc09f1384a3910.jpg',\n   'url': 'http://www.example.com/files/product1.pdf'}),\n (False,\n  Failure(...))]\n重载该函数将下载图片转移到分类目录中，同时关联文件路径到 item 中，保持内容与图片为一个整体：\ndef item_completed(self, results, item, info):\n    image_paths = {x['url'].split('/')[-1]: x['path'] for ok, x in results if ok}\n    if not image_paths:\n        # 下载失败忽略该 Item 的后续处理\n        raise DropItem(\"Item contains no files\")\n    else:\n        # 将图片转移至以 post_id 为名的子目录中\n        for (dest, src) in image_paths.items():\n            dir = settings.IMAGES_STORE\n            newdir = dir + os.path.dirname(src) + '/' + item['post_id'] + '/'\n            if not os.path.exists(newdir):\n                os.makedirs(newdir)\n            os.rename(dir + src, newdir + dest)\n    # 将保存路径保存于 item 中（image_paths 需要在 items.py 中定义）\n    item['image_paths'] = image_paths\n    return item\n接下来在原 TuchongPipeline 类中写入数据库的操作中，通过 item['image_paths'] 路径信息写入本地图片链接。\n除了 ImagesPipeline 处理图片外，还有 FilesPipeline 可以处理文件，使用方法与图片类似，事实上 ImagesPipeline 是 FilesPipeline 的子类，因为图片也是文件的一种。\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "12"}