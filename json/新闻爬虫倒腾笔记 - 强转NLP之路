{"title": "新闻爬虫倒腾笔记 - 强转NLP之路 ", "index": "python,网页爬虫,beautifulsoup", "content": "新闻爬虫1.0 在列表页获取列表和时间\n材料：Pycharm，Chrome开发人员工具，infoLite插件，bs4，request\n安装包\npip install requests\npip install beautifulsoup4\n使用Chrome开发人员工具分析网站\n\n右键或Ctrl+shift+I打开，F5刷新载入\n打开network页签，点选第二排的小漏斗（filter），点入doc页面，因为新闻都有被检索到的需要，所以一般情况下都可以在第一个doc文档中找到需要的信息。我们现在第一个目标是获取这个页面里所有的国内新闻标题、时间、链接。选择第一个doc，通过responce看看是否有所需信息。3.再通过查看header页面可以看到信息被得到的方式是GET，这说明我们可以通过get调用，而通过第一排的Request URL我们就可以知道想要得到网站信息的目标url是什么。\n\n使用request调用网页\nrequest是URL的撷取套件，可以用postputgetdelete来存取网络资源\nimport requests\nnewurl = '目标网站'\nres = requests.get(newurl)\nres.encoding = 'utf-8'\n# 申明编码避免res内容出现乱码\n使用BeautifulSoup4进行加工\n通过request的get得到了网页的所有内容，但是还有很多不需要用到的样式语言等，本来可以用正则表达式来进行删选，但是这里使用第三方包BeautifulSoup4对原材料进行处理。\nsoup = BeautifulSoup(res.text,'html.parser')\n# 需要注释res的类型是HTML，使用parser对HTML进行解析。\n这里需要稍微了解一点HTML/CSS里的DOM Tree知识。（注：DOM Tree是指通过DOM将HTML页面进行解析，并生成的HTML tree树状结构和对应访问方法。）\n\n这方面我自己是理解的，但是理解的不透彻讲不清。详参这里总之，我们所需要的内容是在各种各样的元素中的，它们可能是含有特定标签的HTML元素，也有可能是含有特定CSS属性的元素。（我所理解的是HTML元素是HTML固有的，结构层次已经划分好了的。而特定CSS属性的元素则是因为特殊的格式需要设置的灵活性的一个元素。一般是通过id和类来设置。）select为了得到特定的元素，使用beautifulsoup4中的select。使用方法是：\n# 含有特定标签的HTML元素\nheader = soup.select('h1')\n# 含有特定css属性的元素 ID\nheader = soup.select('#title')\n# 含有特定css属性的元素 class\nlink = soup.select('.link')\nselect的预设方法是传回一个list，所以用解开列表的方式解开并且取出来就可以。\n使用infoLite插件获取\ninfoLite插件下载地址接下来，我们就需要知道爬取目标的元素到底是什么。有两种方法，一是借助infoLite插件，一是借助开发工具的elements自己查找。为了保证查找的准确性，两种方法最好都掌握。首先是使用infoLite插件，点击就可以知道元素的类型。但是有时候情况会比较复杂，比如说，会遇到css类里使用HTML元素的情况。在这种情况下先select CSS 最大共性元素再select出HTML元素。\n标题\n\n（网页其他部分也有h2,可以结合开发工具看更清晰。）\n时间.time\n链接URL链接可以从上面的DOM Tree看到URL是元素的属性，所以需要通过开发工具看。在网页的链接中通常使用a tag去链接其他网页，而href就是连结的桥梁。\n列表里其实是一个字典，利用字典的性质，用key(href)调用出链接。\n完整\nimport requests\nfrom bs4 import BeautifulSoup\nres = requests.get('http://news.sina.com.cn/china/')\nres.encoding = 'utf-8'\nsoup = BeautifulSoup(res.text,'html.parser')\nfor news in soup.select('.news-item'):\nif len(news.select('h2'))>0:\n#保证存在，防止遍历时输出空集\nh2 = news.select('h2')[0].text\n#取出内容\ntime = news.select('.time')[0].text\na = news.select('a')[0]['href']\nprint (time,h2,a)\n学习参考自此网站\n新闻爬虫2.0 抓取新闻内页\n材料：Pycharm，Chrome开发人员工具，infoLite插件，bs4，request\n在前面我们已经将新闻列表页面的所有链接都下载下来了，但是还不够，我们希望可以获取链接所指的各个单个新闻的标题、来源、时间以及内文。\n这里我们从特殊到一般，而用到的方法还是笔记NO.1中对select的活用，先以一个链接为例。\n取得内文页面\n和抓取列表页面的初始步骤一样，我们首先要将原材料整个的放到soup里进行一个剖析，然后再逐个提取出我们想要的元素。取得页面的说明不再赘述。\nimport requests\nfrom bs4 import BeautifulSoup\nres = requests.get('http://news.sina.com.cn/c/nd/2017-01-04/doc-ifxzczff3681310.shtml')\nres.encoding = 'utf-8'\nsoup = BeautifulSoup(res.text,'html.parser')\n获取内文标题\n通过检查工具得知标题是一个特殊的css属性标签，ID是artibodyTitle，通过soup的select传回一个list，再通过取出list里内容的方法取出来。\ntitle= soup.select('#artibodyTitle')[0].text\n取得来源和时间\n本来想处理时间，时间的类是time-source，结果select回传的结果里既有时间又有来源。使用.text也无法将之分开。select的回传列表结果：\n[<span class=\"time-source\" id=\"navtimeSource\">2017年01月04日09:47        <span>\n<span data-sudaclick=\"media_name\">\n\\<a href=\"http://www.cma.gov.cn/2011zwxx/2\n011zyjgl/2011zyjxy/2011zqxyj/201701/\\t20170104_382215.html\n\\\" rel=\"nofollow\" target=\"_blank\">政府网站</a></span></span>\n</span>]\n所以这里要采用contents将时间和来源取出来。\n关于.contents的帮助文档说明\ntag的.contents属性可以将tag的子节点以列表的方式输出。新闻的时间精确到日就可以了，对字符串进行切片处理。\ntime = soup.select('.time-source')[0].contents[:10]\n接下来我们要思考如何取出来源，通过检查工具我们可以看到，来源是在时间的层级之下的，这也很好的解释了为什么用contens处理下来，和来源有关的东西并没有发生变化，因为它是附属其下，并且由一个span里的a tag控制的。\n![Uploading time-source_539741.png . . .]\n所以试试用这个把它select出来\nmedianame = soup.select('.time-source span a')[0].text\n取得内文\n内文位于artibody p里，同样的方法取出。观察由三个成分组成，<p>分隔符、控制符以及我们需要的文字内容。通过strip去除控制符，再遍历输出\narticle = soup.select('#artibody p')[:-1]\n获得编辑名称\n同理，略\nauthor = soup.select('.article-editor')[0].text.strip('责任编辑：')\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}