{"title": "Python 从零开始爬虫(一)——爬虫伪装&反“反爬” - Python 从零开始爬虫 ", "index": "python,网页爬虫", "content": "  之前提到过，有些网站是防爬虫的。其实事实是，凡是有一定规模的网站，大公司的网站，或是盈利性质比较强的网站，都是有高级的防爬措施的。总的来说有两种反爬策略，要么验证身份，把虫子踩死在门口；要么在网站植入各种反爬机制，让爬虫知难而退。  本节内容就着这两种反爬策略提出一些对策。\n身份伪装\n  就算是一些不知名的小网站，多多少少还会检查一下headers验证一下访者的身份，大网站就更不用说了（我一次爬网易云的时候，忘记加headers，直接被红掉）  所以，为了让虫宝们带着信息凯旋归来，我们要教会爬虫们如何伪装；有时光伪装也不行，我们还要教爬虫具体\"如何做人\"，让自己的举止更像普通人而不是比单身汉手速还快的未知生物。\n自定制 Requests Headers\n\n“吾是人！”——修改user-agent：里面储存的是系统和浏览器的型号版本，通过修改它来假装自己是人。\n“我从河北省来”——修改referer：告诉服务器你是通过哪个网址点进来的而不是凭空出现的，有些网站会检查。\n“饼干！”：——带上cookie，有时带不带饼干得到的结果是不同的，试着带饼干去“贿赂”服务器让她给你完整的信息。\n详细数据可以F12捉个包来查看其Requests Headers\n\n\nheaders = {'Referer':'https://accounts.pixiv.net/loginlang=zh&source=pc&view_type=page&ref=wwwtop_accounts_index',#如某些网站（如p站）要检查referer，就给他加上\n           'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.146 Safari/537.36'#每个爬虫必备的伪装\n          }\nr = requests.get(\"https://segmentfault.com/a/1190000014383966\",headers=headers)\nheaders数据通常用这两个即可，而且笔者强烈推荐在爬虫中为每个request都配个user-agent，总比什么都没有好，加了也不会报错。\n降低主IP访问频率\n注意：这是针对长期的，大范围的爬虫的\n有些网站会监视某个ip的访问频率和次数，一但超过某个阈值，就把你当作爬虫嫌犯赶出去了，这时就要想办法降低自己的存在感了。\n\nZzzzz——休眠：爬一段时间后休息一会，不仅是为了自己的成功，也是为服务器着想。\n我不去，我派别人去——ip代理：通过proxies参数来使用，前提是你要有ip，好的ip代理是要花钱的。\n\ntime.sleep(60)#用python自带time模块的休眠功能\nproxies = {'http': 'http://10.10.1.10:3128',#“协议类型：完整ip地址+端号”\n           'https': 'http://10.10.1.10:1080'}#代理ip字典，随机调用\nr = requests.get(url,headers=headers,proxies=proxies)\n反 反爬（简析）\n在系列第零篇中我提到获取网页源码是有坑的，没错，有些时候headers伪装什么的都做足了，可你还是不能如愿以偿的获得正确的网页源码，要么缺，要么给你一堆毫不相关的东西，要么干脆让你红掉。这说明要点不是伪不伪装的问题了，而是如何去解读网页的防爬机制从而推出解决方法，这就要求比较高的观察和分析能力了。\n就我目前遇到的主要有：\n\n\n随机校验码：网页生成随机码，并要求你将其提交才接受你的请求（多用在登录验证中）。——这种校验码通常藏在网页源码中，先取再交是策略。\n\n无序网址：网址后跟着一大串看不出规律的东西。——跟这种东西是没话说的，直接上selenium。\n\n加密/杂乱的源码：你知道你要的东西就在那里，但是不知道怎样提取出来。——推理解谜，看脑子好不好使了。\n\n动态加载：需要和页面交互才能获取更多信息，但是爬虫没法和它交互啊。——直接上selenium/手动捉包分析出目标链接\n\n\najax技术：异步加载，网页内容分次加载，用爬虫只能得到第一次发出的html，导致信息不全。——上selenium/手动捉包分析出目标连接\n\n\n补充：selenium模块，模拟浏览器，强是强但是缺点是慢。其实动态加载是为了方便用户点哪看哪的，但这也加大了爬虫的难度，因为很多信息因此被隐藏了起来。\n最后\n伪装有套路，代码直接套，多加headers总没错。\n内嵌反爬很灵活，没有什么固定的代码格式，要花时间去分析出来。\n本文新出现的方法/模块，后面会有实例，不要方。\n下一篇正式进入网页解析主题，之后就可以开始写小爬虫了♪(＾∀＾●)ﾉ。\n\n                ", "mainLikeNum": ["7 "], "mainBookmarkNum": "5"}