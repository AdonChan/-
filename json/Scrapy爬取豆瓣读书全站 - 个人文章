{"title": "Scrapy爬取豆瓣读书全站 - 个人文章 ", "index": "python", "content": "Scrapy爬取豆瓣读书全站\n分析网页\n\n首先打开豆瓣读书中的分类浏览，可以看到其中有很多的分类\n\n豆瓣应该是一个比较好爬的网站，所有的数据都不是ajax加载的，我们打开谷歌的F12或者是火狐的FireBug可以很轻松的找到每一个分类的链接\n\n这里我们使用scrapy中的一个linkextractors库,这个库的作用是会根据提供的限制，自动爬取和深入每一个页面并且提取需要的链接，如果想要找到每一个分类的url,只需Rule(LinkExtractor(allow='/tag/',restrict_xpaths=\"//div[@class='article']\"),follow=True),这里的allow是一个正则表达式，用来筛选分类url,restrict_xpaths是限制在哪个结构中筛选url,这里限制的是在<div class='article'>这个盒模型中，follow表示是否深入，这里当然是要深入,这里就能得到每一个分类url了，自己可以在回调函数中测试下，输入所得的url,可以使用respose.url\n得到所有的分类url，就可以继续深入到每一步作品所在的页面了，如下图!\n\n\n但是我们需要不止是这一页，我们要爬的时全站，因此这里必须实现翻页，我们可以看到页面底部清楚的写着下一页，我们通过解析页面同样可以得到url,如下图所示\n可以看到所有的url的规则，我们就可以用正则表达式限制，以获取我们的需要，我们可以写出翻页的代码\n\nRule(LinkExtractor(allow=\"\\?start=\\d+\\&type=\",restrict_xpaths=\"//div[@class='pa>ginator']\"),follow=True),\n\n最后一步就是打开每一部书的网页得到所需的信息了，我们就可以通过这里通过解析网页还是可以很清楚的知道url,这里就不再详细的说怎么解析了，这里可以看到所有的url都在li标签中，如下图\n我们打开li标签可以很清楚的看大url的规律，因此这里还是用到上面说的库解析深入，连同上面的代码如下\n\nRule(LinkExtractor(allow='/tag/',restrict_xpaths=\"/ /div[@class='article']\"),follow=True),#第一步\nRule(LinkExtractor(allow=\"\\?start=\\d+\\&type=\",restrict_xpaths=\"//div[@class='pa>ginator']\"),follow=True),  #第二步翻翻页\nRule(LinkExtractor(allow=\"/subject/\\d+/$\",restrict_>xpaths=\"//ul[@class='subject-list']\"),callback='parse_item')#得到所需网页的url\n\n到了这里总算是大功告成了，下面就需要解析自己的所需要的信息了,这里附上网页\n下面就是写自己解析代码了，这里就不需要详细的说了，详细内容请看源码,值得注意的是爬取的网页速度不要太快，豆瓣会禁IP的，这里可以采用一些反爬虫措施,如请求头的更换，ip地址的更换，下一篇会详细解说。\n\n参考文档：\n\nscrapy中文文档\n最后附上本人的github地址,不要忘了给个star哦\n\n本人博客地址\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "4"}