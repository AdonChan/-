{"title": "数据科学 - MAY ", "index": "beautifulsoup,numpy,pandas,python", "content": "有90%的有用数据，都在数据库中。\n数据\n数据类型\n\n定性数据： 叙述特征或种类，例如：种族，区域。\n定量数据： 可以被计数或测量，例如：身高，消费金额。\n\n定量数据\n\n离散数据只能用自然数或整数单位计算。只能按计量单位数计数，可由一般计算方法取得。例如：班级人数\n连续数据一定区间内可以任意取值的数据，其数值是连续不断的，相邻两个数值可以取无限个数值。其数值只能用测量或计量的方法取得。 例如：零件规格尺寸\n\n数据来源\n\n结构化数据每条数据都有固定的字段，固定的格式，方便程序进行后续取用与分析。例如：数据库。\n半结构化数据（要使数据具有弹性，能够存储，也能够便利查找。）数据介于结构化与非结构化之间，数据具有字段，也可以依据字段来查找，使用方便，但每条数据的字段可能不一致。例如：XML，JSON。\n非结构化数据  没有固定的格式，必须整理以后才能存取例如：格式的文字，网页数据，文件数据。\n\n非结构化数据必须透过ETL(Extract抽取, Transfromation转换, Loading储存)工具将数据转为结构化数据才能取用。\n文件处理\n普通操作文件\nwith open('fliename', 'raw') as f:\n    f.write('hello world')\n    f.read()\n    f.readlines()\nCSV格式数据\n方式一：通过文件打开读取数据。\nwith open('./Population.csv', 'r', encoding='UTF-8') as f:\n    # print(f.read())\n    for line in f.readlines():\n        print(line)\n方式二：通过pandas模块读取\nimport pandas as pd\n\n\ndf = pd.read_csv('./Population.csv')\nprint(df.values)\nExcel格式数据\nimport pandas as pd\n\nfilename = 'house_sample.xlsx'\n\ndf = pd.read_excel(filename)\n\nprint(df.values[0][0])\nJSON格式数据\n方式1：通过文件读取，然后json模块读取，转换为list类型数据。\nimport json\n\n\nfilename = 'jd.json'\nwith open(filename, 'r') as f:\n    fc = f.read()\n\ndf = json.loads(fc)\nprint(df)\n\nstrjson = json.dumps(df)\n方式2：通过pandas模块读取\nimport pandas as pd\n\nfilename = 'jd.json'\n\ndf = pd.read_json(filename)\n\nprint(df.values)\nXML格式数据\n通过模块xml处理:\nimport xml.etree.ElementTree as ET\n\nfilename = 'weather.xml'\ntree = ET.parse(filename)\n\nroot = tree.getroot()\n\nfor city in root.iter('city'):\n    print(city.get('cityname'))\n网络爬虫\n需要模块：\n\nBeautifulSoup\n\nrequest：网络获取，可以使用REST操作POST,PUT,GET,DELETE存取网络资源.\n\n简单爬取：\nimport requests\n\nnewurl = 'http://news.qq.com/'\n\nres = requests.get(newurl)\nprint(res.text)\nBeautifulSoup\nbs4模块，可以把抓取的网页变成DOM文档，允许使用CSS选择器来寻找需要的内容。\nimport requests\nfrom bs4 import BeautifulSoup\n\nnewurl = 'http://news.qq.com/'\n\nres = requests.get(newurl)\n\nhtml = res.text\n# print(res.text)\n\nhtml = '\\\n    <h1 title=\"123\" id=\"h\">hello world</h1>\\\n    <h2>数据科学</h2>\n'\nsoup = BeautifulSoup(html, 'html.parser')\n\ns = soup.select('h1') # 获取元素\nprint(s[0]['title']) # 获取属性\n抓取位置实用工具\n\nChrome\nFirefox\nInfoLite\nxpath lxml库\n\n从其它地方获取到数据，存储为.json, .cvs, .xlsx，需要从DataFrame()中获取。\nimport pandas\nimport requests\nfrom bs4 import BeautifulSoup\n\nnewurl = 'http://news.qq.com/'\nhtml = requests.get(newurl).text\n\nsoup = BeautifulSoup(html, 'html.parser')\nwarp = soup.select('.head .Q-tpWrap .text')\n\ndataArr = []\nfor news in warp:\n    dataArr.append({'name': news.select('a')[0].text.encode(), 'herf': news.select('a')[0]['href']})\n\nnewsdf = pandas.DataFrame(dataArr)\nnewsdf.to_json('news.json')\nnewsdf.to_csv('news.csv')\nnewsdf.to_excel('news.xlsx')\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\n\nurl = 'http://xm.esf.fang.com/'\nhtml = requests.get(url).text\n\nsoup = BeautifulSoup(html, 'html.parser')\nresultArr = []\n\nfor house in soup.select('.shop_list dl'):\n    shop = {\n        'tit_shop': house.select('dd:nth-of-type(1) .tit_shop') and house.select('dd:nth-of-type(1) .tit_shop')[0].text,\n        'tel_shop': house.select('dd:nth-of-type(1) .tel_shop') and ''.join( house.select('dd:nth-of-type(1) .tel_shop')[0].text.split('|') ).strip(),\n        'add_shop': house.select('dd:nth-of-type(1) .add_shop') and '小区名字：' + house.select('dd:nth-of-type(1) .add_shop')[0].select('a')[0].text + '； 具体地址：' + house.select('dd:nth-of-type(1) .add_shop')[0].select('span')[0].text,\n        'price_shop': house.select('dd:nth-of-type(2) span b') and house.select('dd:nth-of-type(2) span b')[0].text,\n        'sqm': house.select('dd:nth-of-type(2) span') and house.select('dd:nth-of-type(2) span')[1].text\n    }\n    resultArr.append(shop)\n\nresultArr = json.dumps(resultArr)\n\nwith open('fang.json', 'w') as f:\n    f.write(resultArr)\nprint('ok')\n\n爬取房天下的厦门二手房数据\nimport json\n\nimport requests\nfrom bs4 import BeautifulSoup\n\nurl = 'http://xm.esf.fang.com/'\nhtml = requests.get(url).text\ndomain = 'http://xm.esf.fang.com'\n\n\ndef getUrlDetails(url):\n    dhtml = requests.get(url).text\n    dsoup = BeautifulSoup(dhtml, 'html.parser')\n\n    info = {}\n    info['标题'] = dsoup.select('.title h1')[0] and dsoup.select(\n        '.title h1')[0].text.strip()\n    info['总价'] = dsoup.select('.tab-cont-right .price_esf')[0].text\n\n    for item in dsoup.select('.tab-cont-right .trl-item1'):\n        info[item.select('.font14')[0].text] = item.select(\n            '.tt')[0].text.strip()\n    info['地址'] = dsoup.select(\n        '.tab-cont-right .trl-item2 .rcont')[0].text.strip()[0:-2]\n    for item in dsoup.select('.zf_new_left .cont .text-item'):\n        st_split = item.text.strip().split('\\n')\n        while '' in st_split:\n            st_split.remove('')\n        while '\\r' in st_split:\n            st_split.remove('\\r')\n        if len(st_split) > 2:\n            st_split = [st_split[0]] + [''.join(st_split[1:])]\n        k, v = st_split\n        info[k] = v.strip()\n    return info\n\n\nif __name__ == '__main__':\n    soup = BeautifulSoup(html, 'html.parser')\n    resultArr = []\n    for house in soup.select('.shop_list dl'):\n        if house.select('h4 a'):\n            resUrl = domain + house.select('h4 a')[0]['href']\n            if getUrlDetails(resUrl):\n                resultArr.append(getUrlDetails(resUrl))\n\n    result = json.dumps(resultArr)\n    print('爬取完毕')\n    with open('house.json', 'w') as f:\n        f.write(result)\n    print('写入完毕')\n爬取拉勾网招聘信息 json格式\n# coding=utf-8\n\nimport json\nimport time\n\nimport requests\nimport xlwt\n\nurl = 'https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false'\n\n# 获取存储职位信息的json对象，遍历获得公司名、福利待遇、工作地点、学历要求、工作类型、发布时间、职位名称、薪资、工作年限\ndef getJobRow(url, datas):\n    time.sleep(10)\n    header = {\n        'Accept': 'application/json, text/javascript, */*; q=0.01',\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36',\n        'Host': 'www.lagou.com',\n        'Origin': 'https://www.lagou.com',\n        'Referer': 'https://www.lagou.com/jobs/list_?labelWords=&fromSearch=true&suginput='\n    }\n    cookie = {\n        'Cookie': 'JSESSIONID=ABAAABAAAIAACBI80DD5F5ACDEA0EB9CA0A1B926B8EAD3C; Hm_lvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1539844668; _ga=GA1.2.439735849.1539844668; _gid=GA1.2.491577759.1539844668; user_trace_token=20181018143747-53713f4a-d2a0-11e8-814e-525400f775ce; LGSID=20181018143747-53714082-d2a0-11e8-814e-525400f775ce; PRE_UTM=; PRE_HOST=; PRE_SITE=; PRE_LAND=https%3A%2F%2Fwww.lagou.com%2F; LGUID=20181018143747-53714251-d2a0-11e8-814e-525400f775ce; index_location_city=%E4%B8%8A%E6%B5%B7; TG-TRACK-CODE=index_search; Hm_lpvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1539844675; LGRID=20181018143754-57b25578-d2a0-11e8-bdc4-5254005c3644; SEARCH_ID=d0a97cea1d1d47d0afa41b3f298f41d5'\n    }\n    result_data = requests.post(url=url, cookies=cookie, headers=header, data=datas).json()\n    content = result_data['content']['positionResult']['result']\n\n    info_item = []\n    for i in content:\n        information = []\n        information.append(i['positionId']) # 岗位对应ID\n        information.append(i['companyFullName']) # 公司全名\n        information.append(i['companyLabelList']) # 福利待遇\n        information.append(i['district']) # 工作地点\n        information.append(i['education']) # 学历要求\n        information.append(i['firstType']) # 工作类型\n        information.append(i['formatCreateTime']) # 发布时间\n        information.append(i['positionName']) # 职位名称\n        information.append(i['salary']) # 薪资\n        information.append(i['workYear']) # 工作年限\n\n        info_item.append(information)\n    return info_item\n\ndef main():\n    city = input('请输入爬取的城市：')\n    page = int(input('请输入爬取的页数：'))\n    kd = input('请输入爬取关键词：')\n    info_result = []\n    title = ['岗位id', '公司全名', '福利待遇', '工作地点', '学历要求', '工作类型', '发布时间', '职位名称', '薪资', '工作年限']\n    info_result.append(title)\n\n    for x in range(1, page+1):\n        datas = {\n            'first': True,\n            'pn': x,\n            'kd': kd,\n            'city': city\n        }\n        info = getJobRow(url, datas)\n        info_result = info_result + info\n        print(info_result, 'info_result')\n\n        # 写入excel的数据格式组装成： [[表头数据], [row数据], [row], [row]]\n        workbook = xlwt.Workbook(encoding='utf-8')\n        worksheet = workbook.add_sheet('lagou' + kd, cell_overwrite_ok=True)\n\n        for i, row in enumerate(info_result):\n            # print row\n            for j, col in enumerate(row):\n                # print col\n                worksheet.write(i, j, col) # x,y 位置， col 内容\n\n        workbook.save('lagou' + kd + city + '.xls')\n\nif __name__ == '__main__':\n    main()\n\n数据清理\n\n数据处理\n资料分析\n诠释结果\n\n真正能用在数据分析的时间很少，必须要能够善用工具。\n资料分析：\n\n资料筛选\n侦测缺失值\n补齐缺失值\n资料转换\n处理时间格式数据\n重塑资料\n学习正规运算式\n\npandas处理资料：\n\n\nTable-like格式，\n提供高效能，简易使用的数据格式(Data Frame)让用户可以快速操作及分析资料。\n\npandas底层是numpy\n\n\nnumpy特点：\n\npython数学运算套件\nN维数组对象\n多种数学运算函数\n可整合C/C++和Fortran\n\n\n使用numpy产生结构化信息，有缺陷，而在numpy上完善的pandas，比较合理使用数据。\npandas增加序列Series结构:\n\n类似Array,List的一维物件\n每个Series都可以透过其索引进行存取\n预设Series会以0到Series长度作为索引编号\n\n数据处理\n资料筛选\n存取元素与切割：\ndf.ix[1] # 取一条记录\ndf.ix[1:4] # 取1~4条记录\ndf['name'] # 通过字段取数据\ndf[['name', 'age']] # 通过多个字段取数据，获取每条字段下的数据\n\ndf[1:2, ['name', 'age']] # 根据索引号与字段名筛选数据\n\ndf['gender'] == 'M' # 根据enum特点的值判断筛选数据，返回True 和 False\ndf[df['gender'] == 'M'] # 根据enum特点的值判断筛选数据， 整张表中符合的返回\n\ndf[(df['gender' == 'M']) & (df['age' >= 30])] # 使用 & 取条件交集\ndf[(df['gender' == 'M']) | (df['age' >= 30])] # 使用 | 取条件\n\ndf['employee'] = True # 新增字段\ndel df['employee'] # 删除字段\ndf = df.drop('empyloyee', axis=1) # 删除字段\n\ndf.loc[6] = {'age': 18, 'gender': 'F', 'name': 'aa'} # 新增一条记录\ndf.append(pd.DataFrame([{'age': 18, 'gender': 'F', 'name': 'aa'}]), ignore_index=True) # 新增记录\ndf = df.drop(6) # 删除某条记录\n\ndf['userid'] = range(101, 117) # 设定新的索引\ndf.set_index('userid', inplace=True) # 设定新索引\n\ndf.iloc[1] # 设定新的索引去获取数据\ndf.iloc[[1:3]] # 设定新的索引去获取数据\n获取值的三种方式\ndf.ix[[101, 102]] # 使用ix取值，useid\ndf.loc[[101, 105]] # 使用loc取值，useid\ndf.iloc[1, 2]  # 使用iloc取值，索引\n侦测缺失值\n\n数据中有特定或一个范围的值是不完全的\n缺失值可能会导致数据分析是产生偏误的推论\n\n缺失值可能来自机械的缺失（机械故障，导致数据无法被完整保存）或是人为的缺失（填写信息不完整或数据真假情况）\n\n占位：使用numpy中的numpy.nan占位表示缺失值\npd.DataFrame(['a', numpy.nan])\n检查序列是否有缺失值：\ndf['gender'].notnull() # 检查非缺失值数据\ndf['gender'].isnull() # 检查缺失值数据\n检查字段或Data Frame是否含有缺失值：\ndf.name.isnull().values.any() # 检查字段是否含有缺失值\n\ndf.isnull().values.any() # 检查DataFrame是否含有缺失值，返回True或False\n计算缺失值数量：\ndf.isnull().sum() # 检查字段缺失值的数量\ndf.isnull().sum().sum() # 计算所有缺失值的数量\n补齐缺失值\n\n舍弃缺失值：当缺失值占数据比例很低时。\n使用平均数，中位数，众数等叙述性统计补齐缺失值。\n使用内插法补齐缺失值：如果字段数据呈线性规律。\n\n舍弃缺失值:\ndf.dropna() # 舍弃含有任意缺失值的行\ndf.dropna(how='all') # 舍弃所有都含有缺失值的行，每个字段都是NaN\ndf.dropna(thresh=2) # 舍弃超过两栏缺失值的行\n\ndf['age'] = numpy.nan # 增加一列包含缺失值\ndf.dropna(axis=1, how='all') # 舍弃皆为缺失值的列\n填补缺失值：\ndf.fillna(0) # 用0填补缺失值\ndf['age'].fillna(df['age'].mean()) # 用平均数填补缺失值\ndf['age'].fillna(df.groupby('gender')['age'].transfrom('mean')) # 用各性别年龄平均填补缺失值\n\ndf.fillna(method='pad') # 向后填充缺失值\ndf.fillna(method='bfill', limit=2) # 向前填充缺失值\n维护处理不需要数据或者特殊的数为np.nan:\ndf.loc[df['物业费用'] == '暂无资料', '物业费用'] = np.nan # 修改“暂无资料”为\"np.nan\"\n查看前三行数据：df.head(3)查看后三行数据：df.tail(3)查看DataFrame信息: df.info()查看字段名称: df.columns查看字段类型：df.dtypes叙述性统计：df.describe()检查缺失值: df.isnull().any()缺失值统计: df.isnull().sum()缺失值在整体数据中的比例：df.isnull().sum() / df.count()对特殊字段进行筛选处理： df['volume'].value_counts()缺失值补齐：df['volume'].fillna(0)\n资料转换\n如何清洗，转换该数据？ 使用向量化计算\n计算新值：\ndf['总价'] * 10000 # 计算新价格\n使用物件计算新价格：\nimport numpy as np\nnp.sqrt(df['总价'])\n合并二个字段：\ndf['朝向'] + df['户型']\n计算需要的新值：\ndf['均价'] = df['总价'] * 1000 / df['建筑面积']\nmap: 将函数套用到字段（Series）上的每个元素\ndef removeDollar(e):\n  return e.split('万')[0]\ndf['总价'].map(removeDollar) # 移除“总价”字段中含有的\"万\"字符\n\ndf['总价'].map(lamdba e: e.split('万')[0]) # lamdba的写法\napply: 将函数套用到DataFrame上的行或列\ndf.apply(lambda e: e.max() - e.min(), axis=1) # axis=0（列）axis=1（行） 根据行还是列\napplyMap: 将函数套用到DataFrame上的每个元素\nimport numpy as np\ndf.applymap(lamdba e: np.nan if e == '暂无资料' else e) # 将所有暂无资料的元素替代成缺失值（NaN）\n\n'''\nlamdba e: np.nan if e == '暂无资料' else e\n\ndef convertNaN(e):\n    if e == '暂无资料':\n        return np.nan\n    else:\n        return e\n'''\n处理时间格式\n现在时间：\nfrom datetime import datetime\ncurrent_time =  datetime.now()\n将时间转换成字符串：\ncurrent_time.strftime('%Y-%m-%d')\n将字符串转为时间：\ndatetime.strptime('2018-08-17', '%Y-%m-%d')\n往前回溯一天：\nfrom datetime import timedelta\ncurrent_time - timedelta(day=1)\n往前回溯十天：\nfrom datetime import timedelta\nfor i in range(1, 10):\n    dt = current_time - timedelta(days=i)\n    print(dt.strftime('%Y-%m-%d')) # 取得多天的日期\n    \ncurrent_time - timedelta(day=10)    \n将datetime转换为UNIX timestamp:\nfrom time import mktime\nmktime(current_time.timetuple()) # 需转tuple\n将UNIX timestamp转换为datetime:\ndatetime.fromtimestamp(1538202783)\n在pandas转换时间：\nimport pandas as pd\ndf['日期'] = pd.to_datetime(df['日期'], format='%Y年%m月%d日') # 默认转换为`-` 2018-9-29\n资料重塑\n创建虚拟变量:\npandas.get_dummies(df['朝向']) # 建立虚拟变量\ndf = pandas.concat([df, pandas.get_dummies(df['朝向'])], axis=1) # 合并虚拟变量与原DataFrame，成为数据中的真实数据\ndf.drop(df['朝向'], axis=1) # 舍弃原有字段\n建立透视表pivot_table：\ndf2 = df.pivot_table(index='单价', columns='产权年限', values='参考均价', aggfunc=sum)\ndf2.head() # index，列名字，columns，字段名，values，函数执行后的数据\n# 可以使用 df2.T 可以透视表行列转换\n\ndf3 = df.pivot_table(index=['产权性质', '产权年限'], columns='日期', values='总价', aggfunc=sum)\ndf3.head()\n\n正则\n\n把数据通过正则出来。\n\n比对，对比。\n\n[] 或\n\n*?, +?, ??非贪婪模式(尽可能少的对比)\n\n通过字段名获取捕获到的数据\nm = re.match(r'(?P<first_name>\\w+) (?P<last_name>\\w+)', 'David Chiu')\nprint(m.group('first_name'), m.group('last_name'))\n\nstr1 = 'scp file.text root@10.0.0.1:./'\nm = re.search('^scp ([\\w\\.]+) (\\w+)@([\\w\\.]+):(.+)', str1)\nif m:\n    print(m.group(1), m.group(2), m.group(3), m.group(4))\n在DataFrame中使用正则：\ndf[['室', '厅', '卫']]  = df['户型'].str.extract(r'(\\d+)室(\\d+)厅(\\d+)卫', expand=False)\n# 室,厅,卫等信息\n爬取新浪新闻：\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup as bs\n\n\ndef getDetails(url, idx):\n    if idx > 5:\n        return\n    print(url, 'url')\n    res = requests.get(url)\n    res.encoding = 'utf-8'\n    d = bs(res.text, 'html.parser')\n\n    title = d.select('.main-title')[0].text\n    create_time = d.select('.date-source')[0].select('.date')[0].text\n    source = d.select('.date-source')[0].select('.source')[0].text\n    article = ' '.join(d.select('#article')[0].text.split())\n    keywords = d.select('#keywords')[0].text\n\n    return {\n        'title': title,\n        'create_time': create_time,\n        'source': source,\n        'article': article,\n        'keywords': keywords\n    }\n\n\nif __name__ == '__main__':\n    url = 'https://news.sina.com.cn/china/'\n    res = requests.get(url)\n    res.encoding = 'utf-8'\n    dsoup = bs(res.text, 'html.parser')\n    news_herf = [h['href']\n                 for h in dsoup.select('.left-content-1 div')[3].select('li a')]\n    newArr = []\n    resultArr = []\n    for idx, new in enumerate(news_herf):\n        t = getDetails(new, idx)\n        if t:\n            newArr.append(t)\n\n    df = pd.DataFrame(newArr)\n    df['keywords'] = df['keywords'].apply(lambda i: i.split(':')[1].split())\n    df['create_time'] = pd.to_datetime(df['create_time'], format=r'%Y年%m月%d日 %H:%M')\n    df = df[['title', 'source', 'create_time', 'keywords', 'article']] # 转换字段顺序\n    df.to_json('news.json')\n\n    print('ok')\n可视化数据\n叙述性统计\n有系统的归纳数据，了解数据的轮廓。对数据样本做叙述性，例如：平均数，标准偏差，计次频率，百分比对数据资料的图像化处理，将数据摘要变为图表经常更加偏重于叙述性统计处理可视化数据.\n\n多数资料分析，80%在于如何加总与平均\n\n用SQL做叙述性统计，分割数据，转换数据，聚合数据，探索数据。\nPyton类似的分析工具\n\n获取股价：pandas-datareader\nimport pandas_datareader as pdd\n\ndf = pdd.DataReader('BABA', data_source='yahoo')\nprint(df.tail())\n简易的统计单个字段：算出总和：df['volume'].sum()算出平均：df['volume'].mean()算出标准差：df['volume'].std()取得最小值：df['volume'].min()取得最大值：df['volume'].max()取得记录数：df['volume'].count()取得整体叙述性统计: df.describe()\nimport pandas_datareader as pdd\n\ndf = pdd.DataReader('BABA', data_source='yahoo')\n\n\n# 计算涨跌\ndf['diff'] = df['Close'] - df['Open']\ndf['rise'] = df['diff'] < 0\ndf['fall'] = df['diff'] > 0\n\n# 计算每日报酬\ndf['ret'] = df['Close'].pct_change(1)\n\nprint(df[['rise', 'fall']].sum()) # 计算涨跌次数\n# print(df[df.index > '2018-08-01'])\nprint(df.loc[df.index > '2018-08-01', ['rise', 'fall']].sum()) # 当月的涨跌次数\nprint(df.groupby([df.index.year, df.index.month])['rise', 'fall'].sum()) # 根据年月统计涨跌次数\nprint(df.groupby([df.index.year, df.index.month])['ret'].mean()) # 根据年月统计每月的报酬\n\n推论性统计\n资料模型的建构从样本推论整体资料的概况相关，回归，因素分析\n绘制图表\n人是视觉性的动物，百分之七十的接收数据通过眼睛，百分之三十的接收数据通过其它五官（嘴巴，鼻子，耳朵等）\n信息图表的功能\n\n沟通已知的信息(Storytelling)\n从资料中发现背后的事实(Exploration)\n\n信息可视化\n\n可视化目标：有效沟通清楚完整促进参与者的互动\n\n专注在传达的有效性\n可视化 + 互动 = 成功的可视化\n\npands绘制图表\n需要安装matplotlib模块\nimport pandas_datareader as pdd\n\ndf = pdd.DataReader('BABA', data_source='yahoo')\n\n# 绘制折线图\ndf['Close'].plot(kind='line', figsize=[10, 5], legend=True, title='BABA', grid=True)\n# lengend=True 图表\n# grid 表格\n\n# 绘制移动平均线\ndf['mvg30'] = df['Close'].rolling(window=30).mean()\ndf[['Close', 'mvg30']].plot(kind='line', legend=True, figsize=[10, 5])\n\n# 直方图\ndf.ix[df.index >= '2017-04-01', 'Volume'].plot(kind='bar', figsize[10, 5], title='BABA', legend=True)\n\n# 饼图\ndf['diff'] = df['Close'] - df['Open']\ndf['rise'] = df['diff'] > 0\ndf['fall'] = df['diff'] < 0\n\ndf[['rise', 'fall']].sum().plot(kind='pie', figsize[5,5], counterclock=Ture, startangle=90, legend=True)\n数据存入\n将数据以结构化方式做存储，让用户可以透明结构化查询语言（SQL），快速查询及维护数据。\nACID原则:\n\n不可分割性/原子性（Atomicity）: 交易必须全部完成或全部不完成。\n一致性（Consistency）: 交易开始到结束，数据完整性都符合既设规则与限制\n隔离性（Isolation）: 并行的交易不会影响彼此\n持久性（Durability）: 进行完交易后，对数据库的变更会永久保留在数据库\n\nsqlite3\n套件，组件。\nimport sqlite3 as lite\n\ncon = lite.connect('test.sqlite') # 连接\ncur = con.cursor() # 游标\ncur.execute('SELECT SQLITE_VERSION()') # 语句执行\ndata = cur.fetchone() # 获取一条row\n\nprint(data)\n\ncon.close()\n新增，查询：\nimport sqlite3 as lite\n\nwith lite.connect('test.sqlite') as con:\n    cur = con.cursor()\n    cur.execute('DROP TABLE IF EXISTS PhoneAddress')\n    cur.execute('CREATE TABLE PhoneAddress(phone CHAR(10) PRIMARY KEY, address TEXT, name TEXT unique, age INT NOT NULL)')\n    cur.execute('INSERT INTO PhoneAddress VALUES(\"245345345\", \"United\", \"Jsan\", 50)')\n    cur.execute('SELECT phone,address FROM PhoneAddress')\n    data = cur.fetchall()\n\n    for rec in data:\n        print(rec[0], rec[1])\nfetchone和fetchall获取数据根据游标cursor，来获取对应的数据。操作的逻辑建立在游标之上。\n使用Pandas存储数据\n\n建立DataFrame\n使用Pandas存储数据\n\nimport sqlite3 as lite\nimport pandas\n\nemployee = [{\n    'name': 'Mary',\n    'age': 24,\n    'gender': 'F'\n}]\ndf = pandas.DataFrame(employee)\n\nwith lite.connect('test.sqlite') as db:\n    cur = db.cursor()\n    df.to_sql(name='employee', index=False, con=db, if_exists='replace')\n    d = pandas.read_sql('SELECT * FROM employee', con=db) # 可以使用SQL语句(聚合查询，排序语句)读取数据，pandas转换成DataFrame格式\n    print(d)\n\n    cur.execute('SELECT * FROM employee')\n    data = cur.fetchone()\n    print(data)\n获取国家外汇管理局-人民币汇率中间价\n获取数据并处理数据:\nimport sqlite3 as lite\nfrom datetime import datetime, timedelta\n\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\n\nurl = 'http://www.safe.gov.cn/AppStructured/hlw/RMBQuery.do'\nua = 'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko Core/1.63.6726.400 QQBrowser/10.2.2265.400'\n\nheaders = {\n    'User-Agent': ua,\n    'Content-Type': 'text/html;charset=UTF-8'\n}\n\n\ndef getCurrency(start, end):\n    payload = {\n        'startDate': start,\n        'endDate': end,\n        'queryYN': True\n    }\n    html = requests.post(url, data=payload, headers=headers).text\n\n    soup = BeautifulSoup(html, 'html.parser')\n\n    dfs = pd.read_html(str(soup.select('#InfoTable')[0]), header=0)[0]  # 读取成DataFrame格式数据\n    # soup.select('#InfoTable')[0].prettify('UTF-8') 测试的时候出现中文乱码\n\n    dfs = pd.melt(dfs, col_level=0, id_vars='日期')\n    dfs.columns = ['date', 'currency', 'exchange']\n\n    with lite.connect('currency.sqlite') as db:\n        dfs.to_sql(name='currency', index=None, con=db, if_exists='append')\n\n        cur = db.cursor()\n        cur.execute('SELECT * FROM currency')\n        data = cur.fetchall()\n        print(len(data))\n\n\nif __name__ == '__main__':\n    current_time = datetime.now()\n\n    for i in range(1, 300, 30):\n        start_date = (current_time - timedelta(days=i+30)).strftime('%Y-%m-%d')\n        end_date = (current_time - timedelta(days=i+1)).strftime('%Y-%m-%d')\n        print(start_date, end_date)\n        getCurrency(start_date, end_date)\n\n展示图表数据：\nimport sqlite3 as lite\nimport pandas as pd\n\nwith lite.connect('currency.sqlite') as db:\n    df = pd.read_sql('SELECT * FROM currency', con=db)\n\n    df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n\n    df.index = df.date\n    print(df.head())\n    df.plot(kind='line', rot=30, color='blue')\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}