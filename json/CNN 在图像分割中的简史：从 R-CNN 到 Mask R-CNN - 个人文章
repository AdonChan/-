{"title": "CNN 在图像分割中的简史：从 R-CNN 到 Mask R-CNN - 个人文章 ", "index": "python", "content": "作者：chen_h微信号 & QQ：862251340微信公众号：coderpai简书地址：https://www.jianshu.com/p/867...\n\n自从 Alex Krizhevsky, Geoff Hinton, and Ilya Sutskever  成为了 ImageNet 2012 冠军之后，CNN 已经变成了图像分割的标配。实际上，从那时起，CNN 已经在 ImageNet 挑战上面战胜了人类。\n\n虽然这些分类结果令人印象深刻，但是比真实的人类视觉理解还是要简单很多。\n\n在分类中，通常我们会把图像中一个单一对象作为分类焦点，也就是说主要去识别焦点的物体（比如上面的狗狗）。但是，当我们环顾我们周围世界的时候，我们面对的是一个更加复杂的问题。\n\n我们看到场景是一个非常复杂的场景，而且是多个目标进行重叠的背景，我们不仅需要分类这些不同的对象，而且需要识别他们的边界和彼此的关联。\n\nCNN 可以帮助我们实现这样复杂的任务吗？也就是说，我们给出更复杂的图像，可以利用 CNN 来识别图像中的不同物体之间的边界吗？这个问题，在过去几年里，已经由 Ross Girshick 和他的同事向我们证明了，答案是肯定的。\n这篇文章的目标\n通过这篇文章，我们将介绍一些用于对象检测和分割的主要技术手段，并且了解他们是如何从上一个模型演变到下一个模型的。具体来说，我们将介绍 R-CNN（Regional CNN），一个最早利用CNN解决这个问题的模型，以及其后期的 Fast R-CNN 模型和 Faster R-CNN 模型。最后，我们将介绍 Mask R-CNN 模型，这个模型是由 Facebook Research 最近发布的一篇文章，这篇文章提供了像素级别的分割。以下是各个模型的文章：\n\nR-CNN: https://arxiv.org/abs/1311.2524\n\nFast R-CNN: https://arxiv.org/abs/1504.08083\n\nFaster R-CNN: https://arxiv.org/abs/1506.01497\n\nMask R-CNN: https://arxiv.org/abs/1703.06870\n\n\n2014: R-CNN - An Early Application of CNNs to Object Detection\n.](http://upload-images.jianshu....\n受 Hinton 实验室的启发，UCB 的 Jitendra Malik 的团队问了这样一个问题：\n对象检测到底能如何泛化？\n对象检测是找到图像中的不同对象并且进行分类的任务（如上图所示）。由 Ross Girshick，Jeff Donahue 和 Trevor Darrel 组成的团队发现这个问题可以用 Krizhevsky 的方法在 PASCAL VOC Challenge 上面进行实现，他们写道：\n本文首先显示，与基于 HOG 类特征的简单系统相比，CNN 可以显著提高 PASCAL VOC 上的对象检测性能。\n现在让我们来了解一下 R-CNN 的架构，以及它是如何工作的。\n理解 R-CNN\nR-CNN 模型的目标是根据拍摄的图像，正确识别图像中主要对象（通过边框）的位置。\n\n输入：image\n输出：物体边框 + 每个对象的标签\n\n但是我们如何找出这些边框的位置呢？R-CNN 的做法就是按照人类的直观理解来做的 —— 我们先从图像里把一些物体给框出来，然后来确定这个物体是什么对象。\n\nR-CNN 使用一种称为选择性搜索的技术来创建这些边界框，更多的细节你可以阅读这篇文章。更高层次来说，选择性搜索（如上图所示）是通过不同大小的窗口来查看图像，并且对于每个大小不同的窗口，尝试通过文理，颜色或强度将相邻像素分组在一起以识别对象。\n\n一旦这些边框确定之后，R-CNN 就会将该区域转变到一个标准的平方尺寸，并将其传送到 AlexNet 的一个变体中，如上图所示。\n在 CNN 的最后一层，R-CNN 添加了一个支持向量机（SVM），它简单的分类这是否是一个对象，也就是图中的第四步。\n改进边界框\n现在，我们已经获得了物体的大致边框，那么我们可以将这个边框缩小以适应物体本身真实的尺寸吗？答案是可以的，这是 R-CNN的最后一步。R-CNN 对区域进行一个简单的线性回归，以生成更紧密的边界框坐标以获得结果。以下是回归模型的输入和输出：\n\n\n输入：与物体对应图像的子区域。\n\n输出：子区域中对象的新边界框坐标。\n\n最后，总结一下 R-CNN 的几个步骤：\n\n给图像生成一组边界框。\n通过预先训练的 AlexNet 运行边框中的图像，最后通过 SVM 来进行分类。\n一旦对象被分类，边界框通过运行线性回归模型输出更加紧密的边框坐标。\n\n2015: Fast R-CNN - Speeding up and Simplifying R-CNN\n\nR-CNN 可以很好的工作，但是基于以下几个理由，它非常慢：‘\n\n’它需要 CNN（AlexNet）针对每个图像区域进行运行分类（每个图像大约 2000 次前向传递）。\n它需要分别训练三种不同的模型 —— CNN生成图像特征，SVM来预测分类，最后通过线性回归来收紧边界框。这样设计使得数据管道非常难设计。\n\n2015年，R-CNN 的第一作者 Ross Girshick 解决了这两个问题，也就诞生了第二个算法 —— Fast R-CNN。现在，让我们来看看它的主要思路：\nFast R-CNN Insight 1: RoI (Region of Interest) Pooling\n对于 CNN 的前向传播，Girshick 意识到，对于每个图像，图像的许多分割区域都是重叠的，这就使得我们一次又一次地运行相同的 CNN 计算（大约 2000 次）。他的想法很简单，就是让CNN运行一次图像，然后找到一种共享计算的方法，来区分这 2000 个区域。\n\n这正是 Fast R-CNN 被称之为 PolPool（Region of Interest Pooling）的核心技术，该技术能分享 CNN 在其次区域的前向传递。在上图中，请注意每个 CNN 特征图是从一个原来的大特征图中进行选取的。然后，区域中的特征都被进行合并（一般是采用最大池）。所以需要我们计算的是一个原始图片，而不是那个 2000 次区域。\nFast R-CNN Insight 2: Combine All Models into One Network\n\nFast R-CNN 的第二个改进是在单一模型中同时训练了 CNN，分类器和边界回归。早期的模型我们使用 CNN 来进行图像特征提取，SVM 来进行分类，最后用线性回归来进行边框收紧。Fast R-CNN 使用一个模型来同时达到这三个模型的效果。\n上图展示的就是这个联合模型。Fast R-CNN 用 CNN 顶部的 softmax 层来替代 SVM 分类器的输出分类。它还添加了与 softmax 层平行的线性回归层用来输出边界框坐标。这样，所有我们需要模型的输出都是来自于单一的网络。整个网络模型的输入和输出如下：\n\n\n输入：带有区域目的的图像。\n\n输出：每个区域的对象分类以及更紧密的边界框。\n\n2016: Faster R-CNN - Speeding Up Region Proposal\n即使取得了这些进步，Fast R-CNN 仍然存在一个瓶颈 —— 区域检测。正如我们所看到的，检测对象位置的第一步是产生一对潜在的边界框和区域进行测试。在 Fast R-CNN 中，这些边界框是采用选择性搜索创建的，这是一个相当缓慢的过程，被认为是整个流程额瓶颈。\n\n在2015年中期，一个微软研究员的团队 Shaoqing Ren，Kaiming He，Ross Girshick 和 Jian Sun，找到了一个方法来解决这个瓶颈问题，他们将这个方法命名为 Faster R-CNN。\nFaster R-CNN 的一个重大改进是，在 CNN 第一步分类的时候，后续步骤重用那些相同的 CNN 结构，而不是单独运行选择性搜索算法。\n\n的确，这正是 Faster R-CNN 团队所取得的成就。在上图中，你可以看到如何使用单个 CNN 来对区域进行处理和分类。这样，我们只需要一个 CNN 进行训练，而其他的区域都可以从这个 CNN 网络中获取，作者这样写道：\n我们的观测结果是，用于区域检查（如 Fast R-CNN）的卷积特征图也可以用于区域生成，从而实现几乎无成本的区域生成。\n一下是其模型的输入和输出：\n\n\n输入：图像（注意不需要区域提案）。\n\n输出：图像中对象的分类和边界框坐标。\n\n如何生成区域\n接下来，让我们来看看 Faster R-CNN 是如何从 CNN 特征图中来生成区域。Faster R-CNN 在网络上添加了一个完全卷积网络，创建了一个所谓的区域生成网络。\n\n区域生成网络通过在 CNN 特征图上面的滑动窗口，并在每个窗口中输出 k 个潜在的边界框和分数，以便预测哪些框是最好的。那么，k 个框是代表什么呢？\n\n直观地，我们知道图像中的对象应该符合某些常见的比例和大小。例如，我们知道我们想要的一些类似于人类形状的边框比例和大小。同样，我们知道我们看到的很多的盒子厚度都不会很薄。以这种方式，我们可以创建 k 个这样的盒子，我们称之为锚盒（anchor boxes）。对于每个这样的锚盒，我们在图像中输出一个边界框和每个位置的得分。\n考虑到这些锚盒，我们来看看这个区域生成网络的输入和输出：\n\n\n输入：CNN 特征图。\n\n输出：每个锚盒对应的边界框和该边界框中图像会是某个对象的可能性分数。\n\n然后，我们将每个这样的可能对象边界框传递到 Fast R-CNN 中，以生成分类结果和收紧边界框。\n2017: Mask R-CNN - Extending Faster R-CNN for Pixel Level Segmentation\n\n到目前为止，我们已经看到我们如何使用 CNN 特征图去有效地定位图像中不同对象的边界框。\n我们可以将这些技术进一步扩展，并定位每个对象的精确像素，而不是仅限于边框。这个像素级别的图像分割问题被 Kaiming He 等科学家解决，这个框架被称为 Mask R-CNN。\n\nFast R-CNN 和 Faster R-CNN 都能很好的工作，那么我们能不能将模型进行扩展，使得他们在像素级别进行分割？\n\nMask R-CNN 通过向 Faster R-CNN 添加一个分支来输出一个二进制掩码，说明给定像素是否是对象的一部分。如上所述，分支（在上图中的白色部分）仅仅是基于 CNN 特征图的完全卷积网络，一下是它的输入和输出：\n\n\n输入：CNN特征图\n\n输出：在像素属于对象的所有位置上标记为 1，其他地方标记为 0（这个称为二进制掩码）。\n\n但是 Mask R-CNN 的作者不得不对模型进行一个小的调整，使这个管道可以按预期工作。\nRoiAlign - Realigning RoIPool to be More Accurate\n\n我们需要对原始的 Faster R-CNN 架构进行修改，Mask R-CNN 的作者发现 RoIPool 选择的特征图的区域与原始图形的区域略有一点不对气。由于图像分割需要做到像素级，这与边框分割不同，所以必然导致不准确。\n作者可以通过巧妙的调整 RoIPool 网络来解决这个问题，调整好的模型被称为 RoIAlign。\n\n想象一下，我们有一个大小为 128128 的图像和大小为 2525 的特征图。让我们想象一下，我们想要的是与原始图像中左上角 15*15 像素对应的区域（见上文），那么我们如何从特征图中选取出这些像素呢?\n我们知道原始图像中的每个像素对应于特征图中大约 25/128 像素。要从原始图像中选择 15 像素，所以我们只是选择了 15*25/128 ~= 2.93 像素。\n在 RoIPool 中，我们会将小数点舍去，从而选择 2 个像素，这样就会导致轻微的错位。然而，在 RoIAlign 中，我们避免了这种四舍五入。相反的，我们使用双线性差值来精确地了解像素 2.93 中的所有内容。这在很大程度上让我们避免 RoIPool 所造成的错位。\n一旦这些掩码生成，Mask R-CNN 将它们与来自 Faster R-CNN 的分类和边界框组合起来，以产生如此奇妙的精确分割：\n\n代码\n如果你有兴趣自己尝试这些算法，这里有一些相关的库：\nFaster R-CNN\n\nCaffe: https://github.com/rbgirshick/py-faster-rcnn\n\nPyTorch: https://github.com/longcw/faster_rcnn_pytorch\n\nMatLab: https://github.com/ShaoqingRen/faster_rcnn\n\n\nMask R-CNN\n\nPyTorch: https://github.com/felixgwu/mask_rcnn_pytorch\n\nTensorFlow: https://github.com/CharlesShang/FastMaskRCNN\n\n\n\n作者：chen_h微信号 & QQ：862251340简书地址：https://www.jianshu.com/p/867...\nCoderPai 是一个专注于算法实战的平台，从基础的算法到人工智能算法都有设计。如果你对算法实战感兴趣，请快快关注我们吧。加入AI实战微信群，AI实战QQ群，ACM算法微信群，ACM算法QQ群。长按或者扫描如下二维码，关注 “CoderPai” 微信号（coderpai）\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "2"}