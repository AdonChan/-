{"title": "Python爬虫基础 - VoidKing ", "index": "网页爬虫,python", "content": "前言\nPython非常适合用来开发网页爬虫，理由如下：1、抓取网页本身的接口相比与其他静态编程语言，如java，c#，c++，python抓取网页文档的接口更简洁；相比其他动态脚本语言，如perl，shell，python的urllib包提供了较为完整的访问网页文档的API。（当然ruby也是很好的选择）此外，抓取网页有时候需要模拟浏览器的行为，很多网站对于生硬的爬虫抓取都是封杀的。这是我们需要模拟user agent的行为构造合适的请求，譬如模拟用户登陆、模拟session/cookie的存储和设置。在python里都有非常优秀的第三方包帮你搞定，如Requests，mechanize\n2、网页抓取后的处理抓取的网页通常需要处理，比如过滤html标签，提取文本等。python的beautifulsoap提供了简洁的文档处理功能，能用极短的代码完成大部分文档的处理。其实以上功能很多语言和工具都能做，但是用python能够干得最快，最干净。\nLife is short, you need python.\nPS：python2.x和python3.x有很大不同，本文只讨论python3.x的爬虫实现方法。\n<!--more-->\n爬虫架构\n架构组成\n\nURL管理器：管理待爬取的url集合和已爬取的url集合，传送待爬取的url给网页下载器。网页下载器（urllib）：爬取url对应的网页，存储成字符串，传送给网页解析器。网页解析器（BeautifulSoup）：解析出有价值的数据，存储下来，同时补充url到URL管理器。\n运行流程\n\nURL管理器\n基本功能\n\n添加新的url到待爬取url集合中。\n判断待添加的url是否在容器中（包括待爬取url集合和已爬取url集合）。\n获取待爬取的url。\n判断是否有待爬取的url。\n将爬取完成的url从待爬取url集合移动到已爬取url集合。\n\n存储方式\n1、内存（python内存）待爬取url集合：set()已爬取url集合：set()\n2、关系数据库（mysql）urls(url, is_crawled)\n3、缓存（redis）待爬取url集合：set已爬取url集合：set\n大型互联网公司，由于缓存数据库的高性能，一般把url存储在缓存数据库中。小型公司，一般把url存储在内存中，如果想要永久存储，则存储到关系数据库中。\n网页下载器（urllib）\n将url对应的网页下载到本地，存储成一个文件或字符串。\n基本方法\n新建baidu.py，内容如下：\nimport urllib.request\n\nresponse = urllib.request.urlopen('http://www.baidu.com')\nbuff = response.read()\nhtml = buff.decode(\"utf8\")\nprint(html)\n命令行中执行python baidu.py，则可以打印出获取到的页面。\n构造Request\n上面的代码，可以修改为：\nimport urllib.request\n\nrequest = urllib.request.Request('http://www.baidu.com')\nresponse = urllib.request.urlopen(request)\nbuff = response.read()\nhtml = buff.decode(\"utf8\")\nprint(html)\n携带参数\n新建baidu2.py，内容如下：\nimport urllib.request\nimport urllib.parse\n\nurl = 'http://www.baidu.com'\nvalues = {'name': 'voidking','language': 'Python'}\ndata = urllib.parse.urlencode(values).encode(encoding='utf-8',errors='ignore')\nheaders = { 'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0' }\nrequest = urllib.request.Request(url=url, data=data,headers=headers,method='GET')\nresponse = urllib.request.urlopen(request)\nbuff = response.read()\nhtml = buff.decode(\"utf8\")\nprint(html)\n使用Fiddler监听数据\n我们想要查看一下，我们的请求是否真的携带了参数，所以需要使用fiddler。打开fiddler之后，却意外发现，上面的代码会报错504，无论是baidu.py还是baidu2.py。虽然python有报错，但是在fiddler中，我们可以看到请求信息，确实携带了参数。\n经过查找资料，发现python以前版本的Request都不支持代理环境下访问https。但是，最近的版本应该支持了才对。那么，最简单的办法，就是换一个使用http协议的url来爬取，比如，换成http://www.csdn.net。结果，依然报错，只不过变成了400错误。\n然而，然而，然而。。。神转折出现了！！！当我把url换成http://www.csdn.net/后，请求成功！没错，就是在网址后面多加了一个斜杠/。同理，把http://www.baidu.com改成http://www.baidu.com/，请求也成功了！神奇！！！\n添加处理器\n\nimport urllib.request\nimport http.cookiejar\n\n# 创建cookie容器\ncj = http.cookiejar.CookieJar()\n# 创建opener\nopener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj))\n# 给urllib.request安装opener\nurllib.request.install_opener(opener)\n\n# 请求\nrequest = urllib.request.Request('http://www.baidu.com/')\nresponse = urllib.request.urlopen(request)\nbuff = response.read()\nhtml = buff.decode(\"utf8\")\nprint(html)\nprint(cj)\n网页解析器（BeautifulSoup）\n从网页中提取出有价值的数据和新的url列表。\n解析器选择\n为了实现解析器，可以选择使用正则表达式、html.parser、BeautifulSoup、lxml等，这里我们选择BeautifulSoup。其中，正则表达式基于模糊匹配，而另外三种则是基于DOM结构化解析。\nBeautifulSoup\n安装测试\n1、安装，在命令行下执行pip install beautifulsoup4。2、测试\nimport bs4\nprint(bs4)\n使用说明\n\n基本用法\n1、创建BeautifulSoup对象\nimport bs4\nfrom bs4 import BeautifulSoup\n\n# 根据html网页字符串创建BeautifulSoup对象\nhtml_doc = \"\"\"\n<html><head><title>The Dormouse's story</title></head>\n<body>\n<p class=\"title\"><b>The Dormouse's story</b></p>\n\n<p class=\"story\">Once upon a time there were three little sisters; and their names were\n<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\nand they lived at the bottom of a well.</p>\n\n<p class=\"story\">...</p>\n\"\"\"\nsoup = BeautifulSoup(html_doc)\nprint(soup.prettify())\n2、访问节点\nprint(soup.title)\nprint(soup.title.name)\nprint(soup.title.string)\nprint(soup.title.parent.name)\n\nprint(soup.p)\nprint(soup.p['class'])\n3、指定tag、class或id\nprint(soup.find_all('a'))\nprint(soup.find('a'))\nprint(soup.find(class_='title'))\nprint(soup.find(id=\"link3\"))\nprint(soup.find('p',class_='title'))\n4、从文档中找到所有<a>标签的链接\nfor link in soup.find_all('a'):\n    print(link.get('href'))\n出现了警告，根据提示，我们在创建BeautifulSoup对象时，指定解析器即可。\nsoup = BeautifulSoup(html_doc,'html.parser')\n5、从文档中获取所有文字内容\nprint(soup.get_text())\n6、正则匹配\nlink_node = soup.find('a',href=re.compile(r\"til\"))\nprint(link_node)\n后记\npython爬虫基础知识，至此足够，接下来，在实战中学习更高级的知识。\n书签\nPython开发简单爬虫http://www.imooc.com/learn/563\nThe Python Standard Libraryhttps://docs.python.org/3/lib...\nBeautiful Soup 4.2.0 文档https://www.crummy.com/softwa...\n为什么python适合写爬虫？http://www.cnblogs.com/benzon...\n如何学习Python爬虫[入门篇]？https://zhuanlan.zhihu.com/p/...\n你需要这些：Python3.x爬虫学习资料整理https://zhuanlan.zhihu.com/p/...\n如何入门 Python 爬虫？https://www.zhihu.com/questio...\nPython3.X 抓取网络资源http://www.open-open.com/lib/...\npython网络请求和\"HTTP Error 504:Fiddler - Receive Failure\"http://blog.csdn.net/guoguo52...\n怎么使用Fiddler抓取自己写的爬虫的包？https://www.zhihu.com/questio...\nfiddler对python脚本抓取https包时发生了错误?https://www.zhihu.com/questio...\nHTTPS和HTTP的区别http://blog.csdn.net/whatday/...\n\n                ", "mainLikeNum": ["8 "], "mainBookmarkNum": "45"}