{"title": " Kaggle入门级赛题：房价预测——数据挖掘篇 - 个人文章 ", "index": "数据挖掘,python", "content": "特征工程\n我们注意到 MSSubClass 其实是一个 category 的值：\nall_df['MSSubClass'].dtypes\n\n有：\ndtype('int64')\n它不应该做为数值型的值进行统计。因此，进行强制类型转换，把它变回 string：\ndf['MSSubClass'] =df['MSSubClass'].astype(str) \n然后，统计其出现频次：\nall_df['MSSubClass'].value_counts()\n\n\n就很清楚的了解 MSSubClass 特征了。\n当我们用 numerical 来表达 categorical 的时候要注意，数字本身有大小的含义，所以乱用数字会给之后的模型学习带来麻烦。这里我们可以用 One-Hot 的方法来表达 category。\npandas 自带的get_dummies方法，可以做到一键 One-Hot：\npd.get_dummies(df['MSSubClass'], prefix='MSSubClass').head()\n效果如下：\n此时，MSSubClass 被我们分成了 12 列，每列代表一个 category，是为 1，否为 0。\n所以，同理。接下来，我们需要把所有的 category 数据全部一键 One-Hot：\nall_dummy_df = pd.get_dummies(df)\nall_dummy_df.head()\n此时，数据长这样子：\n\n接下来，我们来处理 numerical 的数据。\n首先查看  One-Hot 后的缺失值：\nall_dummy_df.isnull().sum().sort_values(ascending=False).head(10)\n\n我们需要对这些缺失值进行处理，这里采用平均值来填充空缺：\nmean_cols = all_dummy_df.mean()\nall_dummy_df = all_dummy_df.fillna(mean_cols)\n再次查看是否有缺失值：\nall_dummy_df.isnull().sum().sum()\n显示为 0，即缺失值都已被填充。\n到这里，我们经过以上步骤处理过的数据，就可以喂给分类器进行训练了。为了让数据更加规整化，数据间的差距不要太大，在一个标准分布内，也就是数据平滑化。我们对那些本来就是 numerical 的数据进行处理（与 One-Hot 的 0/1 数据不同）。\n首先，我们来查看哪些是 numerical 的数据：\nnumeric_cols = df.columns[df.dtypes != 'object']\nnumeric_cols\n\n采用公式(X-X')/s，计算标准分布：\nnumeric_col_means = all_dummy_df.loc[:, numeric_cols].mean()\nnumeric_col_std = all_dummy_df.loc[:, numeric_cols].std()\nall_dummy_df.loc[:, numeric_cols] = (all_dummy_df.loc[:, numeric_cols] - numeric_col_means) / numeric_col_std\n得到的数据如下：\nall_dummy_df.head()\n\n以上就完成了对数据的处理。\n建立模型\n首先，把数据集分回训练集和测试集：\ndummy_train_df = all_dummy_df.loc[train_df.index]\ndummy_test_df = all_dummy_df.loc[test_df.index]\n首先采用 Ridge Regression 模型，因为对于多因子的数据集，这种模型可以方便的把所有的变量都一股脑的放进去，我们先用这种模型做实验。\n为了更好的使用 Sklearn，我在这里把 DataFrame 转化成 Numpy Array（这一步不是必须）：\nX_train = dummy_train_df.values\nX_test = dummy_test_df.values\n把数据放到模型里跑一遍，用 Sklearn 自带的 cross validation 方法来测试模型：\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\n\nalphas = np.logspace(-3, 2, 50)\ntest_scores=[]\nfor alpha in alphas:\n    clf = Ridge(alpha)\n    test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=10, scoring='neg_mean_squared_error'))\n    test_scores.append(np.mean(test_score))\n存下所有的 CV 值，看看哪个 alpha 值更好，也就是我们常说的调参数：\nplt.plot(alphas, test_scores)\nplt.title(\"Alpha vs CV Error\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"CV Error\")\n\n可以看到，当 alpha 为 10 到 20 的时候，CV Error 达到最低 0.135 左右。也就是说大约 alpha = 15 的时候给了我们最好的结果。\n一般来说，单个分类器的效果有限。我们会倾向于把多个分类器合在一起，做一个“综合分类器”以达到最好的效果。所以接下来我们要做的事就是 ensemble。\nEnsemble 的方法有 Bagging 和 Boosting 两大类。Bagging 把很多的小分类器放在一起，每个训练随机的一部分数据，然后采用多数投票制把它们的最终结果综合起来。Boosting 比 Bagging 理论上更高级点，它也是揽来一把的分类器。但是把他们线性排列。下一个分类器把上一个分类器分类得不好的地方加上更高的权重，这样下一个分类器就能在这个部分学得更加“深刻”。下面我们分别来看一下。\nBagging\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.model_selection import cross_val_score\n\nparams = [1, 10, 15, 20, 25, 30, 40]\ntest_scores = []\nfor param in params:\n    clf = BaggingRegressor(n_estimators=param, base_estimator=ridge)\n    test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=10, scoring='neg_mean_squared_error'))\n    test_scores.append(np.mean(test_score))\n    \nplt.plot(params, test_scores)\nplt.title(\"n_estimator vs CV Error\")\n\n可以看到，我们用 15 个小的 ridge 分类器就达到了 0.134 以下的效果。\nBoosting\nfrom sklearn.ensemble import AdaBoostRegressor\n\nparams = [10, 15, 20, 25, 30, 35, 40, 45, 50]\ntest_scores = []\nfor param in params:\n    clf = BaggingRegressor(n_estimators=param, base_estimator=ridge)\n    test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=10, scoring='neg_mean_squared_error'))\n    test_scores.append(np.mean(test_score))\n    \nplt.plot(params, test_scores)\nplt.title(\"n_estimator vs CV Error\");\n\n20 个小的 ridge 分类器的效果，达到了 0.133。\n最后，祭出 xgboost 大杀器：\nfrom xgboost import XGBRegressor\n\nparams = [1,2,3,4,5,6]\ntest_scores = []\nfor param in params:\n    clf = XGBRegressor(max_depth=param)\n    test_score = np.sqrt(-cross_val_score(clf, X_train, y_train, cv=10, scoring='neg_mean_squared_error'))\n    test_scores.append(np.mean(test_score))\n\nplt.plot(params, test_scores)\nplt.title(\"max_depth vs CV Error\")\n\n我们看到，当参数为 5 的时候，效果接近 0.125！\n提交结果\n最后，我们将训练好的模型对数据进行训练：\nxgb = XGBRegressor(max_depth=5)\nxgb.fit(X_train, y_train)\ny_xgb = np.expm1(xgb.predict(X_test))\nsubmission_df = pd.DataFrame(data= {'Id' : test_df.index, 'SalePrice': y_xgb})\n\n最终，我们输出的数据长这样子：\nsubmission_df.head()\n\n\n将它存为.csv文件：\nsubmission_df.to_csv('submission_xgb.csv',index=False)\n提交到 kaggle 平台的 Score 是 0.13942，排名在 50% 左右。整个过程没有对特征信息进行太多的处理，还有太多需要改进的地方。\n后记\n第一次完完整整的从头到尾自己做了一个比赛，还是有太多地方浅浅略过，确实，如果只是调参跑模型的话，应该不是难事，但是如何获得更好的效果，数据量大时现有的程序跑不动，需要改进算法等方面，还有太多值得学习的地方，因为这件事好像没有一个最优结果，只有更优的结果。\n\n不足之处，欢迎指正。\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "1"}