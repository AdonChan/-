{"title": "结巴分词原理 源码 ", "index": "分词,python,机器学习,nlp,结巴分词", "content": "介绍\n结巴分词是一个受大家喜爱的分词库，源码地址为github，今天我们就跟进源码，看一下结巴分词的原理\n原理\n    def cut(self, sentence, cut_all=False, HMM=True):\n        '''\n        The main function that segments an entire sentence that contains\n        Chinese characters into separated words.\n\n        Parameter:\n            - sentence: The str(unicode) to be segmented.\n            - cut_all: Model type. True for full pattern, False for accurate pattern.\n            - HMM: Whether to use the Hidden Markov Model.\n        '''\n使用结巴分词的时候，有三种模式，这三种模式的进入条件分别为：\n        if cut_all:\n            cut_block = self.__cut_all\n        elif HMM:\n            cut_block = self.__cut_DAG\n        else:\n            cut_block = self.__cut_DAG_NO_HMM\n首先我们看一下这三种模式\n\n\n__cut_all:\n\n原句：我来到北京清华大学　结果：我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n原句：他来到了网易杭研大厦　结果：他/ 来到/ 了/ 网易/ 杭/ 研/ 大厦\n\n\n\n__cut_DAG:\n\n原句：我来到北京清华大学　结果:我/ 来到/ 北京/ 清华大学\n原句：他来到了网易杭研大厦 结果：他/ 来到/ 了/ 网易/ 杭研/ 大厦\n\n\n\n__cut_DAG_NO_HMM:\n\n原句：我来到北京清华大学　结果:我/ 来到/ 北京/ 清华大学\n原句：他来到了网易杭研大厦 结果：他/ 来到/ 了/ 网易/ 杭/ 研/ 大厦\n\n\n\n下面我们就来分析一下这三种模式：这三种模式有一个共同点，第一步都是先构造DAG，也就是构造有向无环图。源码如下：\n    def get_DAG(self, sentence):\n        self.check_initialized()\n        DAG = {}\n        N = len(sentence)\n        for k in xrange(N):\n            tmplist = []\n            i = k\n            frag = sentence[k]\n            while i < N and frag in self.FREQ:\n                if self.FREQ[frag]:\n                    tmplist.append(i)\n                i += 1\n                frag = sentence[k:i + 1]\n            if not tmplist:\n                tmplist.append(k)\n            DAG[k] = tmplist\n        return DAG\n如果sentence是'我来到北京清华大学‘，那么DAG为\n{0: [0], 1: [1, 2], 2: [2], 3: [3, 4], 4: [4], 5: [5, 6, 8], 6: [6, 7], 7: [7, 8], 8: [8]}\n直观上来看，DAG[5]=[5,6,8]的意思就是，以’清‘开头的话，分别以5、6、8结束时，可以是一个词语，即’清‘、’清华‘、’清华大学‘get_DAG方法中，最重要的也就是self.FREQ了，它是怎么来的呢？\n其实就是通过jieba目录下，dict.txt文件来产生的self.FREQ,方法如下：dict.txt共有349046行，每一行格式为：\n一 217830 m\n一一 1670 m\n一一二 11 m\n一一例 3 m\n一一分 8 m\n一一列举 34 i\n第一部分为词语，第二部分为该词出现的频率，第三部分为该词的词性。以读取’一一列举‘为例子，首先执行self.FREQ['一一列举']=34，然后会检查’一‘、’一一‘、’一一列‘、’一一列举‘之前是否在self.FREQ中存储过，如果之前存储过，则跳过，否则执行self.FREQ['一']=0，self.FREQ['一一']=0，self.FREQ['一一列']=0所以self.FREQ中不止存储了正常的词语和它出现的次数，同时也存储了所有词语的前缀，并将前缀出现的次数设置为0,以和正常词语区别开。\n好了，现在DAG这部分我们介绍完了，然后我们分开来介绍一下这三种模式：\n__cut_all\n源码如下：\n    def __cut_all(self, sentence):\n        dag = self.get_DAG(sentence)\n        old_j = -1\n        for k, L in iteritems(dag):\n            if len(L) == 1 and k > old_j:\n                yield sentence[k:L[0] + 1]\n                old_j = L[0]\n            else:\n                for j in L:\n                    if j > k:\n                        yield sentence[k:j + 1]\n                        old_j = j\n这个具体的遍历方式我们就不细说了，大家自行看源码吧\n__cut_DAG\n    def __cut_DAG(self, sentence):\n        DAG = self.get_DAG(sentence)\n        route = {}\n        self.calc(sentence, DAG, route)\n        ......\n首先我们先看一下self.calc方法\n    def calc(self, sentence, DAG, route):\n        N = len(sentence)\n        route[N] = (0, 0)\n        logtotal = log(self.total)\n        for idx in xrange(N - 1, -1, -1):\n            route[idx] = max((log(self.FREQ.get(sentence[idx:x + 1]) or 1) -\n                              logtotal + route[x + 1][0], x) for x in DAG[idx])\n这里使用了一个技巧，也就是log(a) + log(b) = log(ab)，从而巧妙的避过了乘法，也就避免了溢出的风险。其实calc函数就是实现了vertibi算法，不了解vertibi算法的同学自行百度吧。\n然后再贴上整个__cut_DAG的源码：\n    def __cut_DAG(self, sentence):\n        DAG = self.get_DAG(sentence)\n        route = {}\n        self.calc(sentence, DAG, route)\n        x = 0\n        buf = ''\n        N = len(sentence)\n        while x < N:\n            y = route[x][1] + 1\n            l_word = sentence[x:y]\n            if y - x == 1:\n                buf += l_word\n            else:\n                if buf:\n                    if len(buf) == 1:\n                        yield buf\n                        buf = ''\n                    else:\n                        if not self.FREQ.get(buf):\n                            recognized = finalseg.cut(buf)\n                            for t in recognized:\n                                yield t\n                        else:\n                            for elem in buf:\n                                yield elem\n                        buf = ''\n                yield l_word\n            x = y\n\n        if buf:\n            if len(buf) == 1:\n                yield buf\n            elif not self.FREQ.get(buf):\n                recognized = finalseg.cut(buf)\n                for t in recognized:\n                    yield t\n            else:\n                for elem in buf:\n                    yield elem\n其中，重点关注这一部分\n                        if not self.FREQ.get(buf):\n                            recognized = finalseg.cut(buf)\n                            for t in recognized:\n                                yield t\n什么时候会进入finalseg.cut(buf)呢？实际上，就是当遇到一些dict.txt中没出现的词的时候，才会进入这个函数：在这个函数中，就是使用HMM的方法，对这些未识别成功的词进行标注，然后我们来介绍一下项目中相关的内容：\n其中，prob_start.py存储的是HMM的起始状态相关的信息，文件中的数字都经过log处理过：\nP={'B': -0.26268660809250016,\n 'E': -3.14e+100,\n 'M': -3.14e+100,\n 'S': -1.4652633398537678}\nB代表begin，E代表end，M代表middle，S代表single。所以在开始时，HMM的状态只可能是S或者B，而E和M为负无穷prob_trans.py存储的是状态转移矩阵：\nP={'B': {'E': -0.510825623765990, 'M': -0.916290731874155},\n 'E': {'B': -0.5897149736854513, 'S': -0.8085250474669937},\n 'M': {'E': -0.33344856811948514, 'M': -1.2603623820268226},\n 'S': {'B': -0.7211965654669841, 'S': -0.6658631448798212}}\nprob_emit.py中存储的是在该状态下出现该汉字的概率，例如p('刘'|S)=-0.916\nP={'B': {'\\u4e00': -3.6544978750449433,\n       '\\u4e01': -8.125041941842026,\n       '\\u4e03': -7.817392401429855,\n       '\\u4e07': -6.3096425804013165,\n       '\\u4e08': -8.866689067453933,\n       '\\u4e09': -5.932085850549891,\n       '\\u4e0a': -5.739552583325728,\n       '\\u4e0b': -5.997089097239644,\n       '\\u4e0d': -4.274262055936421,\n       '\\u4e0e': -8.355569307500769,\n       ......\n通过这种方式，也就可以进行分词了。‘我/ 来到/ 北京/ 清华大学’对应的状态应该为'SBEBEBMME'\n__cut_DAG_NO_HMM\n其实__cut_DAG_NO_HMM和__cut_DAG的区别就是：对vertibi未成功切分的部分，__cut_DAG_NO_HMM没有使用HMM进行分词。源码如下：\n    def __cut_DAG_NO_HMM(self, sentence):\n        DAG = self.get_DAG(sentence)\n        route = {}\n        self.calc(sentence, DAG, route)\n        x = 0\n        N = len(sentence)\n        buf = ''\n        while x < N:\n            y = route[x][1] + 1\n            l_word = sentence[x:y]\n            if re_eng.match(l_word) and len(l_word) == 1:\n                buf += l_word\n                x = y\n            else:\n                if buf:\n                    yield buf\n                    buf = ''\n                yield l_word\n                x = y\n        if buf:\n            yield buf\n            buf = ''\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}