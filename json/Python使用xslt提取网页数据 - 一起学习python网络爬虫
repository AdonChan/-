{"title": "Python使用xslt提取网页数据 - 一起学习python网络爬虫 ", "index": "python", "content": "\n1，引言\n在Python网络爬虫内容提取器一文我们详细讲解了核心部件：可插拔的内容提取器类gsExtractor。本文记录了确定gsExtractor的技术路线过程中所做的编程实验。这是第一部分，实验了用xslt方式一次性提取静态网页内容并转换成xml格式。\n2，用lxml库实现网页内容提取\nlxml是python的一个库，可以迅速、灵活地处理 XML。它支持 XML Path Language (XPath) 和 Extensible Stylesheet Language Transformation (XSLT)，并且实现了常见的 ElementTree API。\n这2天测试了在python中通过xslt来提取网页内容，记录如下：\n2.1，抓取目标\n假设要提取集搜客官网旧版论坛的帖子标题和回复数，如下图，要把整个列表提取出来，存成xml格式\n\n2.2，源代码1：只抓当前页，结果显示在控制台\nPython的优势是用很少量代码就能解决一个问题，请注意下面的代码看起来很长，其实python函数调用没有几个，大篇幅被一个xslt脚本占去了，在这段代码中，只是一个好长的字符串而已，至于为什么选择xslt，而不是离散的xpath或者让人挠头的正则表达式，请参看《Python即时网络爬虫项目启动说明》，我们期望通过这个架构，把程序员的时间节省下来一大半。\n可以拷贝运行下面的代码(在windows10， python3.2下测试通过)：\nfrom urllib import request\nfrom lxml import etree\nurl=\"http://www.gooseeker.com/cn/forum/7\"\nconn = request.urlopen(url)\n\ndoc = etree.HTML(conn.read())\n\nxslt_root = etree.XML(\"\"\"\\\n<xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" >\n<xsl:template match=\"/\">\n<列表>\n<xsl:apply-templates select=\"//*[@id='forum' and count(./table/tbody/tr[position()>=1 and count(.//*[@class='topic']/a/text())>0])>0]\" mode=\"列表\"/>\n</列表>\n</xsl:template>\n\n\n\n<xsl:template match=\"table/tbody/tr[position()>=1]\" mode=\"list\">\n<item>\n<标题>\n<xsl:value-of select=\"*//*[@class='topic']/a/text()\"/>\n<xsl:value-of select=\"*[@class='topic']/a/text()\"/>\n<xsl:if test=\"@class='topic'\">\n<xsl:value-of select=\"a/text()\"/>\n</xsl:if>\n</标题>\n<回复数>\n<xsl:value-of select=\"*//*[@class='replies']/text()\"/>\n<xsl:value-of select=\"*[@class='replies']/text()\"/>\n<xsl:if test=\"@class='replies'\">\n<xsl:value-of select=\"text()\"/>\n</xsl:if>\n</回复数>\n</item>\n</xsl:template>\n\n<xsl:template match=\"//*[@id='forum' and count(./table/tbody/tr[position()>=1 and count(.//*[@class='topic']/a/text())>0])>0]\" mode=\"列表\">\n<item>\n<list>\n<xsl:apply-templates select=\"table/tbody/tr[position()>=1]\" mode=\"list\"/>\n</list>\n</item>\n</xsl:template>\n</xsl:stylesheet>\"\"\")\n\ntransform = etree.XSLT(xslt_root)\nresult_tree = transform(doc)\nprint(result_tree)\n\n2.3，抓取结果\n得到的抓取结果如下图：\n2.4，源代码2：翻页抓取，结果存入文件\n我们对2.2的代码再做进一步修改，增加翻页抓取和存结果文件功能，代码如下：\nfrom urllib import request\nfrom lxml import etree\nimport time\n\nxslt_root = etree.XML(\"\"\"\\\n<xsl:stylesheet version=\"1.0\" xmlns:xsl=\"http://www.w3.org/1999/XSL/Transform\" >\n<xsl:template match=\"/\">\n<列表>\n<xsl:apply-templates select=\"//*[@id='forum' and count(./table/tbody/tr[position()>=1 and count(.//*[@class='topic']/a/text())>0])>0]\" mode=\"列表\"/>\n</列表>\n</xsl:template>\n\n\n\n<xsl:template match=\"table/tbody/tr[position()>=1]\" mode=\"list\">\n<item>\n<标题>\n<xsl:value-of select=\"*//*[@class='topic']/a/text()\"/>\n<xsl:value-of select=\"*[@class='topic']/a/text()\"/>\n<xsl:if test=\"@class='topic'\">\n<xsl:value-of select=\"a/text()\"/>\n</xsl:if>\n</标题>\n<回复数>\n<xsl:value-of select=\"*//*[@class='replies']/text()\"/>\n<xsl:value-of select=\"*[@class='replies']/text()\"/>\n<xsl:if test=\"@class='replies'\">\n<xsl:value-of select=\"text()\"/>\n</xsl:if>\n</回复数>\n</item>\n</xsl:template>\n\n<xsl:template match=\"//*[@id='forum' and count(./table/tbody/tr[position()>=1 and count(.//*[@class='topic']/a/text())>0])>0]\" mode=\"列表\">\n<item>\n<list>\n<xsl:apply-templates select=\"table/tbody/tr[position()>=1]\" mode=\"list\"/>\n</list>\n</item>\n</xsl:template>\n</xsl:stylesheet>\"\"\")\n\nbaseurl = \"http://www.gooseeker.com/cn/forum/7\"\nbasefilebegin = \"jsk_bbs_\"\nbasefileend = \".xml\"\ncount = 1\nwhile (count < 12):\n        url = baseurl + \"?page=\" + str(count)\n        conn = request.urlopen(url)\n        doc = etree.HTML(conn.read())\n        transform = etree.XSLT(xslt_root)\n        result_tree = transform(doc)\n        print(str(result_tree))\n        file_obj = open(basefilebegin+str(count)+basefileend,'w',encoding='UTF-8')\n        file_obj.write(str(result_tree))\n        file_obj.close()\n        count += 1\n        time.sleep(2)\n\n我们增加了写文件的代码，还增加了一个循环，构造每个翻页的网址，但是，如果翻页过程中网址总是不变怎么办？其实这就是动态网页内容，下面会讨论这个问题。\n3，总结\n这是开源Python通用爬虫项目的验证过程，在一个爬虫框架里面，其它部分都容易做成通用的，就是网页内容提取和转换成结构化的操作难于通用，我们称之为提取器。但是，借助GooSeeker可视化提取规则生成器MS谋数台 ，提取器的生成过程将变得很便捷，而且可以标准化插入，从而实现通用爬虫，在后续的文章中会专门讲解MS谋数台与Python配合的具体方法。\n4，接下来阅读\n本文介绍的方法通常用来抓取静态网页内容，也就是所谓的html文档中的内容，目前很多网站内容是用javascript动态生成的，一开始html是没有这些内容的，通过后加载方式添加进来，那么就需要采用动态技术，请阅读《Python爬虫使用Selenium+PhantomJS抓取Ajax和动态HTML内容》。\n5，集搜客GooSeeker开源代码下载源\n1.GooSeeker开源Python网络爬虫GitHub源\n6，文档修改历史\n2016-05-26：V2.0，增补文字说明；把跟帖的代码补充了进来2016-05-29：V2.1，增加最后一章源代码下载源\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "14"}