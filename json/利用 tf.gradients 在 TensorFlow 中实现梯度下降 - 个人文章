{"title": "利用 tf.gradients 在 TensorFlow 中实现梯度下降 - 个人文章 ", "index": "python", "content": "作者：chen_h微信号 & QQ：862251340微信公众号：coderpai简书地址：http://www.jianshu.com/p/13e0...\n\n我喜欢 TensorFlow 的其中一个原因是它可以自动的计算函数的梯度。我们只需要设计我们的函数，然后去调用 tf.gradients 函数就可以了。是不是非常简单。\n接下来让我们来举个例子，具体说明一下。\n使用 TensorFlow 内置的优化器对 MNIST 数据集进行 softmax 回归\n在使用 tf.gradients 实现梯度下降之前，我们先尝试使用 TensorFlow 的内置优化器（比如 GradientDescentOptimizer）来解决MNIST数据集分类问题。\nimport tensorflow as tf\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 10\nbatch_size = 100\ndisplay_step = 1\n\n\n# tf Graph Input\nx = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\ny = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n\n# Set model weights\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\n\n# Construct model\npred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n\n# Minimize error using cross entropy\ncost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n\n# Start training\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    # Training cycle\n    for epoch in range(training_epochs):\n        avg_cost = 0.\n        total_batch = int(mnist.train.num_examples/batch_size)\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n            # Fit training using batch data\n            _, c = sess.run([optimizer, cost], feed_dict={x: batch_xs,\n                                                       y: batch_ys})\n            \n#             print(__w)\n            \n            # Compute average loss\n            avg_cost += c / total_batch\n        # Display logs per epoch step\n        if (epoch+1) % display_step == 0:\n#             print(sess.run(W))\n            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n\n    print (\"Optimization Finished!\")\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    # Calculate accuracy for 3000 examples\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print (\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))\n    \n    \n#### Output\n    \n# Extracting /tmp/data/train-images-idx3-ubyte.gz\n# Extracting /tmp/data/train-labels-idx1-ubyte.gz\n# Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n# Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n# Epoch: 0001 cost= 1.184285608\n# Epoch: 0002 cost= 0.665428013\n# Epoch: 0003 cost= 0.552858426\n# Epoch: 0004 cost= 0.498728328\n# Epoch: 0005 cost= 0.465593693\n# Epoch: 0006 cost= 0.442609185\n# Epoch: 0007 cost= 0.425552949\n# Epoch: 0008 cost= 0.412188290\n# Epoch: 0009 cost= 0.401390140\n# Epoch: 0010 cost= 0.392354651\n# Optimization Finished!\n# Accuracy: 0.873333\n所以，我们在这里做的是利用内置的优化器来计算损失值。如果我们想自己计算渐变过程和更新权重，那应该怎么办？这就是 tf.gradients 的作用了。\n使用 tf.gradients 对MNIST数据集进行 softmax 回归\n通过梯度下降公式，权重的更新方式如下：\n\n为了实现梯度下降，我将不使用优化器的代码，而是采用自己写的权重更新。\n因为这里有权重矩阵 w 和偏差项矩阵 b，所以我们需要去计算这些矩阵的梯度。所以实现的代码如下：\n# Computing the gradient of cost with respect to W and b\ngrad_W, grad_b = tf.gradients(xs=[W, b], ys=cost)\n\n# Gradient Step\nnew_W = W.assign(W - learning_rate * grad_W)\nnew_b = b.assign(b - learning_rate * grad_b)\n这三行代码只是替代前面的一行代码，干嘛给自己造成这么大的麻烦呢？因为如果你需要自己的损失函数的梯度，并且你不想编写严格的数学函数，那么 TensorFlow 就可以帮助你了。\n我们已经构建好了计算图，所以接下来我们只需要在会话中运行这个计算图就行了。让我来试试吧。\n# Fit training using batch data\n            _, _,  c = sess.run([new_W, new_b ,cost], feed_dict={x: batch_xs, y: batch_ys})\n我们不需要 new_W 和 new_b 的输出，所以我忽略了这些变量。\n完整代码如下：\nimport tensorflow as tf\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 10\nbatch_size = 100\ndisplay_step = 1\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 10\nbatch_size = 100\ndisplay_step = 1\n\n# tf Graph Input\nx = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\ny = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n\n# Set model weights\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\n\n# Construct model\npred = tf.nn.softmax(tf.matmul(x, W) + b) # Softmax\n\n# Minimize error using cross entropy\ncost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n\ngrad_W, grad_b = tf.gradients(xs=[W, b], ys=cost)\n\n\nnew_W = W.assign(W - learning_rate * grad_W)\nnew_b = b.assign(b - learning_rate * grad_b)\n\n# Initialize the variables (i.e. assign their default value)\ninit = tf.global_variables_initializer()\n\n# Start training\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # Training cycle\n    for epoch in range(training_epochs):\n        avg_cost = 0.\n        total_batch = int(mnist.train.num_examples/batch_size)\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n            # Fit training using batch data\n            _, _,  c = sess.run([new_W, new_b ,cost], feed_dict={x: batch_xs,\n                                                       y: batch_ys})\n            \n            # Compute average loss\n            avg_cost += c / total_batch\n        # Display logs per epoch step\n        if (epoch+1) % display_step == 0:\n#             print(sess.run(W))\n            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n\n    print (\"Optimization Finished!\")\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    # Calculate accuracy for 3000 examples\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print (\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))\n    \n    \n# Output\n# Epoch: 0001 cost= 1.183741399\n# Epoch: 0002 cost= 0.665312284\n# Epoch: 0003 cost= 0.552796521\n# Epoch: 0004 cost= 0.498697014\n# Epoch: 0005 cost= 0.465521633\n# Epoch: 0006 cost= 0.442611256\n# Epoch: 0007 cost= 0.425528946\n# Epoch: 0008 cost= 0.412203073\n# Epoch: 0009 cost= 0.401364554\n# Epoch: 0010 cost= 0.392398663\n# Optimization Finished!\n# Accuracy: 0.874\n使用梯度公式的 softmax 回归\n我们对于权重 w 的梯度处理如下：\n\n如前所示，不使用 tf.gradients 或使用 TensorFlow 的内置优化器，这样可以实现梯度方程。完整代码如下：\nimport tensorflow as tf\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 10\nbatch_size = 100\ndisplay_step = 1\n\n# Parameters\nlearning_rate = 0.01\ntraining_epochs = 10\nbatch_size = 100\ndisplay_step = 1\n\n# tf Graph Input\nx = tf.placeholder(tf.float32, [None, 784]) # mnist data image of shape 28*28=784\ny = tf.placeholder(tf.float32, [None, 10]) # 0-9 digits recognition => 10 classes\n\n# Set model weights\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\n\n# Construct model\npred = tf.nn.softmax(tf.matmul(x, W)) # Softmax\n\n# Minimize error using cross entropy\ncost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))\n\n\nW_grad =  - tf.matmul ( tf.transpose(x) , y - pred) \nb_grad = - tf.reduce_mean( tf.matmul(tf.transpose(x), y - pred), reduction_indices=0)\n\nnew_W = W.assign(W - learning_rate * W_grad)\nnew_b = b.assign(b - learning_rate * b_grad)\n\ninit = tf.global_variables_initializer()\n\n\nwith tf.Session() as sess:\n    sess.run(init)\n\n    # Training cycle\n    for epoch in range(training_epochs):\n        avg_cost = 0.\n        total_batch = int(mnist.train.num_examples/batch_size)\n        # Loop over all batches\n        for i in range(total_batch):\n            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n            # Fit training using batch data\n            _, _, c = sess.run([new_W, new_b, cost], feed_dict={x: batch_xs, y: batch_ys})\n            \n        \n            # Compute average loss\n            avg_cost += c / total_batch\n        # Display logs per epoch step\n        if (epoch+1) % display_step == 0:\n            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n\n    print (\"Optimization Finished!\")\n\n    # Test model\n    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    # Calculate accuracy for 3000 examples\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    print (\"Accuracy:\", accuracy.eval({x: mnist.test.images[:3000], y: mnist.test.labels[:3000]}))\n    \n    \n# Output\n# Extracting /tmp/data/train-images-idx3-ubyte.gz\n# Extracting /tmp/data/train-labels-idx1-ubyte.gz\n# Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n# Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n# Epoch: 0001 cost= 0.432943137\n# Epoch: 0002 cost= 0.330031527\n# Epoch: 0003 cost= 0.313661941\n# Epoch: 0004 cost= 0.306443773\n# Epoch: 0005 cost= 0.300219418\n# Epoch: 0006 cost= 0.298976618\n# Epoch: 0007 cost= 0.293222957\n# Epoch: 0008 cost= 0.291407861\n# Epoch: 0009 cost= 0.288372261\n# Epoch: 0010 cost= 0.286749691\n# Optimization Finished!\n# Accuracy: 0.898\nTensorflow 是如何计算梯度的？\n\n你可以在思考，TensorFlow是如何计算函数的梯度？\nTensorFlow 使用的是一种称为 Automatic Differentiation 的方法，具体你可以查看 Wikipedia。\n我希望这篇文章对你有帮会帮助。\n\n算法直播课：请点击这里\n\n作者：chen_h微信号 & QQ：862251340简书地址：http://www.jianshu.com/p/13e0...\nCoderPai 是一个专注于算法实战的平台，从基础的算法到人工智能算法都有设计。如果你对算法实战感兴趣，请快快关注我们吧。加入AI实战微信群，AI实战QQ群，ACM算法微信群，ACM算法QQ群。长按或者扫描如下二维码，关注 “CoderPai” 微信号（coderpai）\n\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}