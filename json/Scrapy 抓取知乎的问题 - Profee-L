{"title": "Scrapy 抓取知乎的问题 - Profee-L ", "index": "scrapy,python", "content": "\n从如何评价X的话题下开始抓取问题，然后开始爬相关问题再循环\n对于每个问题抓取 标题，关注人数，回答数等数据\n\nzhihuTopicSpider.py\n# -*- coding: utf-8 -*-\n\nimport scrapy\nimport os\nimport time\nimport re\nimport json\n\nfrom ..items import zhihuQuestionItem\n\n# mode 1:tencent   2:free\nmode = 2\nproxy = \"https://web-proxy.oa.com:8080\" if mode == 1 else \"\"\n\n# 设置 用户名和密码\nemail = \"youremail\"\npassword = \"yourpassword\"\n\n\nclass zhihu_topicSpider(scrapy.Spider):\n    name = 'zhihu_topicSpider'\n    zhihu_url = \"https://www.zhihu.com\"\n    login_url = \"https://www.zhihu.com/login/email\"\n    topic = \"https://www.zhihu.com/topic\"\n    domain = \"https://www.zhihu.com\"\n\n    # 设置 Headers\n    headers_dict = {\n        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n        \"Accept-Language\": \"zh-CN,zh;q=0.8\",\n        \"Connection\": \"keep-alive\",\n        \"Host\": \"www.zhihu.com\",\n        \"Upgrade-Insecure-Requests\": \"1\",\n        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.109 Safari/537.36\"\n    }\n\n    def start_requests(self):\n        yield scrapy.Request(\n            url=self.zhihu_url,\n            headers=self.headers_dict,\n            meta={\n                \"proxy\": proxy,\n                \"cookiejar\": 1\n            },\n            callback=self.request_captcha\n        )\n\n    def request_captcha(self, response):\n        # 获取_xsrf值\n        _xsrf = response.css('input[name=\"_xsrf\"]::attr(value)').extract()[0]\n        # 获得验证码的地址\n        captcha_url = \"http://www.zhihu.com/captcha.gif?r=\" + str(time.time() * 1000)\n        # 准备下载验证码\n        # 获取请求\n        yield scrapy.Request(\n            url=captcha_url,\n            headers=self.headers_dict,\n            meta={\n                \"proxy\": proxy,\n                \"cookiejar\": response.meta[\"cookiejar\"],\n                \"_xsrf\": _xsrf\n            },\n            callback=self.download_captcha\n        )\n\n    def download_captcha(self, response):\n        # 下载验证码\n        with open(\"captcha.gif\", \"wb\") as fp:\n            fp.write(response.body)\n        # 打开验证码\n        os.system('open captcha.gif')\n        # 输入验证码\n        print \"请输入验证码:\\n\"\n        captcha = raw_input()\n        # 输入账号和密码\n        yield scrapy.FormRequest(\n            url=self.login_url,\n            headers=self.headers_dict,\n            formdata={\n                \"email\": email,\n                \"password\": password,\n                \"_xsrf\": response.meta[\"_xsrf\"],\n                \"remember_me\": \"true\",\n                \"captcha\": captcha\n            },\n            meta={\n                \"proxy\": proxy,\n                \"cookiejar\": response.meta[\"cookiejar\"],\n            },\n            callback=self.request_zhihu\n        )\n\n    def request_zhihu(self, response):\n        \"\"\"\n            现在已经登录,请求www.zhihu.com的页面\n        \"\"\"\n        yield scrapy.Request(url=self.topic + '/19760570',\n                             headers=self.headers_dict,\n                             meta={\n                                 \"proxy\": proxy,\n                                 \"cookiejar\": response.meta[\"cookiejar\"],\n                             },\n                             callback=self.get_topic_question,\n                             dont_filter=True)\n\n    def get_topic_question(self, response):\n        # with open(\"topic.html\", \"wb\") as fp:\n        #     fp.write(response.body)\n        # 获得话题下的question的url\n        question_urls = response.css(\".question_link[target=_blank]::attr(href)\").extract()\n        length = len(question_urls)\n        k = -1\n        j = 0\n        temp = []\n        for j in range(length/3):\n            temp.append(question_urls[k+3])\n            j+=1\n            k+=3\n        for url in temp:\n            yield scrapy.Request(url = self.zhihu_url+url,\n                    headers = self.headers_dict,\n                    meta = {\n                        \"proxy\": proxy,\n                        \"cookiejar\": response.meta[\"cookiejar\"],\n                    },\n                    callback = self.parse_question_data)\n\n    def parse_question_data(self, response):\n        item = zhihuQuestionItem()\n        item[\"qid\"] = re.search('\\d+',response.url).group()\n        item[\"title\"] = response.css(\".zm-item-title::text\").extract()[0].strip()\n        item[\"answers_num\"] = response.css(\"h3::attr(data-num)\").extract()[0]\n        question_nums = response.css(\".zm-side-section-inner .zg-gray-normal strong::text\").extract()\n        item[\"followers_num\"] = question_nums[0]\n        item[\"visitsCount\"] = question_nums[1]\n        item[\"topic_views\"] = question_nums[2]\n        topic_tags = response.css(\".zm-item-tag::text\").extract()\n        if len(topic_tags) >= 3:\n            item[\"topic_tag0\"] = topic_tags[0].strip()\n            item[\"topic_tag1\"] = topic_tags[1].strip()\n            item[\"topic_tag2\"] = topic_tags[2].strip()\n        elif len(topic_tags) == 2:\n            item[\"topic_tag0\"] = topic_tags[0].strip()\n            item[\"topic_tag1\"] = topic_tags[1].strip()\n            item[\"topic_tag2\"] = '-'\n        elif len(topic_tags) == 1:\n            item[\"topic_tag0\"] = topic_tags[0].strip()\n            item[\"topic_tag1\"] = '-'\n            item[\"topic_tag2\"] = '-'\n        # print type(item[\"title\"])\n        question_links = response.css(\".question_link::attr(href)\").extract()\n        yield item\n        for url in question_links:\n            yield scrapy.Request(url = self.zhihu_url+url,\n                    headers = self.headers_dict,\n                    meta = {\n                        \"proxy\": proxy,\n                        \"cookiejar\": response.meta[\"cookiejar\"],\n                    },\n                    callback = self.parse_question_data)\n\n\npipelines.py\n# -*- coding: utf-8 -*-\n\n# Define your item pipelines here\n#\n# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html\n\n# import json\nimport MySQLdb\n\n# class JsonDumpPipeline(object):\n#     def process_item(self, item, spider):\n#         with open('d.json', 'a') as fp:\n#             fp.write(json.dumps(dict(item), ensure_ascii = False).encode(\"utf-8\") + '\\n')\n\n\n\nclass MySQLPipeline(object):\n    print \"\\n\\n\\n\\n\\n\\n\\n\\n\"\n    sql_questions = (\n            \"INSERT INTO questions(\"\n            \"qid, title, answers_num, followers_num, visitsCount, topic_views, topic_tag0, topic_tag1, topic_tag2) \"\n            \"VALUES ('%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s')\")\n    count = 0\n\n    def open_spider(self, spider):\n        host = \"localhost\"\n        user = \"root\"\n        password = \"wangqi\"\n        dbname = \"zh\"\n        self.conn = MySQLdb.connect(host, user, password, dbname)\n        self.cursor = self.conn.cursor()\n        self.conn.set_character_set('utf8')\n        self.cursor.execute('SET NAMES utf8;')\n        self.cursor.execute('SET CHARACTER SET utf8;')\n        self.cursor.execute('SET character_set_connection=utf8;')\n        print \"\\n\\nMYSQL DB CURSOR INIT SUCCESS!!\\n\\n\"\n        sql = (\n            \"CREATE TABLE IF NOT EXISTS questions (\"\n                \"qid VARCHAR (100) NOT NULL,\"\n                \"title varchar(100),\"\n                \"answers_num INT(11),\"\n                \"followers_num INT(11) NOT NULL,\"\n                \"visitsCount INT(11),\"\n                \"topic_views INT(11),\"\n                \"topic_tag0 VARCHAR (600),\"\n                \"topic_tag1 VARCHAR (600),\"\n                \"topic_tag2 VARCHAR (600),\"\n                \"PRIMARY KEY (qid)\"\n            \")\")\n        self.cursor.execute(sql)\n        print \"\\n\\nTABLES ARE READY!\\n\\n\"\n\n    def process_item(self, item, spider):\n        sql = self.sql_questions % (item[\"qid\"], item[\"title\"], item[\"answers_num\"],item[\"followers_num\"],\n                                item[\"visitsCount\"], item[\"topic_views\"], item[\"topic_tag0\"], item[\"topic_tag1\"], item[\"topic_tag2\"])\n        self.cursor.execute(sql)\n        if self.count % 10 == 0:\n            self.conn.commit()\n        self.count += 1\n        print item[\"qid\"] + \" DATA COLLECTED!\"\nitems.py\n# -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# http://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\nfrom scrapy import Field\n\n\nclass zhihuQuestionItem(scrapy.Item):\n    qid = Field()\n    title = Field()\n    followers_num = Field()\n    answers_num = Field()\n    visitsCount = Field()\n    topic_views = Field()\n    topic_tag0 = Field()\n    topic_tag1 = Field()\n    topic_tag2 = Field()\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "3"}