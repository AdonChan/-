{"title": "python爬虫——写出最简单的网页爬虫 - 数据分析师 ", "index": "python", "content": "\n知识就像碎布，记得“缝一缝”，你才能华丽丽地亮相\n\n最近对python爬虫有了强烈地兴趣，在此分享自己的学习路径，欢迎大家提出建议。我们相互交流，共同进步。\n1.开发工具\n笔者使用的工具是sublime text3，它的短小精悍（可能男人们都不喜欢这个词）使我十分着迷。推荐大家使用，当然如果你的电脑配置不错，pycharm可能更加适合你。\nsublime text3搭建python开发环境推荐查看此博客：\n[sublime搭建python开发环境][http://www.cnblogs.com/codefish/p/4806849.html]\n\n2.爬虫介绍\n爬虫顾名思义，就是像虫子一样，爬在Internet这张大网上。如此，我们便可以获取自己想要的东西。\n既然要爬在Internet上，那么我们就需要了解URL，法号“统一资源定位器”，小名“链接”。其结构主要由三部分组成：\n（1）协议：如我们在网址中常见的HTTP协议。\n（2）域名或者IP地址：域名，如：www.baidu.com，IP地址，即将域名解析后对应的IP。\n（3）路径：即目录或者文件等。\n3.urllib开发最简单的爬虫\n（1）urllib简介\n\n\nModule\nIntroduce\n\n\n\nurllib.error\nException classes raised by urllib.request.\n\n\nurllib.parse\nParse URLs into or assemble them from components.\n\n\nurllib.request\nExtensible library for opening URLs.\n\n\nurllib.response\nResponse classes used by urllib.\n\n\nurllib.robotparser\nLoad a robots.txt file and answer questions about fetchability of other URLs.\n\n\n\n（2）开发最简单的爬虫\n百度首页简洁大方，很适合我们爬虫。\n爬虫代码如下：\n\n\nfrom urllib import request\n\ndef visit_baidu():\n    URL = \"http://www.baidu.com\"\n    # open the URL\n    req = request.urlopen(URL)\n    # read the URL \n    html = req.read()\n    # decode the URL to utf-8\n    html = html.decode(\"utf_8\")\n    print(html)\n\nif __name__ == '__main__':\n    visit_baidu()\n结果如下图：\n\n\n我们可以通过在百度首页空白处右击，查看审查元素来和我们的运行结果对比。\n当然，request也可以生成一个request对象，这个对象可以用urlopen方法打开。\n代码如下：\n\nfrom urllib import request\n\ndef vists_baidu():\n    # create a request obkect\n    req = request.Request('http://www.baidu.com')\n    # open the request object\n    response = request.urlopen(req)\n    # read the response \n    html = response.read()\n    html = html.decode('utf-8')\n    print(html)\n\nif __name__ == '__main__':\n    vists_baidu()\n运行结果和刚才相同。\n（3）错误处理\n错误处理通过urllib模块来处理，主要有URLError和HTTPError错误，其中HTTPError错误是URLError错误的子类，即HTTRPError也可以通过URLError捕获。\nHTTPError可以通过其code属性来捕获。\n处理HTTPError的代码如下：\nfrom urllib import request\nfrom urllib import error\n\ndef Err():\n    url = \"https://segmentfault.com/zzz\"\n    req = request.Request(url)\n\n    try:\n        response = request.urlopen(req)\n        html = response.read().decode(\"utf-8\")\n        print(html)\n    except error.HTTPError as e:\n        print(e.code)\nif __name__ == '__main__':\n    Err()\n运行结果如图：\n\n404为打印出的错误代码，关于此详细信息大家可以自行百度。\n\nURLError可以通过其reason属性来捕获。\nchuliHTTPError的代码如下：\nfrom urllib import request\nfrom urllib import error\n\ndef Err():\n    url = \"https://segmentf.com/\"\n    req = request.Request(url)\n\n    try:\n        response = request.urlopen(req)\n        html = response.read().decode(\"utf-8\")\n        print(html)\n    except error.URLError as e:\n        print(e.reason)\nif __name__ == '__main__':\n    Err()\n运行结果如图：\n\n\n既然为了处理错误，那么最好两个错误都写入代码中，毕竟越细致越清晰。须注意的是，HTTPError是URLError的子类，所以一定要将HTTPError放在URLError的前面，否则都会输出URLError的，如将404输出为Not Found。\n代码如下：\nfrom urllib import request\nfrom urllib import error\n\n# 第一种方法，URLErroe和HTTPError\ndef Err():\n    url = \"https://segmentfault.com/zzz\"\n    req = request.Request(url)\n\n    try:\n        response = request.urlopen(req)\n        html = response.read().decode(\"utf-8\")\n        print(html)\n    except error.HTTPError as e:\n        print(e.code)\n    except error.URLError as e:\n        print(e.reason)\n大家可以更改url来查看各种错误的输出形式。\n\n\n\n\n新人初来乍到不容易，如果您觉得有那么一丢丢好的话，请不要吝啬您的赞赏~撒花。\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "13"}