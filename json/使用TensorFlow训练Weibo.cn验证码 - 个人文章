{"title": "使用TensorFlow训练Weibo.cn验证码 - 个人文章 ", "index": "python,tensorflow,机器学习", "content": "最近在抽时间学习TensorFlow这个DL库的使用，学的断断续续的，看官网上第一个案例就是训练手写字符识别。我之前在做Weibo.cn验证码识别的时候，自己搞了一个数据集，当时用的c++库tiny-dnn进行训练的(见：验证码破解技术四部曲之使用卷积神经网络（四）)，现在我把它移植到TensorFlow上试试。\n完整代码见：weibo.cn/tensorflow-impl\n使用的库\n\nTensorFlow-1.0\nscikit-learn-0.18\npillow\n\n加载数据集\n数据集下载地址：training_set.zip  \n解压过后如下图：  \n我把同一类的图片放到了一个文件夹里，文件夹的名字也就是图片的label，打开文件夹后可以看到字符的图片信息。  \n下面，我们把数据加载到一个pickle文件里面，它需要有train_dataset、train_labels、test_dataset、test_labels四个变量代表训练集和测试集的数据和标签。\n此外，还需要有个label_map，用来把训练的标签和实际的标签对应，比如说3对应字母M，4对应字母N。\n此部分的代码见：load_models.py。注：很多的代码参考自udacity的deeplearning课程。\n首先根据文件夹的来加载所有的数据，index代表训练里的标签，label代表实际的标签，使用PIL读取图片，并转换成numpy数组。\nimport numpy as np\nimport os\nfrom PIL import Image\n\ndef load_dataset():\n    dataset = []\n    labelset = []\n    label_map = {}\n\n    base_dir = \"../trainer/training_set/\"  # 数据集的位置\n    labels = os.listdir(base_dir)\n\n    for index, label in enumerate(labels):\n        if label == \"ERROR\" or label == \".DS_Store\":\n            continue\n        print \"loading:\", label, \"index:\", index\n        try:\n            image_files = os.listdir(base_dir + label)\n            for image_file in image_files:\n                image_path = base_dir + label + \"/\" + image_file\n                im = Image.open(image_path).convert('L')\n                dataset.append(np.asarray(im, dtype=np.float32))\n                labelset.append(index)\n            label_map[index] = label\n        except: pass\n\n    return np.array(dataset), np.array(labelset), label_map\n\n\ndataset, labelset, label_map = load_dataset()\n接下来，把数据打乱。\ndef randomize(dataset, labels):\n    permutation = np.random.permutation(labels.shape[0])\n    shuffled_dataset = dataset[permutation, :, :]\n    shuffled_labels = labels[permutation]\n    return shuffled_dataset, shuffled_labels\n\n\ndataset, labelset = randomize(dataset, labelset)\n然后使用scikit-learn的函数，把训练集和测试集分开。\nfrom sklearn.model_selection import train_test_split\ntrain_dataset, test_dataset, train_labels, test_labels = train_test_split(dataset, labelset)\n在TensorFlow官网给的例子中，会把label进行One-Hot Encoding，并把28*28的图片转换成了一维向量(784)。如下图，查看官网例子的模型。\n我也把数据转换了一下，把32*32的图片转换成一维向量(1024)，并对标签进行One-Hot Encoding。\ndef reformat(dataset, labels, image_size, num_labels):\n    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n    # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n    labels = (np.arange(num_labels) == labels[:, None]).astype(np.float32)\n    return dataset, labels\n\ntrain_dataset, train_labels = reformat(train_dataset, train_labels, 32, len(label_map))\ntest_dataset, test_labels = reformat(test_dataset, test_labels, 32, len(label_map))\nprint \"train_dataset:\", train_dataset.shape\nprint \"train_labels:\", train_labels.shape\nprint \"test_dataset:\", test_dataset.shape\nprint \"test_labels:\", test_labels.shape\n转换后，格式就和minist一样了。\n最后，把数据保存到save.pickle里面。\nsave = {\n    'train_dataset': train_dataset,\n    'train_labels': train_labels,\n    'test_dataset': test_dataset,\n    'test_labels': test_labels,\n    'label_map': label_map\n}\nwith open(\"save.pickle\", 'wb') as f:\n    pickle.dump(save, f)\n验证数据集加载是否正确\n加载完数据后，需要验证一下数据是否正确。我选择的方法很简单，就是把trainset的第1个(或者第2个、第n个)图片打开，看看它的标签和看到的能不能对上。\nimport cPickle as pickle\nfrom PIL import Image\nimport numpy as np\n\ndef check_dataset(dataset, labels, label_map, index):\n    data = np.uint8(dataset[index]).reshape((32, 32))\n    i = np.argwhere(labels[index] == 1)[0][0]\n    im = Image.fromarray(data)\n    im.show()\n    print \"label:\", label_map[i]\n\nif __name__ == '__main__':\n    with open(\"save.pickle\", 'rb') as f:\n        save = pickle.load(f)\n        train_dataset = save['train_dataset']\n        train_labels = save['train_labels']\n        test_dataset = save['test_dataset']\n        test_labels = save['test_labels']\n        label_map = save['label_map']\n\n    # check if the image is corresponding to it's label\n    check_dataset(train_dataset, train_labels, label_map, 0)\n运行后，可以看到第一张图片是Y，标签也是正确的。\n训练\n数据加载好了之后，就可以开始训练了，训练的网络就使用TensorFlow官网在Deep MNIST for Experts里提供的就好了。\n此部分的代码见：train.py。\n先加载一下模型:\nimport cPickle as pickle\nimport numpy as np\nimport tensorflow as tf\n\nwith open(\"save.pickle\", 'rb') as f:\n    save = pickle.load(f)\n    train_dataset = save['train_dataset']\n    train_labels = save['train_labels']\n    test_dataset = save['test_dataset']\n    test_labels = save['test_labels']\n    label_map = save['label_map']\n\nimage_size = 32\nnum_labels = len(label_map)\n\nprint \"train_dataset:\", train_dataset.shape\nprint \"train_labels:\", train_labels.shape\nprint \"test_dataset:\", test_dataset.shape\nprint \"test_labels:\", test_labels.shape\nprint \"num_labels:\", num_labels\nminist的数据都是28*28的，把里面的网络改完了之后，如下：\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\n\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\n\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n                          strides=[1, 2, 2, 1], padding='SAME')\n\n\ngraph = tf.Graph()\nwith graph.as_default():\n    x = tf.placeholder(tf.float32, shape=[None, image_size * image_size])\n    y_ = tf.placeholder(tf.float32, shape=[None, num_labels])\n\n    x_image = tf.reshape(x, [-1, 32, 32, 1])\n\n    # First Convolutional Layer\n    W_conv1 = weight_variable([5, 5, 1, 32])\n    b_conv1 = bias_variable([32])\n\n    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n    h_pool1 = max_pool_2x2(h_conv1)\n\n    # Second Convolutional Layer\n    W_conv2 = weight_variable([5, 5, 32, 64])\n    b_conv2 = bias_variable([64])\n\n    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n    h_pool2 = max_pool_2x2(h_conv2)\n\n    # Densely Connected Layer\n    W_fc1 = weight_variable([image_size / 4 * image_size / 4 * 64, 1024])\n    b_fc1 = bias_variable([1024])\n\n    h_pool2_flat = tf.reshape(h_pool2, [-1, image_size / 4 * image_size / 4 * 64])\n    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n    # Dropout\n    keep_prob = tf.placeholder(tf.float32)\n    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\n    # Readout Layer\n    W_fc2 = weight_variable([1024, num_labels])\n    b_fc2 = bias_variable([num_labels])\n\n    y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n\n    cross_entropy = tf.reduce_mean(\n        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n\n    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n主要改动就是输入层把28*28改成了image_size*image_size（32*32），然后第三层的全连接网络把7*7改成了image_size/4*image_size/4（8*8），以及把10（手写字符一共10类）改成了num_labels。\n然后训练，我这里把batch_size改成了128，训练批次改少了。\nbatch_size = 128\nwith tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    print(\"Initialized\")\n\n    for step in range(2001):\n        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n        # Generate a minibatch.\n        batch_data = train_dataset[offset:(offset + batch_size), :]\n        batch_labels = train_labels[offset:(offset + batch_size), :]\n\n        if step % 50 == 0:\n            train_accuracy = accuracy.eval(feed_dict={\n                x: batch_data, y_: batch_labels, keep_prob: 1.0})\n            test_accuracy = accuracy.eval(feed_dict={\n                x: test_dataset, y_: test_labels, keep_prob: 1.0})\n            print(\"Step %d, Training accuracy: %g, Test accuracy: %g\" % (step, train_accuracy, test_accuracy))\n\n        train_step.run(feed_dict={x: batch_data, y_: batch_labels, keep_prob: 0.5})\n\n    print(\"Test accuracy: %g\" % accuracy.eval(feed_dict={\n        x: test_dataset, y_: test_labels, keep_prob: 1.0}))\n运行，可以看到识别率在不断的上升。\n最后，有了接近98%的识别率，只有4000个训练数据，感觉不错了。\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "11"}