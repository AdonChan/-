{"title": "API例子：用Python驱动Firefox采集网页数据 - 一起学习python网络爬虫 ", "index": "网页爬虫,api,编程语言,python", "content": "\n1，引言\n本文讲解怎样用Python驱动Firefox浏览器写一个简易的网页数据采集器。开源Python即时网络爬虫项目将与Scrapy（基于twisted的异步网络框架）集成，所以本例将使用Scrapy采集淘宝这种含有大量ajax代码的网页数据，但是要注意本例一个严重缺陷：用Selenium加载网页的过程发生在Spider中，破坏了Scrapy的架构原则。所以，本例只是为了测试Firefox驱动和ajax网页数据采集这两个技术点，用于正式运行环境中必须予以修改，后续的文章将专门讲解修正后的实现。\n请注意，本例用到的xslt文件是通过MS谋数台保存提取器后，通过API接口获得，一方面让python代码变得简洁，另一方面，节省调试采集规则的时间。详细操作请查看Python即时网络爬虫：API说明\n2，具体实现\n2.1，环境准备\n需要执行以下步骤，准备Python开发和运行环境：\n\n安装Python--官网下载安装并部署好环境变量 （本文使用Python版本为3.5.1）\n安装lxml-- 官网库下载对应版本的.whl文件，然后命令行界面执行 \"pip install .whl文件路径\"\n安装Scrapy--命令行界面执行 \"pip install Scrapy\"，详细请参考Scrapy：Python3下的第一次运行测试\n安装selenium--命令行界面执行 \"pip install selenium\"\n安装Firefox--官网下载安装\n\n上述步骤展示了两种安装：1，安装下载到本地的wheel包；2，用Python安装管理器执行远程下载和安装。\n2.2，开发和测试过程\n以下代码默认都是在命令行界面执行\n1)，创建scrapy爬虫项目simpleSpider\nE:\\python-3.5.1>scrapy startproject simpleSpider\n\n2)，修改settings.py配置\n有些网站会在根目录下放置一个名字为robots.txt的文件，里面声明了此网站希望爬虫遵守的规范，Scrapy默认遵守这个文件制定的规范，即ROBOTSTXT_OBEY默认值为True。在这里需要修改ROBOTSTXT_OBEY的值，找到E:python-3.5.1simpleSpidersimpleSpider下文件settings.py，更改ROBOTSTXT_OBEY的值为False。\n3)，导入API模块\n在项目目录E:python-3.5.1simpleSpider下创建文件gooseeker.py（也可以在开源Python即时网络爬虫GitHub源 的core文件夹中直接下载），代码如下：\n#!/usr/bin/python\n# -*- coding: utf-8 -*-\n# 模块名: gooseeker\n# 类名: GsExtractor\n# Version: 2.0\n# 说明: html内容提取器\n# 功能: 使用xslt作为模板，快速提取HTML DOM中的内容。\n# released by 集搜客(http://www.gooseeker.com) on May 18, 2016\n# github: https://github.com/FullerHua/jisou/core/gooseeker.py\n\nfrom urllib import request\nfrom urllib.parse import quote\nfrom lxml import etree\nimport time\n\nclass GsExtractor(object):\n    def _init_(self):\n        self.xslt = \"\"\n    # 从文件读取xslt\n    def setXsltFromFile(self , xsltFilePath):\n        file = open(xsltFilePath , 'r' , encoding='UTF-8')\n        try:\n            self.xslt = file.read()\n        finally:\n            file.close()\n    # 从字符串获得xslt\n    def setXsltFromMem(self , xsltStr):\n        self.xslt = xsltStr\n    # 通过GooSeeker API接口获得xslt\n    def setXsltFromAPI(self , APIKey , theme, middle=None, bname=None):\n        apiurl = \"http://www.gooseeker.com/api/getextractor?key=\"+ APIKey +\"&theme=\"+quote(theme)\n        if (middle):\n            apiurl = apiurl + \"&middle=\"+quote(middle)\n        if (bname):\n            apiurl = apiurl + \"&bname=\"+quote(bname)\n        apiconn = request.urlopen(apiurl)\n        self.xslt = apiconn.read()\n    # 返回当前xslt\n    def getXslt(self):\n        return self.xslt\n    # 提取方法，入参是一个HTML DOM对象，返回是提取结果\n    def extract(self , html):\n        xslt_root = etree.XML(self.xslt)\n        transform = etree.XSLT(xslt_root)\n        result_tree = transform(html)\n        return result_tree\n\n4)，创建SimpleSpider爬虫类\n在项目目录E:python-3.5.1simpleSpidersimpleSpiderspiders下创建文件simplespider.py，代码如下：\n# -*- coding: utf-8 -*-\nimport time\nimport scrapy\nfrom lxml import etree\nfrom selenium import webdriver\nfrom gooseeker import GsExtractor\n\nclass SimpleSpider(scrapy.Spider):\n    name = \"simplespider\"\n    allowed_domains = [\"taobao.com\"]\n    start_urls = [\n        \"https://item.taobao.com/item.htm?spm=a230r.1.14.197.e2vSMY&id=44543058134&ns=1&abbucket=10\"\n    ]\n\n    def __init__(self):\n        # use any browser you wish\n        self.browser = webdriver.Firefox()\n    \n    def getTime(self):\n        # 获得当前时间戳\n        current_time = str(time.time())\n        m = current_time.find('.')\n        current_time = current_time[0:m]\n        return current_time\n       \n    def parse(self, response):\n        print(\"start...\")\n        #start browser\n        self.browser.get(response.url)\n        #loading time interval\n        time.sleep(3)\n        #get xslt\n        extra = GsExtractor()\n        extra.setXsltFromAPI(\"API KEY\" , \"淘宝天猫_商品详情30474\")\n        # get doc\n        html = self.browser.execute_script(\"return document.documentElement.outerHTML\");\n        doc = etree.HTML(html)\n        result = extra.extract(doc)\n        # out file\n        file_name = 'F:/temp/淘宝天猫_商品详情30474_' + self.getTime() + '.xml'\n        open(file_name,\"wb\").write(result)\n        self.browser.close()\n        print(\"end\")   \n\n5)，启动爬虫\n在E:python-3.5.1simpleSpider项目目录下执行命令\nE:\\python-3.5.1\\simpleSpider>scrapy crawl simplespider\n\n6)，输出文件\n采集到的网页数据结果文件是：淘宝天猫_商品详情30474_1466064544.xml\n3，展望\n调用Firefox，IE等全特性浏览器显得有点太重量级，很多场合可以考虑轻量级的浏览器内核，比如，casperjs和phantomjs等。同时运行在没有界面的浏览器（headless browser，无头浏览器）模式下，也许可以对网页数据采集性能有所提升。\n然后，最重要的一点是要写一个 Scrapy 的下载器，专门驱动这些浏览器采集网页数据，也就是把这个功能从Spider中迁移出来，这样才符合Scrapy的整体框架原则，实现事件驱动的工作模式。\n4，相关文档\n1， Python即时网络爬虫：API说明2， API例子：用Java/JavaScript下载内容提取器\n5，集搜客GooSeeker开源代码下载源\n1， 开源Python即时网络爬虫GitHub源\n6，文档修改历史\n1，2016-06-28：V1.02，2016-06-28：V1.1，在第一段明显位置注明本案例的缺陷\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "9"}