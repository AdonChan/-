{"title": "Python 爬取CSDN的极客头条 - 小墨鱼 ", "index": "网页爬虫,python", "content": "Python 爬取CSDN的极客头条\n工具\n\nPython\nPython：requests\nPython：BeautifulSoup\n\n分析\n使用浏览器调试面板分析网页结构以及网络请求，容易知道，每一个头条信息结构如图所示\n因此，我们可以通过 dd.tracking-ad > span > a定位元素，同时，根据Network面板的网络请求分析，第一次加载更多数据的请求为\nhttp://geek.csdn.net/service/news/get_news_list?from=-&size=20&type=HackCount\n第二次的为：\nhttp://geek.csdn.net/service/news/get_news_list?from=6:245113&size=20&type=HackCount\n上述请求已精简，删除了原有请求的部分参数\n也就是说，初始加载更多数据的时候，from参数为-，后续的请求，from是前一次请求所返回来的值，因此，我们可以用Python爬取数据了\n代码\n# -*- coding: UTF-8 -*-\nfrom bs4 import BeautifulSoup\nimport requests\nimport time\n\n\nclass CS:\n    def __init__(self):\n        # self.username = username\n        pass\n\n    def geek(self, _from=None, type='HackCount', size=20):\n        \"\"\"\n        url: http://geek.csdn.net/,\n        more: http://geek.csdn.net/service/news/get_news_list?from=-&size=20&type=HackCount\n        :param _from: 加载更多的时候的标志\n        :param type: 极客头条的类型\n        :param size: 每页的数目\n        :return:\n        \"\"\"\n        start = '-'\n        if _from:\n            timestamp = int(time.time())\n            url = 'http://geek.csdn.net/service/news/get_news_list?' \\\n                  'from=%s&size=%d&type=%s&_=%d' % (_from, size, type, timestamp)\n            req = requests.get(url)\n            js = req.json()\n            start = js['from']\n            soup = BeautifulSoup(js['html'], 'lxml')\n        else:\n            url = 'http://geek.csdn.net/'\n            req = requests.get(url)\n            soup = BeautifulSoup(req.content, 'lxml')\n        results = soup.select('dd.tracking-ad > span > a')\n        items = []\n        for result in results:\n            item = {\n                'href': result['href'],\n                'title': result.string\n            }\n            items.append(item)\n        return {\n            'from': start,\n            'items': items\n        }\n\n\ncs = CS()\nitems = []\n_from = ''\ni = 0\n# 这里控制获取多少页的内容\nwhile i < 10:\n    result = cs.geek(_from=_from)\n    items.extend(result['items'])\n    _from = result['from']\n    i = i + 1\nprint(items)\n项目地址： 模拟京东登录\n吐槽QQ群： 173318043\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "2"}