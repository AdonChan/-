{"title": "用python爬取知识星球 - zengdamo ", "index": "python,网页爬虫", "content": "去年我们做过一个叫「学长问答」的社群活动，里面沉淀了大量有价值的互动信息，后来因为各种原因终止了。今天和涂腾聊起来，觉得这些信息就这么沉寂了太浪费。所以就试着用python爬取了知识星球的内容。\n这个过程又学习了一些新的知识，已经在代码中以批注的形式写出。但还有一个没有解决的问题，就是一个提问底下可以跟好几个评论，我暂时还不能在输出结果里将「一个提问+n个评论」以整体的形式给出，而只能把评论的信息以一个字典的形式展现，这算是没有爬取的太彻底。将来再看看有没有新的解决方法。\nimport requests\nimport json\nimport urllib\nimport csv\n\n\n#头信息。网站只提供扫码登陆的方式，没有账号密码。我以为应该比较麻烦，但在header信息里找到了Authorization信息之后，直接可以保持登陆状态了。\n# 令一个标志是直接在浏览器里访问内页网址的话，浏览器的报错是“{\"succeeded\":false,\"code\":401,\"info\":\"\",\"resp_data\":{}}”，这个很像原来node.js的数据中心没有登陆的报错，而数据中心的模拟登陆也是通过在header中添加Authorization来实现的。\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36',\n    'Referer': 'https://wx.zsxq.com/dweb/',\n    'Authorization': '51EC09CA-6BCC-8847-4419-FA04A2FC9E00'\n}\n\n#打开并写入csv文件\n\nf = open('/Users/damo/Desktop/wendatuan.csv', 'w+')\nwriter = csv.writer(f)\nwriter.writerow(['created_time','ask_name','ask_content','comment'])\n\n#定义爬取信息的函数主体\n\ndef get_info(url):\n\n    res = requests.get(url,headers = headers)\n    json_data = json.loads(res.text)\n    datas = json_data['resp_data']['topics']\n\n    for data in datas:\n        if 'talk' in data.keys(): # 判断json中是否包含 talk 这个键\n            ask_name = data['talk']['owner']['name']\n            ask_content = data['talk']['text']\n        else:\n            ask_name = ''\n            ask_content = ''\n\n        if 'show_comments' in data.keys():\n            comment = data['show_comments']\n\n        else:\n            comment = ''\n        created_time = data['create_time']\n        writer.writerow([created_time,ask_name,ask_content,comment])\n\n    # 截止到前面的代码，已经可以实现一个页面的爬取。下面的代码内容主要任务是实现“如何自动实现多页面爬取”\n    # 多页面的爬取是通过Network中Query String Parameters来实现的：这里提供两个参数，观察之后发现count是固定值，而end_time和网址最后的时间是一样的。\n    # 只不过在网页中用到了 urlencode的转化，这部分是新学习的知识。\n    # 在这个爬虫案例中，网页构造的核心逻辑是“上一组最后一个数据的创建时间刚好是下一组数据访问网址中的一个参数”，以此来构造循环抓取的网址\n\n    end_time = datas[19]['create_time']\n    url_encode = urllib.parse.quote(end_time) # urlencode，将网址中的文本转化\n    next_url = 'https://api.zsxq.com/v1.10/groups/518282858584/topics?count=20&end_time='+url_encode # 通过观察构造下一组数据的网址\n    get_info(next_url) # 这里比较巧，直接在函数内部再次调用函数，从而实现不断的自循环\n\nif __name__ == '__main__':\n    url = 'https://api.zsxq.com/v1.10/groups/518282858584/topics?count=20'\n    get_info(url)\n\n\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}