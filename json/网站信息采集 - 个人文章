{"title": "网站信息采集 - 个人文章 ", "index": "网页爬虫,python", "content": "网站信息采集\n在编写爬虫之前可能需要先了解和搜集网站信息\nrobots.txt\nRobots协议（也称为爬虫协议、机器人协议等）的全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。一般的网站都会有这个文件。可以大致了解这个网站存在哪些限制\n下面是知乎的robots.txt，可以通过https://www.zhihu.com/robots....\n# 禁止今日头条和悟空问答爬虫抓取知乎网站内容\nUser-agent: *\nRequest-rate: 1/2 # load 1 page per 2 seconds\nCrawl-delay: 10\n\nDisallow: /login\nDisallow: /logout\nDisallow: /resetpassword\nDisallow: /terms\nDisallow: /search\nDisallow: /notifications\nDisallow: /settings\nDisallow: /inbox\nDisallow: /admin_inbox\nDisallow: /*?guide*\nDisallow: /people/*\n其中User-agent说明了对哪些用户代理的限制，*表示限制所有的爬虫，还设置了请求速率 每两秒访问一个页面，还设置了Crawl-delay，10秒的抓取延时，为了知乎的服务器不过载，我们最好遵循一下？？？？后面的Disallow则限制了访问的路径\n用site:example.com估计网站的大小\n你想抓取信息的网站有九成是被百度或者Google访问过的，通过这条命令可以快速的了解网站的大小，以便在设计爬虫的时候选择合适的方案\n\n识别网站所用的技术\n安装builtwith模块\npip install builtwith\n使用方式\nimport builtwith as bw\nres = bw.parse(\"https://www.zhihu.com/\")\nprint(res)\n# {'javascript-frameworks': ['React', 'RequireJS']}\nres = bw.parse(\"https://www.upc.edu.cn/\")\nprint(res)\n# {'font-scripts': ['Font Awesome'], 'javascript-frameworks': ['jQuery']}\nres = bw.parse(\"http://example.webscraping.com\")\nprint(res)\n#{'web-servers': ['Nginx'], 'web-frameworks': ['Web2py', 'Twitter Bootstrap'], 'programming-languages': ['Python'], 'javascript-frameworks': ['jQuery', 'Modernizr', 'jQuery UI']}\n可以得知知乎使用的是React框架，还能知道网站用什么语言开发的，服务器类型等等\n个人爬虫经验收集地址\nhttps://github.com/No-96/Feng...\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "1"}