{"title": "你真的理解机器学习中偏差 - 方差之间的权衡吗？ - 个人文章 ", "index": "python", "content": "作者：chen_h微信号 & QQ：862251340微信公众号：coderpai简书地址：http://www.jianshu.com/p/f143...\n\n我认为对偏差 - 方差之间的权衡判读对学习机器学习是非常重要的。那么为什么这么说呢？因为这个现象的背后是所有参数，性能和几乎所有机器学习模型的深层原因。如果你能很深刻的理解这个，我保证你能很好的理解机器学习的每一个模型。\n\n所以，我们就不浪费时间在无聊的介绍中，直接深入挖掘吧。理论讲解可能有一点枯燥，但我希望你能耐心看完本文。\n机器学习中的偏差 - 方差之间的权衡\n机器学习全部是关于给定输入数据（X）和给定输出数据（Y），然后去寻找一个最佳映射函数（F），这个映射函数通常也被叫做目标函数。\n任何机器学习算法的预测误差可以分解为三部分，即：偏差误差+方差误差+不可约的误差（对于给定的模型，我们不能进一步减少的误差）。在这个文章中，我们将重点来讨论机器学习中的前两个误差。我们按照如下目录来进行讲解：\n1）偏差误差；\n2）方差误差；\n3）偏差 - 方差之间的权衡；\n4）一些想法的总结；\n1. 偏差误差\n用简单的话来说，这个误差是由于简单的假设所造成的，因为我们的假设越简单，那么我们的模型更加容易去训练。\n一般而言，参数化算法具有较高的偏差，使得学习速度非常快，而且非常容易去理解，但是通常不太灵活。\n注意：对于参数化算法和非参数化算法之间的区别，简单来说，参数化算法对数据进行参数化，形成很多的特征，这种方法训练速度非常快，而且也不需要很多的数据，但是他不是很灵活。非参数化算法对目标函数做出很少或者根本不做任何假设，但是它需要更多的数据，训练速度非常慢，模型复杂度非常高，但是模型非常强大。\n低偏差：对目标函数提出更少的假设；\n高偏差：对目标函数提出更多的假设；\n低偏差模型例子：KNN 和 SVM；\n高偏差模型例子：线性回归和逻辑斯特回归；\n2. 方差误差\n1）如果我们使用不同的数据去训练同一个模型，那么最后我们得到的目标函数估计也是会改变的。\n2）目标函数是由机器学习的训练数据所估计得到的，所以我们期望训练数据拥有一定的方差。理想情况下，我们不希望目标函数从一个训练数据集到另一个训练数据集有太大的变化，也就是说我们的算法需要很好的从训练数据中找到一些映射的特征关系，这样可以保证不同训练集都有一个差不多的目标函数。\n低方差：随着训练数据集的变化，对目标函数估计值的变化非常小；\n高方差：随着训练数据集的变化，对目标函数估计值的变化非常大；\n一般而言，具有很大灵活性的非参数学习算法都具有很高的方差。\n高方差例子：KNN 和 SVM。\n3. 偏差 - 方差之间的权衡\n在上面的例子中我们可以看到一个趋势：参数或者线性的机器学习算法一般都会有一个很高的偏差和一个很低的方差。但是，非参数或者非线性的机器学习算法一般都有一个很低的偏差和一个很高的方差。所有，我们需要在这两者之间找到一个平衡点，来优化我们的算法。\n比如，KNN 算法有很低的偏差和很高的方差，但是我们可以通过调整 k 的值来改变偏差和方差之间的权衡关系，从而达到一个比较平衡的状态。\n因此，我们增加偏差会导致方差的减少，同理，我们增加方差会导致偏差的减少。但在实践中，我们无法去计算真正的偏差和方差值，因为我们不知道实际的目标函数。但是，作为一种方法，偏差和方差为我们提供了一种去判断机器学习算法性能的方法。\n\n\n4. 一些想法的总结\n1）机器学习是去找到一个映射函数（F），这个函数也经常被称之为目标函数；\n2）偏差是模型所做的简化假设，使得目标函数更加容易求解；\n3）方差是在给定不同训练数据集的情况下，目标函数估计值所改变的量；\n4）权衡是去调整一些参数使得偏差和方差之间相对平衡；\n\n作者：chen_h微信号 & QQ：862251340简书地址：http://www.jianshu.com/p/f143...\nCoderPai 是一个专注于算法实战的平台，从基础的算法到人工智能算法都有设计。如果你对算法实战感兴趣，请快快关注我们吧。加入AI实战微信群，AI实战QQ群，ACM算法微信群，ACM算法QQ群。长按或者扫描如下二维码，关注 “CoderPai” 微信号（coderpai）\n\n\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "1"}