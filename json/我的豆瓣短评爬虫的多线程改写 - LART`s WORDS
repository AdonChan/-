{"title": "我的豆瓣短评爬虫的多线程改写 - LART`s WORDS ", "index": "python3.x,python爬虫,python,多线程", "content": "对之前我的那个豆瓣的短评的爬虫，进行了一下架构性的改动。尽可能实现了模块的分离。但是总是感觉不完美。暂时也没心情折腾了。\n同时也添加了多线程的实现。具体过程见下。\n改动\n独立出来的部分：\n\nMakeOpener\nMakeRes\nGetNum\nIOFile\nGetSoup\nmain\n\n将所有的代码都置于函数之中，显得干净了许多。(^__^) 嘻嘻……\n使用直接调用文件入口作为程序的起点\nif __name__ == \"__main__\":\n    main()\n注意，这一句并不代表如果该if之前有其他直接暴露出来的代码时，他会首先执行。\nprint(\"首先执行\")\n\nif __name__ == \"__main__\":\n    print(\"次序执行\")\n\n# 输出如下：\n# 首先执行\n# 次序执行\n该if语句只是代表顺序执行到这句话时进行判断调用者是谁，若是直接运行的该文件，则进入结构，若是其他文件调用，那就跳过。\n多线程\n这里参考了【Python数据分析】Python3多线程并发网络爬虫-以豆瓣图书Top，和我的情况较为类似，参考较为容易。\n仔细想想就可以发现，其实爬10页（每页25本），这10页爬的先后关系是无所谓的，因为写入的时候没有依赖关系，各写各的，所以用串行方式爬取是吃亏的。显然可以用并发来加快速度，而且由于没有同步互斥关系，所以连锁都不用上。\n正如引用博文所说，由于问题的特殊性，我用了与之相似的较为直接的直接分配给各个线程不同的任务，而避免了线程交互导致的其他问题。\n我的代码中多线程的核心代码不多，见下。\nthread = []\nfor i in range(0, 10):\n    t = threading.Thread(\n            target=IOFile,\n            args=(soup, opener, file, pagelist[i], step)\n        )\n    thread.append(t)\n\n# 建立线程\nfor i in range(0, 10):\n    thread[i].start()\n\nfor i in range(0, 10):\n    thread[i].join()\n调用线程库threading，向threading.Thread()类中传入要用线程运行的函数及其参数。\n线程列表依次添加对应不同参数的线程，pagelist[i]，step两个参数是关键，我是分别为每个线程分配了不同的页面链接，这个地方我想了半天，最终使用了一些数学计算来处理了一下。\n同时也简单试用了下列表生成式：\npagelist = [x for x in range(0, pagenum, step)]\n这个和下面是一致的：\npagelist = []\nfor x in range(0, pagenum, step):\n    pagelist.append(x)\nthreading.Thread的几个方法\n值得参考：多线程\n\nstart() 启动线程\njion([timeout])，依次检验线程池中的线程是否结束，没有结束就阻塞直到线程结束，如果结束则跳转执行下一个线程的join函数。在程序中，最后join()方法使得当所调用线程都执行完毕后，主线程才会执行下面的代码。相当于实现了一个结束上的同步。这样避免了前面的线程结束任务时，导致文件关闭。\n\n注意\n使用多线程时，期间的延时时间应该设置的大些，不然会被网站拒绝访问，这时你还得去豆瓣认证下\"我真的不是机器人\"（尴尬）。我设置了10s，倒是没问题，再小些，就会出错了。\n完整代码\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Aug 17 16:31:35 2017\n\n@note: 为了便于阅读，将模块的引用就近安置了\n@author: lart\n\"\"\"\n\nimport time\nimport socket\nimport re\nimport threading\nfrom urllib import parse\nfrom urllib import request\nfrom http import cookiejar\nfrom bs4 import BeautifulSoup\nfrom matplotlib import pyplot\nfrom datetime import datetime\n\n\n# 用于生成短评页面网址的函数\ndef MakeUrl(start):\n    \"\"\"make the next page's url\"\"\"\n    url = 'https://movie.douban.com/subject/26934346/comments?start=' \\\n        + str(start) + '&limit=20&sort=new_score&status=P'\n    return url\n\n\ndef MakeOpener():\n    \"\"\"make the opener of requset\"\"\"\n    # 保存cookies便于后续页面的保持登陆\n    cookie = cookiejar.CookieJar()\n    cookie_support = request.HTTPCookieProcessor(cookie)\n    opener = request.build_opener(cookie_support)\n    return opener\n\n\ndef MakeRes(url, opener, formdata, headers):\n    \"\"\"make the response of http\"\"\"\n    # 编码信息，生成请求，打开页面获取内容\n    data = parse.urlencode(formdata).encode('utf-8')\n    req = request.Request(\n                    url=url,\n                    data=data,\n                    headers=headers\n                )\n    response = opener.open(req).read().decode('utf-8')\n    return response\n\n\ndef GetNum(soup):\n    \"\"\"get the number of pages\"\"\"\n    # 获得页面评论文字\n    totalnum = soup.select(\"div.mod-hd h2 span a\")[0].get_text()[3:-2]\n    # 计算出页数\n    pagenum = int(totalnum) // 20\n    print(\"the number of comments is:\" + totalnum,\n          \"the number of pages is: \" + str(pagenum))\n    return pagenum\n\n\ndef IOFile(soup, opener, file, pagestart, step):\n    \"\"\"the IO operation of file\"\"\"\n    # 循环爬取内容\n    for item in range(step):\n        start = (pagestart + item) * 20\n        print('第' + str(pagestart + item) + '页评论开始爬取')\n        url = MakeUrl(start)\n        # 超时重连\n        state = False\n        while not state:\n            try:\n                html = opener.open(url).read().decode('utf-8')\n                state = True\n            except socket.timeout:\n                state = False\n        # 获得评论内容\n        soup = BeautifulSoup(html, \"html.parser\")\n        comments = soup.select(\"div.comment > p\")\n        for text in comments:\n            file.write(text.get_text().split()[0] + '\\n')\n            print(text.get_text())\n        # 延时1s\n        time.sleep(10)\n\n    print('线程采集写入完毕')\n\n\ndef GetSoup():\n    \"\"\"get the soup and the opener of url\"\"\"\n    main_url = 'https://accounts.douban.com/login?source=movie'\n    formdata = {\n        \"form_email\": \"your-email\",\n        \"form_password\": \"your-password\",\n        \"source\": \"movie\",\n        \"redir\": \"https://movie.douban.com/subject/26934346/\",\n        \"login\": \"登录\"\n            }\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (Windows NT 5.1; U; en; rv:1.8.1)\\\n            Gecko/20061208 Firefox/2.0.0 Opera 9.50\",\n        'Connection': 'keep-alive'\n            }\n    opener = MakeOpener()\n\n    response_login = MakeRes(main_url, opener, formdata, headers)\n    soup = BeautifulSoup(response_login, \"html.parser\")\n\n    if soup.find('img', id='captcha_image'):\n        print(\"有验证码\")\n        # 获取验证码图片地址\n        captchaAddr = soup.find('img', id='captcha_image')['src']\n        # 匹配验证码id\n        reCaptchaID = r'<input type=\"hidden\" name=\"captcha-id\" value=\"(.*?)\"/'\n        captchaID = re.findall(reCaptchaID, response_login)\n        # 下载验证码图片\n        request.urlretrieve(captchaAddr, \"captcha.jpg\")\n        img = pyplot.imread(\"captcha.jpg\")\n        pyplot.imshow(img)\n        pyplot.axis('off')\n        pyplot.show()\n        # 输入验证码并加入提交信息中，重新编码提交获得页面内容\n        captcha = input('please input the captcha:')\n        formdata['captcha-solution'] = captcha\n        formdata['captcha-id'] = captchaID[0]\n        response_login = MakeRes(main_url, opener, formdata, headers)\n        soup = BeautifulSoup(response_login, \"html.parser\")\n\n    return soup, opener\n\n\ndef main():\n    \"\"\"main function\"\"\"\n    timeout = 5\n    socket.setdefaulttimeout(timeout)\n    now = datetime.now()\n    soup, opener = GetSoup()\n\n    pagenum = GetNum(soup)\n    step = pagenum // 9\n    pagelist = [x for x in range(0, pagenum, step)]\n    print('pageurl`s list={}, step={}'.format(pagelist, step))\n\n    # 追加写文件的方式打开文件\n    with open('秘密森林的短评.txt', 'w+', encoding='utf-8') as file:\n        thread = []\n        for i in range(0, 10):\n            t = threading.Thread(\n                    target=IOFile,\n                    args=(soup, opener, file, pagelist[i], step)\n                )\n            thread.append(t)\n\n        # 建立线程\n        for i in range(0, 10):\n            thread[i].start()\n\n        for i in range(0, 10):\n            thread[i].join()\n\n    end = datetime.now()\n    print(\"程序耗时： \" + str(end-now))\n\n\nif __name__ == \"__main__\":\n    main()\n\n运行结果\n效率有提升\n对应的单线程程序在github上。单线程：\n\n可见时间超过30分钟。修改后时间缩短到了11分钟。\n\n文件截图\n\n我的项目\n具体文件和对应的结果截图我放到了我的github上。\nmypython\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}