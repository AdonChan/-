{"title": "MongoDB 4.0 Python3.7 稳定高效的评分制IP代理池APIserver  - 个人文章 ", "index": "python,mongodb,coroutine,multiprocessing,multi-thread", "content": "FooProxy\n稳健高效的评分制 IP代理池 + API服务提供，可以自己插入采集器进行代理IP的爬取，支持 MongoDB 4.0 使用 Python3.7\ngithub 地址: FooProxy\n\n背景\n因为平时爬取某些网站数据时，经常被封IP，同时网上很多的接口又不方便，免费的也少，稳定的更少，所以自己写了一个评分制的ip代理API进行爬虫的供给. 起初对MySQL和MongoDB进行了兼容的编写，后来发现在高并发的情况下，MySQL并不能很好的读写数据，经常莫名其妙的出现死机、读写巨慢、缓执行等各种奇葩现象，对比MongoDB高效的数据文档读写，最终还是放弃了mysql的兼容。(dev分支保留了对mysql的部分支持，如爬取评分)\n环境\n开发环境\n\nPyCharm 2018.2.4 (Professional Edition)\nPython 3.7\nMongoDB 4.0\nWindows 7 64bits\n\n需安装的库\n\npymongo\naiohttp\nflask\nrequests\nbs4\nlxml\n\n项目目录\n\nAPIserver\n一个简单的代理API接口服务器，使用Flask实现，可以自己按需求写路由逻辑。这部分当然可以独立出来写，只是集成写在了项目里面。\ncomponents\n项目的主要运行部分，采集器、验证器、打分检测等功能实现的模块。\nconfig\n其中的DBsettings是数据库的设置，用户名密码之类的，以及存储的数据库名，还有备用有效数据库(standby)的自定义名字和高分稳定数据库(stable)的自定义名字。config文件是整个项目的主要参数配置，可以设置采集器采集间隔、验证器的抓取验证数量间隔和协程并发极值等。\nconst\n项目的静态配置，一般不用动的设置参数\ncustom\n自定义模块，可以编写自己要增加的爬虫采集函数到采集器中进行数据采集\nlog\n项目日志记录模块配置以及日志\ntools\n一些工具函数\nmain.py\n项目入口文件\n\n基本流程\n整个项目的流程其实很简单，采集数据模块主要是编写代理网站的爬虫，可以进行任意的爬虫增减\n\n采集数据\n验证数据\n打分存储\n循环检测\n择优剔劣\nAPI调用\n\n流程图：\n1.采集数据(Collector)\n采集器进程是一个周期循环，使用了多线程对每一个代理网站进行数据采集，即：一个代理网站爬虫一个线程。因为没有数据共享，这里没有GPL。项目内置了两个代理数据采集爬虫，一个是个人网站的nyloner,一个是66ip代理网站的爬虫(在crawlers.py文件中)，如果想要增加代理爬虫，可以自己在custom目录下的custom.py文件中进行增加删减，只需要保证你的爬虫返回的结果数据结构和要求的一致就好。如下:\ndef some_crawler_func():\n    \"\"\"\n    自己定义的一个采集爬虫\n    约定：\n        1.无参数传入\n        2.返回格式是：['<ip>:<port>','<ip>:<port>',...]\n        3.写完把函数名加入下面的my_crawlers列表中，如\n          my_crawlers = [some_crawler_func,...]\n    \"\"\"\n    pass\n\nmy_crawlers = []\n一个数据采集爬虫函数就是一个采集器，写完函数在my_crawlers中加进去就可以。在config中设置一个周期时间，就可以让爬虫定期采集更新数据。\n在采集的过程中，如果出现采集器爬虫被封IP，可以通过自己的APIserver请求代理，然后再继续采集，这一部分没有写，不过可以实现\n2.验证数据(Validator)\n采集器进程和验证器进程共享一个变量:proxyList，是一个MultiProcessing.Manger.List对象。可以在多进程中保持共享数据的同步(理论上)，采集器定期采集的代理数据可以通过proxyList实时的传递给验证器进行有效性验证，因为采集器一次传递的数据比较多，所以验证器使用异步验证，能大大提高效率，具体使用自带的asyncio实现的.\n验证器实现基本上也是调用了一个验证api来判断代理的有效性，可以自己更换api实现，可在validator.py中详细了解。在config中可以配置异步的并发量等来控制验证器。\n3.打分存储(Rator)\n打分器进程主要是与验证器配合，当采集器采集的数据经过验证器验证后，确定有效，则让打分器进行评分，中间可以加入自定义数据处理模块，打分后直接存储在standby数据库，而后供本地检测器进行周期检测，打分器也是一个周期循环，不断的对代理数据进行更新补充。内置在验证器与扫描器中。打分器主要的三个函数:mark_success,mark_fail,mark_update.\n\n\nmark_success  对采集器传递给验证器的代理数据，验证成功后进行一次性的评分并且存储\nmark_fail  对验证器进行验证，代理无效的数据进行失败打分处理(达到删除条件则删除，否则扣分更新数据库)\nmark_update 对非初次打分的代理数据进行更新，即验证有效的数据再次验证时仍有效，则进行加分之类的数据库更新\n\n具体的评分步骤在下面会详细说明，不过还有很大的提升空间，只是初步试了一下。\n\n4.循环扫描(Scanner)\n当验证器的有效数据经过打分器存进本地standby数据库中后，怎么保证这一次存进去的数据以后能保证调用时仍可用呢？使用扫描器周期循环检测！扫描器会在你给定的扫描周期间隔不断地对本地standby数据库进行扫描验证，无效的数据则直接删除，有效的数据会对其得分、响应时间、验证时间等字段进行及时的更新，保证代理数据的实时有效。\n在扫描器内部其实也是有一个验证函数来进行扫描验证。详见scanner.py\n5.择优剔劣(Detector)\n存储在standby数据库中的数据经过扫描器的扫描检测，可以保证其有效性，当是如果想要稳定的代理供给APIserver，那么必须有一个检测器来进行挑拣代理，Detector会周期性的进行扫描standby和stable两个数据库，对其中符合高分稳定条件的代理存进stable数据库，对失效的高分代理进行剔除，这些都可以在config中进行自定义配置高分稳定条件。如：\n#采集器采集数据时间间隔,单位：秒\nCOLLECT_TIME_GAP    = 3600*1\n#验证器的最大并发量\nCONCURRENCY         = 100\n#验证器一次取出多少条 抓取的 代理进行验证\nVALIDATE_AMOUNT     = 500\n#验证器验证抓取数据频率 ： 秒/次\nVALIDATE_F          = 5\n#验证器请求超时重试次数\nVALIDATE_RETRY      = 5\n#扫描器的最大并发协程数量\nCOROUTINE_MAX       = 300\n#扫描器一次取出多少条 本地库 的代理进行验证\nLOCAL_AMOUNT        = 500\n#扫描器验证本地库频率 ： 秒/次\nVALIDATE_LOCAL      = 60*1\n#检测器检测数据库的频率: 秒/次\nDETECT_LOCAL        = 60*1\n#检测器一次取出多少条有效库的代理进行筛选\nDETECT_AMOUNT       = 1000\n#检测器一次取出多少条高分稳定数据库的代理进行检测\nDETECT_HIGH_AMOUNT  = 1000\n#高分稳定数据库代理数据连续多少次无效则从稳定数据库中剔除\nDELETE_COMBO        = 30\n#代理IP成功率的最低要求,低于此要求均删除,100次周期测试 0.2=20%\nMIN_SUCCESS_RATE    = 0.2\n#有效代理数据库数据转至高分稳定数据库的成功率最低要求 0.8=80%\n#以及测试总数的最低要求\nSTABLE_MIN_RATE     = 0.8500\nSTABLE_MIN_COUNT    = 100\n因为是对本地数据库的io操作，使用了异步asyncio可以大大提高效率。\n6.API调用(APIserver)\n有了稳定的高分代理数据，那么就可以挂起一个api server为我们的爬虫保驾护航，这一部分可以单独拿出来编写，使用其他框架django之类的都是不错的选择。项目里只是为了演示使用，使用Flask进行了简单的路由设置，因为测试爬虫在本机，所以使用了下面几个api而已，具体可以自己扩展。\nroot = 'http://localhost:5000'\n# 请求代理 kind为代理种类，anony为高匿，normal为透明\nroot+'/proxy/<string:kind>'\n# 请求代理 直接返回一个高匿代理\nroot+'/proxy'\n可以在apiserver.py中自己实现路由。\n评分\n\n简单的评分可以使代理ip筛选更加简单，其中的具体设置可以再const.settings中更改，一个代理IP数据的得分主要是：\n\n一次请求的基础分 score-basic ：100-10x(响应时间-1)\n请求成功的得分 score-success ： (基础分+测试总数x上一次分数)/(测试总数+1)+自定义成功加分数x成功率x连续成功数\n请求失败的得分 score-fail : (基础分+测试总数x上一次分数)/(测试总数+1)-自定义失败减分数x失败率x连续失败数\n稳定性 stability : 得分x成功率x测试数/自定义精度\n\n与三个变量成正比的稳定性根据得分设置可以很快的两极化稳定与不稳定的代理，从而进行筛选。\n\n使用\n\n确保本机安装MongoDB，并且下载好所有需要安装库,python3.7\n可以先进行自定义的模式，在config中进行配置,可以运行单独的模块进行测试，如：\n\n #运行模式,置 1 表示运行，置 0 表示 不运行,全置 0 表示只运行 API server\n  MODE = {\n   'Collector' : 1,    #代理采集\n   'Validator' : 1,    #验证存储\n   'Scanner'   : 1,    #扫描本地库\n   'Detector'  : 1,    #高分检测\n }\n\n可以在config中的DBsettings配置好数据库设置\n按照自己需求更改评分量（const.setting中,默认不用更改）\n配置后可以直接在DOS下或PyCharm等有标准stdout的环境下运行 python  main.py\n\n运行一段时间就可以看到稳定的效果\n\n不足\n\n稳定性没有很好的标准判断，不过100次测试85%以上的成功率就已经很好了\n没有编写验证器与API服务器的超时请求代理功能\nAPI 服务器没有单独拿出来编写\n还没有加入存活时间的考量\n还没接入爬虫测试\n...\n\n效果\n\n备用有效数据库，开启1.5个小时后:\n\n高分稳定数据库\n\n后话\n\n比较符合预期\n经过连续6天的测试，程序运行正常\n备用有效数据库与高分稳定数据库的同步更新误差在5分钟左右\n只有一个数据采集爬虫的情况下，一个小时采集一次，一次1000条数据[采集66ip代理网]，8个小时内稳定的有效代理995左右，高分稳定的有200条左右，主要在于代理网站的质量\n经过并发爬虫测试，可以使用到实际项目中\n\n\n                ", "mainLikeNum": ["4 "], "mainBookmarkNum": "4"}