{"title": "基于gensim的Wiki百科中文word2vec训练 - 数据分析 ", "index": "python", "content": "Word2Vec简介\nWord2Vec是词（Word）的一种表示方式。不同于one-hot vector，word2vec可以通过计算各个词之间的距离，来表示词与词之间的相似度。word2vec提取了更多的特征，它使得具有相同上下文语义的词尽可能离得近一些，而不太相关的词尽可能离得较远一些。例如，【腾讯】和【网易】两个词向量将会离得很近，同理【宝马】和【保时捷】两个词向量将会离得很近。而【腾讯】和【宝马】/【保时捷】，【网易】和【宝马】/【保时捷】将会离得较远一些。因为【腾讯】和【网易】都同属于互联网类目，而【宝马】和【保时捷】都同属于汽车类目。人以类聚，物以群分嘛！互联网圈子中谈的毕竟都是互联网相关的话题，而汽车圈子中谈论的都是和汽车相关的话题。\n我们怎么得到一个词的word2vec呢？下面我们将介绍如何使用python gensim得到我们想要的词向量。总的来说，包括以下几个步骤：\n\nwiki中文数据预处理\n文本数据分词\ngensim word2vec训练\n\nwiki中文数据预处理\n首先，下载wiki中文数据：zhwiki-latest-pages-articles.xml.bz2。因为zhwiki数据中包含很多繁体字，所以我们想获得简体语料库，接下来需要做以下两件事：\n\n使用gensim模块中的WikiCorpus从bz2中获取原始文本数据\n使用OpenCC将繁体字转换为简体字\n\nWikiCorpus获取原始文本数据\n数据处理的python代码如下：\nfrom __future__ import print_function\nfrom gensim.corpora import WikiCorpus\nimport jieba\nimport codecs\nimport os\nimport six\nfrom gensim.models import Word2Vec\nfrom gensim.models.word2vec import LineSentence\nimport multiprocessing\n\n \nclass Config:\n    data_path = 'xxx/zhwiki'\n    zhwiki_bz2 = 'zhwiki-latest-pages-articles.xml.bz2'\n    zhwiki_raw = 'zhwiki_raw.txt'\n    zhwiki_raw_t2s = 'zhwiki_raw_t2s.txt'\n    zhwiki_seg_t2s = 'zhwiki_seg.txt'\n    embedded_model_t2s = 'embedding_model_t2s/zhwiki_embedding_t2s.model'\n    embedded_vector_t2s = 'embedding_model_t2s/vector_t2s'\n \n \ndef dataprocess(_config):\n    i = 0\n    if six.PY3:\n        output = open(os.path.join(_config.data_path, _config.zhwiki_raw), 'w')\n    output = codecs.open(os.path.join(_config.data_path, _config.zhwiki_raw), 'w')\n    wiki = WikiCorpus(os.path.join(_config.data_path, _config.zhwiki_bz2), lemmatize=False, dictionary={})\n    for text in wiki.get_texts():\n        if six.PY3:\n            output.write(b' '.join(text).decode('utf-8', 'ignore') + '\\n')\n        else:\n            output.write(' '.join(text) + '\\n')\n        i += 1\n        if i % 10000 == 0:\n            print('Saved ' + str(i) + ' articles')\n    output.close()\n    print('Finished Saved ' + str(i) + ' articles')\n\nconfig = Config()\ndataprocess(config)\n使用OpenCC将繁体字转换为简体字\n这里，需要预先安装OpenCC，关于OpenCC在linux环境中的安装方法，请参考这篇文章。仅仅需要两行linux命令就可以完成繁体字转换为简体字的任务，而且速度很快。\n$ cd /xxx/zhwiki/\n$ opencc -i zhwiki_raw.txt -o zhwiki_t2s.txt -c t2s.json\n文本数据分词\n对于分词这个任务，我们直接使用了python的jieba分词模块。你也可以使用哈工大的ltp或者斯坦福的nltk python接口进行分词，准确率及权威度挺高的。不过这两个安装的时候会花费很长时间，尤其是斯坦福的。关于jieba的分词处理代码，参考如下：\ndef is_alpha(tok):\n    try:\n        return tok.encode('ascii').isalpha()\n    except UnicodeEncodeError:\n        return False\n\n\ndef zhwiki_segment(_config, remove_alpha=True):\n    i = 0\n    if six.PY3:\n        output = open(os.path.join(_config.data_path, _config.zhwiki_seg_t2s), 'w', encoding='utf-8')\n    output = codecs.open(os.path.join(_config.data_path, _config.zhwiki_seg_t2s), 'w', encoding='utf-8')\n    print('Start...')\n    with codecs.open(os.path.join(_config.data_path, _config.zhwiki_raw_t2s), 'r', encoding='utf-8') as raw_input:\n        for line in raw_input.readlines():\n            line = line.strip()\n            i += 1\n            print('line ' + str(i))\n            text = line.split()\n            if True:\n                text = [w for w in text if not is_alpha(w)]\n            word_cut_seed = [jieba.cut(t) for t in text]\n            tmp = ''\n            for sent in word_cut_seed:\n                for tok in sent:\n                    tmp += tok + ' '\n            tmp = tmp.strip()\n            if tmp:\n                output.write(tmp + '\\n')\n        output.close()\n\nzhwiki_segment(config)\ngensim word2vec训练\npython的gensim模块提供了word2vec训练，为我们模型的训练提供了很大的方便。关于gensim的使用方法，可以参考基于Gensim的Word2Vec实践。本次训练的词向量大小size为50，训练窗口为5，最小词频为5，并使用了多线程，具体代码如下：\ndef word2vec(_config, saved=False):\n    print('Start...')\n    model = Word2Vec(LineSentence(os.path.join(_config.data_path, _config.zhwiki_seg_t2s)),\n                     size=50, window=5, min_count=5, workers=multiprocessing.cpu_count())\n    if saved:\n        model.save(os.path.join(_config.data_path, _config.embedded_model_t2s))\n        model.save_word2vec_format(os.path.join(_config.data_path, _config.embedded_vector_t2s), binary=False)\n    print(\"Finished!\")\n    return model\n \n \ndef wordsimilarity(word, model):\n    semi = ''\n    try:\n        semi = model.most_similar(word, topn=10)\n    except KeyError:\n        print('The word not in vocabulary!')\n    for term in semi:\n        print('%s,%s' % (term[0],term[1]))\n\nmodel = word2vec(config, saved=True)\nword2vec训练已经完成，我们得到了想要的模型以及词向量，并保存到本地。下面我们分别查看同【宝马】和【腾讯】最相近的前10个词语。可以发现：和【宝马】相近的词大都属于汽车行业，而且是汽车品牌；和【腾讯】相近的词大都属于互联网行业。\n>>> wordsimilarity(word=u'宝马', model=model)\n保时捷,0.92567974329\n固特异,0.888278841972\n劳斯莱斯,0.884045600891\n奥迪,0.881808757782\n马自达,0.881799697876\n亚菲特,0.880708634853\n欧宝,0.877104878426\n雪铁龙,0.876984715462\n玛莎拉蒂,0.868475496769\n桑塔纳,0.865387916565\n\n>>> wordsimilarity(word=u'腾讯', model=model)\n网易,0.880213916302\n优酷,0.873666107655\n腾讯网,0.87026232481\n广州日报,0.859486758709\n微信,0.835543811321\n天涯社区,0.834927380085\n李彦宏,0.832848489285\n土豆网,0.831390202045\n团购,0.829696238041\n搜狐网,0.825544642448\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "5"}