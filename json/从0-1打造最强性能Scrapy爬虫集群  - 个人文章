{"title": " 从0-1打造最强性能Scrapy爬虫集群  - 个人文章 ", "index": "python", "content": "1 项目介绍\n本项目的主要内容是分布式网络新闻抓取系统设计与实现。主要有以下几个部分来介绍：\n（1）深入分析网络新闻爬虫的特点，设计了分布式网络新闻抓取系统爬取策略、抓取字段、动态网页抓取方法、分布式结构、系统监测和数据存储六个关键功能。\n（2）结合程序代码分解说明分布式网络新闻抓取系统的实现过程。包括爬虫编写、爬虫避禁、动态网页数据抓取、部署分布式爬虫、系统监测共六个内容，结合实际定向抓取腾讯新闻数据，通过测试检验系统性能。\n（3）规划设计了包括数据清洗、编码转换、数据分类、对象添加等功能组成的数据处理模块。\n分布式网络新闻抓取系统的设计2.1 系统总体架构设计\n系统采用分布式主从结构，设置 1 个 Master 服务器和多个Slave 服务器，Master管理 Redis 数据库和分发下载任务，Slave 部署 Scrapy 抓取网页和解析提取项目数据。服务器的基本环境是 Ubuntu 操作系统，Master 服务器安装 Redis 数据库服务器和 Graphite，\nSlave 安装 Scrapy 和 Redis 客户端。系统按功能可划分为两个主要模块，一是数据抓取模块，二是数据处理模块。数据抓取模块包含浏览器调用、网页下载、字段提取、爬虫避禁、数据存储和系统监测六个功能；数据处理模块包含数据清洗、对象添加、编码转换和数据分类四个功能。\n2.2 爬取策略的设计\n本项目网络爬虫采用深度优先的爬取策略，根据设定下载网页数据。网页链接处理流程如下：\n1． 手动设置初始下载地址，一般为网站导航地址。\n2． 爬虫开始运行并从初始地址抓取第一批网页链接。\n3． 爬虫根据正则表达式识别新链接中的目录页地址和新闻内容页地址，识别的新地址加入待下载队列，等待抓取，未被识别的网页地址被定义为无用链接丢掉。\n4． 爬虫从待下载队列中依次取出网页链接下载和提取数据。\n5． 下载队列为空，爬虫停止抓取。\n新闻站点的导航页面数量是有限的，这一规律决定了在一定的人工参与下可以轻松获取新闻导航页面的 url，并将其作为爬虫系统的初始 url。2.3 爬取字段的设计\n本项目以网络新闻数据抓取为抓取目标，因此抓取内容必须能够客观准确地反应网络新闻特征。\n以抓取腾讯网络新闻数据为例，通过分析网页结构，本文确定了两步抓取步骤。第一步，抓取新闻内容页，获得新闻标题、新闻来源、新闻内容、发表时间、评论数量、评论地址、相关搜索、用户还喜欢的新闻和喜欢人数共 9 个内容；第二步，在获得评论地址后，抓取评论页，获得评论人 ID、评论人昵称，评论人性别、评论人所在地区、评论时间、评论内容、单条评论支持人数和单条评论回复数量等内容。\n2.4 动态网页抓取方法设计\n腾讯新闻网页使用 Java Script 生成动态网页内容。一些 JS 事件触发的页面内容在打开时发生变化，一些网页在没有 JS 支持的情况下根本不工作。一般的爬虫根本无法从这些网页获取数据。 解决 JavaScript 动态网页的抓取问题有四种方法：\n1．写代码模拟相关 JS 逻辑。\n2．调用有界面的浏览器，类似各种广泛用于测试的，如 Selenium 等。\n3．使用无界面的浏览器，各种基于Webkit的，如 Casperjs、Phantomjs 等。\n4．结合 JS 执行引擎，实现一个轻量级的浏览器。\n本项目由于是基于Python作为主要语言来编写，因此采用使用 Selenium 来处理 JS\n动态新闻页面。它的优点是简单、易于实现。用Python 代码模拟用户对浏览器的操作，将网页先加载到浏览器中打开，再从浏览器缓存中获取网页数据，传递到 spider 解析提取，最后传递目标数据到项目通道。\n2.5爬虫分布式设计\n应用 Redis 数据库实现分布式抓取。基本思想是 Scrapy 爬虫获取到的urls(request)\n都放到一个 Redis Queue中，所有爬虫也都从指定 Redis Queue中获取request(urls)。\nScrapy-Redis 中默认使用Spider Priority Queue 来确定 url 的先后次序，这是由 sorted set\n实现的一种非 FIFO、LIFO方式。\nRedis 中存储了 Scrapy 工程的request 和 stats 信息，根据这些信息可以掌握任务\n情况和爬虫状态，分配任务时便于均衡系统负载，有助于克服爬虫的性能瓶颈。同时\n利用 Redis 的高性能和易于扩展的特点能够轻松实现高效率下载。当 Redis 存储或者\n访问速度遇到问题时，可以通过增大 Redis 集群数和爬虫集群数量改善。Scrapy-Redis\n分布式方案很好解决了中断续抓取以及数据去重问题，爬虫重新启动后，会对照 Redis\n队列中的url 进行抓取，已经抓取的url 将自动过滤掉。\n2.6 基于Graphite系统监测组件设计\n运用 Graphite 监测系统运行状态，实现了一个针对分布式系统的 statscollector，\n将系统的 stats 信息以图表形式动态实时显示，即实时监测。Graphite 监测的信息有：系统的下载信息、日志信息、文件计数、调度信息、爬虫运行信息、爬虫异常信息、文件数量、获得 Item 数量、最大请求深度和收到的回应数量等。\n2.7 数据存储模块的设计\nScrapy 支持数据存储为 json、csv 和 xml 等文本格式，用户可以在运行爬虫时设置，例如：scrapy crawlspider –o items.json –t json，也可以在 Scrapy 工程文件的Item Pipeline\n文件中定义。除此之外，Scrapy 提供了多种数据库 API支持数据库存储。如 Mongo DB、\nRedis 等。数据存储分两个部分，一是网页链接的存储，二是项目数据的存储。网页链接存\n储于 Redis 数据库，用于实现分布式爬虫的下载管理；项目数据包括新闻数据和评论数据，为方便处理，均保存为 JSON 格式的文本文件。评论数据存储时以评论 url 中包含的评论ID 命名，通过这种方法可以将新闻数据与评论数据关联起来。\n3 项目总结\n以上就是分布式网络新闻抓取系统的系统设计部分，采用分布式的设计是因为单机爬虫的爬取量和爬取速度的局限性，总体设计部分如上所示。\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "5"}