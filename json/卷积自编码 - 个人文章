{"title": "卷积自编码 - 个人文章 ", "index": "python", "content": "作者：chen_h微信号 & QQ：862251340微信公众号：coderpai简书地址：https://www.jianshu.com/p/ec4...\n\n这篇教程是翻译Paolo Galeone写的卷积自编码分析教程，作者已经授权翻译，这是原文。\n卷积操作符会对输入信号进行滤波操作，以便提取其内容的一部分。在传统的方法中，自编码没有考虑到信号可以被看做是和其他信号的和。相反，卷积自编码就是使用卷积操作来做信号的叠加之和。他们对一组简单的输入信号进行编码，然后对这些信号再进行重新建模。\n卷积\n\n在一般连续状态，卷积被定义为两个函数（信号）被反转和移位之后的乘积的积分：\n\n作为结果，卷积操作会产生一个新的函数（信号）。卷积满足交换操作，因此：\n\n在一般的 n 维空间输入，自编码可以被用来训练解码（编码）。实际上，自编码通常用于对二维的，有限和离散输入信号进行特征提取，比如数字图像。\n在二维离散空间，卷积操作可以被定义如下：\n\n因为图像的范围有限，所以该公式可以变为：\n\n其中：\n\n\nO(i, j) 表示输出像素，位置是 (i, j)\n\n\n2k+1 是表示矩形奇数卷积核的一条边\n\nF 表示卷积核\n\nI 表示输入图像\n\n对于图1所示，单个卷积核操作在输入图像 I 的每个位置 (i, j) 进行卷积操作。\n\n从图2可以很容易的看出，卷积操作的结果取决于卷积核的值。根据不同的卷积核设置，每个卷积核可以用于不同的图像处理任务，比如去噪，模糊处理等等....\n离散二维卷积操作有两个附加参数：水平和垂直移动步数。它们是在执行单个卷积步骤之后，沿着图像 I 的各个维度跳过的像素的数量。通常，水平和垂直移动步数是相等的，它们被标记为 S 。\n对于一个正方形的图像 Iw = Ih （这是为了简单描述，如果要扩充到一般的矩阵图像，非常方便），以步数 2k+1 ，进行二维的离散卷积操作之后，我们可以得到如下的图像 O ：\n\n到目前为止，我们已经利用了单个卷积核对图像进行灰度级（单通道）操作的情况。如果输入图像具有多个通道，即 D 个通道，那么卷积算子沿着每一个通道都要进行操作。\n一般规则下，一个卷积核的输出通道数必须和输入图像的通道数一样。所以可以概括为，离散二维的卷积是将信号进行堆叠处理。\n各个维度上的卷积\n长方体完全可以由三元组 (W, H, D) 来表示，其中：\n\n\nW≥1 表示长度\n\nH≥1 表示高度\n\nD≥1 表示深度\n\n很明显，一个灰度图像可以看做是深度 D = 1 的长方体，而RGB图像可以看做是深度 D = 3 的长方体。\n一个卷积核也可以看做是一个具有深度 D 的卷积核。特别地，我们可以将图像和滤波器视为单通道图像/滤波器的集合（与顺序无关）。\n\n如果我们考虑图像的深度，那么以前的卷积公式可以概括为：\n\n在图像上进行卷积之后，得到的结果称为激活图（activation map）。激活图是深度 D = 1 的长方体。\n可能听起来很奇怪，在一个三维图像上的卷积得到的结果是一个二维的结果。实际上，对于具有深度 D 的输入信号，卷积核执行精确的 D 个离散的二维卷积操作。所产生的D个二维的激活图，之后将这D个激活图进行处理，从而得到一个二维的卷积结果。以这种方式，所得到的激活图 O 的每个单位 (i, j) 包含的信息是提取该位置所有信息的结果。\n直观地来说，可以将该操作认为是将输入的RGB通道转换成一个单通道进行输出。\n卷积自编码\n卷积自编码（CAE）从不同的角度来定义滤波器的任务：而不像平时我们遇到的那些工程上的卷积滤波器，它们的作用就是让模型学习到最佳滤波器，从而使得重构误差最小。然后，这些训练好的滤波器就可以被使用到任何其他的计算机视觉任务。\n目前利用卷积核进行无监督学习的最先进工具就是卷积自编码（CAE）。一旦这些卷积核被训练学习之后，它们将被应用到任何的输入数据去进行特征提取。然后，这些特征就可以被用于任何的任务，例如分类问题。\nCAE是卷积神经网络（CNN）的一种类型：CNN和CAE之间最主要的区别在于前者是进行端到端的学习滤波器，并且将提取的特征进行组合从而用来分类。事实上，CNN通常被称为是一种监督学习。相反，后者通常被用来训练从输入数据中提取特征，从而重构输入数据。\n由于它们的卷积性质，不管输入数据的维度是多大，CAE产生的激活图的数量都是相同的。因此，CAE完全忽略了二维图像本身的结构，而是作为了一个通用特征提取器。事实上，在自编码（AE）中，图像必须被展开成单个向量，并且网络对输入向量的神经元个数有一定的约束。换句话说，AE迫使每个特征是全局的（即，跨越整个视野），所以它的参数中是存在冗余的，而CAE不是。\n编码器\n很容易理解，单个卷积滤波器不能学会提取图像的各种各样的模式。为此，每个卷积层是由 n 个（超参数）卷积核组成的，每个卷积核的深度是 D ，其中 D 表示输入数据的通道数。\n因此，每个具有深度 D 的输入数据 \n\n和一组 n 个卷积核\n\n之间进行的卷积操作，从而产生一组 n 个激活图，或者等价的特征图。当然，最后产生的特征图的通道数还是 n ，具体如下：\n\n为了提高网络的泛化能力，每个卷积都会被非线性函数 a 激活，以这种方式训练，得到的网络可以学习输入数据的一些非线性特性：\n\n其中，bm^(1) 表示第 m 个特征图的偏差，引入术语 zm 是对 AE 中保持相同的变量名称。\n所产生的激活图是对输入数据 I 进行的一个重新编码，使其可以在低维空间表示。重构好之后的数据维度并不是原来 O 的维度，但是参数的数量是从Om 中学习来的，换句话说，这些参数就是 CAE 需要学习的参数。\n由于我们的目标是从所产生的特征图中对输入数据 I 进行重构。因此我们需要一个解码操作。卷积自编码是一个完全的卷积网络，因此我们的解码操作可以进行再次卷积。\n细心的读者可能认为卷积操作减少了输出的空间范围，因此不可能使用卷积来重建具有相同输入空间范围的信息。\n这是完全正确的，但是我们可以使用输入填充来解决这个问题。如果我们用零向输入数据 I 进行填充，则经过第一个卷积之后的结果具有比输入数据 I 大的空间范围，经过第二个卷积之后就可以产生具有和原始空间 I 相同的空间范围了。\n因此，我们想要输入填充的零是这样的：\n\n从公式1可以看出，我们想要对 I 填充 2(2k+1)-2 个零（每一个边填充 (2k+1) -1 个），以这种方式，卷积编码将产生数据的宽度和高度等于：\n\n解码器\n所产生的 n 个特征图 zm = 1, ..., n 将被用作解码器的输入，以便从该压缩的信息中重建输入图像 I 。\n事实上，解码卷积的超参数是由编码框架确定的：\n\n由于卷积跨越每个特征图，并且产生具有 (2k+1, 2k+1, n) 的维度，因此经过滤波器 F(2) 之后产生相同的空间范围 I 。\n需要学习的滤波器的数量：D个，因为我需要重构具有深度 D 的输入图像。\n\n因此，重构的图像 I_ 是特征图的维度 Z = {zi = 1}^n 和该卷积滤波器 F(2) 之间的进行卷积的结果。\n\n根据前面计算的零进行填充，那么导致解码卷积之后产生的维度是：\n\n我们的目标是使得输入的维度等于输出的维度，然后可以用任何的损失函数来进行计算，例如 MSE：\n\n下一篇，我们来讲怎么利用 TensorFlow 来实现CAE。\n\n作者：chen_h微信号 & QQ：862251340简书地址：https://www.jianshu.com/p/ec4...\nCoderPai 是一个专注于算法实战的平台，从基础的算法到人工智能算法都有设计。如果你对算法实战感兴趣，请快快关注我们吧。加入AI实战微信群，AI实战QQ群，ACM算法微信群，ACM算法QQ群。长按或者扫描如下二维码，关注 “CoderPai” 微信号（coderpai）\n\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}