{"title": "爬虫与swift - Continue ", "index": "python,swift", "content": "分析\n\n使用爬虫爬取网站page，并按事先的要求将需要的项目保存到数据库中，然后再使用python flask框架编写一个web 服务器讲数据库中的数据读出来，最后用swift编写一个应用将数据显示出来。我这里选区的所要爬取的网站是豆瓣电影网。\n\n技术选用\n\n爬虫：使用python的scrapy爬虫\n数据库：使用mongoDB，存储网页只需要key和value形式进行存储就好了，所以在这里选择mongoDB这种NOSQL数据库进行存储\n服务器：使用python的flask框架，用了你就知道几行代码就能完成很多事情，当然特别是flask可以根据需要组装空间，超轻量级。\n\n实现：\n\n\n\nscrapy爬虫实现\n\n\n上图是scrapy的文档结构，下面主要介绍几个文件。\n\n\na. items.py\n\nfrom scrapy.item import Item, Field\nimport scrapy\nclass TopitmeItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    title = Field()\n    dataSrc = Field()\n    dataId = Field()\n    filmReview = Field()\n    startCount = Field()\n这里可以把items.py看作是mvc中的model，在items里我们定义了自己需要的模型。\n\n\nb. pipelines.py\n\nimport pymongo\nfrom scrapy.conf import settings\nfrom scrapy.exceptions import DropItem\nfrom scrapy import log\nclass MongoDBPipeline(object):\n    def __init__(self):\n        connection = pymongo.MongoClient(\n            settings['MONGODB_SERVER'],\n            settings['MONGODB_PORT']\n        )\n        db = connection[settings['MONGODB_DB']]\n        self.collection = db[settings[‘MONGODB_COLLECTION’]]\n    def process_item(self, item, spider):\n        valid = True\n        for data in item:\n            if not data:\n                valid = False\n                raise DropItem(\"Missing {0}!\".format(data))\n        if valid:\n            self.collection.insert(dict(item))\n            log.msg(\"Beauty added to MongoDB database!\",\n                    level=log.DEBUG, spider=spider)\n        return item\n\n俗称管道，这个文件主要用来把我们获取的item类型存入mongodb\n\n\nc. settings.py\n\nBOT_NAME = 'topitme'\nSPIDER_MODULES = ['topitme.spiders']\nNEWSPIDER_MODULE = 'topitme.spiders'\nBOT_NAME = 'topitme'\nITEM_PIPELINES = ['topitme.pipelines.MongoDBPipeline',]\nMONGODB_SERVER = \"localhost\"\nMONGODB_PORT = 27017\nMONGODB_DB = \"topitme\"\nMONGODB_COLLECTION = \"beauty\"\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\n#USER_AGENT = 'topitme (+http://www.yourdomain.com)'\n\n这里需要设置一些常量，例如mongodb的数据库名，数据库地址和数据库端口号等等\n\n\nd. topitme_scrapy.py\n\nfrom scrapy import Spider\nfrom scrapy.selector import Selector\nfrom topitme.items import TopitmeItem\n\nimport sys\nreload(sys)\nsys.setdefaultencoding(‘utf8’)#设置默认编码格式\n\nclass topitmeSpider(Spider):\n    name = \"topitmeSpider\"\n    allowed_domin =[\"movie.douban.com\"]\n    start_urls = [\n        \"http://movie.douban.com/review/latest/\",\n    ]\n    def parse(self, response):\n        results = Selector(response).xpath('//ul[@class=\"tlst clearfix\"]')\n        for result in results:\n            item = TopitmeItem()\n            # item['title'] = result.xpath('li[@class=\"ilst\"]/a/@src').extract()[0]\n            item['title'] = result.xpath('li[@class=\"ilst\"]/a/@title').extract()[0].encode('utf-8')\n            item['dataSrc'] = result.xpath('li[@class=\"ilst\"]/a/img/@src').extract()[0]\n            item['filmReview'] = result.xpath('li[@class=\"clst report-link\"]/div[@class=\"review-short\"]/span/text()').extract()[0].encode('utf-8')\n            item['dataId'] = result.xpath('li[@class=\"clst report-link\"]/div[@class=\"review-short\"]/@id').extract()[0]\n            item['dataId'] = result.xpath('li[@class=\"nlst\"]/h3/a/@title').extract()[0]\n            item['startCount'] = 0\n            yield item\n\n# ul[@class=\"tlst clearfix\"]/li[3]/div[1]\n# //ul[@class=\"tlst clearfix\"]/li[@class=\"ilst\"]/a/img/@src\n\n这个文件是爬虫程序的主要代码，首先我们定义了一个类名为topitmeSpider的类，继承自Spider类，然后这个类有3个基础的属性，name表示这个爬虫的名字，等一下我们在命令行状态启动爬虫的时候，爬虫的名字就是name规定的。\nallowed_domin意思就是指在movie.douban.com这个域名爬东西。\nstart_urls是一个数组，里面用来保存需要爬的页面，目前我们只需要爬首页。所以只有一个地址。\n然后def parse就是定义了一个parse方法（肯定是override的，我觉得父类里肯定有一个同名方法），然后在这里进行解析工作，这个方法有一个response参数，你可以把response想象成，scrapy这个框架在把start_urls里的页面下载了，然后response里全部都是html代码和css代码。这之中最主要的是涉及一个xpath的东西，XPath即为XML路径语言，它是一种用来确定XML（标准通用标记语言的子集）文档中某部分位置的语言。可以通过xpath定位到我们想要获取的元素。\n\n\n\n服务器\n\n使用python的flask框架实现\n\nfrom flask import Flask, request\nimport json\nfrom bson import json_util\nfrom bson.objectid import ObjectId\nimport pymongo\n\napp = Flask(__name__)\n\nclient = pymongo.MongoClient()\ndb = client['topitme']\ndef toJson(data):\n    return json.dumps(data, default=json_util.default)\n\n@app.route('/FilmReview', methods=['GET'])\n\ndef findMovie():\n    if request.method == 'GET':\n        json_results = []\n        for result in results:\n            json_results.append(result)\n        return toJson(json_results)\n\nif __name__ == '__main__':\n    app.run(debug=True)\n\n\n\n首先可以看到代码，client，db两个参量是为了取得数据库连接。\nfindMovie函数响应http request，然后返回数据库数据，以JSON形式返回\n\n\n\nswift\n\nios的实现就不详细介绍了，这里写这部分只是为了，验证结果。\n\n运行：\n\n\n起服务器：\n\n\n\n\n起数据库：\n\n\n\n\n运行爬虫：\n\n\n\n\n访问服务器：http://localhost:5000/FileReview 可以看到数据已经存储到数据库中了\n\n\n\n\nios运行情况：\n\n\n\n\n下面是原网站网页展示，可以看到所要的数据存储到数据库，并且正常显示出来\n\n\n\n                ", "mainLikeNum": ["3 "], "mainBookmarkNum": "15"}