{"title": "scrapy学习笔记 - bomo的学习笔记 ", "index": "python,scrapy", "content": "scrapy是python最有名的爬虫框架之一，可以很方便的进行web抓取，并且提供了很强的定制型，这里记录简单学习的过程和在实际应用中会遇到的一些常见问题\n一、安装\n在安装scrapy之前有一些依赖需要安装，否则可能会安装失败，scrapy的选择器依赖于lxml，还有Twisted网络引擎，下面是ubuntu下安装的过程\n1. linux下安装\n# 1. 安装xml依赖库\n$ sudo apt-get install libxml2 libxml2-dev\n$ sudo apt-get install libxslt1-dev\n$ sudo apt-get install python-libxml2\n\n# 2. 安装lxml\n$ sudo pip install lxml\n\n# 3. 安装Twisted（版本可以换成最新的），用pip也可以，如果失败的话下载源码安装，如下\n$ wget https://pypi.python.org/packages/6b/23/8dbe86fc83215015e221fbd861a545c6ec5c9e9cd7514af114d1f64084ab/Twisted-16.4.1.tar.bz2#md5=c6d09bdd681f538369659111f079c29d\n$ tar xjf Twisted-16.4.1.tar.bz2\n$ cd Twisted-16.4.1\n$ sudo python setup.py install\n\n# 3. 安装scrapy\n$ sudo pip install scrapy\nhttp://lxml.de/installation.html\n2. Mac下安装\n# 安装xml依赖库\n$ xcode-select —install\n\n# 其实相关依赖pip会自动帮我们装上\n$ pip install scrapy\nmac下安装有时候会失败，建议使用virtualenv安装在独立的环境下，可以减少一些问题，因为mac系统自带python，例如一些依赖库依赖的一些新的版本，而升级新版本会把旧版本卸载掉，卸载可能会有权限的问题\n二、基本使用\n1. 初始化scrapy项目\n我们可以使用命令行初始化一个项目\n$ scrapy startproject tutorial\n这里可以查看scrapy更多其他的命令\n初始化完成后，我们得到下面目录结构\nscrapy.cfg:         项目的配置文件\ntutorial/:          该项目的python模块, 在这里添加代码\n    items.py:       项目中的item文件\n    pipelines.py:   项目中的pipelines文件.\n    settings.py:    项目全局设置文件.\n    spiders/        爬虫模块目录\n我们先看一下scrapy的处理流程\nscrapy由下面几个部分组成\n\nspiders：爬虫模块，负责配置需要爬取的数据和爬取规则，以及解析结构化数据\nitems：定义我们需要的结构化数据，使用相当于dict\npipelines：管道模块，处理spider模块分析好的结构化数据，如保存入库等\nmiddlewares：中间件，相当于钩子，可以对爬取前后做预处理，如修改请求header，url过滤等\n\n我们先来看一个例子，在spiders目录下新建一个模块DmozSpider.py\nimport scrapy\n\nclass DmozSpider(scrapy.Spider):\n    # 必须定义\n    name = \"dmoz\"\n    # 初始urls\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    # 默认response处理函数\n    def parse(self, response):\n        # 把结果写到文件中\n        filename = response.url.split(\"/\")[-2]\n        with open(filename, 'wb') as f:\n            f.write(response.body)\n打开终端进入根目录，执行下面命令\n$ scrapy crawl dmoz\n爬虫开始爬取start_urls定义的url，并输出到文件中，最后输出爬去报告，会输出爬取得统计结果\n2016-09-13 10:36:43 [scrapy] INFO: Spider opened\n2016-09-13 10:36:43 [scrapy] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n2016-09-13 10:36:43 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023\n2016-09-13 10:36:44 [scrapy] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/> (referer: None)\n2016-09-13 10:36:45 [scrapy] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n2016-09-13 10:36:45 [scrapy] INFO: Closing spider (finished)\n2016-09-13 10:36:45 [scrapy] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 548,\n 'downloader/request_count': 2,\n 'downloader/request_method_count/GET': 2,\n 'downloader/response_bytes': 16179,\n 'downloader/response_count': 2,\n 'downloader/response_status_count/200': 2,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2016, 9, 13, 2, 36, 45, 585113),\n 'log_count/DEBUG': 3,\n 'log_count/INFO': 7,\n 'response_received_count': 2,\n 'scheduler/dequeued': 2,\n 'scheduler/dequeued/memory': 2,\n 'scheduler/enqueued': 2,\n 'scheduler/enqueued/memory': 2,\n 'start_time': datetime.datetime(2016, 9, 13, 2, 36, 43, 935790)}\n2016-09-13 10:36:45 [scrapy] INFO: Spider closed (finished)\n这里我们完成了简单的爬取和保存的操作，会在根目录生成两个文件Resources和Books\n2. 通过代码运行爬虫\n每次进入控制台运行爬虫还是比较麻烦的，而且不好调试，我们可以通过CrawlerProcess通过代码运行爬虫，新建一个模块run.py\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.project import get_project_settings\n\nfrom spiders.DmozSpider import DmozSpider\n\n# 获取settings.py模块的设置\nsettings = get_project_settings()\nprocess = CrawlerProcess(settings=settings)\n\n# 可以添加多个spider\n# process.crawl(Spider1)\n# process.crawl(Spider2)\nprocess.crawl(DmozSpider)\n\n# 启动爬虫，会阻塞，直到爬取完成\nprocess.start()\n参考：http://doc.scrapy.org/en/latest/topics/practices.html#run-scrapy-from-a-script\n三、Scrapy类\n如上面的DmozSpider类，爬虫类继承自scrapy.Spider，用于构造Request对象给Scheduler\n1. 常用属性与方法\n属性\n\nname：爬虫的名字，必须唯一（如果在控制台使用的话，必须配置）\nstart_urls：爬虫初始爬取的链接列表\nparse：response结果处理函数\ncustom_settings：自定义配置，覆盖settings.py中的默认配置\n\n方法\n\nstart_requests：启动爬虫的时候调用，默认是调用make_requests_from_url方法爬取start_urls的链接，可以在这个方法里面定制，如果重写了该方法，start_urls默认将不会被使用，可以在这个方法里面定制一些自定义的url，如登录，从数据库读取url等，本方法返回Request对象\nmake_requests_from_url：默认由start_requests调用，可以配置Request对象，返回Request对象\nparse：response到达spider的时候默认调用，如果在Request对象配置了callback函数，则不会调用，parse方法可以迭代返回Item或Request对象，如果返回Request对象，则会进行增量爬取\n\n2. Request与Response对象\n每个请求都是一个Request对象，Request对象定义了请求的相关信息（url, method, headers, body, cookie, priority）和回调的相关信息（meta, callback, dont_filter, errback），通常由spider迭代返回\n其中meta相当于附加变量，可以在请求完成后通过response.meta访问\n请求完成后，会通过Response对象发送给spider处理，常用属性有（url, status, headers, body, request, meta, ）\n详细介绍参考官网\n\nhttps://doc.scrapy.org/en/latest/topics/request-response.html#request-objects\nhttps://doc.scrapy.org/en/latest/topics/request-response.html#response-objects\n\n看下面这个例子\nfrom scrapy import Spider\nfrom scrapy import Request\n\nclass TestSpider(Spider):\n    name = 'test'\n    start_urls = [\n        \"http://www.qq.com/\",\n    ]\n\n    def login_parse(self, response):\n        ''' 如果登录成功,手动构造请求Request迭代返回 '''\n        print response\n        for i in range(0, 10):\n            yield Request('http://www.example.com/list/1?page={0}'.format(i))\n\n    def start_requests(self):\n        ''' 覆盖默认的方法(忽略start_urls),返回登录请求页,制定处理函数为login_parse '''\n        return Request('http://www.example.com/login', method=\"POST\" body='username=bomo&pwd=123456', callback=self.login_parse)\n\n\n    def parse(self, response):\n        ''' 默认请求处理函数 '''\n        print response\n四、Selector\n上面我们只是爬取了网页的html文本，对于爬虫，我们需要明确我们需要爬取的结构化数据，需要对原文本进行解析，解析的方法通常有下面这些\n\n普通文本操作\n正则表达式：re\nDom树操作：BeautifulSoup\nXPath选择器：lxml\n\nscrapy默认支持选择器的功能，自带的选择器构建与lxml之上，并对其进行了改进，使用起来更为简洁明了\n1. XPath选择器\nXPpath是标准的XML文档查询语言，可以用于查询XML文档中的节点和内容，关于XPath语法，可以参见这里\n先看一个例子，通过html或xml构造Selector对象，然后通过xpath查询节点，并解析出节点的内容\nfrom scrapy import Selector\n\nhtml = '<html><body><span>good</span><span>buy</span></body></html>'\nsel = Selector(text=html)\nnodes = sel.xpath('//span')\nfor node in nodes:\n    print node.extract()\nSelector相当于节点，通过xpath去到子节点集合（SelectorList），可以继续搜索，通过extract方法可以取出节点的值，extract方法也可以作用于SelectorList，对于SelectorList可以通过extract_first取出第一个节点的值\n\n通过text()取出节点的内容\n通过@href去除节点属性值（这里是取出href属性的值）\n直接对节点取值，则是输出节点的字符串\n\n2. CSS选择器\n除了XPath选择器，scrapy还支持css选择器\nhtml = \"\"\"\n        <html>\n            <body>\n                <span>good</span>\n                <span>buy</span>\n                <ul>\n                    <li class=\"video_part_lists\">aa<li>\n                    <li class=\"video_part_lists\">bb<li>\n                    <li class=\"audio_part_lists\">cc<li>\n                    <li class=\"video_part_lists\">\n                        <a href=\"/\">主页</a>\n                    <li>\n                </ul>\n            </body>\n        </html>\n        \"\"\"\nsel = Selector(text=html)\n\n# 选择class为video_part_lists的li节点\nlis = sel.css('li.video_part_lists')\n\nfor li in lis:\n    # 选择a节点的属性\n    print li.css('a::attr(href)').extract()\n关于css选择器更多的规则，可以见w3c官网\nhttps://www.w3.org/TR/selectors/\n五、Item类\n上面我们只是爬取了网页的html文本，对于爬虫，我们需要明确我们需要爬取的结构化数据，我们定义一个item存储分类信息，scrapy的item继承自scrapy.Item\nfrom scrapy import Item, Field\n\nclass DmozItem(Item):\n    title = Field()\n    link = Field()\n    desc = Field()\nscrapy.Item的用法与python中的字典用法基本一样，只是做了一些安全限制，属性定义使用Field，这里只是进行了声明，而不是真正的属性，使用的时候通过键值对操作，不支持属性访问\nwhat, 好坑爹，这意味着所有的属性赋值都得用字符串了，这里有解释（还是没太明白）\nwhy-is-scrapys-field-a-dict\n修改DmozSpider的parse方法\nclass DmozSpider(scrapy.Spider):\n    ...\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            dmoz_item = DmozItem()\n            dmoz_item['title'] = sel.xpath('a/text()').extract()\n            dmoz_item['link'] = sel.xpath('a/@href').extract()\n            dmoz_item['desc'] = sel.xpath('text()').extract()\n            print dmoz_item\n六、Pipeline\nspider负责爬虫的配置，item负责声明结构化数据，而对于数据的处理，在scrapy中使用管道的方式进行处理，只要注册过的管道都可以处理item数据（处理，过滤，保存）\n下面看看管道的声明方式，这里定义一个预处理管道PretreatmentPipeline.py，如果item的title为None，则设置为空字符串\nclass PretreatmentPipeline(object):\n    def process_item(self, item, spider):\n        if item['title']:\n            # 不让title为空\n            item['title'] = ''\n        return item\n再定义一个过滤重复数据的管道DuplicatesPipeline.py，当link重复，则丢弃\nfrom scrapy.exceptions import DropItem\n\nclass DuplicatesPipeline(object):\n    def __init__(self):\n        self.links = set()\n\n    def process_item(self, item, spider):\n        if item['link'] in self.links:\n            # 跑出DropItem表示丢掉数据\n            raise DropItem(\"Duplicate item found: %s\" % item)\n        else:\n            self.links.add(item['link'])\n            return item\n最后可以定义一个保存数据的管道，可以把数据保存到数据库中\nfrom scrapy.exceptions import DropItem\nfrom Database import Database\n\nclass DatabasePipeline(object):\n    def __init__(self):\n        self.db = Database\n\n    def process_item(self, item, spider):\n        if self.db.item_exists(item['id']):\n            self.db.update_item(item)\n        else:\n            self.db.insert_item(item)\n定义好管道之后我们需要配置到爬虫上，我们在settings.py模块中配置，后面的数字表示管道的顺序\nITEM_PIPELINES = {\n    'pipelines.DuplicatesPipeline.DuplicatesPipeline': 1,\n    'pipelines.PretreatmentPipeline.PretreatmentPipeline': 2,\n}\n我们也可以为spider配置单独的pipeline\nclass TestSpider(Spider):\n    # 自定义配置\n    custom_settings = {\n        # item处理管道\n        'ITEM_PIPELINES': {\n            'tutorial.pipelines.FangDetailPipeline.FangDetailPipeline': 1,\n        },\n    }\n    ...\n除了process_item方法外，pipeline还有open_spider和spider_closed两个方法，在爬虫启动和关闭的时候调用\n七、Rule\n爬虫的通常需要在一个网页里面爬去其他的链接，然后一层一层往下爬，scrapy提供了LinkExtractor类用于对网页链接的提取，使用LinkExtractor需要使用CrawlSpider爬虫类中，CrawlSpider与Spider相比主要是多了rules，可以添加一些规则，先看下面这个例子，爬取链家网的链接\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom scrapy.linkextractors import LinkExtractor\n\nclass LianjiaSpider(CrawlSpider):\n    name = \"lianjia\"\n\n    allowed_domains = [\"lianjia.com\"]\n\n    start_urls = [\n        \"http://bj.lianjia.com/ershoufang/\"\n    ]\n\n    rules = [\n        # 匹配正则表达式,处理下一页\n        Rule(LinkExtractor(allow=(r'http://bj.lianjia.com/ershoufang/pg\\s+$',)), callback='parse_item'),\n\n        # 匹配正则表达式,结果加到url列表中,设置请求预处理函数\n        # Rule(FangLinkExtractor(allow=('http://www.lianjia.com/client/', )), follow=True, process_request='add_cookie')\n    ]\n\n    def parse_item(self, response):\n        # 这里与之前的parse方法一样，处理\n        pass\n1. Rule对象\nRole对象有下面参数\n\nlink_extractor：链接提取规则\ncallback：link_extractor提取的链接的请求结果的回调\ncb_kwargs：附加参数，可以在回调函数中获取到\nfollow：表示提取的链接请求完成后是否还要应用当前规则（boolean），如果为False则不会对提取出来的网页进行进一步提取，默认为False\nprocess_links：处理所有的链接的回调，用于处理从response提取的links，通常用于过滤（参数为link列表）\nprocess_request：链接请求预处理（添加header或cookie等）\n\n2. LinkExtractor\nLinkExtractor常用的参数有：\n\nallow：提取满足正则表达式的链接\ndeny：排除正则表达式匹配的链接（优先级高于allow）\nallow_domains：允许的域名（可以是str或list）\ndeny_domains：排除的域名（可以是str或list）\nrestrict_xpaths：提取满足XPath选择条件的链接（可以是str或list）\nrestrict_css：提取满足css选择条件的链接（可以是str或list）\ntags：提取指定标签下的链接，默认从a和area中提取（可以是str或list）\nattrs：提取满足拥有属性的链接，默认为href（类型为list）\nunique：链接是否去重（类型为boolean）\nprocess_value：值处理函数（优先级大于allow）\n\n关于LinkExtractor的详细参数介绍见官网\n注意：如果使用rules规则，请不要覆盖或重写CrawlSpider的parse方法，否则规则会失效，可以使用parse_start_urls方法\n八、Middleware\n从最开始的流程图可以看到，爬去一个资源链接的流程，首先我们配置spider相关的爬取信息，在启动爬取实例后，scrapy_engine从Spider取出Request（经过SpiderMiddleware），然后丢给Scheduler（经过SchedulerMiddleware），Scheduler接着把请求丢给Downloader（经过DownloadMiddlware），Downloader把请求结果丢还给Spider，然后Spider把分析好的结构化数据丢给Pipeline，Pipeline进行分析保存或丢弃，这里面有4个角色\nscrapy有下面三种middlewares\n\nSpiderMiddleware：通常用于配置爬虫相关的属性，引用链接设置，Url长度限制，成功状态码设置，爬取深度设置，爬去优先级设置等\nDownloadMiddlware：通常用于处理下载之前的预处理，如请求Header（Cookie,User-Agent），登录验证处理，重定向处理，代理服务器处理，超时处理，重试处理等\nSchedulerMiddleware（已经废弃）：为了简化框架，调度器中间件已经被废弃，使用另外两个中间件已经够用了\n\n1. SpiderMiddleware\n爬虫中间件有下面几个方法\n\nprocess_spider_input：当response通过spider的时候被调用，返回None（继续给其他中间件处理）或抛出异常（不会给其他中间件处理，当成异常处理）\nprocess_spider_output：当spider有item或Request输出的时候调动\nprocess_spider_exception：处理出现异常时调用\nprocess_start_requests：spider当开始请求Request的时候调用\n\n下面是scrapy自带的一些中间件（在scrapy.spidermiddlewares命名空间下）\n\nUrlLengthMiddleware\nRefererMiddleware\nOffsiteMiddleware\nHttpErrorMiddleware\nDepthMiddleware\n\n我们自己实现一个SpiderMiddleware\nTODO\n参考链接：http://doc.scrapy.org/en/latest/topics/spider-middleware.html\n2. DownloaderMiddleware\n下载中间件有下面几个方法\n\nprocess_request：请求通过下载器的时候调用\nprocess_response：请求完成后调用\nprocess_exception：请求发生异常时调用\nfrom_crawler：从crawler构造的时候调用\nfrom_settings：从settings构造的时候调用\n``\n\n更多详细的参数解释见这里\n在爬取网页的时候，使用不同的User-Agent可以提高请求的随机性，定义一个随机设置User-Agent的中间件RandomUserAgentMiddleware\nimport random\n\nclass RandomUserAgentMiddleware(object):\n    \"\"\"Randomly rotate user agents based on a list of predefined ones\"\"\"\n\n    def __init__(self, agents):\n        self.agents = agents\n\n    # 从crawler构造，USER_AGENTS定义在crawler的配置的设置中\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(crawler.settings.getlist('USER_AGENTS'))\n\n    # 从settings构造，USER_AGENTS定义在settings.py中\n    @classmethod\n    def from_settings(cls, settings):\n        return cls(settings.getlist('USER_AGENTS'))\n\n    def process_request(self, request, spider):\n        # 设置随机的User-Agent\n        request.headers.setdefault('User-Agent', random.choice(self.agents))\n在settings.py设置USER_AGENTS参数\nUSER_AGENTS = [\n    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n    \"Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)\",\n    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n    \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n    \"Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6\",\n    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n    \"Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5\",\n    \"Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6\",\n    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\",\n    \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\",\n]\n配置爬虫中间件的方式与pipeline类似，第二个参数表示优先级\n# 配置爬虫中间件\nSPIDER_MIDDLEWARES = {\n    'myproject.middlewares.CustomSpiderMiddleware': 543,\n    # 如果想禁用默认的中间件的话，可以设置其优先级为None\n    'scrapy.spidermiddlewares.offsite.OffsiteMiddleware': None,\n}\n\n# 配置下载中间件\nDOWNLOADER_MIDDLEWARES = {\n    'myproject.middlewares.RandomUserAgentMiddleware': 543,\n    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None,\n}\n3. 代理服务器\n爬虫最怕的就是封ip，这时候就需要代理服务器来爬取，scrapy设置代理服务器非常简单，只需要在请求前设置Request对象的meta属性，添加proxy值即可，通常我们可以通过中间件来做\nclass ProxyMiddleware(object):\n    def process_request(self, request, spider):\n        proxy = 'https://178.33.6.236:3128'     # 代理服务器\n        request.meta['proxy'] = proxy\n九、缓存\nscrapy默认已经自带了缓存的功能，通常我们只需要配置即可，打开settings.py\n# 打开缓存\nHTTPCACHE_ENABLED = True\n\n# 设置缓存过期时间（单位：秒）\n#HTTPCACHE_EXPIRATION_SECS = 0\n\n# 缓存路径(默认为：.scrapy/httpcache)\nHTTPCACHE_DIR = 'httpcache'\n\n# 忽略的状态码\nHTTPCACHE_IGNORE_HTTP_CODES = []\n\n# 缓存模式(文件缓存)\nHTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n更多参数参见这里\n十、多线程\nscrapy网络请求是基于Twisted，而Twisted默认支持多线程，而且scrapy默认也是通过多线程请求的，并且支持多核CPU的并发，通常只需要配置一些参数即可\n# 默认Item并发数：100\nCONCURRENT_ITEMS = 100\n\n# 默认Request并发数：16\nCONCURRENT_REQUESTS = 16\n\n# 默认每个域名的并发数：8\nCONCURRENT_REQUESTS_PER_DOMAIN = 8\n\n# 每个IP的最大并发数：0表示忽略\nCONCURRENT_REQUESTS_PER_IP = 0\n更多参数参见这里\n十一、常见问题\n1. 项目名称问题\n在使用的时候遇到过一个问题，在初始化scrapy startproject tutorial的时候，如果使用了一些特殊的名字，如：test, fang等单词的话，通过get_project_settings方法获取配置的时候会出错，改成tutorial或一些复杂的名字的时候不会\nImportError: No module named tutorial.settings\n这是一个bug，在github上有提到：https://github.com/scrapy/scrapy/issues/428，但貌似没有完全修复，修改一下名字就好了（当然scrapy.cfg和settings.py里面也需要修改）\n2. 为每个pipeline配置spider\n上面我们是在settings.py里面配置pipeline，这里的配置的pipeline会作用于所有的spider，我们可以为每一个spider配置不同的pipeline，设置Spider的custom_settings对象\nclass LianjiaSpider(CrawlSpider):\n    ...\n    # 自定义配置\n    custom_settings = {\n        'ITEM_PIPELINES': {\n            'tutorial.pipelines.TestPipeline.TestPipeline': 1,\n        }\n    }\n3. 获取提取链接的节点信息\n通过LinkExtractor提取的scrapy.Link默认不带节点信息，有时候我们需要节点的其他attribute属性，scrapy.Link有个text属性保存从节点提取的text值，我们可以通过修改lxmlhtml._collect_string_content变量为etree.tostring，这样可以在提取节点值就变味渲染节点scrapy.Link.text，然后根据scrapy.Link.text属性拿到节点的html，最后提取出我们需要的值\nfrom lxml import etree\nimport scrapy.linkextractors.lxmlhtml\nscrapy.linkextractors.lxmlhtml._collect_string_content = etree.tostring\n4. 从数据库中读取urls\n有时候我们已经把urls下载到数据库了，而不是在start_urls里配置，这时候可以重载spider的start_requests方法\ndef start_requests(self):\n    for u in self.db.session.query(User.link):\n        yield Request(u.link)\n我们还可以在Request添加元数据，然后在response中访问\ndef start_requests(self):\n    for u in self.db.session.query(User):\n        yield Request(u.link, meta={'name': u.name})\n\ndef parse(self, response):\n    print response.url, response.meta['name']\n5. 如何进行循环爬取\n有时候我们需要爬取的一些经常更新的页面，例如：间隔时间为2s，爬去一个列表前10页的数据，从第一页开始爬，爬完成后重新回到第一页\n目前的思路是，通过parse方法迭代返回Request进行增量爬取，由于scrapy默认由缓存机制，需要修改\n6. 关于去重\nscrapy默认有自己的去重机制，默认使用scrapy.dupefilters.RFPDupeFilter类进行去重，主要逻辑如下\nif include_headers:\n    include_headers = tuple(to_bytes(h.lower())\n                             for h in sorted(include_headers))\ncache = _fingerprint_cache.setdefault(request, {})\nif include_headers not in cache:\n    fp = hashlib.sha1()\n    fp.update(to_bytes(request.method))\n    fp.update(to_bytes(canonicalize_url(request.url)))\n    fp.update(request.body or b'')\n    if include_headers:\n        for hdr in include_headers:\n            if hdr in request.headers:\n                fp.update(hdr)\n                for v in request.headers.getlist(hdr):\n                    fp.update(v)\n    cache[include_headers] = fp.hexdigest()\nreturn cache[include_headers]\n默认的去重指纹是sha1(method + url + body + header)，这种方式并不能过滤很多，例如有一些请求会加上时间戳的，基本每次都会不同，这时候我们需要自定义过滤规则\nfrom scrapy.dupefilter import RFPDupeFilter\n\nclass CustomURLFilter(RFPDupeFilter):\n    \"\"\" 只根据url去重\"\"\"\n\n    def __init__(self, path=None):\n        self.urls_seen = set()\n        RFPDupeFilter.__init__(self, path)\n\n    def request_seen(self, request):\n        if request.url in self.urls_seen:\n            return True\n        else:\n            self.urls_seen.add(request.url)\n配置setting\nDUPEFILTER_CLASS = 'tutorial.custom_filters.CustomURLFilter'\n7. 如何在Pipeline中处理不同的Item\nscrapy所有的迭代出来的的Item都会经过所有的Pipeline，如果需要处理不同的Item，只能通过isinstance()方法进行类型判断，然后分别进行处理，暂时没有更好的方案\n8. url按顺序执行\n我们可以通过Request的priority控制url的请求的执行顺序，但由于网络请求的不确定性，不能保证返回也是按照顺序进行的，如果需要进行逐个url请求的话，吧url列表放在meta对象里面，在response的时候迭代返回下一个Request对象到调度器，达到顺序执行的目的，暂时没有更好的方案\n十二、总结\nscrapy虽然是最有名的python爬虫框架，但是还是有很多不足，例如，item不能单独配置给制定的pipeline，每一个爬取的所有item都会走遍所有的管道，需要在管道里面去判断不同类型的item，如果在pipelines和items比较多的项目，将会让项目变得非常臃肿\n如有问题欢迎到我的博客留言\n十三、参考链接\n\n官方文档\n中文教程\nscrapy五大模块\n\n最后安利一下自己的博客：http://zhengbomo.github.com\n\n                ", "mainLikeNum": ["4 "], "mainBookmarkNum": "23"}