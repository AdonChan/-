{"title": "Reinventing the wheel：决策树算法的实现 - Solaris ", "index": "大数据,人工智能,python,机器学习", "content": "数据描述\n每条数据项储存在列表中，最后一列储存结果多条数据项形成数据集\ndata=[[d1,d2,d3...dn,result],\n      [d1,d2,d3...dn,result],\n                .\n                .\n      [d1,d2,d3...dn,result]]\n\n决策树数据结构\nclass DecisionNode:\n    '''决策树节点\n    '''\n    \n    def __init__(self,col=-1,value=None,results=None,tb=None,fb=None):\n        '''初始化决策树节点\n        \n        args：        \n        col -- 按数据集的col列划分数据集\n        value -- 以value作为划分col列的参照\n        result -- 只有叶子节点有，代表最终划分出的子数据集结果统计信息。｛‘结果’：结果出现次数｝\n        rb,fb -- 代表左右子树\n        '''\n        self.col=col\n        self.value=value\n        self.results=results\n        self.tb=tb\n        self.fb=fb\n决策树分类的最终结果是将数据项划分出了若干子集，其中每个子集的结果都一样，所以这里采用｛‘结果’：结果出现次数｝的方式表达每个子集\n\ndef divideset(rows,column,value):\n    '''依据数据集rows的column列的值，判断其与参考值value的关系对数据集进行拆分\n       返回两个数据集\n    '''\n    split_function=None\n    #value是数值类型\n    if isinstance(value,int) or isinstance(value,float):\n        #定义lambda函数当row[column]>=value时返回true\n        split_function=lambda row:row[column]>=value\n    #value是字符类型\n    else:\n        #定义lambda函数当row[column]==value时返回true\n        split_function=lambda row:row[column]==value\n    #将数据集拆分成两个\n    set1=[row for row in rows if split_function(row)]\n    set2=[row for row in rows if not split_function(row)]\n    #返回两个数据集\n    return (set1,set2)\n\ndef uniquecounts(rows):\n    '''计算数据集rows中有几种最终结果，计算结果出现次数，返回一个字典\n    '''\n    results={}\n    for row in rows:\n        r=row[len(row)-1]\n        if r not in results: results[r]=0\n        results[r]+=1\n    return results\n\ndef giniimpurity(rows):\n    '''返回rows数据集的基尼不纯度\n    '''\n    total=len(rows)\n    counts=uniquecounts(rows)\n    imp=0\n    for k1 in counts:\n        p1=float(counts[k1])/total\n        for k2 in counts:\n            if k1==k2: continue\n            p2=float(counts[k2])/total\n            imp+=p1*p2\n    return imp\n\ndef entropy(rows):\n    '''返回rows数据集的熵\n    '''\n    from math import log\n    log2=lambda x:log(x)/log(2)  \n    results=uniquecounts(rows)\n    ent=0.0\n    for r in results.keys():\n        p=float(results[r])/len(rows)\n        ent=ent-p*log2(p)\n    return ent\n\ndef build_tree(rows,scoref=entropy):\n    '''构造决策树\n    '''\n    if len(rows)==0: return DecisionNode()\n    current_score=scoref(rows)\n\n    # 最佳信息增益\n    best_gain=0.0\n    #\n    best_criteria=None\n    #最佳划分\n    best_sets=None\n\n    column_count=len(rows[0])-1\n    #遍历数据集的列，确定分割顺序\n    for col in range(0,column_count):\n        column_values={}\n        # 构造字典\n        for row in rows:\n            column_values[row[col]]=1\n        for value in column_values.keys():\n            (set1,set2)=divideset(rows,col,value)\n            p=float(len(set1))/len(rows)\n            # 计算信息增益\n            gain=current_score-p*scoref(set1)-(1-p)*scoref(set2)\n            if gain>best_gain and len(set1)>0 and len(set2)>0:\n                best_gain=gain\n                best_criteria=(col,value)\n                best_sets=(set1,set2)\n    # 如果划分的两个数据集熵小于原数据集，进一步划分它们\n    if best_gain>0:\n        trueBranch=build_tree(best_sets[0])\n        falseBranch=build_tree(best_sets[1])\n        return DecisionNode(col=best_criteria[0],value=best_criteria[1],\n                        tb=trueBranch,fb=falseBranch)\n    # 如果划分的两个数据集熵不小于原数据集，停止划分\n    else:\n        return DecisionNode(results=uniquecounts(rows))\n\ndef print_tree(tree,indent=''):\n    if tree.results!=None:\n        print(str(tree.results))\n    else:\n        print(str(tree.col)+':'+str(tree.value)+'? ')\n        print(indent+'T->',end='')\n        print_tree(tree.tb,indent+'  ')\n        print(indent+'F->',end='')\n        print_tree(tree.fb,indent+'  ')\n\n\ndef getwidth(tree):\n    if tree.tb==None and tree.fb==None: return 1\n    return getwidth(tree.tb)+getwidth(tree.fb)\n\ndef getdepth(tree):\n    if tree.tb==None and tree.fb==None: return 0\n    return max(getdepth(tree.tb),getdepth(tree.fb))+1\n\n\ndef drawtree(tree,jpeg='tree.jpg'):\n    w=getwidth(tree)*100\n    h=getdepth(tree)*100+120\n\n    img=Image.new('RGB',(w,h),(255,255,255))\n    draw=ImageDraw.Draw(img)\n\n    drawnode(draw,tree,w/2,20)\n    img.save(jpeg,'JPEG')\n\ndef drawnode(draw,tree,x,y):\n    if tree.results==None:\n        # Get the width of each branch\n        w1=getwidth(tree.fb)*100\n        w2=getwidth(tree.tb)*100\n\n        # Determine the total space required by this node\n        left=x-(w1+w2)/2\n        right=x+(w1+w2)/2\n\n        # Draw the condition string\n        draw.text((x-20,y-10),str(tree.col)+':'+str(tree.value),(0,0,0))\n\n        # Draw links to the branches\n        draw.line((x,y,left+w1/2,y+100),fill=(255,0,0))\n        draw.line((x,y,right-w2/2,y+100),fill=(255,0,0))\n    \n        # Draw the branch nodes\n        drawnode(draw,tree.fb,left+w1/2,y+100)\n        drawnode(draw,tree.tb,right-w2/2,y+100)\n    else:\n        txt=' \\n'.join(['%s:%d'%v for v in tree.results.items()])\n        draw.text((x-20,y),txt,(0,0,0))\n\n\n\n\n对测试数据进行分类（附带处理缺失数据）\ndef mdclassify(observation,tree):\n    '''对缺失数据进行分类\n    \n    args：\n    observation -- 发生信息缺失的数据项\n    tree -- 训练完成的决策树\n    \n    返回代表该分类的结果字典\n    '''\n\n    # 判断数据是否到达叶节点\n    if tree.results!=None:\n        # 已经到达叶节点，返回结果result\n        return tree.results\n    else:\n        # 对数据项的col列进行分析\n        v=observation[tree.col]\n\n        # 若col列数据缺失\n        if v==None:\n            #对tree的左右子树分别使用mdclassify，tr是左子树得到的结果字典，fr是右子树得到的结果字典\n            tr,fr=mdclassify(observation,tree.tb),mdclassify(observation,tree.fb)\n\n            # 分别以结果占总数比例计算得到左右子树的权重\n            tcount=sum(tr.values())\n            fcount=sum(fr.values())\n            tw=float(tcount)/(tcount+fcount)\n            fw=float(fcount)/(tcount+fcount)\n            result={}\n\n            # 计算左右子树的加权平均\n            for k,v in tr.items(): \n                result[k]=v*tw\n            for k,v in fr.items(): \n                # fr的结果k有可能并不在tr中，在result中初始化k\n                if k not in result: \n                    result[k]=0 \n                # fr的结果累加到result中  \n                result[k]+=v*fw\n            return result\n\n        # col列没有缺失，继续沿决策树分类\n        else:\n            if isinstance(v,int) or isinstance(v,float):\n                if v>=tree.value: branch=tree.tb\n                else: branch=tree.fb\n            else:\n                if v==tree.value: branch=tree.tb\n                else: branch=tree.fb\n            return mdclassify(observation,branch)\n\ntree=build_tree(my_data)\nprint(mdclassify(['google',None,'yes',None],tree))\nprint(mdclassify(['google','France',None,None],tree))\n决策树剪枝\ndef prune(tree,mingain):\n    '''对决策树进行剪枝\n    \n    args：\n    tree -- 决策树\n    mingain -- 最小信息增益\n    \n   返回\n    '''\n    # 修剪非叶节点\n    if tree.tb.results==None:\n        prune(tree.tb,mingain)\n    if tree.fb.results==None:\n        prune(tree.fb,mingain)\n    #合并两个叶子节点\n    if tree.tb.results!=None and tree.fb.results!=None:\n        tb,fb=[],[]\n        for v,c in tree.tb.results.items():\n            tb+=[[v]]*c\n        for v,c in tree.fb.results.items():\n            fb+=[[v]]*c\n        #计算熵减少情况\n        delta=entropy(tb+fb)-(entropy(tb)+entropy(fb)/2)\n        #熵的增加量小于mingain，可以合并分支\n        if delta<mingain:\n            tree.tb,tree.fb=None,None\n            tree.results=uniquecounts(tb+fb)\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}