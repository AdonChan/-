{"title": "如何在多个queue多台server上部署Celery 以及任务状态监控flower - pythoner ", "index": "redis,rabbitmq,python,分布式任务", "content": "Celery是分布式任务队列，能实时处理任务， 同时支持task scheduling. 官方文档Celery工作原理如下：\n\ncelery client发送message给broker\nworker 从broker中消费消息，并将结果存储在result_end中\n\n本文中使用的broker是Rabbit MQ，result_end使用的是Redis.\nScenario\n现在有两个task，分别是加法运算和乘法运算。假定乘法运算的事件优先级高&事件也很多，对于加法运算，要求每分钟最多处理10个事件。\n框架\n\nCelery Worker：   在2 台server上部署worker，其中：   server1上的worker处理queue priority_low和priority_high上的事件   server2上的worker只处理priority_high上的事件\nCelery Client：在应用中调用\nRabbit MQ：在server3上启动\nRedis：在localhost启动\n\n\nCode\ntasks.py & callback\n对两个任务加上callback的处理，如果成功，打印“----[task_id] is done”\nfrom celery import Celery\nfrom kombu import Queue\nimport time\n\n\napp = Celery('tasks', backend='redis://127.0.0.1:6379/6')\napp.config_from_object('celeryconfig')\n\n\nclass CallbackTask(Task):\n    def on_success(self, retval, task_id, args, kwargs):\n        print \"----%s is done\" % task_id\n\n    def on_failure(self, exc, task_id, args, kwargs, einfo):\n        pass\n\n@app.task(base=CallbackTask) \ndef add(x, y):\n    return x + y\n\n\n@app.task(base=CallbackTask) \ndef multiply(x,y):\n    return x * y\n\nceleryconfig.py\nfrom kombu import Queue\nfrom kombu import Exchange\n\nresult_serializer = 'json'\n\n\nbroker_url = \"amqp://guest:guest@192.168.xx.xxx:5672/%2f\"\n\ntask_queues = (\n    Queue('priority_low',  exchange=Exchange('priority', type='direct'), routing_key='priority_low'),\n    Queue('priority_high',  exchange=Exchange('priority', type='direct'), routing_key='priority_high'),\n)\n\ntask_routes = ([\n    ('tasks.add', {'queue': 'priority_low'}),\n    ('tasks.multiply', {'queue': 'priority_high'}),\n],)\n\ntask_annotations = {\n    'tasks.add': {'rate_limit': '10/m'}\n}\n\nCelery Server and Client\nWorker on Server1\n消费priority_high事件\ncelery -A tasks worker -Q priority_high --concurrency=4 -l info -E -n worker1@%h\nWorker on Server2\n消费priority_high和priority_low事件\ncelery -A tasks worker -Q priority_high,priority_low --concurrency=4  -l info -E -n worker2@%h\nClient\n生产者，pushlish 事件到broker\nfrom tasks import add\nfrom tasks import multiply\n\n\nfor i in xrange(50):\n    add.delay(2, 2)\n    multiply.delay(10,10)\n\n监控\ninstall\npip install flower\n启动flower\n假设在server2上启动flower，flower默认的端口是5555.\ncelery  flower --broker=amqp://guest:guest@192.168.xx.xxx:5672//\n监控界面\n在浏览器上输入 http://server2_ip:5555, 可以看到如下界面：从queued tasks途中，可以看出 priority_high中的task先消费完，和预期是一样的。\n\n                ", "mainLikeNum": ["5 "], "mainBookmarkNum": "11"}