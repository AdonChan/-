{"title": "Python 爬虫实战（二）：使用 requests-html - 吴小龙同学 ", "index": "网页爬虫,python", "content": "Python 爬虫实战（一）：使用 requests 和 BeautifulSoup，我们使用了 requests 做网络请求，拿到网页数据再用 BeautifulSoup 解析，就在前不久，requests 作者 kennethreitz 出了一个新库 requests-html，Pythonic HTML Parsing for Humans™，它可以用于解析 HTML 文档的。requests-html 是基于现有的框架 PyQuery、Requests、lxml 等库进行了二次封装，更加方便开发者调用。\n安装\nMac：\npip3 install requests-html\nWindows：\npip install requests-html\n实例\n\n代码撸多了，让我们看会妹纸，爬的网站我选的是 http://www.win4000.com/zt/xin... ，打开网站，观察到这是个列表，图片是缩略图，要想保存图片到本地，当然需要高清大图，因此得进入列表详情，进一步解析，完整代码如下：\nfrom requests_html import HTMLSession\nimport requests\nimport time\n\nsession = HTMLSession()\n\n\n# 解析图片列表\ndef get_girl_list():\n    # 返回一个 response 对象\n    response = session.get('http://www.win4000.com/zt/xinggan.html')  # 单位秒数\n\n    content = response.html.find('div.Left_bar', first=True)\n\n    li_list = content.find('li')\n\n    for li in li_list:\n        url = li.find('a', first=True).attrs['href']\n        get_girl_detail(url)\n\n\n# 解析图片详细\ndef get_girl_detail(url):\n    # 返回一个 response 对象\n    response = session.get(url)  # 单位秒数\n    content = response.html.find('div.scroll-img-cont', first=True)\n    li_list = content.find('li')\n    for li in li_list:\n        img_url = li.find('img', first=True).attrs['data-original']\n        img_url = img_url[0:img_url.find('_')] + '.jpg'\n        print(img_url + '.jpg')\n        save_image(img_url)\n\n\n# 保持大图\ndef save_image(img_url):\n    img_response = requests.get(img_url)\n    t = int(round(time.time() * 1000))  # 毫秒级时间戳\n    f = open('/Users/wuxiaolong/Desktop/Girl/%d.jpg' % t, 'ab')  # 存储图片，多媒体文件需要参数b（二进制文件）\n    f.write(img_response.content)  # 多媒体存储content\n    f.close()\n\n\nif __name__ == '__main__':\n    get_girl_list()\n\n代码就这么多，是不是感觉很简单啊。\n说明：\n1、requests-html 与 BeautifulSoup 不同，可以直接通过标签来 find，一般如下：标签标签.someClass标签#someID标签[target=_blank]参数 first 是 True，表示只返回 Element 找到的第一个，更多使用：http://html.python-requests.org/ ；\n2、这里保存本地路径 /Users/wuxiaolong/Desktop/Girl/我写死了，需要读者改成自己的，如果直接是文件名，保存路径将是项目目录下。\n遗留问题\n示例所爬网站是分页的，没有做，可以定时循环来爬妹纸哦，有兴趣的读者自己玩下。\n参考\nrequests-html\n今天用了一下Requests-HTML库（Python爬虫）\n公众号\n我的公众号：吴小龙同学，欢迎交流～\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "1"}