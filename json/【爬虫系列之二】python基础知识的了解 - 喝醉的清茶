{"title": "【爬虫系列之二】python基础知识的了解 - 喝醉的清茶 ", "index": "python", "content": "了解了 前面的环境搭建，以及python的基础知识的学习，咱们接下来学习一下，如何将网页扒下来\n一、案例介绍\n当我们打开浏览器，打开页面，看到的是好看的页面，但是其实是由浏览器解释才呈现的，实际上这好看的页面，是通过html，css，js组合形成的。\n接下来，我们就写个小例子来扒下来网页（本篇文章，以python2.7为例子）\n# coding:utf-8\n\nimport urllib2\nresponse = urllib2.urlopen(\"http://music.163.com/\")\nprint response.read()\n二、分析案例的方法\n然后让我们来分析一下上述例子的代码是干啥的\n第一行：import urllib2 \n该行代码是通过import将python的urllib2库引入\n第二行：response = urllib2.urlopen(\"http://music.163.com/\")\n这行代码是调用urllib2库中的urlopen方法,然后传入了一个网易云音乐的url地址,urlopen方法为`urlopen(url, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,\n        cafile=None, capath=None, cadefault=False, context=None)`\n其中第一个参数就是url，然后其它几个参数都是可以不传送的，其中第二个参数表示的访问该网址需要的传送的数据，第三个参数timeout表示的是过期时间，默认值是socket._GLOBAL_DEFAULT_TIMEOUT\n第三行：print response.read()其中response对象有一个read方法，可以返回获取到的页面内容，千万记得添加上read方法，不然会出现打印出来是对象。\n三、Request的构造\n将上述例子修改下，如下，其中urlopen是可以传入一个request请求，它是一个Request类的实例,构造的时候可以传入url，data,header等参数,它可以让我们构造一个request对象，在request请求中添加相关的header头文件。\n# coding:utf-8\n\nimport urllib2\n\nrequest = urllib2.Request(\"http://music.163.com/\")\nresponse = urllib2.urlopen(request)\n\nprint response.read()\n四、POST和GET数据传送\nGET 数据传送\nGET方式我们可以直接把参数写到网址上面，直接构建一个带参数的URL出来即可。\nimport urllib\nimport urllib2\n\nvalues = {}\nvalues['wd'] = \"zs\"\ndata = urllib.urlencode(values)\nurl = \"http://www.baidu.com/s\" + '?' + data\nrequest = urllib2.Request(url)\nresponse = urllib2.urlopen(request)\n\nprint response.read()\nPOST 数据传送\n上面介绍了data参数 ，这里就是将需要的参数 通过data方式传入\nimport urllib\nimport urllib2\n \nvalues = {}\nvalues['username'] = \"1016903103@qq.com\"\nvalues['password'] = \"XXXX\"\ndata = urllib.urlencode(values) \nurl = \"https://passport.jd.com/new/login.aspx?ReturnUrl=https%3A%2F%2Fwww.jd.com%2F\"\nrequest = urllib2.Request(url,data)\nresponse = urllib2.urlopen(request)\nprint response.read()\n上述代码引入了urllib库，现在我们模拟登陆京东商场，但是应该是无法登陆的，一般都是需要设置一些头部header的工作，或者其它的参数，这里使用了urllib库对参数，进行urlencode一下。\n五、设置headers\n上面描述了一下，一般请求都需要带上各种header属性。看下图，是京东的登陆页面，然后可以打开谷歌浏览器F12，然后可以看到请求，上面headers设置了挺多的参数，其中agent是请求的身份，还有refer，都是用来反盗链。看下面例子\nimport urllib  \nimport urllib2  \n \nurl = 'https://passport.jd.com/new/login.aspx?ReturnUrl=https%3A%2F%2Fwww.jd.com%2F'\nuser_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36'\nrefer ='https://www.jd.com/'\nvalues = {'username' : 'cqc',  'password' : 'XXXX' }  \nheaders = { 'User-Agent' : user_agent,'Refer':refer }  \ndata = urllib.urlencode(values)  \nrequest = urllib2.Request(url, data, headers)  \nresponse = urllib2.urlopen(request)  \npage = response.read()\n一般进行爬虫的时候，可以考虑检查浏览器的headers的内容\n六、Proxy（代理）的设置\nurllib2 默认会使用 http_proxy 来设置 HTTP Proxy。假如一个网站它会某一段时间某个IP 的访问次数，如果访问次数过多，它会禁止你的访问。所以你可以设置一些来帮助你做工作，每隔一段时间换一个代理，网站君都不知道是谁在捣鬼了，这酸爽！下面一段代码说明设置方法\nimport urllib2\nenable_proxy = True\nproxy_handler = urllib2.ProxyHandler({\"http\" : 'http://some-proxy.com:8080'})\nnull_proxy_handler = urllib2.ProxyHandler({})\nif enable_proxy:\n    opener = urllib2.build_opener(proxy_handler)\nelse:\n    opener = urllib2.build_opener(null_proxy_handler)\nurllib2.install_opener(opener)\n七、使用 HTTP 的 PUT 和 DELETE 方法\n下面的案例是用来单独设置http的put和delete，请求\nimport urllib2\nrequest = urllib2.Request(uri, data=data)\nrequest.get_method = lambda: 'PUT' # or 'DELETE'\nresponse = urllib2.urlopen(request)\n推荐阅读：\n【爬虫系列之一】爬虫开发环境的搭建\n更多精彩内容，欢迎大家关注我的微信公众号：喝醉的清茶\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}