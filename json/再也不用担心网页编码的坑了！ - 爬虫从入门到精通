{"title": "再也不用担心网页编码的坑了！ - 爬虫从入门到精通 ", "index": "requests,网页爬虫,python", "content": "大家爬取网页的时候，应该都遇到过这种情况 \n当我打印网页源代码的时候\n发现 全部是乱码的\n\n那这个时候应该怎么办呢？\nrequests是如何判断编码\n首先，response.content返回的内容 是二进制内容\nresponse.text 则是根据设置的encoding来解码\n# Try charset from content-type  \ncontent = None  \nencoding = self.encoding  \n  \nif not self.content:  \n  return str('')  \n  \n# Fallback to auto-detected encoding.  \nif self.encoding is None:  \n    encoding = self.apparent_encoding  \n  \n# Decode unicode from given encoding.  \ntry:  \n    content = str(self.content, encoding, errors='replace')  \nexcept (LookupError, TypeError):\n\n我们可以看到 ，当encoding为None的时候，  \n编码是通过chardet.detect来获取的,\ndef apparent_encoding(self):  \n\"\"\"The apparent encoding, provided by the chardet library.\"\"\"  \n    return chardet.detect(self.content)['encoding']\n那么chardet.detect 又是干嘛的呢？\n简单的讲，就是根据给定的字节，来返回他的编码\n至于他是如何实现的，欢迎去看源代码。。。\n上面说到了当encoding为None的时候,requests是如何设置encoding的\n那么encoding 默认编码是啥呢？继续查看源代码\n我们在adapters.py 里面找到了~\nresponse.encoding = get_encoding_from_headers(response.headers)\n\ndef get_encoding_from_headers(headers):  \n\"\"\"Returns encodings from given HTTP Header Dict.  \n    :param headers: dictionary to extract encoding from.  \n    :rtype: str  \n    \"\"\"  \n    content_type = headers.get('content-type')  \n  \nif not content_type:  \n  return None  \ncontent_type, params = cgi.parse_header(content_type)  \n\nif 'charset' in params:  \n  return params['charset'].strip(\"'\\\"\")  \n  \nif 'text' in content_type:  \n  return 'ISO-8859-1'\n简单讲就是 如何返回头里面没有content_type，则encoding为None\n如果charset在参数里面的话，则使用charset设置的值（看下图，github返回的）\n\n如果text在参数里面的话，则使用ISO-8859-1\n然后你打印下 你乱码网页的encoding，发现，还真是ISO-8859-1\n\n你会很奇怪，为啥当content-type为text/html的时候，编码为iso-8859-1呢？\n\n现在常见的编码不是utf8么，requests怎么这么傻*呢...\n然后发现是rfc2016的规定。。。\nrfc2016的链接在 https://www.ietf.org/rfc/rfc2...\n感兴趣的同学可以自行查阅...\n\n最后总结\n当返回头没有content_type 的时候，encoding使用chardet.detect 猜测出来的编码（一般都是很准的）  \n当返回头里面有content_type 的时候，如果有charset=xxx，则encoding的编码为chatset的值。如果只是text/html,则编码为ISO-8859-1\n那么当你发现response.text返回乱码的时候，怎么办呢。。。\n只要先设置编码为None...\n再打印.text就可以了..\nresponse.encoding = None  \nresponse.text\n本来呢，本篇文章到此结束了。。。但是呢。。。\n科普个小知识\n有几种方法可以知道网页的编码呢？\n\n我们上面讲过的 response.headers中的content_type\n通过chardet.detect猜测出来（上面讲过的）\n网页源代码中的 meta（且有charset的值）如下面的，则表示网页编码为gb2312（不过呢，有时候并不是很准，这个是前端瞎xx写的，这时候就可以用chardet.detect来猜测了...）\n\n方法3的代码如何写呢（如下）\ndef get_encodings_from_content(content):  \n\"\"\"Returns encodings from given content string.  \n    :param content: bytestring to extract encodings from.  \n    \"\"\"  \n    warnings.warn((  \n'In requests 3.0, get_encodings_from_content will be removed. For '  \n        'more information, please see the discussion on issue #2266. (This'  \n        ' warning should only appear once.)'),  \n        DeprecationWarning)  \n  \n    charset_re = re.compile(r'&lt;meta.*?charset=[\"\\']*(.+?)[\"\\'&gt;]', flags=re.I)  \n    pragma_re = re.compile(r'&lt;meta.*?content=[\"\\']*;?charset=(.+?)[\"\\'&gt;]', flags=re.I)  \n    xml_re = re.compile(r'^&lt;\\?xml.*?encoding=[\"\\']*(.+?)[\"\\'&gt;]')  \n  \n    return (charset_re.findall(content) +  \n            pragma_re.findall(content) +  \n            xml_re.findall(content))\n你会看到requests3.0版本的时候，这个方法会去掉，这又是为什么呢。。。\n截图自己看把，地址在https://github.com/requests/r...\n\n如果还有猜测编码的方法，欢迎留言\n完...\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "2"}