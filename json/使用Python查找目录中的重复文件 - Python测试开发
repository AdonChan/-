{"title": "使用Python查找目录中的重复文件 - Python测试开发 ", "index": "python", "content": "是这样的，电脑上的堆积的照片有点多，而且重复的照片被放在了不同的目录，占用的空间越来越大，数量也多得已经不太适合人工分辨整理，写个Python脚本来处理吧。\n文件的唯一标识 - MD5\n假如你要处理的重复文件有不同的文件名，最简单的办法就是通过MD5来确定两个文件是不是一样的。\ndef md5sum(filename, blocksize=65536):\n    hash = hashlib.md5()\n    with open(filename, \"rb\") as f:\n        for block in iter(lambda: f.read(blocksize), b\"\"):\n            hash.update(block)\n    return hash.hexdigest()\n这个方法可以快速获得一个文件的MD5值，blocksize 可以根据文件大小和CPU性能调整，一般选择的值约等于文件的平均大小。\n保存所有文件标识和路径\n接下来遍历所有文件，使用MD5作为key，路径作为value，保存起来。\ndup = {}\n\ndef build_hash_dict(dir_path, pattern='*.jpg'):\n    \n    def save(file):\n        hash = md5sum(file)\n        if hash not in dup.keys():\n            dup[hash] = [file]\n        else:\n            dup[hash].append(file)\n\n    p = Path(dir_path)\n    for item in p.glob('**/' + pattern):\n        save(str(item))\n处理重复文件\n最后一步非常简单，把上一步建立的字典做一个简单的过滤就能找到重复文件。\ndef get_duplicate():\n    return {k: v for k, v in dup.items() if len(v) > 1}\n\nfor hash, files in get_duplicate().items():\n    print(\"{}: {}\".format(hash, files))\n接下来你可以根据自己的需要删除或者保留某个路径下的文件，本文到此为止。\n完整的脚本代码： https://gist.github.com/tobyq...\n\n\n关于作者：Toby Qin, Python 技术爱好者，目前从事测试开发相关工作，转载请注明原文出处。\n欢迎关注我的博客 https://betacat.online，你可以到我的公众号中去当吃瓜群众。\n\n\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "0"}