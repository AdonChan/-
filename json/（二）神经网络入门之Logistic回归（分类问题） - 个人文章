{"title": "（二）神经网络入门之Logistic回归（分类问题） - 个人文章 ", "index": "python", "content": "作者：chen_h微信号 & QQ：862251340微信公众号：coderpai简书地址：https://www.jianshu.com/p/d94...\n\n这篇教程是翻译Peter Roelants写的神经网络教程，作者已经授权翻译，这是原文。\n该教程将介绍如何入门神经网络，一共包含五部分。你可以在以下链接找到完整内容。\n\n（一）神经网络入门之线性回归\nLogistic分类函数\n（二）神经网络入门之Logistic回归（分类问题）\n（三）神经网络入门之隐藏层设计\nSoftmax分类函数\n（四）神经网络入门之矢量化\n（五）神经网络入门之构建多层网络\n\nLogistic回归（分类问题）\n\n这部分教程将介绍一部分：\nLogistic分类模型\n我们在上次的教程中给出了一个很简单的模型，只有一个输入和一个输出。在这篇教程中，我们将构建一个二分类模型，输入参数是两个变量。这个模型在统计上被称为Logistic回归模型，网络结构可以被描述如下：\n\n我们先导入教程需要使用的软件包。\nimport numpy as np \nimport matplotlib.pyplot as plt \nfrom matplotlib.colors import colorConverter, ListedColormap\nfrom matplotlib import cm\n定义类分布\n在教程中，目标分类t将从两个独立分布中产生，当t=1时，用蓝色表示。当t=0时，用红色表示。输入参数X是一个N*2的矩阵，目标分类t是一个N * 1的向量。更直观的表现，见下图。\n# Define and generate the samples\nnb_of_samples_per_class = 20  # The number of sample in each class\nred_mean = [-1,0]  # The mean of the red class\nblue_mean = [1,0]  # The mean of the blue class\nstd_dev = 1.2  # standard deviation of both classes\n# Generate samples from both classes\nx_red = np.random.randn(nb_of_samples_per_class, 2) * std_dev + red_mean\nx_blue = np.random.randn(nb_of_samples_per_class, 2) * std_dev + blue_mean\n\n# Merge samples in set of input variables x, and corresponding set of output variables t\nX = np.vstack((x_red, x_blue))\nt = np.vstack((np.zeros((nb_of_samples_per_class,1)), np.ones((nb_of_samples_per_class,1))))\n# Plot both classes on the x1, x2 plane\nplt.plot(x_red[:,0], x_red[:,1], 'ro', label='class red')\nplt.plot(x_blue[:,0], x_blue[:,1], 'bo', label='class blue')\nplt.grid()\nplt.legend(loc=2)\nplt.xlabel('$x_1$', fontsize=15)\nplt.ylabel('$x_2$', fontsize=15)\nplt.axis([-4, 4, -4, 4])\nplt.title('red vs. blue classes in the input space')\nplt.show()\n\nLogistic函数和交叉熵损失函数\nLogistic函数\n我们设计的网络的目的是从输入的x去预测目标t。假设，输入x = [x1, x2]，权重w = [w1, w2]，预测目标t = 1。那么，概率P(t = 1|x, w)将是神经网络输出的y，即y = σ(x∗wT)。其中，σ表示Logistic函数，定义如下：\n\n如果，对于Logistic函数和它的导数还不是很清楚的，可以查看这个教程，里面进行了详细描述。\n交叉熵损失函数\n对于这个分类问题的损失函数优化，我们使用交叉熵误差函数来解决，对于每个训练样本i，交叉熵误差函数定义如下：\n\n如果我们要计算整个训练样本的交叉熵误差，那么只需要把每一个样本的值进行累加就可以了，即：\n\n关于交叉熵误差函数更加详细的介绍可以看这个教程。\nlogistic(z)函数实现了Logistic函数，cost(y, t)函数实现了损失函数，nn(x, w)实现了神经网络的输出结果，nn_predict(x, w)实现了神经网络的预测结果。\n# Define the logistic function\ndef logistic(z): \n    return 1 / (1 + np.exp(-z))\n\n# Define the neural network function y = 1 / (1 + numpy.exp(-x*w))\ndef nn(x, w): \n    return logistic(x.dot(w.T))\n\n# Define the neural network prediction function that only returns\n#  1 or 0 depending on the predicted class\ndef nn_predict(x,w): \n    return np.around(nn(x,w))\n    \n# Define the cost function\ndef cost(y, t):\n    return - np.sum(np.multiply(t, np.log(y)) + np.multiply((1-t), np.log(1-y)))\n# Plot the cost in function of the weights\n# Define a vector of weights for which we want to plot the cost\nnb_of_ws = 100 # compute the cost nb_of_ws times in each dimension\nws1 = np.linspace(-5, 5, num=nb_of_ws) # weight 1\nws2 = np.linspace(-5, 5, num=nb_of_ws) # weight 2\nws_x, ws_y = np.meshgrid(ws1, ws2) # generate grid\ncost_ws = np.zeros((nb_of_ws, nb_of_ws)) # initialize cost matrix\n# Fill the cost matrix for each combination of weights\nfor i in range(nb_of_ws):\n    for j in range(nb_of_ws):\n        cost_ws[i,j] = cost(nn(X, np.asmatrix([ws_x[i,j], ws_y[i,j]])) , t)\n# Plot the cost function surface\nplt.contourf(ws_x, ws_y, cost_ws, 20, cmap=cm.pink)\ncbar = plt.colorbar()\ncbar.ax.set_ylabel('$\\\\xi$', fontsize=15)\nplt.xlabel('$w_1$', fontsize=15)\nplt.ylabel('$w_2$', fontsize=15)\nplt.title('Cost function surface')\nplt.grid()\nplt.show()\n\n梯度下降优化损失函数\n梯度下降算法的工作原理是损失函数ξ对于每一个参数的求导，然后沿着负梯度方向进行参数更新。\n参数w按照一定的学习率沿着负梯度方向更新，即w(k+1)=w(k)−Δw(k+1)，其中Δw可以表示为：\n\n对于每个训练样本i，∂ξi/∂w计算如下：\n\n其中，yi=σ(zi)是神经元的Logistic输出，zi=xi∗wT是神经元的输入。\n在详细推导损失函数对于权重的导数之前，我们先这个教程中摘取几个推导。\n\n参考上面的分步推导，我们可以得到下面的详细推导：\n\n因此，对于每个权重的更新Δwj可以表示为：\n\n在批处理中，我们需要将N个样本的梯度都进行累加，即：\n\n在开始梯度下降算法之前，你需要对参数都进行一个随机数赋值过程，然后采用梯度下降算法更新参数，直至收敛。\ngradient(w, x, t)函数实现了梯度∂ξ/∂w，delta_w(w_k, x, t, learning_rate)函数实现了Δw。\n# define the gradient function.\ndef gradient(w, x, t):\n    return (nn(x, w) - t).T * x\n\n# define the update function delta w which returns the \n#  delta w for each weight in a vector\ndef delta_w(w_k, x, t, learning_rate):\n    return learning_rate * gradient(w_k, x, t)\n梯度下降更新\n我们在训练集X上面运行10次去做预测，下图中画出了前三次的结果，图中蓝色的点表示在第k次，w(k)的值。\n# Set the initial weight parameter\nw = np.asmatrix([-4, -2])\n# Set the learning rate\nlearning_rate = 0.05\n\n# Start the gradient descent updates and plot the iterations\nnb_of_iterations = 10  # Number of gradient descent updates\nw_iter = [w]  # List to store the weight values over the iterations\nfor i in range(nb_of_iterations):\n    dw = delta_w(w, X, t, learning_rate)  # Get the delta w update\n    w = w-dw  # Update the weights\n    w_iter.append(w)  # Store the weights for plotting\n# Plot the first weight updates on the error surface\n# Plot the error surface\nplt.contourf(ws_x, ws_y, cost_ws, 20, alpha=0.9, cmap=cm.pink)\ncbar = plt.colorbar()\ncbar.ax.set_ylabel('cost')\n\n# Plot the updates\nfor i in range(1, 4): \n    w1 = w_iter[i-1]\n    w2 = w_iter[i]\n    # Plot the weight-cost value and the line that represents the update\n    plt.plot(w1[0,0], w1[0,1], 'bo')  # Plot the weight cost value\n    plt.plot([w1[0,0], w2[0,0]], [w1[0,1], w2[0,1]], 'b-')\n    plt.text(w1[0,0]-0.2, w1[0,1]+0.4, '$w({})$'.format(i), color='b')\nw1 = w_iter[3]  \n# Plot the last weight\nplt.plot(w1[0,0], w1[0,1], 'bo')\nplt.text(w1[0,0]-0.2, w1[0,1]+0.4, '$w({})$'.format(4), color='b') \n# Show figure\nplt.xlabel('$w_1$', fontsize=15)\nplt.ylabel('$w_2$', fontsize=15)\nplt.title('Gradient descent updates on cost surface')\nplt.grid()\nplt.show()\n\n训练结果可视化\n下列代码，我们将训练的结果进行可视化。\n# Plot the resulting decision boundary\n# Generate a grid over the input space to plot the color of the\n#  classification at that grid point\nnb_of_xs = 200\nxs1 = np.linspace(-4, 4, num=nb_of_xs)\nxs2 = np.linspace(-4, 4, num=nb_of_xs)\nxx, yy = np.meshgrid(xs1, xs2) # create the grid\n# Initialize and fill the classification plane\nclassification_plane = np.zeros((nb_of_xs, nb_of_xs))\nfor i in range(nb_of_xs):\n    for j in range(nb_of_xs):\n        classification_plane[i,j] = nn_predict(np.asmatrix([xx[i,j], yy[i,j]]) , w)\n# Create a color map to show the classification colors of each grid point\ncmap = ListedColormap([\n        colorConverter.to_rgba('r', alpha=0.30),\n        colorConverter.to_rgba('b', alpha=0.30)])\n\n# Plot the classification plane with decision boundary and input samples\nplt.contourf(xx, yy, classification_plane, cmap=cmap)\nplt.plot(x_red[:,0], x_red[:,1], 'ro', label='target red')\nplt.plot(x_blue[:,0], x_blue[:,1], 'bo', label='target blue')\nplt.grid()\nplt.legend(loc=2)\nplt.xlabel('$x_1$', fontsize=15)\nplt.ylabel('$x_2$', fontsize=15)\nplt.title('red vs. blue classification boundary')\nplt.show()\n\n完整代码，点击这里\n\n作者：chen_h微信号 & QQ：862251340简书地址：https://www.jianshu.com/p/d94...\nCoderPai 是一个专注于算法实战的平台，从基础的算法到人工智能算法都有设计。如果你对算法实战感兴趣，请快快关注我们吧。加入AI实战微信群，AI实战QQ群，ACM算法微信群，ACM算法QQ群。长按或者扫描如下二维码，关注 “CoderPai” 微信号（coderpai）\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}