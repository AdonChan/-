{"title": "Naïve Bayes Classifiers - Machine Learning ", "index": "python", "content": "1.1 Exact Bayes Classifier\nWe would like to classify categorical output $(k_1,k_2,...,k_3)$ given some attributes$(x_1, x_2, ..., x_n)$\nFor example, we would like to predict the output is $k_1$ or $k_2$ given three attributes $A,B,C$\nIf  $P(k_1|A, B, C)$ > $P(k_2|A, B, C)$\nwe would like to say A, B, C are more likely to belong to $k_1$; vice versa\nNotation:\nIf A exists, A; if A does not exist, -A\nIf B exists, B; if B does not exist, -B\nIf C exists, C; if C does not exist, -C\nThen, if we apply Bayes' Theorm, \n$$P(k_1|A, B, C)$$ =$$\\frac{P(k_1)P(A,B,C|k_1)}{P(A,B,C)}$$\nBy applying total probability law, \n$ \\Longrightarrow$ $$\\frac{P(k_1)P(A,B,C|k_1)}{P(k_1)P(A,B,C|k_1)+P(k_2)P(A,B,C|k_2)}$$\nHowever, to calculate $P(A,B,C|k_1)$ needs $2^i$ spaces, where i = 3 in this case, to calculate $P(A,B,C|k_2)$ needs another $2^2$ spaces\nThe frequency table is like below:\n\n\nFreuency\nA, B, C\nA, B, -C\nA, -B, C\nA, -B, -C\n-A, B, C\n-A, B, -C\n-A, -B, C\n-A, -B, C\n\n\n\nk1\n1\n2\n3\n4\n5\n6\n7\n8\n\n\nk2\n9\n8\n7\n6\n5\n4\n3\n2\n\n\n\nTherefore, we introduce Naive Bayes Algorithm to reduce the storing space and computational speed.\n# 1.2 Naive Bayes Classifier\nWe assume class conditional independence, so that\n$P(A,B,C|k_1)$ is equal to $P(A|k_1)P(B|k_1)P(C|k_1)$\n$P(A,B,C|k_2)$ is equal to $P(A|k_2)P(B|k_2)P(C|k_2)$\nAnd now, we need only 2in records, where i is the number of attributes, and i being number of categorical output we will predict\n\n\nFreuency\nA\n-A\nB\n-B\nC\n-C\n\n\n\nk1\n1\n2\n3\n4\n5\n6\n\n\nk2\n7\n6\n5\n4\n3\n2\n\n\n\nTherefore, our problem \n$$P(k_1|A, B, C)$$= $$\\frac{P(k_1)P(A,B,C|k_1)}{P(k_1)P(A,B,C|k_1)+P(k_2)P(A,B,C|k_2)}$$$\\Longrightarrow$$$\\frac{P(k_1)[P(A|k_1)P(B|k_1)P(C|k_1)]}{P(k_1)[P(A|k_1)P(B|k_1)P(C|k_1)]+P(k_2)[P(A|k_2)P(B|k_2)P(C|k_2)]} \\ (i)$$\n$$P(k_2|A, B, C)$$= $$\\frac{P(k_2)P(A,B,C|k_2)}{P(k_1)P(A,B,C|k_1)+P(k_2)P(A,B,C|k_2)}$$$\\Longrightarrow$$$\\frac{P(k_2)[P(A|k_2)P(B|k_2)P(C|k_2)]}{P(k_1)[P(A|k_1)P(B|k_1)P(C|k_1)]+P(k_2)[P(A|k_2)P(B|k_2)P(C|k_2)]} \\ (ii)$$\nWe notice that (i),(ii) share the same numerator, we can focus only on the denominator\n$$P(k_1|A, B, C)$$= $$\\frac{P(k_1)P(A,B,C|k_1)}{P(k_1)P(A,B,C|k_1)+P(k_2)P(A,B,C|k_2)}$$$\\propto$$$P(k_1)[P(A|k_1)P(B|k_1)P(C|k_1)]$$\n$$P(k_2|A, B, C)$$= $$\\frac{P(k_2)P(A,B,C|k_2)}{P(k_1)P(A,B,C|k_1)+P(k_2)P(A,B,C|k_2)}$$$\\propto$$$P(k_2)[P(A|k_2)P(B|k_2)P(C|k_2)]$$\nIf $P(k_1)[P(A|k_1)P(B|k_1)P(C|k_1)]$ > $P(k_2)[P(A|k_2)P(B|k_2)P(C|k_2)]$, we say that A, B, C are more likely to belong to $k_1$; vice versa\n1.3 why not P(C | A, B) = P(C | A) * P(C | B)\nFrom 1.2, we know that from Naive Bayes Algorithm, we assume class conditional independence, so that\n$P(A,B | C)$ = $P( A | C) * P(B | C)$\nbuy why not diretly say that\n$P(C | A, B)$ = $P(C | A) * P(C | B)$\nThis is because it happens only when P(C) = 0 or P(1), meaningless\n$$P(C | A, B)=P(C | A) * P(C | B)$$ \n$\\Longrightarrow$\n$$\\frac{P(C,(A,B))}{P(A,B)} = \\frac{P(C,A)}{P(A)}*\\frac{P(C,B)}{P(B)} $$\n$\\Longrightarrow if\\ B\\ and\\ C\\ are\\ class\\ conditional\\ independent$\n$$\\frac{P(C)*P(A)*P(B)}{P(A)*P(B)} = \\frac{P(C)*P(A)}{P(A)}*\\frac{P(C)*P(B)}{P(B)} $$\n$\\Longrightarrow$\n$$P(C) = P(C)*P(C)$$\nwhere only possible if P(C) = 0, or, 1\nTherefore, we use Bayes therom to swap $P(C | A, B)$ to $P(A, B | C) $ before applying naive bayes algorithm\n2. Example\nConsider the following 4 SMS messages:\n\n\nmessage\nLabel\n\n\n\nI am not coming\nham\n\n\nGood work\nham\n\n\nDo you need viagra\nspam\n\n\nwin an IMac\nspam\n\n\n\n2.1 Compute the prior probabilities of a new SMS message being ‘spam’ or ‘ham’.\nLet $p(spam)$ be the probability of a new SMS message being \"spam\"\nLet $p(ham)$ be the probability of a new SMS message being \"ham\"\nTherefore$$p(spam)= 0.5$$ $$p(ham)=0.5$$\n2.2 For each de-capitalised keyword that appears in your training set (that is, ‘i’, ‘am’,‘not’, ‘coming’, ‘good’, ‘work’, ‘do’, ‘you’, ‘need’, ‘viagra’, ‘win’, ‘an’ and ‘imac’), build a frequency table that records the likelihoods P(W|ham), P(-W|ham), P(W|spam) and P(-W|spam).\nEach de-capitalised keyword are put into two rows(word row, and -word row): \nwe mark the number of ham massage that the certain keyword exists on the (word row, ham column);\nwe mark the number of ham massage that the certain keyword does not exist on the (-word row, ham column);\nwe mark the number of spam massage that the certain keyword exists on the (word row, spam column);\nwe mark the number of spam massage that the certain keyword does not exist on the (-word row, spam column);\nWe can construct a frequency table following:\n\n\nFrequency\nHam\nSpam\n\n\n\n-am\n1\n2\n\n\n-an\n2\n1\n\n\n-coming\n1\n2\n\n\n-do\n2\n1\n\n\n-good\n1\n2\n\n\n-i\n1\n2\n\n\n-imac\n2\n1\n\n\n-need\n2\n1\n\n\n-not\n1\n2\n\n\n-viagra\n2\n1\n\n\n-win\n2\n1\n\n\n-work\n1\n2\n\n\n-you\n2\n1\n\n\nam\n1\n0\n\n\nan\n0\n1\n\n\ncoming\n1\n0\n\n\ndo\n0\n1\n\n\ngood\n1\n0\n\n\ni\n1\n0\n\n\nimac\n0\n1\n\n\nneed\n0\n1\n\n\nnot\n1\n0\n\n\nviagra\n0\n1\n\n\nwin\n0\n1\n\n\nwork\n1\n0\n\n\nyou\n0\n1\n\n\n\nThen, to record the likelihoods P(W|ham), P(-W|ham), P(W|spam) and P(-W|spam), we divide each entry in ham column by 2 (the total number of ham messages), and divide each entry in spam column by 2 (the total number of spam messages).\nIn addition, to prevent the likelihood of 1 and 0, we replace any likelihood smaller than 0.05 (larger than 0.95) with 0.05 (0.95) by using one of the variants of the Laplace estimator.\nTherefore, we get the following likelihood table:\n\n\nProbability of the row name given the column name\nHam\nSpam\n\n\n\n-am\n0.50\n0.95\n\n\n-an\n0.95\n0.50\n\n\n-coming\n0.50\n0.95\n\n\n-do\n0.95\n0.50\n\n\n-good\n0.50\n0.95\n\n\n-i\n0.50\n0.95\n\n\n-imac\n0.95\n0.50\n\n\n-need\n0.95\n0.50\n\n\n-not\n0.50\n0.95\n\n\n-viagra\n0.95\n0.50\n\n\n-win\n0.95\n0.50\n\n\n-work\n0.50\n0.95\n\n\n-you\n0.95\n0.50\n\n\nam\n0.50\n0.05\n\n\nan\n0.05\n0.50\n\n\ncoming\n0.50\n0.05\n\n\ndo\n0.05\n0.50\n\n\ngood\n0.50\n0.05\n\n\ni\n0.50\n0.05\n\n\nimac\n0.05\n0.50\n\n\nneed\n0.05\n0.50\n\n\nnot\n0.50\n0.05\n\n\nviagra\n0.05\n0.50\n\n\nwin\n0.05\n0.50\n\n\nwork\n0.50\n0.05\n\n\nyou\n0.05\n0.50\n\n\n\n2.3 Predict if the following two SMS messages \"Coming home ?\" and \"Get Viagra now\" are ham or spam?\n2.3.1 For message \"coming home\":\nIf \n$$P(ham | -i , -am , -not , coming , -good , -work , -do , -you , -need , -viagra , -win , -an , -imac)$$\nis greater than \n$$P(spam | -i , -am , -not , coming , -good , -work , -do , -you , -need , -viagra , -win , -an , -imac)$$\nwe say that the message \"coming home\" is more likely to be a ham message; vice versa.\nAccording to Bayes’ Theorem,\n$$P(ham | -i , -am , -not , coming , -good , -work , -do , -you , -need , -viagra , -win , -an , -imac)$$\n$Bayes' Therom \\Longrightarrow$\n$$\\frac{P(ham)P(-i , -am , -not , coming , -good , -work , -do , -you , -need , -viagra , -win , -an , -imac| ham)}{P( -i , -am , -not , coming , -good , -work , -do , -you , -need , -viagra , -win , -an , -imac)}$$\n$Total probability law  \\Longrightarrow$\n$$frac{P(ham)P(-i , -am , -not , coming , -good , -work , -do , -you , -need , -viagra , -win , -an , -imac| ham)}{P(ham)P(-i , -am , -not , coming , -good , -work , -do , -you , -need , -viagra , -win , -an , -imac| ham)+P(spam)P(-i , -am , -not , coming , -good , -work , -do , -you , -need , -viagra , -win , -an , -imac| spam)} $$\n$naive bayes simplify \\Longrightarrow$\n$$\n\\frac{P(ham)[P(-i|ham)P(-am|ham)P(-not|ham)P(coming|ham)P(-good|ham)P(-work|ham)\nP(-do|ham)P(-you|ham)P(-need|ham)P(-viagra|ham)P(-win|ham)(-an|ham)P(-imac|ham)]}{P(ham)[P(-i|ham)P(-am|ham)P(-not|ham)P(coming|ham)P(-good|ham)P(-work|ham)\nP(-do|ham)P(-you|ham)P(-need|ham)P(-viagra|ham)P(-win|ham)(-an|ham)P(-imac|ham)]+P(spam)[P(-i|spam)P(-am|spam)P(-not|spam)P(coming|spam)P(-good|spam)P(-work|spam)\nP(-do|spam)P(-you|spam)P(-need|spam)P(-viagra|spam)P(-win|spam)(-an|spam)P(-imac|spam)]}$$\n\n=\n\n$$\\frac\n{0.5*(0.5*0.5*0.5*0.5*0.5*0.5*0.95*0.95*0.95*0.95*0.95*0.95*0.95)}{0.5*(0.5*0.5*0.5*0.5*0.5*0.5*0.95*0.95*0.95*0.95*0.95*0.95*0.95)+0.5*(0.95*0.95*0.95*0.05*0.95*0.95*0.5*0.5*0.5*0.5*0.5*0.5*0.5)}$$\n\n=\n\n$$\\frac{0.00545576012}{0.00545576012+0.00015112908}=0.97304582369$$\n\nAlternatively, we can focus only on the propensities which are proportional to posterior probability.\n\n$$P(ham | -i , -am , -not , coming , -good , -work , -do , -you , -need , -viagra , -win , -an , -imac)$$\n\n$\\propto$\n\n$$P(ham)[P(-i|ham)P(-am|ham)P(-not|ham)P(coming|ham)P(-good|ham)P(-work|ham)\nP(-do|ham)P(-you|ham)P(-need|ham)P(-viagra|ham)P(-win|ham)(-an|ham)P(-imac|ham)]$$\n\n= \n\n$$ 0.5*(0.5*0.5*0.5*0.5*0.5*0.5*0.95*0.95*0.95*0.95*0.95*0.95*0.95)$$\n\n= \n\n$$0.00545576012$$\n\nSimilarly, \n\nAccording to Bayes’ Theorem,\n\n$$P(spam | -i , -am , -not , coming , -good , -work , -do , -you , -need , -viagra , -win , -an , -imac)$$\n\n$Bayes' Therom \\Longrightarrow$\n\n$$\\frac{P(spam)P(-i , -am , -not , coming , -good , -work , -do , -you , -need , -viagra , -win , -an , -imac| spam)}{P( -i , -am , -not , coming , -good , -work , -do , -you , -need , -viagra , -win , -an , -imac)}$$\n\n$Total probability law  \\Longrightarrow$\n\n$$\\frac\n{P(spam)P(-i , -am , -not , coming , -good , -work , -do , -you , -need , -viagra , -win , -an , -imac| spam)}\n{P(ham)P(-i , -am , -not , coming , -good , -work , -do , -you , -need , -viagra , -win , -an , -imac| ham)\n+P(spam)P(-i , -am , -not , coming , -good , -work , -do , -you , -need , -viagra , -win , -an , -imac| spam)} $$\n\n$naive bayes simplify \\Longrightarrow$\n\n$$\\frac{P(spam)[P(-i|spam)P(-am|spam)P(-not|spam)P(coming|spam)P(-good|spam)P(-work|spam)\nP(-do|spam)P(-you|spam)P(-need|spam)P(-viagra|spam)P(-win|spam)(-an|spam)P(-imac|spam)]}{P(ham)[P(-i|ham)P(-am|ham)P(-not|ham)P(coming|ham)P(-good|ham)P(-work|ham)\nP(-do|ham)P(-you|ham)P(-need|ham)P(-viagra|ham)P(-win|ham)(-an|ham)P(-imac|ham)]+P(spam)[P(-i|spam)P(-am|spam)P(-not|spam)P(coming|spam)P(-good|spam)P(-work|spam)\nP(-do|spam)P(-you|spam)P(-need|spam)P(-viagra|spam)P(-win|spam)(-an|spam)P(-imac|spam)]}$$\n\n=\n\n$$\\frac{0.5*(0.95*0.95*0.95*0.05*0.95*0.95*0.5*0.5*0.5*0.5*0.5*0.5*0.5)}{0.5*(0.5*0.5*0.5*0.5*0.5*0.5*0.95*0.95*0.95*0.95*0.95*0.95*0.95)+0.5*(0.95*0.95*0.95*0.05*0.95*0.95*0.5*0.5*0.5*0.5*0.5*0.5*0.5)}\n$$\n=\n$$\\frac{0.00015112908}{0.00545576012+0.00015112908}=0.0269541763$$\nAlternatively, we can focus only on the propensities which are proportional to posterior probability.\n$$P(spam | -i , -am , -not , coming , -good , -work , -do , -you , -need , -viagra , -win , -an , -imac)$$\n$\\propto$\n$$P(spam)[P(-i|spam)P(-am|spam)P(-not|spam)P(coming|spam)P(-good|spam)P(-work|spam)P(-do|spam)P(-you|spam)P(-need|spam)P(-viagra|spam)P(-win|spam)(-an|spam)P(-imac|spam)]$$\n= \n$$ 0.5*(0.95*0.95*0.95*0.05*0.95*0.95*0.5*0.5*0.5*0.5*0.5*0.5*0.5)$$\n= \n$$0.00015112908$$\nSince probability 0.00545576012 > 0.00015112908, or propensity 0.97304582369 > 0.0269541763, we conclude that the message \"coming home\" is more likely to be a ham message.\n2.3.2 For message\"Get Viagra now\":\n$$P(ham | -i , -am , -not , -coming , -good , -work , -do , -you , -need , viagra , -win , -an , -imac)$$\n$\\propto$\n$$P(ham)[P(-i|ham)P(-am|ham)P(-not|ham)P(-coming|ham)P(-good|ham)P(-work|ham)P(-do|ham)P(-you|ham)P(-need|ham)P(viagra|ham)P(-win|ham)(-an|ham)P(-imac|ham)]$$\n=\n$$0.5*(0.5*0.5*0.5*0.5*0.5*0.5*0.95*0.95*0.95*0.05*0.95*0.95*0.95)=0.00028714526$$\n$$P(spam | -i , -am , -not , -coming , -good , -work , -do , -you , -need , viagra , -win , -an , -imac) $$\n$\\propto$\n$$P(spam)[P(-i|spam)P(-am|spam)P(-not|spam)P(-coming|spam)P(-good|spam)P(-work|spam)P(-do|spam)P(-you|spam)P(-need|spam)P(viagra|spam)P(-win|spam)(-an|spam)P(-imac|spam)] $$\n=\n$$0.5*(0.95*0.95*0.95*0.95*0.95*0.95*0.5*0.5*0.5*0.5*0.5*0.5*0.5)=0.00287145269$$\nTo calculate the probability of we divide 0.00028714526 and 0.00287145269 by (0.00028714526+0.00287145269), respectively, and the probability is 0.09090908831 and 0.90909091168, which again suggested \"Get Viagra now\" is more likely to be a spam message.\nSince the propensity 0.00028714526 < 0.00287145269, or the probability 0.09090908831< 0.90909091168, we believe that the message \"Get Viagra now\" is more likely to be a spam message.\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}