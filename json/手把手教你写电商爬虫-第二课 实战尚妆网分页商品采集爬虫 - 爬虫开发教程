{"title": "手把手教你写电商爬虫-第二课 实战尚妆网分页商品采集爬虫 - 爬虫开发教程 ", "index": "网页爬虫,电商网站,python,github,javascript", "content": "系列教程\n手把手教你写电商爬虫-第一课 找个软柿子捏捏\n如果没有看过第一课的朋友，请先移步第一课，第一课讲了一些基础性的东西，通过软柿子\"切糕王子\"这个电商网站好好的练了一次手，相信大家都应该对写爬虫的流程有了一个大概的了解，那么这课咱们就话不多说，正式上战场，对垒尚妆网。\n首先，向我们被爬网站致敬，没有他们提供数据，我们更是无从爬起，所以先安利一下尚妆网：\n经营化妆品时尚购物，大数据为驱动，并依托智能首饰为入口的新一代智慧美妆正品电子商务平台。其创始团队来自天猫、支付宝、欧莱雅、薇姿等互联网公司和化妆品集团。好吧，我很懒，直接从百度知道里抄过来的，不过不代表我没有诚意。OK，言归正传，我们先把我们的工具包拿出来：\n1、神箭手云爬虫，2、Chrome浏览器 3、Chrome的插件XpathHelper 不知道是干嘛的同学请移步第一课\n古代战士上战场前，必须先好好的观察对手，所谓知己知彼，百战不殆。我们先来观察一下尚妆网\n\n从首页大家能看出什么？说美女很美的，还有说美女表情很到位的同学，你们可以先回家了。\n剩下的同学，我们继续了：\n可以看出，作为一个完善的电商网站，尚妆网有着普通电商网站所拥有的主要的元素，包括分类，分页，主题等等。首先我们要确定我们希望要爬取哪一类数据，当然作为爬虫来说，全部爬下来不是不行，不过对于做实验来说，就没必要了。好，我们假设：我们要爬护肤里的面膜品类所有商品，价格和销量，至于为什么是面膜，你们猜呢？\n废话太多了，我们开始爬虫三步走，跟着我再背诵一遍：1、选入口Url 2、限定内容页和中间页 3、写内容页抽取规则\n1、选定入口url\n这个简单，找到面膜页面的地址：\nhttp://list.showjoy.com/search/?q=cateIds%3A1,cateName%3A%E9%9D%A2%E8%86%9C\n\n好，就是它了。\n2、区分内容页和中间页\n好，重点来了，尚妆网的列表页面，是通过ajax动态加载了，这个怎么实现呢？我们先不着急，先看下内容页\nhttp://item.showjoy.com/sku/26551.html\n\nhttp://item.showjoy.com/sku/100374.html\n\n内容页很简单，我们直接提取成正则表达式\nhttp://item\\\\.showjoy\\\\.com/sku/\\\\d+\\\\.html\n\n那么列表页呢？首先，第一个当然是：\nhttp://list.showjoy.com/search/?q=cateIds%3A1,cateName%3A%E9%9D%A2%E8%86%9C\n\n下一页的链接是什么呢？这个时候就需要借助chrome浏览器的开发者工具，我们打开工具，切换到network选项卡，向下滑动加载下一页，可以看到展示出的连接地址：\n\n注意，可以忽略掉png这些图片的文件，直接看到下一页的连接，我们将链接复制出来：\nhttp://list.showjoy.com/search/?q=cateIds%3A1,cateName%3A%E9%9D%A2%E8%86%9C&stock=1&page=4&_synToken=59a6c555b0947486769f35d010353cd5\n\n看着好像很复杂，不过page我认识，其他的可以去掉吗？我们试一下访问\nhttp://list.showjoy.com/search/?q=cateIds%3A1,cateName%3A%E9%9D%A2%E8%86%9C&page=4\n\n貌似正常打开，而且也可以显示不同的商品，就此我们可以看出来，这个ajax加载下一页不过是一个纸老虎，根本没什么可怕的。我们将这个提取成正则表达式，另外 值得注意的是，由于我们第一页可能是没有page的，所以也需要考虑没有page参数的情况 6\nhttp://list\\\\.showjoy\\\\.com/search/\\\\?q=cateIds%3A1,cateName%3A%E9%9D%A2%E8%86%9C(&page=\\\\d+)?\n\n第三步：就是写内容页的抽取规则了，我们就抽取商品名称，评价数和成交数这三项数据吧，有人要问了，为啥不要价格呢。我只能说，too young too native，你打开商品页面的时候，有没有注意到价格的地方也一个快速的异步加载。考虑到咱们毕竟才第二课，而且刚刚还没那个ajax搞得虎躯一震，差一点把这节课改成第三课，所以咱们这里先降低点难度，下一课咱们用一节课的时间来探讨下这个价格该怎么提取。\n\n根据前面课程教的方案，我们同样的方法，写出xpath：\n标题：  //h3[contains(@class,\"choose-hd\")]\n\n评价： //div[contains(@class,\"dtabs-hd\")]/ul/li[2]\n\n成交记录：//div[contains(@class,\"dtabs-hd\")]/ul/li[3]\n\n通过xpath helper进行验证之后没有问题，这样我们可以组合代码得到下面的结果\nvar configs = {  \n    domains: [\"www.showjoy.com\",\"list.showjoy.com\",\"item.showjoy.com\"],  \n    scanUrls: [\"http://list.showjoy.com/search/?q=cateIds%3A1,cateName%3A%E9%9D%A2%E8%86%9C\"],  \n    contentUrlRegexes: [\"http://item\\\\.showjoy\\\\.com/sku/\\\\d+\\\\.html\"],  \n    helperUrlRegexes: [\"http://list\\\\.showjoy\\\\.com/search/\\\\?q=cateIds%3A1,cateName%3A%E9%9D%A2%E8%86%9C(\\\\&page=\\\\d+)?\"],//可留空  \n    fields: [  \n        {  \n            // 第一个抽取项  \n            name: \"title\",  \n            selector: \"//h3[contains(@class,'choose-hd')]\",//默认使用XPath  \n            required: true //是否不能为空  \n        },  \n        {  \n            // 第二个抽取项  \n            name: \"comment\",  \n            selector: \"//div[contains(@class,'dtabs-hd')]/ul/li[2]\",//使用正则的抽取规则  \n            required: false //是否不能为空  \n        },  \n        {  \n            // 第三个抽取项  \n            name: \"sales\",  \n            selector: \"//div[contains(@class,'dtabs-hd')]/ul/li[3]\",//使用正则的抽取规则  \n            required: false //是否不能为空  \n        }  \n    ]  \n};  \n  \nstart(configs);  \n\n可以看到在domains里 我填入了三个域名，这里是一定要注意的，因为他的列表页和详情页的域名都不一致，因此需要把每一个域名都写进去。好了，代码运行正常，但是启动任务之后发现，怎么第二页的内容没有采集到呢？ 还有前面说的价格咱们也采集不到，到底还能不能愉快的玩耍了呢？ 我们第三课就给大家讲讲如何解决ajax页面的url发现和ajax加载内容的提取。\n对爬虫感兴趣的童鞋可以加qq群讨论：342953471。\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "5"}