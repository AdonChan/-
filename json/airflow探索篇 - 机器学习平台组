{"title": "airflow探索篇 - 机器学习平台组 ", "index": "python", "content": "airflow是一个 Airbnb 的 Workflow 开源项目，在Github 上已经有超过两千星。data pipeline调度和监控工作流的平台，用于用来创建、监控和调整data pipeline。类似的产品有：Azkaban、oozie \npip方式安装\n默认已经安装python >= 2.7 以及 pip安装可以参考这篇，比较详细。airflow安装以及celery方式启动\n重要说明\n使用mysql需要安装\npython 2 : pip install MySQL-python\npython 3 : pip install PyMySQL\nAIRFLOW_HOME配置说明\n上篇在.bashrc中配置的export AIRFLOW_HOME=/home/airflow/airflow01。AIRFLOW_HOME设置目录在airflow initdb的时候初始化，存放airflow的配置文件airflow.cfg及相关文件。\nDAG说明-管理建议\n默认$AIRFLOW_HOME/dags存放定义的dag，可以分目录管理dag。常用管理dag做法，dag存放另一个目录通过git管理，并设置软连接映射到$AIRFLOW_HOME/dag。好处方便dag编辑变更，同时dag变更不会出现编辑到一半的时候就加载到airflow中。\nplugins说明-算子定义\n默认$AIRFLOW_HOME/plugins存放定义的plugins,自定义组件。可以自定义operator，hook等等。我们希望可以直接使用这种模式定义机器学习的一个算子。下面定义了一个简单的加法算子。\n# -*- coding: UTF-8 -*-\n# !/usr/bin/env python\n\nfrom airflow.plugins_manager import AirflowPlugin\nfrom airflow.models import BaseOperator\nfrom airflow.utils.decorators import apply_defaults\n\n# Will show up under airflow.operators.plus_plugin.PluginOperator\nclass PlusOperator(BaseOperator):\n\n    @apply_defaults\n    def __init__(self, op_args=None, params=None, provide_context=False, set_context=False, *args, **kwargs):\n        super(PlusOperator, self).__init__(*args, **kwargs)\n        self.params = params or {}\n        self.set_context = set_context\n\n    def execute(self, context):\n        if self.provide_context:\n            context.update(self.op_kwargs)\n            self.op_kwargs = context\n\n        puls = self.op_kwargs['a'] + self.op_kwargs['b']\n        print \"a =\", self.op_kwargs['a'], \". b=\", self.op_kwargs['a']\n        return_value = self.main()\n        context[self.task_id].xcom_push(key='return_value', value=return_value)\n        return puls\n\n\n# Defining the plugin class\nclass PlusPlugin(AirflowPlugin):\n    name = \"plus_plugin\"\n    operators = [PlusOperator]\n在dag中使用案例如下\nfrom airflow.operators.plus_plugin import PlusOperator\nplus_task = PlusOperator(task_id='plus_task', provide_context=True, params={'a': 1,'b':2},dag=dag)\n\n一些命令说明\n\n\n命令\n说明\n\n\n\nairflow webserver -p 8091\n8091启动webserver,通过页面查询不需要可以不启动\n\n\nairflow scheduler\n调度器，必须启动，不然dag没法run起来(使用CeleryExecutor、LocalExecutor时)\n\n\nairflow run dagid [time]\nrun task instance\n\n\nairflow backfill [dagid] -s[startTime] -e [endTime]\nrun a backfill over 2 days\n\n\n\nrun的demo\n# run your first task instance\nairflow run example_bash_operator runme_0 2018-01-11\n\n# run a backfill over 2 days\nairflow backfill example_bash_operator -s 2018-01-10 -e 2018-01-11\n\n基于CeleryExecutor方式的系统架构\n使用celery方式的系统架构图(官方推荐使用这种方式，同时支持mesos方式部署)。turing为外部系统，GDags服务帮助拼接成dag，可以忽略。\n\n1.master节点webui管理dags、日志等信息。scheduler负责调度，只支持单节点，多节点启动scheduler可能会挂掉\n2.worker负责执行具体dag中的task。这样不同的task可以在不同的环境中执行。\n\n\n基于LocalExecutor方式的系统架构图\n另一种启动方式的思考，一个dag分配到1台机器上执行。如果task不复杂同时task环境相同，可以采用这种方式，方便扩容、管理，同时没有master单点问题。\n\n基于源码的启动以及二次开发\n很多情况airflow是不满足我们需求，就需要自己二次开发，这时候就需要基于源码方式启动。比如日志我们期望通过http的方式提供出来，同其他系统查看。airflow自动的webserver只提供页面查询的方式。\n下载源码\ngithub源码地址 : [https://github.com/apache/inc...]git clone git@github.com:apache/incubator-airflow.git\n切换分支\nmaster分支的表初始化有坑，mysql设置的sql校验安全级别过高一直建表不成功。这个坑被整的有点惨。v1-8-stable或者v1-9-stable分支都可以。git checkout v1-8-stable\n安装必要Python包\n进入incubator-airflow，python setup.py install (没啥文档说明，又是一个坑。找了半天)\n初始化\n直接输入airflow initdb(python setup.py install这个命令会将airflow安装进去)\n修改配置\n进入$AIRFLOE_HOME (默认在~/airflow),修改airflow.cfg，修改mysql配置。可以查看上面推荐的文章以及上面的[使用mysql需要安装]\n启动\nairflow webserver -p 8085airflow scheduler\n获取日志信息的改造\n1.进入incubator-airflow/airflow/www/2.修改views.py在 class Airflow(BaseView)中添加下面代码\n@expose('/logs')\n    @login_required\n    @wwwutils.action_logging\n    def logs(self):\n        BASE_LOG_FOLDER = os.path.expanduser(\n            conf.get('core', 'BASE_LOG_FOLDER'))\n        dag_id = request.args.get('dag_id')\n        task_id = request.args.get('task_id')\n        execution_date = request.args.get('execution_date')\n        dag = dagbag.get_dag(dag_id)\n        log_relative = \"{dag_id}/{task_id}/{execution_date}\".format(\n            **locals())\n        loc = os.path.join(BASE_LOG_FOLDER, log_relative)\n        loc = loc.format(**locals())\n        log = \"\"\n        TI = models.TaskInstance\n        session = Session()\n        dttm = dateutil.parser.parse(execution_date)\n        ti = session.query(TI).filter(\n            TI.dag_id == dag_id, TI.task_id == task_id,\n            TI.execution_date == dttm).first()\n        dttm = dateutil.parser.parse(execution_date)\n        form = DateTimeForm(data={'execution_date': dttm})\n\n        if ti:\n            host = ti.hostname\n            log_loaded = False\n\n            if os.path.exists(loc):\n                try:\n                    f = open(loc)\n                    log += \"\".join(f.readlines())\n                    f.close()\n                    log_loaded = True\n                except:\n                    log = \"*** Failed to load local log file: {0}.\\n\".format(loc)\n            else:\n                WORKER_LOG_SERVER_PORT = \\\n                    conf.get('celery', 'WORKER_LOG_SERVER_PORT')\n                url = os.path.join(\n                    \"http://{host}:{WORKER_LOG_SERVER_PORT}/log\", log_relative\n                ).format(**locals())\n                log += \"*** Log file isn't local.\\n\"\n                log += \"*** Fetching here: {url}\\n\".format(**locals())\n                try:\n                    import requests\n                    timeout = None  # No timeout\n                    try:\n                        timeout = conf.getint('webserver', 'log_fetch_timeout_sec')\n                    except (AirflowConfigException, ValueError):\n                        pass\n\n                    response = requests.get(url, timeout=timeout)\n                    response.raise_for_status()\n                    log += '\\n' + response.text\n                    log_loaded = True\n                except:\n                    log += \"*** Failed to fetch log file from worker.\\n\".format(\n                        **locals())\n\n            if not log_loaded:\n                # load remote logs\n                remote_log_base = conf.get('core', 'REMOTE_BASE_LOG_FOLDER')\n                remote_log = os.path.join(remote_log_base, log_relative)\n                log += '\\n*** Reading remote logs...\\n'\n\n                # S3\n                if remote_log.startswith('s3:/'):\n                    log += log_utils.S3Log().read(remote_log, return_error=True)\n\n                # GCS\n                elif remote_log.startswith('gs:/'):\n                    log += log_utils.GCSLog().read(remote_log, return_error=True)\n\n                # unsupported\n                elif remote_log:\n                    log += '*** Unsupported remote log location.'\n\n            session.commit()\n            session.close()\n\n        if PY2 and not isinstance(log, unicode):\n            log = log.decode('utf-8')\n\n        title = \"Log\"\n\n        return wwwutils.json_response(log)\n3.重启服务,访问url如:\nhttp://localhost:8085/admin/airflow/logs?task_id=run_after_loop&dag_id=example_bash_operator&execution_date=2018-01-11\n就可以拿到这个任务在execution_date=2018-01-11的日志\n异步任务思考\n案例：task通过http请求大数据操作，拆分一些数据，存入一些临时表。方案：1.新建一张task实例的状态表如：task_instance_state。2.扩展一个plugins,如：AsyncHttpOperator。AsyncHttpOperator实现逻辑：\n\n在task_instance_state插入一条running状态记录running。\n发送http请求给大数据平台，操作数据。\n轮询查询task_instance_state状态是成功、失败、running。如是running则继续轮询，成功、失败操作相应后续操作。\n\n3.提供一个restful api update task_instance_state,供大数据平台回调，修改任务实例状态。\n不错的文章推荐\n瓜子云的任务调度系统Get started developing workflows with Apache Airflow官网地址生产环境使用可能遇到的坑初探airflow焦油坑系统研究Airbnb开源项目airflow\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "2"}