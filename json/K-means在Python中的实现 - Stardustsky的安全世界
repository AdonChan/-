{"title": "K-means在Python中的实现 - Stardustsky的安全世界 ", "index": "python,k-means,机器学习", "content": "K-means算法简介\nK-means是机器学习中一个比较常用的算法，属于无监督学习算法，其常被用于数据的聚类，只需为它指定簇的数量即可自动将数据聚合到多类中，相同簇中的数据相似度较高，不同簇中数据相似度较低。\nK-menas的优缺点：\n优点：\n\n原理简单\n速度快\n对大数据集有比较好的伸缩性\n\n缺点：\n\n需要指定聚类 数量K\n对异常值敏感\n对初始值敏感\n\nK-means的聚类过程\n其聚类过程类似于梯度下降算法，建立代价函数并通过迭代使得代价函数值越来越小\n\n适当选择c个类的初始中心；\n在第k次迭代中，对任意一个样本，求其到c个中心的距离，将该样本归到距离最短的中心所在的类；\n利用均值等方法更新该类的中心值；\n对于所有的c个聚类中心，如果利用（2）（3）的迭代法更新后，值保持不变，则迭代结束，否则继续迭代。\n\n\n该算法的最大优势在于简洁和快速。算法的关键在于初始中心的选择和距离公式。\nK-means 实例展示\npython中km的一些参数：\nsklearn.cluster.KMeans(\n    n_clusters=8,\n    init='k-means++', \n    n_init=10, \n    max_iter=300, \n    tol=0.0001, \n    precompute_distances='auto', \n    verbose=0, \n    random_state=None, \n    copy_x=True, \n    n_jobs=1, \n    algorithm='auto'\n    )\nn_clusters: 簇的个数，即你想聚成几类\ninit: 初始簇中心的获取方法\nn_init: 获取初始簇中心的更迭次数，为了弥补初始质心的影响，算法默认会初始10个质心，实现算法，然后返回最好的结果。\nmax_iter: 最大迭代次数（因为kmeans算法的实现需要迭代）\ntol: 容忍度，即kmeans运行准则收敛的条件\nprecompute_distances:是否需要提前计算距离，这个参数会在空间和时间之间做权衡，如果是True 会把整个距离矩阵都放到内存中，auto 会默认在数据样本大于featurs*samples 的数量大于12e6 的时候False,False 时核心实现的方法是利用Cpython 来实现的\nverbose: 冗长模式（不太懂是啥意思，反正一般不去改默认值）\nrandom_state: 随机生成簇中心的状态条件。\ncopy_x: 对是否修改数据的一个标记，如果True，即复制了就不会修改数据。bool 在scikit-learn 很多接口中都会有这个参数的，就是是否对输入数据继续copy 操作，以便不修改用户的输入数据。这个要理解Python 的内存机制才会比较清楚。\nn_jobs: 并行设置\nalgorithm: kmeans的实现算法，有：’auto’, ‘full’, ‘elkan’, 其中 ‘full’表示用EM方式实现\n虽然有很多参数，但是都已经给出了默认值。所以我们一般不需要去传入这些参数,参数的。可以根据实际需要来调用。\n下面展示一个代码例子\nfrom sklearn.cluster import KMeans\nfrom sklearn.externals import joblib\nfrom sklearn import cluster\nimport numpy as np\n\n# 生成10*3的矩阵\ndata = np.random.rand(10,3)\nprint data\n# 聚类为4类\nestimator=KMeans(n_clusters=4)\n# fit_predict表示拟合+预测，也可以分开写\nres=estimator.fit_predict(data)\n# 预测类别标签结果\nlable_pred=estimator.labels_\n# 各个类别的聚类中心值\ncentroids=estimator.cluster_centers_\n# 聚类中心均值向量的总和\ninertia=estimator.inertia_\n\nprint lable_pred\nprint centroids\nprint inertia\n\n代码执行结果\n[0 2 1 0 2 2 0 3 2 0]\n\n[[ 0.3028348   0.25183096  0.62493622]\n [ 0.88481287  0.70891813  0.79463764]\n [ 0.66821961  0.54817207  0.30197415]\n [ 0.11629904  0.85684903  0.7088385 ]]\n \n0.570794546829\n为了更直观的描述，这次在图上做一个展示，由于图像上绘制二维比较直观，所以数据调整到了二维，选取100个点绘制，聚类类别为3类\nfrom sklearn.cluster import KMeans\nfrom sklearn.externals import joblib\nfrom sklearn import cluster\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = np.random.rand(100,2)\nestimator=KMeans(n_clusters=3)\nres=estimator.fit_predict(data)\nlable_pred=estimator.labels_\ncentroids=estimator.cluster_centers_\ninertia=estimator.inertia_\n#print res\nprint lable_pred\nprint centroids\nprint inertia\n\nfor i in range(len(data)):\n    if int(lable_pred[i])==0:\n        plt.scatter(data[i][0],data[i][1],color='red')\n    if int(lable_pred[i])==1:\n        plt.scatter(data[i][0],data[i][1],color='black')\n    if int(lable_pred[i])==2:\n        plt.scatter(data[i][0],data[i][1],color='blue')\nplt.show()\n\n可以看到聚类效果还是不错的，对k-means的聚类效率进行了一个测试，将维度扩宽到50维\n\n\n数据规模\n消耗时间\n数据维度\n\n\n\n10000条\n4s\n50维\n\n\n100000条\n30s\n50维\n\n\n1000000条\n4'13s\n50维\n\n\n\n对于百万级的数据，拟合时间还是能够接受的，可见效率还是不错，对模型的保存与其它的机器学习算法模型保存类似\nfrom sklearn.externals import joblib\njoblib.dump(km,\"model/km_model.m\")\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "4"}