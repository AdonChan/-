{"title": "Python3爬虫下载pdf（二） - 个人文章 ", "index": "线程池,网页爬虫,python", "content": "Python3爬虫下载pdf（二）\n最近在学习python的爬虫，并且玩的不亦说乎，因此写个博客，记录并分享一下。\n需下载下载以下模块\n\nbs4模块\nrequests模块\n\n一、源码\nfrom concurrent.futures import ThreadPoolExecutor\nimport requests,argparse,re,os\nfrom bs4 import BeautifulSoup as Soup\n\nheaders = {\n        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:54.0) Gecko/20100101 Firefox/54.0'\n    }\n    \n## 设置命令行参数\ndef setArgs():\n    parser = argparse.ArgumentParser(description=\"功能：下载pdf\")\n    parser.add_argument(\"url\",help=\"目标url\")\n    parser.add_argument(\"-t\",\"--thread\",help=\"最大的线程数。默认为3\",default=3,type=int)\n    parser.add_argument(\"-f\",\"--filedir\",help=\"文件保存的路径.默认为当前目录下的downloads文件夹.如果不存在，便自动新建\",default=\"downloads\")\n    return parser.parse_args()\n    \n## 获取所有pdf的url\ndef getPdfUrl(root_url):\n    response = requests.get(root_url, headers=headers)\n    ## 如果requests没有从页面中获得字符编码，那么设置为utf-8\n    if \"charset\" not in response.headers:\n        response.encoding = \"utf-8\"\n    bsObj = Soup(response.text, \"html.parser\")\n    pdfs = bsObj.find_all(\"a\", {\"href\": re.compile(r'.pdf$')})\n    ## 获得一个字典，key为pdf完整url，value为pdf名称\n    url_pdfName = {root_url[:root_url.rfind(\"/\")+1]+pdf[\"href\"]:pdf.string for pdf in pdfs}\n    return url_pdfName\n\n## 显示正在下载的pdf的名称\ndef showPdf(pdf_name):\n    print(pdf_name+\"...\")\n\n## 下载pdf\ndef savePdf(url,pdf_name):\n    response = requests.get(url,headers=headers,stream=True)\n    ## 如果指定的文件夹，那么便新建\n    if not os.path.exists(FILE_DIR):\n        os.makedirs(FILE_DIR)\n    ## os.path.join(a,b..)如果a字符串没有以/结尾，那么自动加上\\\\。（windows下）\n    with open(os.path.join(FILE_DIR,pdf_name),\"wb\") as pdf_file:\n        for content in response.iter_content():\n            pdf_file.write(content)\n\n## 设置要下载一个pdf要做的事情，作为线程的基本\ndef downOne(url,pdf_name):\n    showPdf(pdf_name)\n    savePdf(url,pdf_name)\n    print(pdf_name+\" has been downloaded!!\")\n    \n## 开始线程\ndef downPdf(root_url,max_thread):\n    url_pdfName = getPdfUrl(root_url)\n    with ThreadPoolExecutor(max_thread) as executor:\n        executor.map(downOne,url_pdfName.keys(),url_pdfName.values())\n\ndef main():\n    ## 获得参数\n    args = setArgs()\n    ## 如果没有输入必须的参数，便结束，返回简略帮助\n    try:\n        global FILE_DIR\n        FILE_DIR = args.filedir\n        downPdf(args.url,args.thread)\n    except:\n        exit()\n\nif __name__ == \"__main__\":\n    main()\n\n效果图\n\n例子：\n备注\nwith ThreadPoolExecutor(max_thread) as executor:\n     executor.map(downOne,url_pdfName.keys(),url_pdfName.values())\n\n使用工作的线程实例化ThreadPoolExecutor 类；executor._exit_ 方法会调用executor.shutdown(wait=True) 方法，它会在所有线程都执行完毕前阻塞线程。\nmap方法的作用与内置map函数类似，不过downOne函数会在多个线程中并发调用；map方法返回一个生成器。\n\nglobal FILE_DIR\nFILE_DIR = args.filedir\n\n设置了全局参数，用来接收文件路径的值\n因为后面用executor.map() 传参的时候，参数必须是iterabe，不知道咋放了，所以就设了个全局变量\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "1"}