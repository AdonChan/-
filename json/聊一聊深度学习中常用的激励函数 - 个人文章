{"title": "聊一聊深度学习中常用的激励函数 - 个人文章 ", "index": "python", "content": " \n大家都知道，人脑的基本计算单元叫做神经元。现代生物学表明，人的神经系统中大概有860亿神经元，而这数量巨大的神经元之间大约是通过1014−1015个突触连接起来的。上面这一幅示意图，粗略地描绘了一下人体神经元与我们简化过后的数学模型。每个神经元都从树突接受信号，同时顺着某个轴突传递信号。而每个神经元都有很多轴突和其他的神经元树突连接。而我们可以看到右边简化的神经元计算模型中，信号也是顺着轴突(比如x0)传递，然后在轴突处受到激励(w0倍)然后变成w0x0。我们可以这么理解这个模型：在信号的传导过程中，突触可以控制传导到下一个神经元的信号强弱(数学模型中的权重w)，而这种强弱是可以学习到的。在基本生物模型中，树突传导信号到神经元细胞，然后这些信号被加和在一块儿了，如果加和的结果被神经元感知超过了某种阈值，那么神经元就被激活，同时沿着轴突向下一个神经元传导信号。在我们简化的数学计算模型中，我们假定有一个『激励函数』来控制加和的结果对神经元的刺激程度，从而控制着是否激活神经元和向后传导信号。比如说，我们在逻辑回归中用到的sigmoid函数就是一种激励函数，因为对于求和的结果输入，sigmoid函数总会输出一个0-1之间的值，我们可以认为这个值表明信号的强度、或者神经元被激活和传导信号的概率。\n \n我们知道深度学习的理论基础是神经网络，在单层神经网络中（感知机），输入和输出计算关系如下图所示： \n可见，输入与输出是一个线性关系，对于增加了多个神经元之后，计算公式也是类似，如下图： \n这样的模型就只能处理一些简单的线性数据，而对于非线性数据则很难有效地处理（也可通过组合多个不同线性表示，但这样更加复杂和不灵活），如下图所示： \n那么，通过在神经网络中加入非线性激励函数后，神经网络就有可能学习到平滑的曲线来实现对非线性数据的处理了。如下图所示： \n因此，神经网络中激励函数的作用通俗上讲就是将多个线性输入转换为非线性的关系。如果不使用激励函数的话，神经网络的每层都只是做线性变换，即使是多层输入叠加后也还是线性变换。通过激励函数引入非线性因素后，使神经网络的表示能力更强了。\n下面介绍几个常用的激励函数这z1、sigmoid 函数 \n这应该是神经网络中使用最频繁的激励函数了，它把一个实数压缩至0到1之间，当输入的数字非常大的时候，结果会接近1，当输入非常大的负数时，则会得到接近0的结果。在早期的神经网络中使用得非常多，因为它很好地解释了神经元受到刺激后是否被激活和向后传递的场景（0：几乎没有被激活，1：完全被激活），不过近几年在深度学习的应用中比较少见到它的身影，因为使用sigmoid函数容易出现梯度弥散或者梯度饱和。当神经网络的层数很多时，如果每一层的激励函数都采用sigmoid函数的话，就会产生梯度弥散的问题，因为利用反向传播更新参数时，会乘以它的导数，所以会一直减小。如果输入的是比较大或者比较小的数（例如输入100，经Sigmoid函数后结果接近于1，梯度接近于0），会产生饱和效应，导致神经元类似于死亡状态。\n【小白科普】什么是饱和呢？\n\n2、tanh 函数 \ntanh函数将输入值压缩至-1到1之间。该函数与Sigmoid类似，也存在着梯度弥散或梯度饱和的缺点。\n3、ReLU函数 \nReLU是修正线性单元（The Rectified Linear Unit）的简称，近些年来在深度学习中使用得很多，可以解决梯度弥散问题，因为它的导数等于1或者就是0。相对于sigmoid和tanh激励函数，对ReLU求梯度非常简单，计算也很简单，可以非常大程度地提升随机梯度下降的收敛速度。（因为ReLU是线性的，而sigmoid和tanh是非线性的）。但ReLU的缺点是比较脆弱，随着训练的进行，可能会出现神经元死亡的情况，例如有一个很大的梯度流经ReLU单元后，那权重的更新结果可能是，在此之后任何的数据点都没有办法再激活它了。如果发生这种情况，那么流经神经元的梯度从这一点开始将永远是0。也就是说，ReLU神经元在训练中不可逆地死亡了。\n4、Leaky ReLU 函数 \nLeaky ReLU主要是为了避免梯度消失，当神经元处于非激活状态时，允许一个非0的梯度存在，这样不会出现梯度消失，收敛速度快。它的优缺点跟ReLU类似。\n5、ELU 函数 \nELU在正值区间的值为x本身，这样减轻了梯度弥散问题（x>0区间导数处处为1），这点跟ReLU、Leaky ReLU相似。而在负值区间，ELU在输入取较小值时具有软饱和的特性，提升了对噪声的鲁棒性下图是ReLU、LReLU、ELU的曲线比较图：\n\n6、Maxout 函数 \nMaxout也是近些年非常流行的激励函数，简单来说，它是ReLU和Leaky ReLU的一个泛化版本，当w1、b1设置为0时，便转换为ReLU公式。因此，Maxout继承了ReLU的优点，同时又没有“一不小心就挂了”的担忧。但相比ReLU，因为有2次线性映射运算，因此计算量也会翻倍。（原文出处https://blog.csdn.net/Stephen...）\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}