{"title": "scrapy学习之路1.1(正则表达式) - 个人文章 ", "index": "scrapy,python,正则表达式", "content": "^   以某个开头\n$    以某个结尾\n *     某个任意多次，大于等于0\n?    让某个取消贪婪匹配，可以理解为改为从左到右匹配到某个为止\n+    某个至少为一次，大于等于1\n{ }例,{2,5},某个出现2到5次.....{2},{2,}等\n|    或者(a|b),选a或者b\n[ ]有三种意思，1.[13567]中括号的任选一个-------2.[0-9],[a-z]-------3.[.]就代表.号，不代表任意字符了\n[^]例,[^1]非1\n[a-z]同上\n/s空格\n/S非空格\n/w代表[A-Za-z0-9_]\n/W代表非[A-Za-z0-9_]\n[u4E00-u9FA5]代表汉字\n( )略\n/d数字\nimport re\nline1 = \"你出生在2016-09-01\"\nline2 = \"你出生在2016-9-1\"\nline3 = \"你出生在2016/09/01\"\nline4 = \"你出生在2016年9月1号\"\nline5 = \"你出生在2016-09\"\n\nregex_str = \"(你出生在\\d{4}(-|/|年)\\d{1,2}($|(月|/|-)\\d{1,2}($|号)))\"\nmatch_obj = re.match(regex_str, line5)\nif match_obj:\n    print(match_obj.group(1))\n    \n#五个line都能匹配\n简单的爬虫\n# -*- coding: utf-8 -*-\n# @Author: Lai\n\nimport re\nimport os\nimport requests\n\nBASE_PATH = \"E:/EW/\"\n\n\ndef get_block_link(url):\n    html_obj = requests.get(url)\n    reg = '<a target=\"_blank\" href=\"(.*?)\" class=\"l mr10\"><img  onerror'\n    link_list = re.findall(reg, html_obj.text, re.S)\n    return link_list\n\n\n# 得到标题和多章节的链接\ndef get_child_link(url):\n    links = []\n    for link in get_block_link(url):\n        html_obj = requests.get(link)\n        reg = '<a href=\"(.*?)\" class=\"reader\" title=\".*?\">.*?</a>'\n        link = re.findall(reg, html_obj.text)[0]\n        links.append(link)\n    return links\n\n\ndef get_charterName_and_content(url):\n    html_obj = requests.get(url)\n    html_obj.encoding = \"GBK\"\n    reg_content = 'id=\"content\"><script type=\"text/javascript\">style5\\(\\);</script>(.*?)<script type=\"text/javascript\">'\n    if re.findall(reg_content, html_obj.text, re.S):\n        content = re.findall(reg_content, html_obj.text, re.S)[0]\n    else:\n        content = \"空\"\n    reg = ' class=\"article_title\">.*?</a>  >.*?\\s(.*?)</div>'\n    if re.findall(reg, html_obj.text, re.S):\n        charter_name = re.findall(reg, html_obj.text, re.S)[0]\n    else:\n        charter_name = \"空\"\n    res = (charter_name, content)\n    return res\n\n\n# 得到每个章节的链接\ndef get_child_in_info(url):\n    for link in get_child_link(url):\n        html_obj = requests.get(link)\n        html_obj.encoding = \"GBK\"\n        reg = '<div class=\"chapName\"><span class=\"r\">.*?</span><strong>(.*?)</strong><div class=\"clear\">'\n        if re.findall(reg, html_obj.text, re.S):\n            title = re.findall(reg, html_obj.text, re.S)[0]\n        else:\n            title = \"空\"\n        path = os.path.join(BASE_PATH, title)\n        reg_url = '<li><a href=\"(.*?)\" title=\".*?\">.*?</a></li>'\n        urls = re.findall(reg_url, html_obj.text)\n        for url in urls:\n            chart_name, content = get_charterName_and_content(url)\n            if not os.path.exists(path):\n                os.mkdir(path)\n            else:\n                pass\n            position = os.path.join(path, chart_name+\".html\")\n            with open(position, \"w\") as f:\n                f.write(content)\n\n\nif __name__ == \"__main__\":\n    real_url = \"http://www.quanshuwang.com/list/1_1.html\"\n    # get_block_link(real_url)\n    # print(get_child_link(real_url))\n    get_child_in_info(real_url)\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "1"}