{"title": "纪念我的第一个完整的小说爬虫 - LART`s WORDS ", "index": "python,网页爬虫", "content": "纪念我的第一个爬虫程序，一共写了三个白天，其中有两个上午没有看，中途遇到了各种奇怪的问题，伴随着他们的解决，对于一些基本的操作也弄清楚了。果然，对于这些东西的最号的学习方式，就是在使用中学习，通过解决问题的方式来搞定这些知识。按需索取，才能更有针对性。\n大体记录下整个过程。\n\n准备构思\n出于对于python的热爱，想要尝试一些练手的项目，但是不论是看书，还是直接尝试别人的项目，到最后都会沦为不停地复制粘贴...最实际的就是自己来上手亲自写代码。思路都是一样的，但是具体的实现还得靠自己。\n以前的复制粘贴给我的帮助也就是告诉了我大致的流程。\n确定目标网址\n目标网址是关键。我梦想中的爬虫是那种偏向于更智能的，直接给他一个想要获取的关键词，一步步的流程直接自己完成，可以自己给定范围，也可以直接爬取整个互联网或者更实际的就是整个百度上的内容，但是，目前就我而言，见到的爬虫，都是给定目标网址，通过目标页面上的内容进一步执行规定的操作，所以现在来看，我们在写爬虫之前，需要确定一个基准页面，这个是需要我们事先制定的。在考虑我们需要程序完成怎样的功能，获取页面文本还是相关链接内容还是其他的目的。\n我这个程序想要获取的是《剑来》小说，把各个章节的内容爬去下载存储到文件里。\n编程只是实现目的的工具。\n所以重点是分析我们的需求。\n获取小说目录页面是基本。这里有各个章节的链接，标题等等内容。这是我们需要的。\n有了各个章节的链接，就需要进入其中获得各个章节的内容。\n所以，我们需要获得页面内容，需要从中获得目标内容。\n所以使用 urllib.request，re 库。\n前者用来获得网页内容，后者获得目标信息。\nheaders\n直接使用urllib.request的urlopen()，read()方法是会报类似以下的错误（这里是网上查找过来的，都是类似的）：\nraise HTTPError(req.get_full_url(), code, msg, hdrs, fp)\nHTTPError: HTTP Error 403: Forbidden\n\n出现urllib2.HTTPError: HTTP Error 403: Forbidden错误是由于网站禁止爬虫，可以在请求加上头信息，伪装成浏览器。\nheaders = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:55.0) Gecko/20100101 Firefox/55.0'}\nrequest = url_req.Request(url, headers=headers)\nresponse = url_req.urlopen(request, data=None, timeout=3)\nhtml = response.read().decode('GBK')\n\n注意：这里存在两个容易出问题的地方。\n编码：编码问题是使用爬虫中有时候会很头痛的问题，由于网页源代码编码格式不明确，所以这里尝试了许久。\n使用chardet库的detect()方法可以检测字节字符串的编码。所以直接检测这里的html(先不要解码)。输出的是GB2312，但是在后面页面的爬取中，会出现提示有的字符的编码异常，所以这里采取了比其范围更广的中文字符集GBK，解决了这个问题。\n设置超时范围：由于频繁的获取网页内容，目标网站有时候会出现没有响应的问题。\n（这个问题可以见我在CSDN上的提问：关于python爬虫程序中途停止的问题）\n于是我采取了捕获 urlopen()的socket.timeout异常，并在出现异常的时候再循环访问，直到获得目标页面。\n获得目标内容\n这里使用的是正则表达式。re模块。这里的使用并不复杂。\n首先需要一个模式字符串。以re.I指定忽略大小写，编译后的对象拥有本身匹配的方法，这里使用的是findall()，返回一个所有结果组成的列表。可以及时返回输出其内容，进而选择合适的部分进行处理。\npython 正则表达式\n通过查看相关的符号，这里使用(.+?)来实现匹配非贪婪模式(尽量少的)下任意无限字符，对之使用()，进而匹配括号内的模式。\n文件写入\n使用with open() as file:，进而可以处理文件。并且可以自动执行打开和关闭文件，更为便捷安全。\nwith open(findall_title[0] + '.txt', 'w+', encoding='utf-8') as open_file:\n\n\n这里也要注意编码的问题，指定utf-8。会避免一些问题。\n这里使用w+模式，追加写文件。\n\n完整代码\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Aug 11 16:31:42 2017\n@author: lart\n\"\"\"\n\nimport urllib.request as url_req\nimport re, socket, time\n\n\ndef r_o_html(url):\n    print('r_o_html begin')\n\n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:55.0) Gecko/20100101 Firefox/55.0'}\n\n    request = url_req.Request(url, headers=headers)\n\n    NET_STATUS = False\n    while not NET_STATUS:\n        try:\n            response = url_req.urlopen(request, data=None, timeout=3)\n            html = response.read().decode('GBK')\n            print('NET_STATUS is good')\n            print('r_o_html end')\n            return html\n        except socket.timeout:\n            print('NET_STATUS is not good')\n            NET_STATUS = False\n\ndef re_findall(re_string, operation, html):\n\n    print('re_findall begin')\n    pattern = re.compile(re_string, re.I)\n\n    if operation == 'findall':\n        result = pattern.findall(html)\n    else:\n        print('this operation is invalid')\n        exit(-1)\n\n    print('re_findall end')\n    return result\n\n\nif __name__ == '__main__':\n    url_base = 'http://www.7kankan.la/book/1/'\n\n    html = r_o_html(url_base)\n\n    findall_title = re_findall(r'<title>(.+?)</title>', 'findall', html)\n\n    findall_chapter = re_findall(r'<dd class=\"col-md-3\"><a href=[\\',\"](.+?)[\\',\"] title=[\\',\"](.+?)[\\',\"]>', 'findall', html)\n\n    with open(findall_title[0] + '.txt', 'w+', encoding='utf-8') as open_file:\n        print('article文件打开', findall_chapter)\n        for i in range(len(findall_chapter)):\n            print('第' + str(i) + '章')\n\n            open_file.write('\\n\\n\\t' + findall_chapter[i][1] + '\\n --------------------------------------------------------------------- \\n')\n\n            url_chapter = url_base + findall_chapter[i][0]\n\n            html_chapter = r_o_html(url_chapter)\n\n            findall_article = re_findall(r'&nbsp;&nbsp;&nbsp;&nbsp;(.+?)<br />', 'findall', html_chapter)\n\n            findall_article_next = findall_chapter[i][0].replace('.html', '_2.html')\n\n            url_nextchapter = url_base + findall_article_next\n\n            html_nextchapter = r_o_html(url_nextchapter)\n\n            if html_nextchapter:\n                findall_article.extend(re_findall(r'&nbsp;&nbsp;&nbsp;&nbsp;(.+?)<br />', 'findall', html_nextchapter))\n\n                for text in findall_article:\n                    open_file.write(text + '\\n')\n\n            time.sleep(1)\n\n    print('文件写入完毕')\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "9"}