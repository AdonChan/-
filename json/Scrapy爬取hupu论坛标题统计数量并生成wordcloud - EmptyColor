{"title": "Scrapy爬取hupu论坛标题统计数量并生成wordcloud - EmptyColor ", "index": "词性分析,python,网页爬虫", "content": "爬取数据\nhuputitle_spiders.py\n#coding:utf-8    \nimport scrapy   \nfrom huputitle.items import HuputitleItem\n\nfrom scrapy.crawler import CrawlerProcess \n\nclass hupuSpider(scrapy.Spider):\n    name = 'huputitle'    \n    allowed_domains = [\"bbs.hupu.com\"]    \n    start_urls = [\"https://bbs.hupu.com/bxj\"]    \n         \n    def parse(self, response):    \n        item = HuputitleItem()    \n        item['titles'] = response.xpath('//a[@id=\"\"]/text()').extract()#提取标题\n        # print 'titles',item['titles']  \n        yield item \n        new_url = \"https://bbs.hupu.com\" + response.xpath('//a[@id=\"j_next\"]/@href').extract_first()\n        if new_url:    \n            yield scrapy.Request(new_url,callback=self.parse)  \nitems.py\n# -*- coding: utf-8 -*-\nimport scrapy\n\n\nclass HuputitleItem(scrapy.Item):\n    # define the fields for your item here like:\n    titles = scrapy.Field()\n\n\npipelines.py\n# -*- coding: utf-8 -*-\nimport os    \nimport urllib   \n  \nfrom huputitle import settings  \n\nimport sys  \nreload(sys)  \nsys.setdefaultencoding( \"utf-8\" )  \n\nclass HuputitlePipeline(object):\n    def process_item(self, item, spider):\n        for title in item['titles']:\n            # print 'title',title\n            fo = open(\"foo.txt\", \"a\")\n            fo.write(\"\".join(title)+\"\\r\\n\")\n        fo.close()\n        return item\n\nsettings.py\nBOT_NAME = 'huputitle'\n\nSPIDER_MODULES = ['huputitle.spiders']\nNEWSPIDER_MODULE = 'huputitle.spiders'\n\nITEM_PIPELINES = {  \n    'huputitle.pipelines.HuputitlePipeline': 1,  \n}\n\nUSER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_3) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.54 Safari/536.5'\n最终爬取了100页2W多个标题\n\n分词并统计词的数量\n这里我使用了 jieba 这个库来分词hupudivide.py\n#encoding=utf-8  \nimport jieba  \nimport sys\nreload(sys)\nsys.setdefaultencoding('utf-8')\n\nfo = open(\"hupu.txt\", \"r\")\nfi = open(\"hupudi.txt\", \"w\")\nlines = fo.readlines()\nfor line in lines:\n    seg_list = jieba.cut_for_search(line)\n    fi.write(\" \\n\".join(seg_list)) \n\n分出了17w个词然后统计数量huPuCounter.py\n#encoding=utf-8  \nimport jieba  \nimport jieba.analyse\nimport time\nfrom collections import Counter\nimport sys\nreload(sys)\nsys.setdefaultencoding('utf-8')\n\nfo = open(\"hupudi.txt\", \"r\")\nfi = open(\"hupunum.txt\", \"w\")\nfl = open(\"hupunumword.txt\", \"w\")\nf = open(\"hupuword.txt\", \"w\")\n\nlines = fo.readlines()\n\nd = {}\n\nfor line in lines:\n    if line not in d:\n        d[line] = 1\n    else:\n        d[line] = d[line] + 1\n\nd = sorted(d.items(),key=lambda item:item[1],reverse=True)\n\nfor k in d:\n    fi.write(\"%s%d\\n\" % (k[0][:-1].encode('utf-8'),k[1]))\n    if len(k[0][:-1].encode('utf-8')) >= 6:\n        fl.write(\"%s%d\\n\" % (k[0][:-1].encode('utf-8'),k[1]))\n        f.write(\"%s\" % (k[0][:-1].encode('utf-8')))\n这里我统计了两个词以下和两个词以上的词的量分配如图\n\n生成词云以及其他数据图表\nmakeHupuCloud.py\n#encoding=utf-8 \nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport jieba\n\ntext_from_file_with_apath = open('foo.txt').read()\n\nwordlist_after_jieba = jieba.cut(text_from_file_with_apath, cut_all = False)\nwl_space_split = \" \".join(wordlist_after_jieba)\n\nbackgroud_Image = plt.imread('huputag.jpg')\nmy_wordcloud = WordCloud(background_color = 'white',    \n                mask = backgroud_Image,  ).generate(wl_space_split)\n\nplt.imshow(my_wordcloud)\nplt.axis(\"off\")\nplt.show()\n这里我是用python的wordcloud库生成的词云，图片是hupu的logo使用jieba的分词分出词性 生成的图表\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "10"}