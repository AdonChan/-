{"title": "机器学习从入门到放弃之逻辑回归 - 肥兔子de碎碎语 ", "index": "python,机器学习", "content": "分类问题\n回到本系列的第一篇文章机器学习从入门到放弃之KNN算法，在里面有这样的一个问题\n黄点代表1类电影的分布，绿色代表0类电影的分布，紫色代表需要分类的电影样本。那么该怎么判别紫色的那颗点所在的类别呢？\n之前给出的是KNN算法，通过计算紫色点都周边的剧场的长短，来判断紫色点属于哪个类别。现在有这样一种极端情况，黄点和绿点在紫点周围呈圆周分布，距离一样，咋办？\n\n图画得不是太好，大家理会我的意思就行。\n在这种情况，假如像下图这样的情况，就容易处理得多了。\n\n红线的下方是黄色种类，上方时绿色种类。\n这种情况我们称之为线性分类，关于如何拟合出这条线程函数下面会讲述。现在先来说说，既然这叫线性分类，那么必然会有非线性的情况啊，那咋办呢？\n没错，如果特征可以被线性函数全部表达，这自然是理想情况，但实际问题中更多的非线性分类。这时，我们需要将线性函数转换为非线性函数。那怎么转换呢，很简单，将线性函数（假设叫z），扔到某一非线性函数f(x)内，得到新的表达式y = f(z),就是我们所需的非线性分类器了，而f(x)也就作激活函数，它有很多种，本文只介绍逻辑回归所使用到的sigmoid函数，其表达式是\n\n其图像有一个漂亮的S型\n\n可见在x的取值范围足够大的时候，其从0变1的过程可以忽略不计，因此，我们习惯的把>0.5归为1类，<0.5归为0类，那么恰好是0.5怎么办？这个概率是极低的，如果真的是0.5，那就随机归类，然后出门买张彩票吧，说不定就不用继续当程序员了。 (/≥▽≤/)\n上面函数图像引用云深不知处的博客\n算法介绍\n回到表达式上，可知函数的变量是z其余都是常量，所要要求解该分类函数的值，就是要确定z的值而z是线性方程，基本的数学知识不难知道，\n$$z=a1x1+a2x2……an*xn$$\n其中[x1……xn]是输入向量，所以训练的过程就是确定于[a1,a2……an]的值，使得该表达式对于多个输入向量的输出值正确率最高。\n下面开始讲述求最佳的[a1,a2……an]的方法\n显然，我们可以设计一个函数来衡量[a1,a2……an]是否最佳，比如说这样的\n$$J(a) = sum_{n=0}(f_a(xi)-y)^2$$\n显然当J(a)达到最小值时,a的值最佳。方法如下，\n\n初始化weight,可以使用随机值\n代入式子得到err = y - predict\nweight = weight + alpha * error * x_train[i],其中alpha称为学习速率，太小会影响函数的收敛速度，太大刚才就不收敛了。\n为了解决上述问题，在《机器学习实战中》使用了动态更新alpha的方法，式子为alpha = 4/(1+i)+0.01\n\n上述修改weight的过程称为梯度下降法，其中我故意略去了数学证明部分，需要的同学请自行查找专业资料。\n代码实现\ngithub\n文章描述如有错误，欢迎指正。\n关注我\n我的个人公众号开启啦，微信公众号搜索肥兔子的碎碎语，你能查看到更多学习心得的分享噢⊙ω⊙。\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "4"}