{"title": "基于 Python 的简单自然语言处理实践 - 某熊的全栈之路 ", "index": "自然语言处理,机器学习,python", "content": "基于 Python 的简单自然语言处理实践 从属于笔者的 程序猿的数据科学与机器学习实战手册。\n基于 Python 的简单自然语言处理\n本文是对于基于 Python 进行简单自然语言处理任务的介绍，本文的所有代码放置在这里。建议前置阅读  Python 语法速览与机器学习开发环境搭建，更多机器学习资料参考机器学习、深度学习与自然语言处理领域推荐的书籍列表以及面向程序猿的数据科学与机器学习知识体系及资料合集。\nTwenty News Group 语料集处理\n20 Newsgroup 数据集包含了约 20000 篇来自于不同的新闻组的文档，最早由 Ken Lang 搜集整理。本部分包含了对于数据集的抓取、特征提取、简单分类器训练、主题模型训练等。本部分代码包括主要的处理代码封装库与基于 Notebook 的交互示范。我们首先需要进行数据抓取：\n    def fetch_data(self, subset='train', categories=None):\n        \"\"\"return data\n        执行数据抓取操作\n        Arguments:\n        subset -> string -- 抓取的目标集合 train / test / all\n        \"\"\"\n        rand = np.random.mtrand.RandomState(8675309)\n        data = fetch_20newsgroups(subset=subset,\n                                  categories=categories,\n                                  shuffle=True,\n                                  random_state=rand)\n\n        self.data[subset] = data\n然后在 Notebook 中交互查看数据格式：\n# 实例化对象\ntwp = TwentyNewsGroup()\n# 抓取数据\ntwp.fetch_data()\ntwenty_train = twp.data['train']\nprint(\"数据集结构\", \"->\", twenty_train.keys())\nprint(\"文档数目\", \"->\", len(twenty_train.data))\nprint(\"目标分类\", \"->\",[ twenty_train.target_names[t] for t in twenty_train.target[:10]])\n\n数据集结构 -> dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR', 'description'])\n文档数目 -> 11314\n目标分类 -> ['sci.space', 'comp.sys.mac.hardware', 'sci.electronics', 'comp.sys.mac.hardware', 'sci.space', 'rec.sport.hockey', 'talk.religion.misc', 'sci.med', 'talk.religion.misc', 'talk.politics.guns']\n接下来我们可以对语料集中的特征进行提取：\n# 进行特征提取\n\n# 构建文档-词矩阵（Document-Term Matrix）\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncount_vect = CountVectorizer()\n\nX_train_counts = count_vect.fit_transform(twenty_train.data)\n\nprint(\"DTM 结构\",\"->\",X_train_counts.shape)\n\n# 查看某个词在词表中的下标\nprint(\"词对应下标\",\"->\", count_vect.vocabulary_.get(u'algorithm'))\n\nDTM 结构 -> (11314, 130107)\n词对应下标 -> 27366\n为了将文档用于进行分类任务，还需要使用 TF-IDF 等常见方法将其转化为特征向量：\n# 构建文档的 TF 特征向量\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ntf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\nX_train_tf = tf_transformer.transform(X_train_counts)\n\nprint(\"某文档 TF 特征向量\",\"->\",X_train_tf)\n\n# 构建文档的 TF-IDF 特征向量\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ntf_transformer = TfidfTransformer().fit(X_train_counts)\nX_train_tfidf = tf_transformer.transform(X_train_counts)\n\nprint(\"某文档 TF-IDF 特征向量\",\"->\",X_train_tfidf)\n\n某文档 TF 特征向量 ->   (0, 6447)    0.0380693493813\n  (0, 37842)    0.0380693493813\n我们可以将特征提取、分类器训练与预测封装为单独函数：\n    def extract_feature(self):\n        \"\"\"\n        从语料集中抽取文档特征\n        \"\"\"\n\n        # 获取训练数据的文档-词矩阵\n        self.train_dtm = self.count_vect.fit_transform(self.data['train'].data)\n\n        # 获取文档的 TF 特征\n\n        tf_transformer = TfidfTransformer(use_idf=False)\n\n        self.train_tf = tf_transformer.transform(self.train_dtm)\n\n        # 获取文档的 TF-IDF 特征\n\n        tfidf_transformer = TfidfTransformer().fit(self.train_dtm)\n\n        self.train_tfidf = tf_transformer.transform(self.train_dtm)\n\n    def train_classifier(self):\n        \"\"\"\n        从训练集中训练出分类器\n        \"\"\"\n\n        self.extract_feature();\n\n        self.clf = MultinomialNB().fit(\n            self.train_tfidf, self.data['train'].target)\n\n    def predict(self, docs):\n        \"\"\"\n        从训练集中训练出分类器\n        \"\"\"\n\n        X_new_counts = self.count_vect.transform(docs)\n\n        tfidf_transformer = TfidfTransformer().fit(X_new_counts)\n        \n        X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n\n        return self.clf.predict(X_new_tfidf)\n然后执行训练并且进行预测与评价：\n# 训练分类器\ntwp.train_classifier()\n\n# 执行预测\ndocs_new = ['God is love', 'OpenGL on the GPU is fast']\npredicted = twp.predict(docs_new)\n\nfor doc, category in zip(docs_new, predicted):\n    print('%r => %s' % (doc, twenty_train.target_names[category]))\n    \n# 执行模型评测\ntwp.fetch_data(subset='test')\n\npredicted = twp.predict(twp.data['test'].data)\n\nimport numpy as np\n\n# 误差计算\n\n# 简单误差均值\nnp.mean(predicted == twp.data['test'].target)   \n\n# Metrics\n\nfrom sklearn import metrics\n\nprint(metrics.classification_report(\n    twp.data['test'].target, predicted,\n    target_names=twp.data['test'].target_names))\n\n# Confusion Matrix\nmetrics.confusion_matrix(twp.data['test'].target, predicted)\n\n'God is love' => soc.religion.christian\n'OpenGL on the GPU is fast' => rec.autos\n                          precision    recall  f1-score   support\n\n             alt.atheism       0.79      0.50      0.61       319\n           ...\n      talk.religion.misc       1.00      0.08      0.15       251\n\n             avg / total       0.82      0.79      0.77      7532\n\nOut[16]:\narray([[158,   0,   1,   1,   0,   1,   0,   3,   7,   1,   2,   6,   1,\n          8,   3, 114,   6,   7,   0,   0],\n       ...\n       [ 35,   3,   1,   0,   0,   0,   1,   4,   1,   1,   6,   3,   0,\n          6,   5, 127,  30,   5,   2,  21]])\n我们也可以对文档集进行主题提取：\n# 进行主题提取\n\ntwp.topics_by_lda()\n\nTopic 0 : stream s1 astronaut zoo laurentian maynard s2 gtoal pem fpu\nTopic 1 : 145 cx 0d bh sl 75u 6um m6 sy gld\nTopic 2 : apartment wpi mars nazis monash palestine ottoman sas winner gerard\nTopic 3 : livesey contest satellite tamu mathew orbital wpd marriage solntze pope\nTopic 4 : x11 contest lib font string contrib visual xterm ahl brake\nTopic 5 : ax g9v b8f a86 1d9 pl 0t wm 34u giz\nTopic 6 : printf null char manes behanna senate handgun civilians homicides magpie\nTopic 7 : buf jpeg chi tor bos det que uwo pit blah\nTopic 8 : oracle di t4 risc nist instruction msg postscript dma convex\nTopic 9 : candida cray yeast viking dog venus bloom symptoms observatory roby\nTopic 10 : cx ck hz lk mv cramer adl optilink k8 uw\nTopic 11 : ripem rsa sandvik w0 bosnia psuvm hudson utk defensive veal\nTopic 12 : db espn sabbath br widgets liar davidian urartu sdpa cooling\nTopic 13 : ripem dyer ucsu carleton adaptec tires chem alchemy lockheed rsa\nTopic 14 : ingr sv alomar jupiter borland het intergraph factory paradox captain\nTopic 15 : militia palestinian cpr pts handheld sharks igc apc jake lehigh\nTopic 16 : alaska duke col russia uoknor aurora princeton nsmca gene stereo\nTopic 17 : uuencode msg helmet eos satan dseg homosexual ics gear pyron\nTopic 18 : entries myers x11r4 radar remark cipher maine hamburg senior bontchev\nTopic 19 : cubs ufl vitamin temple gsfc mccall astro bellcore uranium wesleyan\n常见自然语言处理工具封装\n经过上面对于 20NewsGroup 语料集处理的介绍我们可以发现常见自然语言处理任务包括，数据获取、数据预处理、数据特征提取、分类模型训练、主题模型或者词向量等高级特征提取等等。笔者还习惯用 python-fire 将类快速封装为可通过命令行调用的工具，同时也支持外部模块调用使用。本部分我们主要以中文语料集为例，譬如我们需要对中文维基百科数据进行分析，可以使用 gensim 中的维基百科处理类：\nclass Wiki(object):\n    \"\"\"\n    维基百科语料集处理\n    \"\"\"\n    \n    def wiki2texts(self, wiki_data_path, wiki_texts_path='./wiki_texts.txt'):\n        \"\"\"\n        将维基百科数据转化为文本数据\n        Arguments:\n        wiki_data_path -- 维基压缩文件地址\n        \"\"\"\n        if not wiki_data_path:\n            print(\"请输入 Wiki 压缩文件路径或者前往 https://dumps.wikimedia.org/zhwiki/ 下载\")\n            exit()\n\n        # 构建维基语料集\n        wiki_corpus = WikiCorpus(wiki_data_path, dictionary={})\n        texts_num = 0\n\n        with open(wiki_text_path, 'w', encoding='utf-8') as output:\n            for text in wiki_corpus.get_texts():\n                output.write(b' '.join(text).decode('utf-8') + '\\n')\n                texts_num += 1\n                if texts_num % 10000 == 0:\n                    logging.info(\"已处理 %d 篇文章\" % texts_num)\n\n        print(\"处理完毕，请使用 OpenCC 转化为简体字\")\n抓取完毕后，我们还需要用 OpenCC 转化为简体字。抓取完毕后我们可以使用结巴分词对生成的文本文件进行分词，代码参考这里，我们直接使用 python chinese_text_processor.py tokenize_file /output.txt 直接执行该任务并且生成输出文件。获取分词之后的文件，我们可以将其转化为简单的词袋表示或者文档-词向量，详细代码参考这里：\nclass CorpusProcessor:\n    \"\"\"\n    语料集处理\n    \"\"\"\n\n    def corpus2bow(self, tokenized_corpus=default_documents):\n        \"\"\"returns (vocab,corpus_in_bow)\n        将语料集转化为 BOW 形式\n        Arguments:\n        tokenized_corpus -- 经过分词的文档列表\n        Return:\n        vocab -- {'human': 0, ... 'minors': 11}\n        corpus_in_bow -- [[(0, 1), (1, 1), (2, 1)]...]\n        \"\"\"\n        dictionary = corpora.Dictionary(tokenized_corpus)\n\n        # 获取词表\n        vocab = dictionary.token2id\n\n        # 获取文档的词袋表示\n        corpus_in_bow = [dictionary.doc2bow(text) for text in tokenized_corpus]\n\n        return (vocab, corpus_in_bow)\n\n    def corpus2dtm(self, tokenized_corpus=default_documents, min_df=10, max_df=100):\n        \"\"\"returns (vocab, DTM)\n        将语料集转化为文档-词矩阵\n        - dtm -> matrix: 文档-词矩阵\n                I    like    hate    databases\n        D1    1      1          0            1\n        D2    1      0          1            1\n        \"\"\"\n\n        if type(tokenized_corpus[0]) is list:\n            documents = [\" \".join(document) for document in tokenized_corpus]\n        else:\n            documents = tokenized_corpus\n\n        if max_df == -1:\n            max_df = round(len(documents) / 2)\n\n        # 构建语料集统计向量\n        vec = CountVectorizer(min_df=min_df,\n                              max_df=max_df,\n                              analyzer=\"word\",\n                              token_pattern=\"[\\S]+\",\n                              tokenizer=None,\n                              preprocessor=None,\n                              stop_words=None\n                              )\n\n        # 对于数据进行分析\n        DTM = vec.fit_transform(documents)\n\n        # 获取词表\n        vocab = vec.get_feature_names()\n\n        return (vocab, DTM)\n我们也可以对分词之后的文档进行主题模型或者词向量提取，这里使用分词之后的文件就可以忽略中英文的差异：\n    def topics_by_lda(self, tokenized_corpus_path, num_topics=20, num_words=10, max_lines=10000, split=\"\\s+\", max_df=100):\n        \"\"\"\n        读入经过分词的文件并且对其进行 LDA 训练\n        Arguments:\n        tokenized_corpus_path -> string -- 经过分词的语料集地址\n        num_topics -> integer -- 主题数目\n        num_words -> integer -- 主题词数目\n        max_lines -> integer -- 每次读入的最大行数\n        split -> string -- 文档的词之间的分隔符\n        max_df -> integer -- 避免常用词，过滤超过该阈值的词\n        \"\"\"\n\n        # 存放所有语料集信息\n        corpus = []\n\n        with open(tokenized_corpus_path, 'r', encoding='utf-8') as tokenized_corpus:\n\n            flag = 0\n\n            for document in tokenized_corpus:\n\n                # 判断是否读取了足够的行数\n                if(flag > max_lines):\n                    break\n\n                # 将读取到的内容添加到语料集中\n                corpus.append(re.split(split, document))\n\n                flag = flag + 1\n\n        # 构建语料集的 BOW 表示\n        (vocab, DTM) = self.corpus2dtm(corpus, max_df=max_df)\n\n        # 训练 LDA 模型\n\n        lda = LdaMulticore(\n            matutils.Sparse2Corpus(DTM, documents_columns=False),\n            num_topics=num_topics,\n            id2word=dict([(i, s) for i, s in enumerate(vocab)]),\n            workers=4\n        )\n\n        # 打印并且返回主题数据\n        topics = lda.show_topics(\n            num_topics=num_topics,\n            num_words=num_words,\n            formatted=False,\n            log=False)\n\n        for ti, topic in enumerate(topics):\n            print(\"Topic\", ti, \":\", \" \".join(word[0] for word in topic[1]))\n\n该函数同样可以使用命令行直接调用，传入分词之后的文件。我们也可以对其语料集建立词向量，代码参考这里；如果对于词向量基本使用尚不熟悉的同学可以参考基于 Gensim 的 Word2Vec 实践：\n    def wv_train(self, tokenized_text_path, output_model_path='./wv_model.bin'):\n        \"\"\"\n        对于文本进行词向量训练，并将输出的词向量保存\n        \"\"\"\n\n        sentences = word2vec.Text8Corpus(tokenized_text_path)\n\n        # 进行模型训练\n        model = word2vec.Word2Vec(sentences, size=250)\n\n        # 保存模型\n        model.save(output_model_path)\n\n    def wv_visualize(self, model_path, word=[\"中国\", \"航空\"]):\n        \"\"\"\n        根据输入的词搜索邻近词然后可视化展示\n        参数：\n            model_path: Word2Vec 模型地址\n        \"\"\"\n\n        # 加载模型\n        model = word2vec.Word2Vec.load(model_path)\n\n        # 寻找出最相似的多个词\n        words = [wp[0] for wp in model.most_similar(word, topn=20)]\n\n        # 提取出词对应的词向量\n        wordsInVector = [model[word] for word in words]\n\n        # 进行 PCA 降维\n        pca = PCA(n_components=2)\n        pca.fit(wordsInVector)\n        X = pca.transform(wordsInVector)\n\n        # 绘制图形\n        xs = X[:, 0]\n        ys = X[:, 1]\n\n        plt.figure(figsize=(12, 8))\n        plt.scatter(xs, ys, marker='o')\n\n        # 遍历所有的词添加点注释\n        for i, w in enumerate(words):\n            plt.annotate(\n                w,\n                xy=(xs[i], ys[i]), xytext=(6, 6),\n                textcoords='offset points', ha='left', va='top',\n                **dict(fontsize=10)\n            )\n        plt.show()\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "4"}