{"title": "Python爬虫之多线程下载程序类电子书 - 个人文章 ", "index": "python,网页爬虫", "content": "  近段时间，笔者发现一个神奇的网站：http://www.allitebooks.com/ ，该网站提供了大量免费的编程方面的电子书，是技术爱好者们的福音。其页面如下：\n\n  那么我们是否可以通过Python来制作爬虫来帮助我们实现自动下载这些电子书呢？答案是yes.  笔者在空闲时间写了一个爬虫，主要利用urllib.request.urlretrieve()函数和多线程来下载这些电子书。  首先呢，笔者的想法是先将这些电子书的下载链接网址储存到本地的txt文件中，便于永久使用。其Python代码（Ebooks_spider.py）如下， 该代码仅下载第一页的10本电子书作为示例：\n# -*- coding:utf-8 -*-\n# 本爬虫用来下载http://www.allitebooks.com/中的电子书\n# 本爬虫将需要下载的书的链接写入txt文件，便于永久使用\n# 网站http://www.allitebooks.com/提供编程方面的电子书\n\n#  导入必要的模块\nimport urllib.request\nfrom bs4 import BeautifulSoup\n\n#  获取网页的源代码\ndef get_content(url):\n    html = urllib.request.urlopen(url)\n    content = html.read().decode('utf-8')\n    html.close()\n    return content\n\n# 将762个网页的网址储存在list中\nbase_url = 'http://www.allitebooks.com/'\nurls = [base_url]\nfor i in range(2, 762):\n    urls.append(base_url + 'page/%d/' % i)\n\n# 电子书列表，每一个元素储存每本书的下载地址和书名\nbook_list =[]\n\n# 控制urls的数量,避免书下载过多导致空间不够!!!\n# 本例只下载前3页的电子书作为演示\n# 读者可以通过修改url[:3]中的数字,爬取自己想要的网页书，最大值为762\nfor url in urls[:1]:\n    try:\n        # 获取每一页书的链接\n        content = get_content(url)\n        soup = BeautifulSoup(content, 'lxml')\n        book_links = soup.find_all('div', class_=\"entry-thumbnail hover-thumb\")\n        book_links = [item('a')[0]['href'] for item in book_links]\n        print('\\nGet page %d successfully!' % (urls.index(url) + 1))\n    except Exception:\n        book_links = []\n        print('\\nGet page %d failed!' % (urls.index(url) + 1))\n\n    # 如果每一页书的链接获取成功\n    if len(book_links):\n        for book_link in book_links:\n            # 下载每一页中的电子书\n            try:\n                content = get_content(book_link)\n                soup = BeautifulSoup(content, 'lxml')\n                # 获取每本书的下载网址\n                link = soup.find('span', class_='download-links')\n                book_url = link('a')[0]['href']\n\n                # 如果书的下载链接获取成功\n                if book_url:\n                    # 获取书名\n                    book_name = book_url.split('/')[-1]\n                    print('Getting book: %s' % book_name)\n                    book_list.append(book_url)\n            except Exception as e:\n                print('Get page %d Book %d failed'\n                      % (urls.index(url) + 1, book_links.index(book_link)))\n\n# 文件夹\ndirectory = 'E:\\\\Ebooks\\\\'\n# 将书名和链接写入txt文件中，便于永久使用\nwith open(directory+'book.txt', 'w') as f:\n    for item in book_list:\n        f.write(str(item)+'\\n')\n\nprint('写入txt文件完毕!')\n可以看到，上述代码主要爬取的是静态页面，因此效率非常高！运行该程序，显示结果如下：\n\n在book.txt文件中储存了这10本电子书的下载地址，如下：\n\n  接着我们再读取这些下载链接，用urllib.request.urlretrieve()函数和多线程来下载这些电子书。其Python代码（download_ebook.py）如下：\n# -*- coding:utf-8 -*-\n# 本爬虫读取已写入txt文件中的电子书的链接，并用多线程下载\n\nimport time\nfrom concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETED\nimport urllib.request\n\n# 利用urllib.request.urlretrieve()下载PDF文件\ndef download(url):\n    # 书名\n    book_name = 'E:\\\\Ebooks\\\\'+url.split('/')[-1]\n    print('Downloading book: %s'%book_name) # 开始下载\n    urllib.request.urlretrieve(url, book_name)\n    print('Finish downloading book: %s'%book_name) #完成下载\n\ndef main():\n    start_time = time.time() # 开始时间\n\n    file_path = 'E:\\\\Ebooks\\\\book.txt' # txt文件路径\n    # 读取txt文件内容，即电子书的链接\n    with open(file_path, 'r') as f:\n        urls = f.readlines()\n    urls = [_.strip() for _ in urls]\n\n    # 利用Python的多线程进行电子书下载\n    # 多线程完成后，进入后面的操作\n    executor = ThreadPoolExecutor(len(urls))\n    future_tasks = [executor.submit(download, url) for url in urls]\n    wait(future_tasks, return_when=ALL_COMPLETED)\n\n    # 统计所用时间\n    end_time = time.time()\n    print('Total cost time:%s'%(end_time - start_time))\n\nmain()\n运行上述代码，结果如下：\n\n再去文件夹中查看文件：\n\n可以看到这10本书都已成功下载，总共用时327秒，每本书的平均下载时间为32.7，约半分钟，而这些书的大小为87.7MB，可见效率相当高的！  怎么样，看到爬虫能做这些多有意思的事情，不知此刻的你有没有心动呢？心动不如行动，至理名言~~  本次代码已上传github, 地址为： https://github.com/percent4/E... .\n注意：本人现已开通两个微信公众号： 用Python做数学（微信号为：python_math）以及轻松学会Python爬虫（微信号为：easy_web_scrape）， 欢迎大家关注哦~~\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "0"}