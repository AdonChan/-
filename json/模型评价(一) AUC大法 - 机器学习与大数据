{"title": "模型评价(一) AUC大法 - 机器学习与大数据 ", "index": "搜索引擎,python,数据挖掘,机器学习", "content": "问题：\n\nAUC是什么\nAUC能拿来干什么\nAUC如何求解（深入理解AUC）\n\nAUC是什么\n混淆矩阵(Confusion matrix)\n混淆矩阵是理解大多数评价指标的基础，毫无疑问也是理解AUC的基础。丰富的资料介绍着混淆矩阵的概念，这里用一个经典图来解释混淆矩阵是什么。显然，混淆矩阵包含四部分的信息：\n\nTrue negative(TN)，称为真阴率，表明实际是负样本预测成负样本的样本数\nFalse positive(FP)，称为假阳率，表明实际是负样本预测成正样本的样本数\nFalse negative(FN)，称为假阴率，表明实际是正样本预测成负样本的样本数\nTrue positive(TP)，称为真阳率，表明实际是正样本预测成正样本的样本数\n\n对照着混淆矩阵，很容易就能把关系、概念理清楚，但是久而久之，也很容易忘记概念。不妨我们按照位置前后分为两部分记忆，前面的部分是True／False表示真假，即代表着预测的正确性，后面的部分是positive／negative表示正负样本，即代表着预测的结果，所以，混淆矩阵即可表示为正确性－预测结果的集合。现在我们再来看上述四个部分的概念（均代表样本数，下述省略）：\n\nTN，预测是负样本，预测对了\nFP，预测是正样本，预测错了\nFN，预测是负样本，预测错了\nTP，预测是正样本，预测对了\n\n几乎我所知道的所有评价指标，都是建立在混淆矩阵基础上的，包括准确率、精准率、召回率、F1-score，当然也包括AUC。\nROC曲线\n事实上，要一下子弄清楚什么是AUC并不是那么容易，首先我们要从ROC曲线说起。对于某个二分类分类器来说，输出结果标签（0还是1）往往取决于输出的概率以及预定的概率阈值，比如常见的阈值就是0.5，大于0.5的认为是正样本，小于0.5的认为是负样本。如果增大这个阈值，预测错误（针对正样本而言，即指预测是正样本但是预测错误，下同）的概率就会降低但是随之而来的就是预测正确的概率也降低；如果减小这个阈值，那么预测正确的概率会升高但是同时预测错误的概率也会升高。实际上，这种阈值的选取也一定程度上反映了分类器的分类能力。我们当然希望无论选取多大的阈值，分类都能尽可能地正确，也就是希望该分类器的分类能力越强越好，一定程度上可以理解成一种鲁棒能力吧。为了形象地衡量这种分类能力，ROC曲线横空出世！如下图所示，即为一条ROC曲线（该曲线的原始数据第三部分会介绍）。现在关心的是：\n\n横轴：False Positive Rate（假阳率，FPR）\n纵轴：True Positive Rate（真阳率，TPR）\n\n\n\n假阳率，简单通俗来理解就是预测为正样本但是预测错了的可能性，显然，我们不希望该指标太高。$$FPR=\\frac{FP}{TN+FP}$$\n真阳率，则是代表预测为正样本但是预测对了的可能性，当然，我们希望真阳率越高越好。$$TPR=\\frac{TP}{TP+FN}$$\n\n显然，ROC曲线的横纵坐标都在[0,1]之间，自然ROC曲线的面积不大于1。现在我们来分析几个特殊情况，从而更好地掌握ROC曲线的性质：\n\n(0,0)：假阳率和真阳率都为0，即分类器全部预测成负样本\n(0,1)：假阳率为0，真阳率为1，全部完美预测正确，happy\n(1,0)：假阳率为1，真阳率为0，全部完美预测错误，悲剧\n(1,1)：假阳率和真阳率都为1，即分类器全部预测成正样本\nTPR＝FPR，斜对角线，预测为正样本的结果一半是对的，一半是错的，代表随机分类器的预测效果\n\n于是，我们可以得到基本的结论：ROC曲线在斜对角线以下，则表示该分类器效果差于随机分类器，反之，效果好于随机分类器，当然，我们希望ROC曲线尽量除于斜对角线以上，也就是向左上角（0,1）凸。\nAUC(Area under the ROC curve)\nROC曲线一定程度上可以反映分类器的分类效果，但是不够直观，我们希望有这么一个指标，如果这个指标越大越好，越小越差，于是，就有了AUC。AUC实际上就是ROC曲线下的面积。AUC直观地反映了ROC曲线表达的分类能力。\n\nAUC ＝ 1，代表完美分类器\n0.5 < AUC < 1，优于随机分类器\n0 < AUC < 0.5，差于随机分类器\n\nAUC能拿来干什么\n从作者有限的经历来说，AUC最大的应用应该就是点击率预估（CTR）的离线评估。CTR的离线评估在公司的技术流程中占有很重要的地位，一般来说，ABTest和转全观察的资源成本比较大，所以，一个合适的离线评价可以节省很多时间、人力、资源成本。那么，为什么AUC可以用来评价CTR呢？我们首先要清楚两个事情：\n\nCTR是把分类器输出的概率当做是点击率的预估值，如业界常用的LR模型，利用sigmoid函数将特征输入与概率输出联系起来，这个输出的概率就是点击率的预估值。内容的召回往往是根据CTR的排序而决定的。\nAUC量化了ROC曲线表达的分类能力。这种分类能力是与概率、阈值紧密相关的，分类能力越好（AUC越大），那么输出概率越合理，排序的结果越合理。\n\n我们不仅希望分类器给出是否点击的分类信息，更需要分类器给出准确的概率值，作为排序的依据。所以，这里的AUC就直观地反映了CTR的准确性（也就是CTR的排序能力）\nAUC如何求解\n步骤如下：\n\n得到结果数据，数据结构为：（输出概率，标签真值）\n对结果数据按输出概率进行分组，得到（输出概率，该输出概率下真实正样本数，该输出概率下真实负样本数）。这样做的好处是方便后面的分组统计、阈值划分统计等\n对结果数据按输出概率进行从大到小排序\n从大到小，把每一个输出概率作为分类阈值，统计该分类阈值下的TPR和FPR\n微元法计算ROC曲线面积、绘制ROC曲线\n\n代码如下所示：\nimport pylab as pl\nfrom math import log,exp,sqrt\nimport itertools\nimport operator\n\ndef read_file(file_path, accuracy=2):\n    db = []  #(score,nonclk,clk)\n    pos, neg = 0, 0 #正负样本数量\n    #读取数据\n    with open(file_path,'r') as fs:\n        for line in fs:\n            temp = eval(line)\n            #精度可控\n            #score = '%.1f' % float(temp[0])\n            score = float(temp[0])\n            trueLabel = int(temp[1])\n            sample = [score, 0, 1] if trueLabel == 1 else [score, 1, 0]\n            score,nonclk,clk = sample\n            pos += clk #正样本\n            neg += nonclk #负样本\n            db.append(sample)\n    return db, pos, neg\n\ndef get_roc(db, pos, neg):\n    #按照输出概率，从大到小排序\n    db = sorted(db, key=lambda x:x[0], reverse=True)\n    file=open('data.txt','w')\n    file.write(str(db))\n    file.close()\n    #计算ROC坐标点\n    xy_arr = []\n    tp, fp = 0., 0.\n    for i in range(len(db)):\n        tp += db[i][2]\n        fp += db[i][1]\n        xy_arr.append([fp/neg,tp/pos])\n    return xy_arr\n\ndef get_AUC(xy_arr):\n    #计算曲线下面积\n    auc = 0.\n    prev_x = 0\n    for x,y in xy_arr:\n        if x != prev_x:\n            auc += (x - prev_x) * y\n            prev_x = x\n    return auc\n\ndef draw_ROC(xy_arr):\n    x = [_v[0] for _v in xy_arr]\n    y = [_v[1] for _v in xy_arr]\n    pl.title(\"ROC curve of %s (AUC = %.4f)\" % ('clk',auc))\n    pl.xlabel(\"False Positive Rate\")\n    pl.ylabel(\"True Positive Rate\")\n    pl.plot(x, y)# use pylab to plot x and y\n    pl.show()# show the plot on the screen\n数据：提供的数据为每一个样本的（预测概率，真实标签）tuple数据链接：https://pan.baidu.com/s/1c1FUzVM，密码1ax8计算结果：AUC＝0.747925810016，与Spark MLLib中的roc_AUC计算值基本吻合当然，选择的概率精度越低，AUC计算的偏差就越大\n总结\n\nROC曲线反映了分类器的分类能力，结合考虑了分类器输出概率的准确性\nAUC量化了ROC曲线的分类能力，越大分类效果越好，输出概率越合理\nAUC常用作CTR的离线评价，AUC越大，CTR的排序能力越强\n\n参考资料\n很多大牛对AUC都有自己的认识和理解，这里围绕和AUC的意义是什么，给出一些能帮助自己理解AUC的 大牛们的回答［1］From  机器学习和统计里面的auc怎么理解？\n\n［2］From  机器学习和统计里面的auc怎么理解？\n\n［3］From 精确率、召回率、F1 值、ROC、AUC 各自的优缺点是什么？\n\n［4］From 多高的AUC才算高？\n\n其他一些参考资料：利用Python画ROC曲线和AUC值计算精确率与召回率，RoC曲线与PR曲线ROC和AUC介绍以及如何计算AUC基于混淆矩阵的评价指标机器学习性能评估指标\n\n                ", "mainLikeNum": ["10 "], "mainBookmarkNum": "10"}