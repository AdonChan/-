{"title": "K近邻算法的kd树实现 - 算法渐进 ", "index": "python,c++,c,eclipse,windows", "content": "k近邻算法的介绍\nk近邻算法是一种基本的分类和回归方法，这里只实现分类的k近邻算法。k近邻算法的输入为实例的特征向量，对应特征空间的点；输出为实例的类别，可以取多类。k近邻算法不具有显式的学习过程，实际上k近邻算法是利用训练数据集对特征向量空间进行划分。将划分的空间模型作为其分类模型。\n\nk近邻算法的三要素\n\nk值的选择：即分类决策时选择k个最近邻实例；\n距离度量：即预测实例点和训练实例点间的距离，一般使用L2距离即欧氏距离；\n分类决策规则。\n\n下面对三要素进行一下说明：1.欧氏距离即欧几里得距离，高中数学中用来计算点和点间的距离公式；2.k值选择：k值选择会对k近邻法结果产生重大影响，如果选择较小的k值，相当于在较小的邻域中训练实例进行预测，这样有点是“近似误差”会减小，即只与输入实例较近（相似）的训练实例才会起作用，缺点是“估计误差”会增大，即对近邻的实例点很敏感。而k值过大则相反。实际中取较小的k值通过交叉验证的方法取最优k值。3.k近邻法的分类决策规则往往采用多数表决的方式，这等价于“经验风险最小化”。\n\nk近邻算法的实现：kd树\n\n实现k近邻法是要考虑的主要问题是如何退训练数据进行快速的k近邻搜索，当训练实例数很大是显然通过一般的线性搜索方式效率低下，因此为了提高搜索效率，需要构造特殊的数据结构对训练实例进行存储。kd树就是一种不错的数据结构，可以大大提高搜索效率。本质商kd树是对k维空间的一个划分，构造kd树相当与使用垂直于坐标轴的超平面将k维空间进行切分，构造一系列的超矩形，kd树的每一个结点对应一个这样的超矩形。kd树本质上是一棵二叉树，当通过一定规则构造是他是平衡的。下面是过早kd树的算法：\n\n开始：构造根结点，根节点对应包含所有训练实例的k为空间。 选择第1维为坐标轴，以所有训练实例的第一维数据的中位数为切分点，将根结点对应的超矩形切分为两个子区域。由根结点生成深度为1的左右子结点，左结点对应第一维坐标小于切分点的子区域，右子结点对应第一位坐标大于切分点的子区域。\n重复：对深度为j的结点选择第l维为切分坐标轴，l=j(modk)+1,以该区域中所有训练实例的第l维的中位数为切分点，重复第一步。\n直到两个子区域没有实例存在时停止。形成kd树。\n\n\n以下是kd树的python实现\n准备工作\n#读取数据准备\ndef file2matrix(filename):\n    fr = open(filename)\n    returnMat = []          #样本数据矩阵\n    for line in fr.readlines():\n        line = line.strip().split('\\t')\n        returnMat.append([float(line[0]),float(line[1]),float(line[2]),float(line[3])])\n    return returnMat\n    \n#将数据归一化，避免数据各维度间的差异过大\ndef autoNorm(data):\n    #将data数据和类别拆分\n    data,label = np.split(data,[3],axis=1)\n    minVals = data.min(0)     #data各列的最大值\n    maxVals = data.max(0)       #data各列的最小值\n    ranges = maxVals - minVals\n    normDataSet = np.zeros(np.shape(data))\n    m = data.shape[0]\n    #tile函数将变量内容复制成输入矩阵同样大小的矩阵\n    normDataSet = data - np.tile(minVals,(m,1))        \n    normDataSet = normDataSet/np.tile(ranges,(m,1))\n    #拼接\n    normDataSet = np.hstack((normDataSet,label))\n    return normDataSet\n//数据实例\n40920    8.326976    0.953952    3\n14488    7.153469    1.673904    2\n26052    1.441871    0.805124    1\n75136    13.147394    0.428964    1\n38344    1.669788    0.134296    1\n72993    10.141740    1.032955    1\n35948    6.830792    1.213192    3\n42666    13.276369    0.543880    3\n67497    8.631577    0.749278    1\n35483    12.273169    1.508053    3\n//每一行是一个数据实例，前三维是数据值，第四维是类别标记\n\n树结构定义\n#构建kdTree将特征空间划分\nclass kd_tree:\n    \"\"\"\n    定义结点\n    value:节点值\n    dimension：当前划分的维数\n    left:左子树\n    right:右子树\n    \"\"\"\n    def __init__(self, value):\n        self.value = value\n        self.dimension = None       #记录划分的维数\n        self.left = None\n        self.right = None\n    \n    def setValue(self, value):\n        self.value = value\n    \n    #类似Java的toString()方法\n    def __str__(self):\n        return str(self.value)\n\nkd树构造\ndef creat_kdTree(dataIn, k, root, deep):\n    \"\"\"\n    data:要划分的特征空间（即数据集）\n    k:表示要选择k个近邻\n    root:树的根结点\n    deep:结点的深度\n    \"\"\"\n    #选择x(l)(即为第l个特征)为坐标轴进行划分，找到x(l)的中位数进行划分\n#     x_L = data[:,deep%k]        #这里选取第L个特征的所有数据组成一个列表\n    #获取特征值中位数，这里是难点如果numpy没有提供的话\n    \n    if(dataIn.shape[0]>0):      #如果该区域还有实例数据就继续\n        dataIn = dataIn[dataIn[:,int(deep%k)].argsort()]       #numpy的array按照某列进行排序\n        data1 = None; data2 = None\n        #拿取根据xL排序的中位数的数据作为该子树根结点的value\n        if(dataIn.shape[0]%2 == 0):     #该数据集有偶数个数据\n            mid = int(dataIn.shape[0]/2)\n            root = kd_tree(dataIn[mid,:])\n            root.dimension = deep%k\n            dataIn = np.delete(dataIn,mid, axis = 0)\n            data1,data2 = np.split(dataIn,[mid], axis=0) \n            #mid行元素分到data2中，删除放到根结点中\n        elif(dataIn.shape[0]%2 == 1):\n            mid = int((dataIn.shape[0]+1)/2 - 1)    #这里出现递归溢出，当shape为(1,4)时出现，原因是np.delete时没有赋值给dataIn\n            root = kd_tree(dataIn[mid,:])\n            root.dimension = deep%k\n            dataIn = np.delete(dataIn,mid, axis = 0)\n            data1,data2 = np.split(dataIn,[mid], axis=0) #mid行元素分到data1中，删除放到根结点中\n        #深度加一\n        deep+=1\n        #递归构造子树\n        #这里犯了严重错误，递归调用是将root传递进去，造成程序混乱，应该给None\n        root.left = creat_kdTree(data1, k, None, deep)\n        root.right = creat_kdTree(data2, k, None, deep)\n    return root\n前序遍历测试\n#前序遍历kd树\ndef preorder(kd_tree,i):\n    print(str(kd_tree.value)+\" :\"+str(kd_tree.dimension)+\":\"+str(i))\n    if kd_tree.left != None:\n        preorder(kd_tree.left,i+1)\n    if kd_tree.right != None:\n        preorder(kd_tree.right,i+1)\n\nkd树的最近邻搜索\n最近邻搜索算法，k近邻搜索在此基础上实现原理：首先找到包含目标点的叶节点；然后从该也结点出发，一次退回到父节点，不断查找与目标点最近的结点，当确定不可能存在更近的结点是停止。\ndef findClosest(kdNode,closestPoint,x,minDis,i=0):\n    \"\"\"\n    这里存在一个问题，当传递普通的不可变对象minDis时，递归退回第一次找到\n    最端距离前，minDis改变，最后结果混乱，这里传递一个可变对象进来。\n    kdNode:是构造好的kd树。\n    closestPoint：是存储最近点的可变对象，这里是array\n    x：是要预测的实例\n    minDis：是当前最近距离。\n    \"\"\"\n    if kdNode == None:\n        return\n    #计算欧氏距离\n    curDis = (sum((kdNode.value[0:3]-x[0:3])**2))**0.5\n    if minDis[0] < 0 or curDis < minDis[0] :\n        i+=1\n        minDis[0] = curDis \n        closestPoint[0] = kdNode.value[0]\n        closestPoint[1] = kdNode.value[1]\n        closestPoint[2] = kdNode.value[2]\n        closestPoint[3] = kdNode.value[3]\n        print(str(closestPoint)+\" : \"+str(i)+\" : \"+str(minDis))\n    #递归查找叶节点\n    if kdNode.value[kdNode.dimension] >= x[kdNode.dimension]:\n        findClosest(kdNode.left,closestPoint,x,minDis,i)\n    else:\n        findClosest(kdNode.right, closestPoint, x, minDis,i) \n    #计算测试点和分隔超平面的距离，如果相交进入另一个叶节点重复\n    rang = abs(x[kdNode.dimension] - kdNode.value[kdNode.dimension])\n    if rang > minDis[0] :\n        return\n    if kdNode.value[kdNode.dimension] >= x[kdNode.dimension]:\n        findClosest(kdNode.right,closestPoint,x,minDis,i)\n    else:\n        findClosest(kdNode.left, closestPoint, x, minDis,i) \n测试：\ndata = file2matrix(\"datingTestSet2.txt\")\ndata = np.array(data)\nnormDataSet = autoNorm(data)\nsys.setrecursionlimit(10000)            #设置递归深度为10000\ntrainSet,testSet = np.split(normDataSet,[900],axis=0) \nkdTree = creat_kdTree(trainSet, 3, None, 0)\nnewData = testSet[1,0:3]\nclosestPoint = np.zeros(4)\nminDis = np.array([-1.0])\nfindClosest(kdTree, closestPoint, newData, minDis)\nprint(closestPoint)\nprint(testSet[1,:])\nprint(minDis)\n测试结果\n[0.35118819 0.43961918 0.67110669 3.        ] : 1 : [0.40348346]\n[0.11482037 0.13448927 0.48293309 2.        ] : 2 : [0.30404792]\n[0.12227055 0.07902201 0.57826697 2.        ] : 3 : [0.22272422]\n[0.0645755  0.10845299 0.83274698 2.        ] : 4 : [0.07066192]\n[0.10020488 0.15196271 0.76225551 2.        ] : 5 : [0.02546591]\n[0.10020488 0.15196271 0.76225551 2.        ]\n[0.08959933 0.15442555 0.78527657 2.        ]\n[0.02546591]\n\nk近邻搜索实现\n在最近邻的基础上进行改进得到：这里的closestPoint和minDis合并，一同处理\n#k近邻搜索\ndef findKNode(kdNode, closestPoints, x, k):\n    \"\"\"\n    k近邻搜索，kdNode是要搜索的kd树\n    closestPoints:是要搜索的k近邻点集合,将minDis放入closestPoints最后一列合并\n    x：预测实例\n    minDis：是最近距离\n    k:是选择k个近邻\n    \"\"\"\n    if kdNode == None:\n        return\n    #计算欧式距离\n    curDis = (sum((kdNode.value[0:3]-x[0:3])**2))**0.5\n    #将closestPoints按照minDis列排序,这里存在一个问题，排序后返回一个新对象\n    #不能将其直接赋值给closestPoints\n    tempPoints = closestPoints[closestPoints[:,4].argsort()]\n    for i in range(k):\n        closestPoints[i] = tempPoints[i]\n    #每次取最后一行元素操作\n    if closestPoints[k-1][4] >=10000  or closestPoints[k-1][4] > curDis:\n        closestPoints[k-1][4] = curDis\n        closestPoints[k-1,0:4] = kdNode.value \n        \n    #递归搜索叶结点\n    if kdNode.value[kdNode.dimension] >= x[kdNode.dimension]:\n        findKNode(kdNode.left, closestPoints, x, k)\n    else:\n        findKNode(kdNode.right, closestPoints, x, k)\n    #计算测试点和分隔超平面的距离，如果相交进入另一个叶节点重复\n    rang = abs(x[kdNode.dimension] - kdNode.value[kdNode.dimension])\n    if rang > closestPoints[k-1][4]:\n        return\n    if kdNode.value[kdNode.dimension] >= x[kdNode.dimension]:\n        findKNode(kdNode.right, closestPoints, x, k)\n    else:\n        findKNode(kdNode.left, closestPoints, x, k)  \n测试\ndata = file2matrix(\"datingTestSet2.txt\")\ndata = np.array(data)\nnormDataSet = autoNorm(data)\nsys.setrecursionlimit(10000)            #设置递归深度为10000\ntrainSet,testSet = np.split(normDataSet,[900],axis=0) \nkdTree = creat_kdTree(trainSet, 3, None, 0)\nnewData = testSet[1,0:3]\nprint(\"预测实例点：\"+str(newData))\nclosestPoints = np.zeros((3,5))         #初始化参数\nclosestPoints[:,4] = 10000.0            #给minDis列赋值\nfindKNode(kdTree, closestPoints, newData, 3)\nprint(\"k近邻结果：\"+str(closestPoints))\n测试结果\n预测实例点：[0.08959933 0.15442555 0.78527657]\nk近邻结果：[[0.10020488 0.15196271 0.76225551 2.         0.02546591]\n [0.10664709 0.13172159 0.83777837 2.         0.05968697]\n [0.09616206 0.20475001 0.75047289 2.         0.06153793]]\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}