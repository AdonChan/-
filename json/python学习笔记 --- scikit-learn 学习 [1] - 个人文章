{"title": "python学习笔记 --- scikit-learn 学习 [1] - 个人文章 ", "index": "python", "content": "Feature extraction\n详细讲解记录在 传送门\n我在这里只是大概整理我使用过学习过的api。\nLoading features from dicts\n这个方便提取数据特征，比如我们的数据是dict形式的，里面有city是三种不同城市，就可以one-hot encode。\n使用的是 DictVectorizer 这个模块\n>>> measurements = [\n...     {'city': 'Dubai', 'temperature': 33.},\n...     {'city': 'London', 'temperature': 12.},\n...     {'city': 'San Fransisco', 'temperature': 18.},\n... ]\n\n>>> from sklearn.feature_extraction import DictVectorizer\n>>> vec = DictVectorizer()\n\n>>> vec.fit_transform(measurements).toarray()\narray([[  1.,   0.,   0.,  33.],\n       [  0.,   1.,   0.,  12.],\n       [  0.,   0.,   1.,  18.]])\n\n>>> vec.get_feature_names()\n['city=Dubai', 'city=London', 'city=San Fransisco', 'temperature']\n\n\n下面官网又举了个使用例子，是关于pos_window的，词性这方面我也没做过，但是我一开始以为的是在讲这种方式在这种情况下不行，因为有很多0,但是细看后又觉得不是，希望有人能帮我解答。\n以下英文是原文摘抄。\nFor example, suppose that we have a first algorithm that extracts Part of Speech (PoS) tags that we want to use as complementary tags for training a sequence classifier (e.g. a chunker). The following dict could be such a window of features extracted around the word ‘sat’ in the sentence ‘The cat sat on the mat.’:\n    \n>>>\n\n>>> pos_window = [\n...     {\n...         'word-2': 'the',\n...         'pos-2': 'DT',\n...         'word-1': 'cat',\n...         'pos-1': 'NN',\n...         'word+1': 'on',\n...         'pos+1': 'PP',\n...     },\n...     # in a real application one would extract many such dictionaries\n... ]\n\nThis description can be vectorized into a sparse two-dimensional matrix suitable for feeding into a classifier (maybe after being piped into a text.TfidfTransformer for normalization):\n\n>>>\n\n>>> vec = DictVectorizer()\n>>> pos_vectorized = vec.fit_transform(pos_window)\n>>> pos_vectorized                \n<1x6 sparse matrix of type '<... 'numpy.float64'>'\n    with 6 stored elements in Compressed Sparse ... format>\n>>> pos_vectorized.toarray()\narray([[ 1.,  1.,  1.,  1.,  1.,  1.]])\n>>> vec.get_feature_names()\n['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat', 'word-2=the']\n\nAs you can imagine, if one extracts such a context around each individual word of a corpus of documents the resulting matrix will be very wide (many one-hot-features) with most of them being valued to zero most of the time. So as to make the resulting data structure able to fit in memory the DictVectorizer class uses a scipy.sparse matrix by default instead of a numpy.ndarray.\n这部分先放过，接下讲。\nFeature hashing\nFeatureHasher 这个类使用来高速低占用内存向量化，使用的技术是feature hashing，由于现在还没怎么接触这个方面，不细聊了。\n基于murmurhash，这个蛮出名的，以前接触过。由于scipy.sparse的限制，最大的feature个数上限是\n$$2^{31}-1$$\nText feature extraction 文本特征提取\nCommon Vectorizer usage 普通用法\nvectorization ，也就是将文本集合转化成数字向量。这种特殊的策略也叫 \"Bag of words\" 或是 \"Bag of n-grams\"，完全忽略词在文中位置关系。\n第一个介绍 CountVectorizer。\n >>> from sklearn.feature_extraction.text import CountVectorizer\n\n有很多的参数\n >>> vectorizer = CountVectorizer(min_df=1)\n>>> vectorizer                     \nCountVectorizer(analyzer=...'word', binary=False, decode_error=...'strict',\n    dtype=<... 'numpy.int64'>, encoding=...'utf-8', input=...'content',\n    lowercase=True, max_df=1.0, max_features=None, min_df=1,\n    ngram_range=(1, 1), preprocessor=None, stop_words=None,\n    strip_accents=None, token_pattern=...'(?u)\\\\b\\\\w\\\\w+\\\\b',\n    tokenizer=None, vocabulary=None)\n    \n    \n下面稍微使用一下\n>>> corpus = [\n...     'This is the first document.',\n...     'This is the second second document.',\n...     'And the third one.',\n...     'Is this the first document?',\n... ]\n>>> X = vectorizer.fit_transform(corpus)\n>>> X                              \n<4x9 sparse matrix of type '<... 'numpy.int64'>'\n    with 19 stored elements in Compressed Sparse ... format>\n    \n结果\n>>> vectorizer.get_feature_names() == (\n...     ['and', 'document', 'first', 'is', 'one',\n...      'second', 'the', 'third', 'this'])\nTrue\n\n>>> X.toarray()           \narray([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n       [0, 1, 0, 1, 0, 2, 1, 0, 1],\n       [1, 0, 0, 0, 1, 0, 1, 1, 0],\n       [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)\n\n可以看出这是根据单词来统计feature个数，属于one-hot，一般来讲不实用。\nTf–idf term weighting\n这个能好点，tf-idf我就不讲了，原理很简单。\n下面可贴一个实例，count里面就是计算好了的单词出现的个数，只有三个单词。\n>>> counts = [[3, 0, 1],\n...           [2, 0, 0],\n...           [3, 0, 0],\n...           [4, 0, 0],\n...           [3, 2, 0],\n...           [3, 0, 2]]\n...\n>>> tfidf = transformer.fit_transform(counts)\n>>> tfidf                         \n<6x3 sparse matrix of type '<... 'numpy.float64'>'\n    with 9 stored elements in Compressed Sparse ... format>\n\n>>> tfidf.toarray()                        \narray([[ 0.81940995,  0.        ,  0.57320793],\n       [ 1.        ,  0.        ,  0.        ],\n       [ 1.        ,  0.        ,  0.        ],\n       [ 1.        ,  0.        ,  0.        ],\n       [ 0.47330339,  0.88089948,  0.        ],\n       [ 0.58149261,  0.        ,  0.81355169]])\n\n具体在项目中是如下使用。\n>>> from sklearn.feature_extraction.text import TfidfVectorizer\n>>> vectorizer = TfidfVectorizer(min_df=1)\n>>> vectorizer.fit_transform(corpus)\n\n\nVectorizing a large text corpus with the hashing trick\n使用hash技巧来适配大数据集，没用过，看上去很牛\n<font color=#00aaaa >The above vectorization scheme is simple but the fact that it holds an in- memory mapping from the string tokens to the integer feature indices (the vocabulary_ attribute) causes several problems when dealing with large datasets:\nthe larger the corpus, the larger the vocabulary will grow and hence the memory use too,\nfitting requires the allocation of intermediate data structures of size proportional to that of the original dataset.\nbuilding the word-mapping requires a full pass over the dataset hence it is not possible to fit text classifiers in a strictly online manner.\npickling and un-pickling vectorizers with a large vocabulary_ can be very slow (typically much slower than pickling / un-pickling flat data structures such as a NumPy array of the same size),\nit is not easily possible to split the vectorization work into concurrent sub tasks as the vocabulary_ attribute would have to be a shared state with a fine grained synchronization barrier: the mapping from token string to feature index is dependent on ordering of the first occurrence of each token hence would have to be shared, potentially harming the concurrent workers’ performance to the point of making them slower than the sequential variant.</font>\n\n>>> from sklearn.feature_extraction.text import HashingVectorizer\n>>> hv = HashingVectorizer(n_features=10)\n>>> hv.transform(corpus)\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}