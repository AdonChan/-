{"title": "tornado stream upload - dreambei ", "index": "tornado,python", "content": "tornado 4.0 新加tornado.web.stream_request_body decorator ，用于stream request\nStreaming uploads let you handle large requests without buffering everything into memory, but there is still generally some limits to what you're willing to handle.  The max_buffer_size and max_body_size parameters are now separate, but they both default to 100MB.  With streaming uploads, you can increase max_body_size as much as you want without increasing your memory requirements, but make sure you have enough disk space (or s3 budget, etc) to handle the uploads you'll get.  You can even set max_body_size on a per-request basis by calling self.request.connection.set_max_body_size() from prepare()\n\nimport tornado.web\nimport tornado.ioloop\n\nMB = 1024 * 1024\nGB = 1024 * MB\nTB = 1024 * GB\n\nMAX_STREAMED_SIZE = 1*GB\n\n@tornado.web.stream_request_body\nclass MainHandler(tornado.web.RequestHandler):\n    def prepare(self):\n        self.f = open(\"xxxxxxxx\", \"wb\")\n        \n        # 如果不设max_body_size, 不能上传>100MB的文件\n        self.request.connection.set_max_body_size(MAX_STREAMED_SIZE)\n        \n    def post(self):\n        print(\"upload completed\")\n        self.f.close()\n\n    def data_received(self, data):\n        self.f.write(data)\n\n\nif __name__ == \"__main__\":\n    application = tornado.web.Application([\n        (r\"/\", MainHandler),\n    ])\n    application.listen(7777)\n    tornado.ioloop.IOLoop.instance().start()\ntornado.web.stream_request_body 源码\n测试:\ncurl -v -XPOST --data-binary @presto-server-0.144.2.tar.gz -127.0.0.1:7777/\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "2"}