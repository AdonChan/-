{"title": "celery动态添加任务 - Hello World ", "index": "celery,linux,python", "content": "celery是一个基于Python的分布式调度系统，文档在这 ,最近有个需求,想要动态的添加任务而不用重启celery服务,找了一圈没找到什么好办法(也有可能是文档没看仔细)，所以只能自己实现囉\n为celery动态添加任务，首先我想到的是传递一个函数进去，让某个特定任务去执行这个传递过去的函数，就像这样\n@app.task\ndef execute(func, *args, **kwargs):\n    return func(*args, **kwargs)\n\n很可惜，会出现这样的错误\nkombu.exceptions.EncodeError: Object of type 'function' is not JSON serializable\n\n换一种序列化方式\n@app.task(serializer='pickle')\ndef execute(func, *args, **kwargs):\n    return func(*args, **kwargs)\n\n结果又出现一大串错误信息\nERROR/MainProcess] Pool callback raised exception: ContentDisallowed('Refusing to deserialize untrusted content of type pickle (application/x-python-serialize)',)\nTraceback (most recent call last):\n  File \"/home/jl/.virtualenvs/test/lib/python3.6/site-packages/kombu/utils/objects.py\", line 42, in __get__\n    return obj.__dict__[self.__name__]\nKeyError: 'chord'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/jl/.virtualenvs/test/lib/python3.6/site-packages/kombu/utils/objects.py\", line 42, in __get__\n    return obj.__dict__[self.__name__]\nKeyError: '_payload'\n\n换一种思路\nfunc = import_string(func)\n\n不知道这样是否可以，结果测试: No\n哎，流年不利.\n最后一直测试，一直测试，终于找到了一种办法,直接上代码\nfrom importlib import import_module, reload\n\napp.conf.CELERY_IMPORTS = ['task', 'task.all_task']\n\ndef import_string(import_name):\n    import_name = str(import_name).replace(':', '.')\n    modules = import_name.split('.')\n    mod = import_module(modules[0])\n    for comp in modules[1:]:\n        if not hasattr(mod, comp):\n            reload(mod)\n        mod = getattr(mod, comp)\n    return mod\n\n@app.task\ndef execute(func, *args, **kwargs):\n    func = import_string(func)\n    return func(*args, **kwargs)\n\n项目结构是这样的\n├── celery_app.py  ├── config.py  ├── task  │   ├── all_task.py  │   ├── __init__.py\n注意: 任务必须大于等于两层目录\n以后每次添加任务都可以先添加到all_task.py里，调用时不用再重启celery服务\n# task/all_task.py\n\ndef ee(c, d):\n    return c, d, '你好'\n\n# example\nfrom celery_app import execute\n\nexecute.delay('task.all_task.ee', 2, 444)\n\nok，另外发现celery也支持任务定时调用,就像这样\nexecute.apply_async(args=['task.all_task.aa'], eta=datetime(2017, 7, 9, 8, 12, 0))\n\n简单实现一个任务重复调用的功能\n@app.task\ndef interval(func, seconds, args=(), task_id=None):\n    next_run_time = current_time() + timedelta(seconds=seconds)\n    kwargs = dict(args=(func, seconds, args), eta=next_run_time)\n    if task_id is not None:\n        kwargs.update(task_id=task_id)\n    interval.apply_async(**kwargs)\n    func = import_string(func)\n    return func(*args)\n\n大概意思就是先计算下次运行的时间,然后把任务添加到celery队列里,这里有个task_id有些问题,因为假设添加了每隔3s执行一个任务,它的task_id默认会使用uuid生成，如果想要再移除这个任务就不太方便，自定task_id可能会好一些，另外也许需要判断task_id是否存在\nAsyncResult(task_id).state\n\nok,再献上一个好用的函数\nfrom inspect import getmembers, isfunction\n\ndef get_tasks(module='task'):\n    return [{\n        'name': 'task:{}'.format(f[1].__name__),\n        'doc': f[1].__doc__,\n    } for f in getmembers(import_module(module), isfunction)]\n\n就这样.\n\n                ", "mainLikeNum": ["3 "], "mainBookmarkNum": "5"}