{"title": "人工智能术语表 - 个人文章 ", "index": "python", "content": "作者：chen_h微信号 & QQ：862251340微信公众号：coderpai简书地址：https://www.jianshu.com/p/b5c...\n\n介绍一些人工智能技术的术语，如果你还有术语补充，请访问 Github\n\n\n\nEnglish Terminology\n中文术语\n\n\n\nneural networks\n神经网络\n\n\nactivation function\n激活函数\n\n\nhyperbolic tangent\n双曲正切函数\n\n\nbias units\n偏置项\n\n\nactivation\n激活值\n\n\nforward propagation\n前向传播\n\n\nfeedforward neural network\n前馈神经网络\n\n\nBackpropagation Algorithm\n反向传播算法\n\n\n(batch) gradient descent\n（批量）梯度下降法\n\n\n(overall) cost function\n（整体）代价函数\n\n\nsquared-error\n方差\n\n\naverage sum-of-squares error\n均方差\n\n\nregularization term\n规则化项\n\n\nweight decay\n权重衰减\n\n\nbias terms\n偏置项\n\n\nBayesian regularization method\n贝叶斯规则化方法\n\n\nGaussian prior\n高斯先验概率\n\n\nMAP\n极大后验估计\n\n\nmaximum likelihood estimation\n极大似然估计\n\n\nactivation function\n激活函数\n\n\ntanh function\n双曲正切函数\n\n\nnon-convex function\n非凸函数\n\n\nhidden (layer) units\n隐藏层单元\n\n\nsymmetry breaking\n对称失效\n\n\nlearning rate\n学习速率\n\n\nforward pass\n前向传导\n\n\nhypothesis\n假设值\n\n\nerror term\n残差\n\n\nweighted average\n加权平均值\n\n\nfeedforward pass\n前馈传导\n\n\nHadamard product\n阿达马乘积\n\n\nforward propagation\n前向传播\n\n\noff-by-one error\n缺位错误\n\n\nbias term\n偏置项\n\n\nnumerically checking\n数值检验\n\n\nnumerical roundoff errors\n数值舍入误差\n\n\nsignificant digits\n有效数字\n\n\nunrolling\n组合扩展\n\n\nlearning rate\n学习速率\n\n\nHessian matrix Hessian\n矩阵\n\n\nNewton's method\n牛顿法\n\n\nconjugate gradient\n共轭梯度\n\n\nstep-size\n步长值\n\n\nAutoencoders\n自编码算法\n\n\nSparsity\n稀疏性\n\n\nneural networks\n神经网络\n\n\nsupervised learning\n监督学习\n\n\nunsupervised learning\n无监督学习\n\n\nhidden units\n隐藏神经元\n\n\nthe pixel intensity value\n像素灰度值\n\n\nIID\n独立同分布\n\n\nPCA\n主元分析\n\n\nactive\n激活\n\n\ninactive\n抑制\n\n\nactivation function\n激活函数\n\n\nactivation\n激活度\n\n\nthe average activation\n平均活跃度\n\n\nsparsity parameter\n稀疏性参数\n\n\npenalty term\n惩罚因子\n\n\nKL divergence\nKL 散度\n\n\nBernoulli random variable\n伯努利随机变量\n\n\noverall cost function\n总体代价函数\n\n\nbackpropagation\n后向传播\n\n\nforward pass\n前向传播\n\n\ngradient descent\n梯度下降\n\n\nthe objective\n目标函数\n\n\nthe derivative checking method\n梯度验证方法\n\n\nVisualizing\n可视化\n\n\nAutoencoder\n自编码器\n\n\nhidden unit\n隐藏单元\n\n\nnon-linear feature\n非线性特征\n\n\nactivate\n激励\n\n\ntrivial answer\n平凡解\n\n\nnorm constrained\n范数约束\n\n\nsparse autoencoder\n稀疏自编码器\n\n\nnorm bounded\n有界范数\n\n\ninput domains\n输入域\n\n\nvectorization\n矢量化\n\n\nLogistic Regression\n逻辑回归\n\n\nbatch gradient ascent\n批量梯度上升法\n\n\nintercept term\n截距\n\n\nthe log likelihood\n对数似然函数\n\n\nderivative\n导函数\n\n\ngradient\n梯度\n\n\nvectorization\n向量化\n\n\nforward propagation\n正向传播\n\n\nbackpropagation\n反向传播\n\n\ntraining examples\n训练样本\n\n\nactivation function\n激活函数\n\n\nsparse autoencoder\n稀疏自编码网络\n\n\nsparsity penalty\n稀疏惩罚\n\n\naverage firing rate\n平均激活率\n\n\nPrincipal Components Analysis\n主成份分析\n\n\nwhitening\n白化\n\n\nintensity\n亮度\n\n\nmean\n平均值\n\n\nvariance\n方差\n\n\ncovariance matrix\n协方差矩阵\n\n\nbasis\n基\n\n\nmagnitude\n幅值\n\n\nstationarity\n平稳性\n\n\nnormalization\n归一化\n\n\neigenvector\n特征向量\n\n\nredundant\n冗余\n\n\nvariance\n方差\n\n\nsmoothing\n平滑\n\n\ndimensionality reduction\n降维\n\n\nregularization\n正则化\n\n\nreflection matrix\n反射矩阵\n\n\ndecorrelation\n去相关\n\n\nPrincipal Components Analysis (PCA)\n主成分分析\n\n\nzero-mean\n均值为零\n\n\nmean value\n均值\n\n\neigenvalue\n特征值\n\n\nsymmetric positive semi-definite matrix\n对称半正定矩阵\n\n\nnumerically reliable\n数值计算上稳定\n\n\nsorted in decreasing order\n降序排列\n\n\nsingular value\n奇异值\n\n\nsingular vector\n奇异向量\n\n\nvectorized implementation\n向量化实现\n\n\ndiagonal\n对角线\n\n\nSoftmax Regression\nSoftmax回归\n\n\nsupervised learning\n有监督学习\n\n\nunsupervised learning\n无监督学习\n\n\ndeep learning\n深度学习\n\n\nlogistic regression\nlogistic回归\n\n\nintercept term\n截距项\n\n\nbinary classification\n二元分类\n\n\nclass labels\n类型标记\n\n\nhypothesis\n估值函数/估计值\n\n\ncost function\n代价函数\n\n\nmulti-class classification\n多元分类\n\n\nweight decay\n权重衰减\n\n\nself-taught learning\n自我学习/自学习\n\n\nunsupervised feature learning\n无监督特征学习\n\n\nautoencoder\n自编码器\n\n\nsemi-supervised learning\n半监督学习\n\n\ndeep networks\n深层网络\n\n\nfine-tune\n微调\n\n\nunsupervised feature learning\n非监督特征学习\n\n\npre-training\n预训练\n\n\nDeep Networks\n深度网络\n\n\ndeep neural networks\n深度神经网络\n\n\nnon-linear transformation\n非线性变换\n\n\nrepresent compactly\n简洁地表达\n\n\npart-whole decompositions\n“部分-整体”的分解\n\n\nparts of objects\n目标的部件\n\n\nhighly non-convex optimization problem\n高度非凸的优化问题\n\n\nconjugate gradient\n共轭梯度\n\n\ndiffusion of gradients\n梯度的弥散\n\n\nGreedy layer-wise training\n逐层贪婪训练方法\n\n\nautoencoder\n自动编码器\n\n\nGreedy layer-wise training\n逐层贪婪训练法\n\n\nStacked autoencoder\n栈式自编码神经网络\n\n\nFine-tuning\n微调\n\n\nRaw inputs\n原始输入\n\n\nHierarchical grouping\n层次型分组\n\n\nPart-whole decomposition\n部分-整体分解\n\n\nFirst-order features\n一阶特征\n\n\nSecond-order features\n二阶特征\n\n\nHigher-order features\n更高阶特征\n\n\nLinear Decoders\n线性解码器\n\n\nSparse Autoencoder\n稀疏自编码\n\n\ninput layer\n输入层\n\n\nhidden layer\n隐含层\n\n\noutput layer\n输出层\n\n\nneuron\n神经元\n\n\nrobust\n鲁棒\n\n\nsigmoid activation function\nS型激励函数\n\n\ntanh function\ntanh激励函数\n\n\nlinear activation function\n线性激励函数\n\n\nidentity activation function\n恒等激励函数\n\n\nhidden unit\n隐单元\n\n\nweight\n权重\n\n\nerror term\n偏差项\n\n\nFull Connected Networks\n全连接网络\n\n\nSparse Autoencoder\n稀疏编码\n\n\nFeedforward\n前向输送\n\n\nBackpropagation\n反向传播\n\n\nLocally Connected Networks\n部分联通网络\n\n\nContiguous Groups\n连接区域\n\n\nVisual Cortex\n视觉皮层\n\n\nConvolution\n卷积\n\n\nStationary\n固有特征\n\n\nPool\n池化\n\n\nfeatures\n特征\n\n\nexample\n样例\n\n\nover-fitting\n过拟合\n\n\ntranslation invariant\n平移不变性\n\n\npooling\n池化\n\n\nextract\n提取\n\n\nobject detection\n物体检测\n\n\nDC component\n直流分量\n\n\nlocal mean subtraction\n局部均值消减\n\n\nsparse autoencoder\n消减归一化\n\n\nrescaling\n缩放\n\n\nper-example mean subtraction\n逐样本均值消减\n\n\nfeature standardization\n特征标准化\n\n\nstationary\n平稳\n\n\nzero-mean\n零均值化\n\n\nlow-pass filtering\n低通滤波\n\n\nreconstruction based models\n基于重构的模型\n\n\nRBMs\n受限Boltzman机\n\n\nk-Means\nk-均值\n\n\nlong tail\n长尾\n\n\nloss function\n损失函数\n\n\northogonalization\n正交化\n\n\nSparse Coding\n稀疏编码\n\n\nunsupervised method\n无监督学习\n\n\nover-complete bases\n超完备基\n\n\ndegeneracy\n退化\n\n\nreconstruction term\n重构项\n\n\nsparsity penalty\n稀疏惩罚项\n\n\nnorm\n范式\n\n\ngenerative model\n生成模型\n\n\nlinear superposition\n线性叠加\n\n\nadditive noise\n加性噪声\n\n\nbasis feature vectors\n特征基向量\n\n\nthe empirical distribution\n经验分布函数\n\n\nthe log-likelihood\n对数似然函数\n\n\nGaussian white noise\n高斯白噪音\n\n\nthe prior distribution\n先验分布\n\n\nprior probability\n先验概率\n\n\nsource features\n源特征\n\n\nthe energy function\n能量函数\n\n\nregularized\n正则化\n\n\nleast squares\n最小二乘法\n\n\nconvex optimization software\n凸优化软件\n\n\nconjugate gradient methods\n共轭梯度法\n\n\nquadratic constraints\n二次约束\n\n\nthe Lagrange dual\n拉格朗日对偶函数\n\n\nfeedforward architectures\n前馈结构算法\n\n\nIndependent Component Analysis\n独立成分分析\n\n\nOver-complete basis\n超完备基\n\n\nOrthonormal basis\n标准正交基\n\n\nSparsity penalty\n稀疏惩罚项\n\n\nUnder-complete basis\n不完备基\n\n\nLine-search algorithm\n线搜索算法\n\n\nTopographic cost term\n拓扑代价项\n\n\n\n\n作者：chen_h微信号 & QQ：862251340简书地址：https://www.jianshu.com/p/b5c...\nCoderPai 是一个专注于算法实战的平台，从基础的算法到人工智能算法都有设计。如果你对算法实战感兴趣，请快快关注我们吧。加入AI实战微信群，AI实战QQ群，ACM算法微信群，ACM算法QQ群。长按或者扫描如下二维码，关注 “CoderPai” 微信号（coderpai）\n\n\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "1"}