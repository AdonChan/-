{"title": "爬虫基础练习－python批量下载图片之达盖尔的旗帜 - Python 高手之路 ", "index": "python", "content": "三个重点，我隐藏了1024的地址，本爬虫只适用于1024的抓取。每个帖子以帖子名为文件名新建一个文件并把帖子内图片保存下来。\nurl_start设置起始页  url_end设置结束页有问题的留言，我看到就会回复\n1编码\n2文章页链接匹配\n3文件夹操作\n\n\nimport requests\nimport re\nimport time\nfrom bs4 import BeautifulSoup\nimport os\n\nurl_start = 'url1'\nurl_end = 'url2'\n\n\n# 获取图片链接并保存到文件夹的函数\ndef getIMG(article_url):\n    # time.sleep(1)\n    urls = []\n    try:\n        html = requests.get(article_url)\n        html.encoding = 'gbk'\n        soup = BeautifulSoup(html.text, 'html.parser')\n        part_picURL = re.findall(\"src='http://img(.+?\\.jpg)'\",html.text,re.S)\n        for each in part_picURL:\n            picURL = 'http://img' + each\n            urls.append(picURL)\n        i=0\n        for each in urls:\n            try:\n                pic = requests.get(each, timeout = 10)\n                folder_name = soup.select('h4')[0].text\n                if os.path.isdir(folder_name):\n                    pass\n                else:\n                    os.mkdir(folder_name)\n                    print('文件夹'+ '$ ' + folder_name + '$' + '创建完成')\n                file_name = folder_name+'/' + folder_name + str(i) + '.jpg'\n                fp = open(file_name,'wb')\n                fp.write(pic.content)\n                fp.close()\n                i += 1\n            except:\n                pass\n        print('图片下载完成')\n    except:\n        pass\n    return urls\n\n\nurl_list = []\n#获取当前页面文章列表链接并翻页\ndef getlist(url_Start):\n    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.80 Safari/537.36'}\n    req = requests.get(url_Start)\n    req.encoding = 'gbk'\n    url_index = re.findall('\"打開新窗口\" href=\"htm_(.+?\\.html)\" target=\"_blank\">',req.text,re.S)\n    for p in url_index:\n        full_url = 'http://cl.gtta.pw/htm_' + p\n        url_list.append(full_url)\n    #判断是否要翻页\n    urls_next = re.findall('false;}\"></a><a href=\"(.*?)\">下一頁',req.text,re.S)[0]\n    url_next = 'http://cl.gtta.pw/' + urls_next\n    if url_next != url_end:\n        getlist(url_next)\n    else:\n        print('已到达末页')\n    return url_list\n\n\nlists = getlist(url_start)\nprint(len(lists))\nfor list in lists:\n    img = getIMG(list)\n    print(img)\n\n\n\n\n\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "5"}