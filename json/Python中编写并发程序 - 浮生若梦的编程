{"title": "Python中编写并发程序 - 浮生若梦的编程 ", "index": "python", "content": "GIL\n\n在Python中,由于历史原因(GIL),使得Python中多线程的效果非常不理想.GIL使得任何时刻Python只能利用一个CPU核,并且它的调度算法简单粗暴:多线程中,让每个线程运行一段时间t,然后强行挂起该线程,继而去运行其他线程,如此周而复始,直到所有线程结束.\n\n这使得无法有效利用计算机系统中的\"局部性\",频繁的线程切换也对缓存不是很友好,造成资源的浪费.\n\n据说Python官方曾经实现了一个去除GIL的Python解释器,但是其效果还不如有GIL的解释器,遂放弃.后来Python官方推出了\"利用多进程替代多线程\"的方案,在Python3中也有concurrent.futures这样的包,让我们的程序编写可以做到\"简单和性能兼得\".\n\n多进程/多线程+Queue\n\n一般来说,在Python中编写并发程序的经验是:计算密集型任务使用多进程,IO密集型任务使用多进程或者多线程.另外,因为涉及到资源共享,所以需要同步锁等一系列麻烦的步骤,代码编写不直观.另外一种好的思路是利用多进程/多线程+Queue的方法,可以避免加锁这样麻烦低效的方式.\n\n现在在Python2中利用Queue+多进程的方法来处理一个IO密集型任务.\n假设现在需要下载多个网页内容并进行解析,单进程的方式效率很低,所以使用多进程/多线程势在必行.\n我们可以先初始化一个tasks队列,里面将要存储的是一系列dest_url,同时开启4个进程向tasks中取任务然后执行,处理结果存储在一个results队列中,最后对results中的结果进行解析.最后关闭两个队列.\n\n下面是一些主要的逻辑代码.\n\npython# -*- coding:utf-8 -*-\n\n#IO密集型任务\n#多个进程同时下载多个网页\n#利用Queue+多进程\n#由于是IO密集型,所以同样可以利用threading模块\n\nimport multiprocessing\n\ndef main():\n    tasks = multiprocessing.JoinableQueue()\n    results = multiprocessing.Queue()\n    cpu_count = multiprocessing.cpu_count()  #进程数目==CPU核数目\n\n    create_process(tasks, results, cpu_count)   #主进程马上创建一系列进程,但是由于阻塞队列tasks开始为空,副进程全部被阻塞\n    add_tasks(tasks)  #开始往tasks中添加任务\n    parse(tasks, results)  #最后主进程等待其他线程处理完成结果\n\n\ndef create_process(tasks, results, cpu_count):\n    for _ in range(cpu_count):\n        p = multiprocessing.Process(target=_worker, args=(tasks, results)) #根据_worker创建对应的进程\n        p.daemon = True  #让所有进程可以随主进程结束而结束\n        p.start() #启动\n\ndef _worker(tasks, results):\n    while True:   #因为前面所有线程都设置了daemon=True,故不会无限循环\n        try:\n            task = tasks.get()   #如果tasks中没有任务,则阻塞\n            result = _download(task)\n            results.put(result)   #some exceptions do not handled\n        finally:\n            tasks.task_done()\n\ndef add_tasks(tasks):\n    for url in get_urls():  #get_urls() return a urls_list\n        tasks.put(url)\n\ndef parse(tasks, results):\n    try: \n        tasks.join()\n    except KeyboardInterrupt as err:\n        print \"Tasks has been stopped!\"\n        print err\n\n    while not results.empty():\n        _parse(results)\n\n\n\nif __name__ == '__main__':\n    main()\n\n\n\n利用Python3中的concurrent.futures包\n\n在Python3中可以利用concurrent.futures包,编写更加简单易用的多线程/多进程代码.其使用感觉和Java的concurrent框架很相似(借鉴?)\n比如下面的简单代码示例\n\npythondef handler():\n    futures = set()\n\n    with concurrent.futures.ProcessPoolExecutor(max_workers=cpu_count) as executor:\n        for task in get_task(tasks):\n            future = executor.submit(task)\n            futures.add(future)\n\ndef wait_for(futures):\n    try:\n        for future in concurrent.futures.as_completed(futures):\n            err = futures.exception()\n            if not err:\n                result = future.result()\n            else:\n                raise err\n    except KeyboardInterrupt as e:\n        for future in futures:\n            future.cancel()\n        print \"Task has been canceled!\"\n        print e\n    return result\n\n\n总结\n\n要是一些大型Python项目也这般编写,那么效率也太低了.在Python中有许多已有的框架使用,使用它们起来更加高效.\n但是自己的一些\"小打小闹\"的程序这样来编写还是不错的.:)\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "38"}