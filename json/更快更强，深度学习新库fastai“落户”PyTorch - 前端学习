{"title": "更快更强，深度学习新库fastai“落户”PyTorch - 前端学习 ", "index": "python", "content": "\n几天前，有人统计了历年ICLR论文录用者使用的深度学习框架，发现虽然TensorFlow还高居榜首，但PyTorch近一年来的使用数据已经翻了3倍，可以和TF比肩。这是个令人惊讶的消息，也让不少从业者开始正视这一发展趋势，筹备“双修”事宜。在下文中，论智给读者带来的是fast.ai发布的一个简便、好用的PyTorch库——对PyTorch感兴趣的读者不妨先从这个库开始试手。\n随着互联网和知识传播的深度结合，现在在线课程对许多人来说已经不是新鲜事物。在深度学习领域，最受学生欢迎的MOOC课程平台有三个：Fast.ai、deeplearning.ai /Coursera和Udacity。其中，因为Jeremy Howard化繁为简、实战为上的独特授课风格，Fast.ai给人的印象一直很“接地气”：\n研究如何快速、可靠地把最先进的深度学习应用于实际问题。提供Fast.ai库，它不仅是让新手快速构建深度学习实现的工具包，也是提供最佳实践的一个强大而便捷的资源。课程内容简洁易懂，以便尽可能多的人从研究成果和软件中收益。国庆期间，Fast.ai发布一个新的、面向深度学习的免费开源库——fastai。这是个PyTorch库，虽然还是预览版，但它目前已经为最重要的深度学习应用程序和数据类型提供了一致的API，且相比其他深度学习库，它在准确性和速度上有显着提高，同时所需的代码大大减少。\n感兴趣的开发者可以访问fastai的 GitHub 进行安装： github.com/fastai/fastai/\nfastai库从去年宣布开发开始，历时18个月，fastai深度学习库v1.0终于和大家见面了。在项目启动之初，开发人员就曾介绍过PyTorch作为一个平台的优势：可以利用常规python代码的灵活性和各种函数构建、训练神经网络，可以解决更广泛的问题……\n现在，经过Fast.ai团队和PyTorch团队的共同努力，我们迎来了一个为计算机视觉、文本、表格数据、时间序列、协同过滤等常见深度学习应用提供单一一致界面的深度学习库。这意味着，如果你已经学会用fastai创建实用的计算机视觉（CV）模型，那你就可以用同样的方法创建自然语言处理（NLP）模型，或是软件支持的其他模型。\n早期用户使用反馈GitHub上的语义代码搜索\nFast.ai的课程是GitHub的数据科学家和高管（包括CEO在内）提高数据素养的一个重要途径，其中，Github的高级机器学习科学家Hithl Husain在过去两年中一直通过Fast.ai学习深度学习，他认为这些MOOC课程开启了Github的数据新时代，使数据科学家们更有信心解决机器学习中的最新问题。\n作为fastai的第一批使用者，Hithl Husain和他的同事Ho-Hsiang Wu最近发布了一个实验版工具“语义代码搜索”，允许开发者直接通过意义而不是关键词匹配来查找代码， 这意味着最佳搜索结果不一定包含你搜索的单词 。在 官方博客文章 中，他们介绍了自己弃用Tensorflow Hub转而投向fastai的原因，称后者能更轻松地访问最先进的架构（如AWD LSTMs）和技术（如随机重启循环学习率）。\n\n语义代码搜索\n在过去的12个月里，Husain一直在体验预发布版本的fastai库。他表示：\n我之所以选择fast.ai，是因为它能在保证相同性能的情况下，用模块化、高级API实现最先进的技术和创新，同时减少计算量。语义代码搜索只是冰山一角，销售、营销、反欺诈，人们能用fastai为各行各业带去革命性的变化。\n生成音乐\nChristine McLeavey Payne是从上一期Fast.ai深度学习课程中脱颖而出的一名学生。她的人生经历非常丰富：从旧金山交响乐团的古典钢琴师，到金融领域的HPC专家，再到斯坦福大学的神经科学和医学研究员。现在，她已经在OpenAI开启了又一段人生旅途，而在近期的OpenAI项目中，她用fastai创建了一个能生成钢琴曲和室内音乐的LSTM——Clara。\nfastai是一个了不起的资源，即便是我这样刚接触深度学习的新手，也能用短短几行代码就得到fastai模型。我不完全知道这些先进技术背后的原理，但我的模型能运行，而且训练用时更短，性能也更好。\n她的音乐生成模型基于上课期间她构建的一个语言模型，利用fastai库对NLP最新技术的支持，她在短短两周内就完成了这个音乐生成项目，并取得了很好的初步成果。这是fastai库实用性的一个典例，只需少量修改，开发者就能把文本分类模型改成音乐生成模型，这在实践中能节省大量时间和精力。\n\nIBM Watson高级研究员对音乐生成器Clara的评价\n艺术创作\n建筑师、投资者Miguel Pérez Michaus一直在用预发布版本的fastai进行他的“Style Reversion（风格还原）”实验。所谓“风格还原”，就是把风格迁移后的图像恢复成原本的样子，如下图所示：\n\n风格还原\n他表示：“我喜欢用fastai创作，因为它能实现Keras不能实现的东西，比如生成‘不标准’的东西。”作为早期用户，他在过去12个月中目睹了fastai的更新迭代：\n我很幸运地体验了fastai的A测版本，虽然只是Alpha版，但它充分展示了自己的实用性和灵活性，而且允许我这样具有领域知识但没有正式计算机科学背景的人上手操作。fastai会变得越来越好。对于深度学习的未来，我个人有一点粗浅的认识，就是我们必须要详细掌握黑盒背后的真实技术原理，在这种情况下，我认为fastai会广受欢迎。\n学术研究\n在NLP领域，波兰语一直是一个挑战，因为它是一种形态丰富的语言，如波兰语形容词会根据名词的数和性而变化。企业家Piotr Czapla和Marcin Kardas是深度学习咨询公司n-wave的联合创始人，基于 Cutting Edge Deep Learning For Coders 这门课程中显示的思路，他们用fastai开发了一种新的波兰语文本分类算法，并在波兰顶级NLP学术竞赛中获得一等奖，有关这项新研究的论文即将发布。\n根据Czapla的说法，fastai库对他们的成功至关重要：\nfastai适合那些没有上百台服务器的普通人，这是我很喜欢它的一点。它支持快速开发和原型设计，并融入了所有最好的深度学习实践。同时，Fast.ai课程是我开始学习深度学习的指路明灯，从上课的那天起，我才开始思考深度学习能做什么。\n示例：计算机视觉领域的迁移学习Kaggle上有一个非常受欢迎的竞赛项目：Dogs vs Cats。参赛者需要编写一个算法来分类图像是包含狗还是猫。这也是Fast.ai课程中经常涉及的一个竞赛，因为它代表了一类重要问题：基于预训练模型的迁移学习。\n我们将以此为例，从 所需代码量、准确性和速度 三个指标上比较Keras和fastai的差异。以下是用fastai进行2-stage微调时的所有代码——不仅要编写的代码非常少，设置的参数也非常少：\ndata = data_from_imagefolder(Path('data/dogscats'),\nds_tfms=get_transforms(), tfms=imagenet_norm, size=224)\nlearn = ConvLearner(data, tvm.resnet34, metrics=accuracy)\nlearn.fit_one_cycle(6)\nlearn.unfreeze()\nlearn.fit_one_cycle(4, slice(1e-5,3e-4))\n下表是两个深度学习库的差异对比：\n\nKeras是现在最流行的训练神经网络的方法之一，以上数据虽然是片面的，但fastai的改进能从侧面说明Keras并不完美，它还有很大的改善空间。而无论是Keras还是其他深度学习库，要完成同样的任务，它们所需的代码量都远远超过fastai，相应的，它们的训练时间会更长，且模型性能不一定会更好。\n此外，fastai在NLP任务上也有强劲表现。下表是ULMFiT论文中的一幅截图，显示了文本分类算法ULMFiT与IMDb数据集中排名靠前的算法的相对误差：\n\n文本分类性能总结\nfastai是目前唯一提供此算法的库，由于该算法是内置的，你可以直接参考上面的Dogs vs Cats代码复现论文结果。以下是训练ULMFiT语言模型的方法：\ndata = data_from_textcsv(LM_PATH, Tokenizer(), data_func=lm_data)\nlearn = RNNLearner.language_model(data, drop_mult=0.3,\npretrained_fnames=['lstm_wt103', 'itos_wt103'])\nlearn.freeze()\nlearn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))\nlearn.unfreeze()\nlearn.fit_one_cycle(10, 1e-3, moms=(0.8,0.7), pct_start=0.25)\n来源： www.fast.ai/2018/10/02/fastai-ai/\n编译：Bot\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "2"}