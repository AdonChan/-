{"title": "机器学习实战，使用朴素贝叶斯来做情感分析 - 肥兔子de碎碎语 ", "index": "机器学习,python", "content": "前言\n前段时间更新了一系列基础的机器学习算法，感觉有些无味，而且恰好那时买了了国内某公司的云服务器，就打算部署一套文本处理的WEB API，顺别应用一下之前学习到的机器学习算法。(文末放出地址)\n本文不会涉及过于复杂的数学原理，主要讲述如何提取特征与调用模型。\n实现了的api\n\n分词  访问/cut?sentence=&method=，其中sentence参数指明需要分词的句子，method参数指明分词的方式。\n统计词频访问/count?sentence=&=method=,其中的参数说明和分词的api一致。\n情感分析访问/count?sentence=，因为目前是针对句子的，因此sentence的范围限制了在1~200词之间。\n\n本文重点讲述情感分析的实现。\n情感分析的实现\n情感分析的应用是多种多样的，往大了说，可以用于国家对某个热点进行舆情监控，选举的选情分析，电商对产品的售后意向调查，往小了说还可以写一个脚本对你女神的微博进行关心,在女神不开心的时候及时送上关心。\n至于情感分析的实现，在学术论文上均有提及，大致过程都可以分为提取情感极性词,将语句转化为向量,扔进你训练好的模型里\n在这里我们针对实际情况做出一定的修改，提取情感极性词，必然是需要词典来参考的，虽然各大语言机构都有公开的词典，，但并不建议使用这些词典。其中一个原因是因为这些公开的词典太过书面化，和“礼貌”，不太接近日常生活。因此这这里我采用的是自定义词典，另外一点是，在提取关键词部分不仅仅是提取情感相关的词语，同时也使用其他日常用语，比如草泥马,虽然不像开心,伤心这样的情感极性词，但草泥马显然具有明确的情感偏向。\n值得说明的是，我们既然在提取特征词的时候考虑到平常的词语，同样也就意味这一些乱七八糟的词语会混进来，所以我们在提取完特征词后，同时计算它们的词频，只取频率排名的前20%，(当然这个数字可以根据自己需要调整)。\n挑选的出来的特征词就构成了[word1,word2,word3……]，同时检测训练样本，若样本中出现了特征词，则该样本的特征向量对应位置置1，否则为0。\n构成特征向量后，我选取的算法是朴素贝叶斯，关于其原理，可以查看我支持的专栏机器学习从入门到放弃之朴素贝叶斯。至于为什么选取朴素贝叶斯，很大一个原因是因为朴素贝叶斯在垃圾邮件分类上有不错的效果，而确定一个句子属于那种情感，和判断一封邮件是否为垃圾邮件有异曲同工之妙。\n在sklearn中，只要添加如下代码即可。\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb = gnb.fit(feature, label)\nresult = gnb.predice(test)\n上述api中涉及的分词操作均是使用 结巴分词 完成。\n测试效果\n下面测试用例又黄又暴力，未成年观众在家长的陪同下观看。\n\n\n\n开源代码\ndudulu\nWEB API测试页面\n目前只有情感分析的api的测试页面\ndudulu 不得不说使用了必应的背景图来做自己网站的背景图效果相当不错。\n后话\n希望大家多多调戏（目前语料只支持中文），后端的日志会记录下测试记录（算是收集数据），我会周期性上去更新模型，效果理论上会越变越好。\n\n                ", "mainLikeNum": ["3 "], "mainBookmarkNum": "15"}