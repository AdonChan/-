{"title": "【数据科学系统学习】机器学习算法 # 西瓜书学习记录 [3] Logistic 回归实践 - 个人文章 ", "index": "逻辑回归,python,机器学习", "content": "本篇内容为《机器学习实战》第 5 章 Logistic 回归程序清单。\n书中所用代码为 python2，下面给出的程序清单是在 python3 中实践改过的代码，希望对你有帮助。\n\n训练算法：使用梯度上升找到最佳参数\n梯度上升法的伪代码如下：\n每个回归系数初始化为 1 重复 R 次：————计算整个数据集的梯度————使用 alpha $\\times$ gradient 更新回归系数的向量返回回归系数\n程序清单 5-1：Logisitc 回归梯度上升优化算法\n# 打开文本文件 testSet.txt 并逐行读取\ndef loadDataSet():\n    dataMat = []; lableMat = []\n    fr = open('testSet.txt')\n    for line in fr.readlines():\n        lineArr = line.strip().split()\n        # 为计算方便，将 X0 的值设为 1.0\n        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\n        lableMat.append(int(lineArr[2]))\n    return dataMat, lableMat\n\ndef sigmoid(intX):\n    return 1.0/(1+exp(-intX))\n\n# 参数 dataMatIn 是一个 2 维 numpy 数组，存放的是一个 100*3 的矩阵\n# 每列分别代表每个不同的特征，每行代表每个训练样本\ndef gradAscent(dataMatIn, classLabels):\n    # 转换为 numpy 矩阵的数据类型\n    dataMatrix = mat(dataMatIn)\n    # classLabels 类别标签\n    labelMat = mat(classLabels).transpose()\n    # 得到矩阵大小\n    m,n = shape(dataMatrix)\n\n    # 设置梯度上升法所需参数\n    # alpha 步长，maxCycles 迭代次数\n    alpha = 0.001\n    maxCycles = 500\n    weights = ones((n,1))\n\n    for k in range(maxCycles):\n        h = sigmoid(dataMatrix * weights)\n        # 计算真实类别与预测类的差值，然后按照该差值的方向调整回归系数\n        error = (labelMat - h)\n        weights = weights + alpha * dataMatrix.transpose() * error\n    # 返回训练好的回归系数\n    return weights\n在 python 提示符下，执行代码并得到结果：\n>>> import logRegres\n>>> dataArr, labelMat=logRegres.loadDataSet()\n>>> logRegres.gradAscent(dataArr, labelMat)\nmatrix([[ 4.12414349],\n        [ 0.48007329],\n        [-0.6168482 ]])\n\n分析数据：画出决策边界\n程序清单 5-2：画出数据集和 Logistic 回归最佳拟合直线的函数\n# 画出数据集和logistic回归最佳拟合直线的函数\ndef plotBestFit(weights):\n    import matplotlib.pyplot as plt\n    dataMat, labelMat = loadDataSet()\n    dataArr = array(dataMat)\n    n = shape(dataArr)[0]\n    xcord1 = []\n    ycord1 = []\n\n    xcord2 = []\n    ycord2 = []\n\n    for i in range(n):\n        if int(labelMat[i]) == 1:\n            xcord1.append(dataArr[i,1])\n            ycord1.append(dataArr[i,2])\n        else:\n            xcord2.append(dataArr[i,1])\n            ycord2.append(dataArr[i,2])\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    # 形状参数 marker 's'：正方形，参数 s：点的大小\n    ax.scatter(xcord1, ycord1, s=30, c='red', marker='s')\n    ax.scatter(xcord2, ycord2, s=30, c='green')\n\n    # arange(start, end, step)，返回一个array对象\n    x = arange(-3.0, 3.0, 0.1)\n    # 设置 sigmoid 函数为0\n    y = (-weights[0]-weights[1]*x)/weights[2]\n    ax.plot(x, y)\n    plt.xlabel('X1')\n    plt.ylabel('X2')\n    plt.show()\n在 python 提示符下，执行代码并得到结果：\n>>> import importlib\n>>> importlib.reload(logRegres)\n<module 'logRegres' from '/Users/Desktop/Coding/logRegres.py'>\n>>> weights = logRegres.gradAscent(dataArr, labelMat)\n>>> logRegres.plotBestFit(weights.getA())\n\n\n训练算法：随机梯度上升\n随机梯度上升算法的伪代码如下：所有回归系数初始化为 1对数据集中每个样本————计算该样本的梯度————使用 alpha $\\times$ gradient 更新回归系数值返回回归系数值\n程序清单 5-3：随机梯度上升算法\n# 随机梯度上升算法\ndef stocGradAscent0(dataMatrix, classLabels):\n    m,n = shape(dataMatrix)\n    alpha = 0.01\n    weights = ones(n)\n    for i in range(m):\n        # 变量 h 和误差 error 都是向量\n        h = sigmoid(sum(dataMatrix[i]*weights))\n        error = classLabels[i] - h\n        weights = weights + alpha * error * dataMatrix[i]\n    return weights\n在 python 提示符下，执行代码并得到结果：\n>>> from numpy import *\n>>> importlib.reload(logRegres)\n<module 'logRegres' from '/Users/Desktop/Coding/logRegres.py'>\n>>> dataArr, labelMat=logRegres.loadDataSet()\n>>> weights = logRegres.stocGradAscent0(array(dataArr), labelMat)\n>>> logRegres.plotBestFit(weights)\n\n\n程序清单 5-4：改进的随机梯度上升算法\n# 改进的随机梯度上升算法\ndef stocGradAscent1(dataMatrix, classLabels, numIter=150):\n    m,n = shape(dataMatrix)\n    weights = ones(n)\n    for j in range(numIter):\n        dataIndex = list(range(m))\n        for i in range(m):\n            alpha = 4/(1.0+j+i)+0.01\n            # uniform() 方法将随机生成下一个实数\n            randIndex = int(random.uniform(0, len(dataIndex)))\n            h = sigmoid(sum(dataMatrix[randIndex]*weights))\n            error = classLabels[randIndex] - h\n            weights = weights + alpha * error * dataMatrix[randIndex]\n            del(dataIndex[randIndex])\n    return weights\n在 python 提示符下，执行代码并得到结果：\n>>> importlib.reload(logRegres)\n<module 'logRegres' from '/Users/Desktop/Coding/logRegres.py'>\n>>> dataArr, labelMat=logRegres.loadDataSet()\n>>> weights = logRegres.stocGradAscent1(array(dataArr), labelMat)\n>>> logRegres.plotBestFit(weights)\n\n\n示例：从疝气病症预测病马的死亡率\n使用 Logistic 回归估计马疝病的死亡率\n\n收集数据：给定数据文件。\n准备数据：用 Python 解析文本文件并填充缺失值。\n分析数据：可视化并观察数据。\n训练算法：使用优化算法，找到最佳的系数。\n测试算法：为了量化回归的效果，需要观察错误率。根据错误率决定是否回退到训练阶段，通过改变迭代的次数和步长等参数来得到更好的回归系数。\n使用算法：实现一个简单的命令行程序来收集马的症状并输出预测结果。\n\n使用 Logistic 回归方法进行分类所需做的是把测试集上每个特征向量乘以最优化方法得来的回归系数，再将该乘积结果求和，最后输入到 sigmoid 函数即可。如果对应的 sigmoid 值大于 0.5 就预测类别标签为 1，否则为 0。\n程序清单 5-5：Logistic 回归分类函数\n# 以回归系数和特征向量作为输入来计算对应的 sigmoid 值\ndef classifyVector(inX, weights):\n    prob = sigmoid(sum(inX*weights))\n    if prob > 0.5: return 1.0\n    else: return 0.0\n\n# 打开测试集和训练集，并对数据进行格式化处理\ndef colicTest():\n    frTrain = open('horseColicTraining.txt')\n    frTest = open('horseColicTest.txt')\n    trainingSet = []\n    trainingLabels = []\n\n    for line in frTrain.readlines():\n        currLine = line.strip().split('\\t')\n        lineArr = []\n\n        for i in range(21):\n            lineArr.append(float(currLine[i]))\n        trainingSet.append(lineArr)\n        trainingLabels.append(float(currLine[21]))\n    \n    # 计算回归系数向量\n    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 500)\n    errorCount = 0\n    numTestVec = 0.0\n    \n    # 导入测试集并计算分类错误率\n    for line in frTest.readlines():\n        numTestVec += 1.0\n        currLine = line.strip().split('\\t')\n        lineArr = []\n\n        for i in range(21):\n            lineArr.append(float(currLine[i]))\n        if int(classifyVector(array(lineArr), trainWeights)) !=int(currLine[21]):\n            errorCount += 1\n\n    errorRate = (float(errorCount) / numTestVec)\n    \n    print('这个测试集的错误率是：%f' % errorRate)\n    return errorRate\n\ndef multiTest():\n    numTests = 10\n    errorSum = 0.0\n    \n    for k in range(numTests):\n        errorSum += colicTest()\n        \n    print('经过 %d 次迭代后平均错误率是：%f' % (numTests, errorSum/float(numTests)))\n\n在 python 提示符下，执行代码并得到结果：\n>>> importlib.reload(logRegres)\n<module 'logRegres' from '/Users/Desktop/Coding/logRegres.py'>\n>>> logRegres.multiTest()\n这个测试集的错误率是：0.358209\n这个测试集的错误率是：0.373134\n这个测试集的错误率是：0.253731\n这个测试集的错误率是：0.402985\n这个测试集的错误率是：0.358209\n这个测试集的错误率是：0.298507\n这个测试集的错误率是：0.343284\n这个测试集的错误率是：0.298507\n这个测试集的错误率是：0.402985\n这个测试集的错误率是：0.417910\n经过 10 次迭代后平均错误率是：0.350746\n\nLogistic 回归的目的是寻找一个非线性函数 sigmoid 的最佳拟合参数，求解过程可以由最优化算法来完成。\n\n不足之处，欢迎指正。\n$$$$\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "1"}