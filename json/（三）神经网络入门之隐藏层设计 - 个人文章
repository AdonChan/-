{"title": "（三）神经网络入门之隐藏层设计 - 个人文章 ", "index": "python", "content": "作者：chen_h微信号 & QQ：862251340微信公众号：coderpai简书地址：https://www.jianshu.com/p/8e1...\n\n这篇教程是翻译Peter Roelants写的神经网络教程，作者已经授权翻译，这是原文。\n该教程将介绍如何入门神经网络，一共包含五部分。你可以在以下链接找到完整内容。\n\n（一）神经网络入门之线性回归\nLogistic分类函数\n（二）神经网络入门之Logistic回归（分类问题）\n（三）神经网络入门之隐藏层设计\nSoftmax分类函数\n（四）神经网络入门之矢量化\n（五）神经网络入门之构建多层网络\n\n隐藏层\n\n这部分教程将介绍三部分：\n\n隐藏层设计\n非线性激活函数\nBP算法\n\n在前面几个教程中，我们已经介绍了一些很简单的教程，就是单一的回归模型或者分类模型。在这个教程中，我们也将设计一个二分类神经网络模型，其中输入数据是一个维度，隐藏层只有一个神经元，并且使用非线性函数作为激活函数，模型结构能用图表示为：\n\n我们先导入教程需要使用的软件包。\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import colorConverter, ListedColormap \nfrom mpl_toolkits.mplot3d import Axes3D \nfrom matplotlib import cm\n定义数据集\n在这篇教程中，我们将输入数据x分类成两个类别，用蓝色表示t = 1，用红色表示t = 0。其中，红色分类样本是一个多峰分布，被蓝色分类样本包围。这些数据都是一维的，但是数据之间的间隔并不是线性的分割。这些数据特性将在下图中表示出来。\n这个二分类模型不会完全准确的分类处理啊，因为我们在其中加入了一个神经元，并且采用的是非线性函数。\n# Define and generate the samples\nnb_of_samples_per_class = 20  # The number of sample in each class\nblue_mean = [0]  # The mean of the blue class\nred_left_mean = [-2]  # The mean of the red class\nred_right_mean = [2]  # The mean of the red class\n\nstd_dev = 0.5  # standard deviation of both classes\n# Generate samples from both classes\nx_blue = np.random.randn(nb_of_samples_per_class, 1) * std_dev + blue_mean\nx_red_left = np.random.randn(nb_of_samples_per_class/2, 1) * std_dev + red_left_mean\nx_red_right = np.random.randn(nb_of_samples_per_class/2, 1) * std_dev + red_right_mean\n\n# Merge samples in set of input variables x, and corresponding set of\n# output variables t\nx = np.vstack((x_blue, x_red_left, x_red_right))\nt = np.vstack((np.ones((x_blue.shape[0],1)), \n               np.zeros((x_red_left.shape[0],1)), \n               np.zeros((x_red_right.shape[0], 1))))\n# Plot samples from both classes as lines on a 1D space\nplt.figure(figsize=(8,0.5))\nplt.xlim(-3,3)\nplt.ylim(-1,1)\n# Plot samples\nplt.plot(x_blue, np.zeros_like(x_blue), 'b|', ms = 30) \nplt.plot(x_red_left, np.zeros_like(x_red_left), 'r|', ms = 30) \nplt.plot(x_red_right, np.zeros_like(x_red_right), 'r|', ms = 30) \nplt.gca().axes.get_yaxis().set_visible(False)\nplt.title('Input samples from the blue and red class')\nplt.xlabel('$x$', fontsize=15)\nplt.show()\n\n非线性激活函数\n在这里，我们使用的非线性转换函数是Gaussian radial basis function (RBF)。除了径向基函数网络，RBF函数在神经网络中不经常被作为激活函数。比较常见的激活函数是sigmoid函数。但我们根据设计的输入数据x，在这里RBF函数能很好地将蓝色样本数据从红色样本数据中分类出来，下图画出了RBF函数的图像。RBF函数给定义为：\n\nRBF函数的导数为定义为：\n\n# Define the rbf function\ndef rbf(z):\n    return np.exp(-z**2)\n# Plot the rbf function\nz = np.linspace(-6,6,100)\nplt.plot(z, rbf(z), 'b-')\nplt.xlabel('$z$', fontsize=15)\nplt.ylabel('$e^{-z^2}$', fontsize=15)\nplt.title('RBF function')\nplt.grid()\nplt.show()\n\nBP算法\n在训练模型的时候，我们使用BP算法来进行模型优化，这是一种很典型的优化算法。BP算法的每次迭代分为两步：\n\n正向传播去计算神经网络的输出。\n利用神经网络得出的结果和真实结果之间的误差进行反向传播来更新神经网络的参数。\n\n1. 正向传播\n在计算正向传播中，输入数据被一层一层的计算，最后从模型中得出输出结果。\n计算隐藏层的激活函数\n隐藏层h经激活函数之后，输出结果为：\n\n其中，wh是权重参数。hidden_activations(x, wh)函数实现了该功能。\n计算输出结果的激活函数\n神经网络的最后一层的输出，是将隐藏层的输出h作为数据参数，并且利用Logistic函数来作为激活函数。\n\n其中，w0是输出层的权重，output_activations(h, w0)函数实现了该功能。我们在公式中添加了一个偏差项-1，因为如果不添加偏差项，那么Logistic函数只能学到一个经过原点的分类面。因为，隐藏层中的RBF函数的输入值得范围是从零到正无穷，那么如果我们不在输出层加上偏差项的话，模型不可能学出有用的分类结果，因为没有样本的值将小于0，从而归为决策树的左边。因此，我们增加了一个截距，即偏差项。正常情况下，偏差项也和权重参数一样，需要被训练，但是由于这个例子中的模型非常简单，所以我们就用一个常数来作为偏差项。\n# Define the logistic function\ndef logistic(z): \n    return 1 / (1 + np.exp(-z))\n\n# Function to compute the hidden activations\ndef hidden_activations(x, wh):\n    return rbf(x * wh)\n\n# Define output layer feedforward\ndef output_activations(h , wo):\n    return logistic(h * wo - 1)\n\n# Define the neural network function\ndef nn(x, wh, wo): \n    return output_activations(hidden_activations(x, wh), wo)\n\n# Define the neural network prediction function that only returns\n#  1 or 0 depending on the predicted class\ndef nn_predict(x, wh, wo): \n    return np.around(nn(x, wh, wo))\n2. 反向传播\n在反向传播过程中，我们需要先计算出神经网络的输出与真实值之间的误差。这个误差会一层一层的反向传播去更新神经网络中的各个权重。\n在每一层中，使用梯度下降算法按照负梯度方向对每个参数进行更新。\n参数wh和wo利用w(k+1)=w(k)−Δw(k+1)更新，其中Δw=μ∗∂ξ/∂w，μ是学习率，∂ξ/∂w是损失函数ξ对参数w的梯度。\n计算损失函数\n在这个模型中，损失函数ξ与交叉熵损失函数一样，具体解释在这里：\n\n损失函数对于参数wh和wo的表示如下图所示。从图中，我们发现误差面不是一个凸函数，而且沿着wh = 0这一轴，参数wh将是损失函数的一个映射。\n从图中发现，沿着wh = 0，从wo > 0开始，损失函数有一个非常陡峭的梯度，并且我们要按照图形的下边缘进行梯度下降。如果学习率取得过大，那么在梯度更新的时候，可能跳过最小值，从一边的梯度方向跳到另一边的梯度方向。因为梯度的方向太陡峭了，每次对参数的更新跨度将会非常大。因此，在开始的时候我们需要将学习率取一个比较小的值。\n# Define the cost function\ndef cost(y, t):\n    return - np.sum(np.multiply(t, np.log(y)) + np.multiply((1-t), np.log(1-y)))\n\n# Define a function to calculate the cost for a given set of parameters\ndef cost_for_param(x, wh, wo, t):\n    return cost(nn(x, wh, wo) , t)\n# Plot the cost in function of the weights\n# Define a vector of weights for which we want to plot the cost\nnb_of_ws = 200 # compute the cost nb_of_ws times in each dimension\nwsh = np.linspace(-10, 10, num=nb_of_ws) # hidden weights\nwso = np.linspace(-10, 10, num=nb_of_ws) # output weights\nws_x, ws_y = np.meshgrid(wsh, wso) # generate grid\ncost_ws = np.zeros((nb_of_ws, nb_of_ws)) # initialize cost matrix\n# Fill the cost matrix for each combination of weights\nfor i in range(nb_of_ws):\n    for j in range(nb_of_ws):\n        cost_ws[i,j] = cost(nn(x, ws_x[i,j], ws_y[i,j]) , t)\n# Plot the cost function surface\nfig = plt.figure()\nax = Axes3D(fig)\n# plot the surface\nsurf = ax.plot_surface(ws_x, ws_y, cost_ws, linewidth=0, cmap=cm.pink)\nax.view_init(elev=60, azim=-30)\ncbar = fig.colorbar(surf)\nax.set_xlabel('$w_h$', fontsize=15)\nax.set_ylabel('$w_o$', fontsize=15)\nax.set_zlabel('$\\\\xi$', fontsize=15)\ncbar.ax.set_ylabel('$\\\\xi$', fontsize=15)\nplt.title('Cost function surface')\nplt.grid()\nplt.show()\n\n输出层更新\n∂ξi/∂wo是每个样本i的输出梯度，参照第二部分教程的方法，我们可以得出相应的推导公式：\n\n其中，zoi=hi∗wo，hi是样本i经过激活函数之后输出的值，∂ξi/∂zoi=δoi是输出层误差的求导。\ngradient_output(y, t)函数实现了δo，gradient_weight_out(h, grad_output)函数实现了∂ξ/∂wo。\n隐藏层更新\n∂ξi/∂wh是每个样本i在影藏层的梯度，具体计算如下：\n\n其中，\n∂ξi/∂zhi=δhi表示误差对于隐藏层输入的梯度。这个误差也可以解释为，zhi对于最后误差的贡献。那么，接下来我们定义一下这个误差梯度δhi：\n\n又应为∂zhi/∂wh=xi，那么我们能计算最后的值为：\n\n在批处理中，对每个对应参数的梯度进行累加，就是最后的梯度。\ngradient_hidden(wo, grad_output)函数实现了δh。gradient_weight_hidden(x, zh, h, grad_hidden)函数实现了∂ξ/∂wh。backprop_update(x, t, wh, wo, learning_rate)函数实现了BP算法的每次迭代过程。\n# Define the error function\ndef gradient_output(y, t):\n    return y - t\n\n# Define the gradient function for the weight parameter at the output layer\ndef gradient_weight_out(h, grad_output): \n    return  h * grad_output\n\n# Define the gradient function for the hidden layer\ndef gradient_hidden(wo, grad_output):\n    return wo * grad_output\n\n# Define the gradient function for the weight parameter at the hidden layer\ndef gradient_weight_hidden(x, zh, h, grad_hidden):\n    return x * -2 * zh * h * grad_hidden\n\n# Define the update function to update the network parameters over 1 iteration\ndef backprop_update(x, t, wh, wo, learning_rate):\n    # Compute the output of the network\n    # This can be done with y = nn(x, wh, wo), but we need the intermediate \n    #  h and zh for the weight updates.\n    zh = x * wh\n    h = rbf(zh)  # hidden_activations(x, wh)\n    y = output_activations(h, wo)\n    # Compute the gradient at the output\n    grad_output = gradient_output(y, t)\n    # Get the delta for wo\n    d_wo = learning_rate * gradient_weight_out(h, grad_output)\n    # Compute the gradient at the hidden layer\n    grad_hidden = gradient_hidden(wo, grad_output)\n    # Get the delta for wh\n    d_wh = learning_rate * gradient_weight_hidden(x, zh, h, grad_hidden)\n    # return the update parameters\n    return (wh-d_wh.sum(), wo-d_wo.sum())\nBP算法更新\n下面的代码，我们模拟了一个50次的循环。白色的点表示，参数wh和wo在误差面上面的第k次迭代。\n在更新过程中，我们不断的线性减小学习率。这是为了在更新到最后的时候，学习率能是0。这样能保证最后的参数更新不会在最小值附近徘徊。\n# Run backpropagation\n# Set the initial weight parameter\nwh = 2\nwo = -5\n# Set the learning rate\nlearning_rate = 0.2\n\n# Start the gradient descent updates and plot the iterations\nnb_of_iterations = 50  # number of gradient descent updates\nlr_update = learning_rate / nb_of_iterations # learning rate update rule\nw_cost_iter = [(wh, wo, cost_for_param(x, wh, wo, t))]  # List to store the weight values over the iterations\nfor i in range(nb_of_iterations):\n    learning_rate -= lr_update # decrease the learning rate\n    # Update the weights via backpropagation\n    wh, wo = backprop_update(x, t, wh, wo, learning_rate) \n    w_cost_iter.append((wh, wo, cost_for_param(x, wh, wo, t)))  # Store the values for plotting\n\n# Print the final cost\nprint('final cost is {:.2f} for weights wh: {:.2f} and wo: {:.2f}'.format(cost_for_param(x, wh, wo, t), wh, wo))\n在我们的机器上面，最后输出的结果是：final cost is 10.81 for weights wh: 1.20 and wo: 5.56\n但由于参数初始化的不同，可能在你的机器上面运行会有不同的结果。\n# Plot the weight updates on the error surface\n# Plot the error surface\nfig = plt.figure()\nax = Axes3D(fig)\nsurf = ax.plot_surface(ws_x, ws_y, cost_ws, linewidth=0, cmap=cm.pink)\nax.view_init(elev=60, azim=-30)\ncbar = fig.colorbar(surf)\ncbar.ax.set_ylabel('$\\\\xi$', fontsize=15)\n\n# Plot the updates\nfor i in range(1, len(w_cost_iter)):\n    wh1, wo1, c1 = w_cost_iter[i-1]\n    wh2, wo2, c2 = w_cost_iter[i]\n    # Plot the weight-cost value and the line that represents the update \n    ax.plot([wh1], [wo1], [c1], 'w+')  # Plot the weight cost value\n    ax.plot([wh1, wh2], [wo1, wo2], [c1, c2], 'w-')\n# Plot the last weights\nwh1, wo1, c1 = w_cost_iter[len(w_cost_iter)-1]\nax.plot([wh1], [wo1], c1, 'w+')\n# Shoz figure\nax.set_xlabel('$w_h$', fontsize=15)\nax.set_ylabel('$w_o$', fontsize=15)\nax.set_zlabel('$\\\\xi$', fontsize=15)\nplt.title('Gradient descent updates on cost surface')\nplt.grid()\nplt.show()\n\n分类结果的可视化\n下面的代码可视化了最后的分类结果。在输入空间域里面，蓝色和红色代表了最后的分类颜色。从图中，我们发现所有的样本都被正确分类了。\n# Plot the resulting decision boundary\n# Generate a grid over the input space to plot the color of the\n#  classification at that grid point\nnb_of_xs = 100\nxs = np.linspace(-3, 3, num=nb_of_xs)\nys = np.linspace(-1, 1, num=nb_of_xs)\nxx, yy = np.meshgrid(xs, ys) # create the grid\n# Initialize and fill the classification plane\nclassification_plane = np.zeros((nb_of_xs, nb_of_xs))\nfor i in range(nb_of_xs):\n    for j in range(nb_of_xs):\n        classification_plane[i,j] = nn_predict(xx[i,j], wh, wo)\n# Create a color map to show the classification colors of each grid point\ncmap = ListedColormap([\n        colorConverter.to_rgba('r', alpha=0.25),\n        colorConverter.to_rgba('b', alpha=0.25)])\n\n# Plot the classification plane with decision boundary and input samples\nplt.figure(figsize=(8,0.5))\nplt.contourf(xx, yy, classification_plane, cmap=cmap)\nplt.xlim(-3,3)\nplt.ylim(-1,1)\n# Plot samples from both classes as lines on a 1D space\nplt.plot(x_blue, np.zeros_like(x_blue), 'b|', ms = 30) \nplt.plot(x_red_left, np.zeros_like(x_red_left), 'r|', ms = 30) \nplt.plot(x_red_right, np.zeros_like(x_red_right), 'r|', ms = 30) \nplt.gca().axes.get_yaxis().set_visible(False)\nplt.title('Input samples and their classification')\nplt.xlabel('x')\nplt.show()\n\n输入域的转换\n为什么神经网络模型能利用最后的线性Logistic实现非线性的分类呢？关键原因是隐藏层的非线性RBF函数。RBF转换函数可以将靠近原点的样本（蓝色分类）的输出值大于0，而远离原点的样本（红色样本）的输出值接近0。如下图所示，红色样本的位置都在左边接近0的位置，蓝色样本的位置在远离0的位置。这个结果就是使用线性Logistic分类的。\n同时注意，我们使用的高斯函数的峰值偏移量是0，也就是说，高斯函数产生的值是一个关于原点分布的数据。\n# Plot projected samples from both classes as lines on a 1D space\nplt.figure(figsize=(8,0.5))\nplt.xlim(-0.01,1)\nplt.ylim(-1,1)\n# Plot projected samples\nplt.plot(hidden_activations(x_blue, wh), np.zeros_like(x_blue), 'b|', ms = 30) \nplt.plot(hidden_activations(x_red_left, wh), np.zeros_like(x_red_left), 'r|', ms = 30) \nplt.plot(hidden_activations(x_red_right, wh), np.zeros_like(x_red_right), 'r|', ms = 30) \nplt.gca().axes.get_yaxis().set_visible(False)\nplt.title('Projection of the input samples by the hidden layer.')\nplt.xlabel('h')\nplt.show()\n\n完整代码，点击这里\n\n作者：chen_h微信号 & QQ：862251340简书地址：https://www.jianshu.com/p/8e1...\nCoderPai 是一个专注于算法实战的平台，从基础的算法到人工智能算法都有设计。如果你对算法实战感兴趣，请快快关注我们吧。加入AI实战微信群，AI实战QQ群，ACM算法微信群，ACM算法QQ群。长按或者扫描如下二维码，关注 “CoderPai” 微信号（coderpai）\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}