{"title": "tensorboard简介（含代码） - 个人文章 ", "index": "python", "content": "一.tensorboard简介\ntensorboard是tensorflow自带的一个强大的可视化工具，也是一个web应用套件支持七种可视化包括 SCALARS(标量)、IMAGES(图像)、AUDIO(音频)、GRAPHS(数据流图)、DISTRIBUTIONS(训练数据分布图)、HISOGRAMS(训练过程中数据的柱状图)和EMBEDDINGS(展示词向量的投影分布)\n一.基础知识\nTensorflow的计算表现为数据流图，因此tf.Graph类中包含了一系列表示计算的操作对象（tf.Operation）,以及操作之间的流动的数据(tf.Tensor)。tf.Operation代表图中的一个节点，用于计算张量数据可以由节点构造器（tf.add）与tf.Graph.creat_op()产生。tf.Tensor是操作输出的符号句柄，不包含输出的的值，而是提供tf.Session来计算这些值的方法，这样就能构建一个数据流连接。例如c = tf.matmul(a, b)表示创建了一个类型为MatMul的Operation，该Operation接收Tensor a和Tensor b作为输入，而产生Tensor c作为输出。在可视化的时候还需要给必要的节点添加摘要（summary），摘要会收集节点的数据，并且标记上第几步时间戳等写入事件文件(event file),tf.sunmmary.FileWriter类用于在目录中创建事件文件，并且向摘要中添加摘要和事件。如train_writer.add_summary(summary,i)。\n二.mnist小程序分析\nimport tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_dataimport numpy as npclass tensorboard(object):\n#用来生成核函数\ndef weight_variable(self,shape):\n    weight=tf.truncated_normal(shape=shape,stddev=0.1)\n    return tf.Variable(weight)\n#用来生成偏置项的分布\ndef bias_variable(self,shape):\n    bias=tf.constant(0.0,dtype=tf.float32,shape=shape)\n    return tf.Variable(bias)\n    \n\n初始化某个结构层，其中我们使用tf.name_scope来创建作用域，它和tf.variable_scope的区别在于tf.name_scope只能给op_name加前缀，tf.variable_scope不仅仅可以给op_name还能给variable_name加前缀同时简要说明tf.Variable和tf.get_Variable的区别，简而言之就是tf.Variable每次都直接新建一个变量，但是tf.get_Variable先检查，如果有的话就直接返回该变量对象，否则创建对象。是否能在该作用域内创建新对象由tf.get_Variable决定。再使用variable_summaries函数来创建该层的一些信息\n\ndef nn_layer(self,input_tensor,input_dim,output_dim,layer_name,act=tf.nn.relu):\n    with tf.name_scope(layer_name):\n        with tf.name_scope('weight'):\n            weights=self.weight_variable([input_dim,output_dim])\n            self.variable_summaries(weights)\n        with tf.name_scope('bias'):\n            bias=self.weight_variable([output_dim])\n            self.variable_summaries(bias)\n        with tf.name_scope('Wx_plus_b'):\n            preactivate=tf.nn.bias_add(tf.matmul(input_tensor,weights),bias)\n            #tf.summary是给必要的节点添加摘要，摘要会收集该节点的数据，在这个位置收集数据并              #且使用histogram绘制出来，效果如下图\n            tf.summary.histogram('preactivate',preactivate)\n        activations=act(preactivate,name='activation')\n        tf.summary.histogram('activation',activations)\n        return activations\n\n\n接下来说明我们摘要应该包含的数据，即标量由均值、方差、最小值、最大值，同时绘制输入的张量数据的图分布\ndef variable_summaries(self,var):\n    with tf.name_scope('sunmmaries'):\n        mean=tf.reduce_mean(var)\n        tf.summary.scalar('mean',mean)\n        with tf.name_scope('stddev'):\n            stddev=tf.sqrt(tf.reduce_mean(tf.square(var-mean)))\n        tf.summary.scalar('stddev',stddev)\n        tf.summary.scalar('max',tf.reduce_max(var))\n        tf.summary.scalar('min',tf.reduce_min(var))\n        tf.summary.histogram('histogram',var)\n        \n\n\n效果如下图，虽说训练存在问题，但是目标仅仅是为了画图\n\n接下来是初始化函数，不做多讲\ndef __init__(self,max_steps=500,learning_rate=0.01,drop_out=0.9,batch_size=256):\n    self.max_steps=max_steps\n    self.learning_rate=learning_rate\n    self.drop_out=drop_out\n    self.log_dir='mnist_with_summaries'\n    self.batch_size=batch_size\n    self.mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True,source_url='http://yann.lecun.com/exdb/mnist/')\n    \n\n\n\n以下定义了结构信息，定义了图中的节点\ndef construct(self):\n    with tf.name_scope('input'):\n        self.x=tf.placeholder(dtype=tf.float32,shape=[self.batch_size,784],name='X_input')\n        self.y_=tf.placeholder(dtype=tf.float32,shape=[self.batch_size,10],name='y_input')\n    with tf.name_scope('input_shape'):\n        image_shaped_input=tf.reshape(self.x,shape=[self.batch_size,28,28,1])\n        tf.summary.image('input',image_shaped_input,10)\n    hidden1=self.nn_layer(self.x,784,500,'layer1')\n    with tf.name_scope('drop_out'):\n        self.keep_prob=tf.placeholder(dtype=tf.float32)\n        tf.summary.scalar('dropout_keep_prob',self.keep_prob)\n        dropped=tf.nn.dropout(hidden1,keep_prob=self.keep_prob)\n    y=self.nn_layer(dropped,500,10,'layer2',act=tf.identity)\n    with tf.name_scope('cross_entropy'):\n        diff=tf.nn.softmax_cross_entropy_with_logits(logits=y,labels=self.y_)\n        with tf.name_scope('total'):\n            cross_entropy=tf.reduce_mean(diff)\n    tf.summary.scalar('cross_entropy',cross_entropy)\n    with tf.name_scope('train'):\n        self.train_step=tf.train.AdamOptimizer(self.learning_rate).minimize(cross_entropy)\n    with tf.name_scope('accuracy'):\n        with tf.name_scope('correct_prediction'):\n            correct_prediction=tf.equal(tf.argmax(y),tf.argmax(self.y_))\n        with tf.name_scope('accuracy'):\n            self.accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n    tf.summary.scalar('accuracy',self.accuracy)\n    \n\n\n\n\n上面的代码定义了大部分的节点名称，并且也添加了标量的摘要如accuracy等,accuracy的图不再赘述，接下来就是在运行的过程中绘制刚刚的标量和图表信息\n\n    #为了释放TensorBoard所使用的事件文件（events file），所有的即时数据（在这里只有一个）都要在图表构建阶段合并至一个操作（op）中。\n    merged=tf.summary.merge_all()\n    session=tf.Session()\n    #用于将Summary写入磁盘，需要制定存储路径logdir，如果传递了Graph对象，则在Graph Visualization会显示Tensor Shape Information\n    train_writer=tf.summary.FileWriter(self.log_dir+'/train',session.graph)\n    test_writer=tf.summary.FileWriter(self.log_dir+'/test')\n    init=tf.global_variables_initializer()\n    session.run(init)\n    saver=tf.train.Saver()\n    for i in range(self.max_steps):\n        if i%100==0:\n            #绘制之前的所有的图表以及标量信息\n            summary,acc=session.run([merged,self.accuracy],feed_dict=self.feed_dict(False))\n            test_writer.add_summary(summary,i)\n            print('Accuracy at step %s: %s'%(i,acc))\n        else:\n            if i==499:\n                #定义本模型的运行选项，并不加限制并且操作元数据，为描述数据的数据（data about data），主要是描述数据属性（property）的信息，用来支持如指示存储位置、历史数据、资源查找、文件记录等功能。\n                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n                run_metadata=tf.RunMetadata()\n                summary,_=session.run([merged,self.train_step],feed_dict=self.feed_dict(True),options=run_options,run_metadata=run_metadata)\n                train_writer.add_run_metadata(run_metadata,'step%03d'%i)\n                train_writer.add_summary(summary,i)\n                print('Adding run metadata for',i)\n            else:\n                summary,_=session.run([merged,self.train_step],feed_dict=self.feed_dict(True))\n                train_writer.add_summary(summary,i)\n                if i%99==0:\n                    print(\"at train step\")\n    train_writer.close()\n    test_writer.close()\ndef extract(self,x,y):\n    sample_num=self.mnist.train.num_examples\n    idx=np.random.randint(low=0,high=(sample_num-self.batch_size))\n    return x[idx:(idx+self.batch_size)],y[idx:(idx+self.batch_size)]\ndef feed_dict(self,train):\n    if train:\n        xs,ys=self.mnist.train.next_batch(self.batch_size)\n        k=self.drop_out\n    else:\n        xs,ys=self.extract(self.mnist.train.images,self.mnist.train.labels)\n        k=1.0\n    return {self.x:xs,self.y_:ys,self.keep_prob:k}\n    \n\n二.结论\nTensorboard作为对TensorFlow训练过程的分析，可以对模型的问题作出更快的判断，可以加快模型的优化，应当尽量多采用\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}