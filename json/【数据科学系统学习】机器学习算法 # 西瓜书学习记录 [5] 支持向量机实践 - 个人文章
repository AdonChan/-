{"title": "【数据科学系统学习】机器学习算法 # 西瓜书学习记录 [5] 支持向量机实践 - 个人文章 ", "index": "python,支持向量机", "content": "本篇内容为《机器学习实战》第 6 章 支持向量机部分程序清单。所用代码为 python3。\n\n支持向量机优点：泛化错误率低，计算开销不大，结果易解释。 缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二分类问题。适用数据类型：数值型和标称型数据。\n1996 年，John Platt 发布了一个称为SMO的强大算法，用于训练 SVM。SMO表示序列最小优化 (Sequential Minimal Optimization)。\nSMO算法的工作原理是：每次循环中选择两个alpha进行优化处理。一旦找到一对合适的alpha，那么就增大其中一个同时减小另一个。这里的“合适”是指两个alpha必须要符合一定的条件，第一个条件是这两个alpha必须要在间隔边界之外，第二个条件是这两个alpha还没有进行过区间化处理或者不在边界上。\n应用简化版 SMO 算法处理小规模数据集\n下面给出简化版的SMO算法程序清单。\n该SMO函数的伪代码如下：创建一个alpha向量并将其初始化为 0 向量当迭代次数小于最大迭代次数时（外循环）···对数据集中的每个数据向量（内循环）：······如果该数据向量可以被优化：·········随机选择另外一个数据向量·········同时优化这两个向量·········如果两个向量都不能被优化，退出内循环···如果所有向量都没被优化，增加迭代数目，继续下一次循环\n程序清单 6-1 SMO算法中的辅助函数\n# coding=utf-8\n# import sys\nfrom numpy import *\n\ndef loadDataSet():\n    dataMat = []\n    labelMat = []\n    fr = open('testSet.txt')\n\n    for line in fr.readlines():\n        lineArr = line.strip().split('\\t')\n        dataMat.append([float(lineArr[0]), float(lineArr[1])])\n        labelMat.append(float(lineArr[2]))\n    return dataMat, labelMat\n\n\n# i 是第一个 alpha 的下标， m 是所有 alpha 的数目\n# 只要函数值不等于输入值 i，函数就会进行随机选择\ndef selectJrand(i, m):\n    j = i\n    while (j == i):\n        j = int(random.uniform(0, m))\n    return j\n\n# 用于调整大于 H 或小于 L 的 alpha 值\ndef clipAlpha(aj, H, L):\n    if aj > H:\n        aj = H\n    if L > aj:\n        aj = L\n    return aj\n\n在 python 提示符下，执行代码并得到结果：\n>>> import svmMLiA\n>>> dataArr, labelArr = svmMLiA.loadDataSet()\n>>> labelArr\n[-1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0]\n\n可以看出，这里采用的类别标签是 -1 和 1。\n\n程序清单 6-2 简化版SMO算法\n# 参数：数据集，类别标签，常数C，容错率，退出前最大的循环次数\ndef smoSimple(dataMatIn, classLabels, C, toler, maxIter):\n    dataMatrix = mat(dataMatIn)\n    # 由于转置了类别标签，我们得到的是一个列向量而不是列表\n    labelMat = mat(classLabels).transpose()\n    b = 0\n    m,n = shape(dataMatrix)\n    # 构建一个 alpha 列矩阵，矩阵中元素都初始化为0\n    alphas = mat(zeros((m, 1)))\n    # iter 变量存储的是在没有任何 alpha 改变的情况下便利数据集的次数\n    # 当这个变量达到输入值 maxIter 时，函数结束运行并退出\n    iter = 0\n\n    while(iter < maxIter):\n\n        # 每次循环当中，将 alphaPairsChanged 先设为0，在对整个集合顺序遍历\n        # 变量 alphaPairsChanged 用于记录 alpha 是否已经进行优化\n        alphaPairsChanged = 0\n        for i in range(m):\n            # 计算 fXi，即我们预测的类别\n            fXi = float(multiply(alphas, labelMat).T * (dataMatrix*dataMatrix[i,:].T) + b)\n            # 与真实值比对，计算误差 Ei\n            Ei = fXi - float(labelMat[i])\n            # 如果误差很大，可以对该数据实例所对应的 alpha 值进行优化\n\n            # 不论正间隔还是负间隔都会被测试\n            # 检查 alpha 值，保证其不能等于 0 或 C\n            if((labelMat[i]*Ei < -toler) and (alphas[i] < C)\\\n               or (labelMat[i]*Ei > toler) and (alphas[i] > 0)):\n\n                # 用辅助函数 selectJrand 随机选择第二个 alpha 值，即 alpha[j]\n                j = selectJrand(i,m)\n                # 同样计算误差\n                fXj = float(multiply(alphas, labelMat).T * (dataMatrix*dataMatrix[j,:].T)) + b\n                Ej = fXj - float(labelMat[j])\n                alphaIold = alphas[i].copy()\n                alphaJold = alphas[j].copy()\n\n                # 计算 L 和 H，调整 alpha 到 0 与 C 之间\n                if(labelMat[i] != labelMat[j]):\n                    L = max(0, alphas[j] - alphas[i])\n                    H = min(C, C + alphas[j] - alphas[i])\n                else:\n                    L = max(0, alphas[j] + alphas[i] - C)\n                    H = min(C, alphas[j] + alphas[i])\n                if L==H:\n                    print('L == H')\n                    continue\n\n                # eta 是 alpha[j] 的最优修改量\n                eta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T - \\\n                    dataMatrix[i,:]*dataMatrix[i,:].T - \\\n                    dataMatrix[j,:]*dataMatrix[j,:].T\n\n                if eta >= 0:\n                    print('eta >= 0')\n                    continue\n\n                # 计算出一个新的 alpha[j]，并进行调整\n                alphas[j] -= labelMat[j] * (Ei - Ej) / eta\n                alphas[j] = clipAlpha(alphas[j], H, L)\n\n                # 检查 alpha[j] 是否有轻微改变，是的话则退出 for 循环\n                if(abs(alphas[j] - alphaJold) < 0.00001):\n                    print('j not moving enough')\n                    continue\n\n                # 对 alpha[i] 进行和 alpha[j] 同样的改变\n                # 改变的大小一样，方向正好相反\n                alphas[i] += labelMat[j] * labelMat[i] * (alphaJold - alphas[j])\n\n                # 对 alpha[i] 和 alpha[j] 进行优化之后，给它们设置一个常数项 b\n                b1 = b - Ei - \\\n                     labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - \\\n                     labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T\n                b2 = b - Ej - \\\n                     labelMat[i] * (alphas[i] - alphaIold) * dataMatrix[i, :] * dataMatrix[j, :].T - \\\n                     labelMat[j] * (alphas[j] - alphaJold) * dataMatrix[j, :] * dataMatrix[j, :].T\n\n        if(alphaPairsChanged == 0):\n            iter += 1\n        else:\n            iter = 0\n        print('iteration number: %d' % iter)\n    return b, alphas\n\n在 python 提示符下，执行代码并得到结果：\nb, alphas = svmMLiA.smoSimple(dataArr, labelArr, 0.6, 0.001, 40)\n再执行：\n>>> for i in range(100):\n...     if alphas[i] > 0.0:\n...             print(dataArr[i], labelArr[i])\n... \n[3.542485, 1.977398] -1.0\n[7.108772, -0.986906] 1.0\n[4.658191, 3.507396] -1.0\n[7.40786, -0.121961] 1.0\n[3.457096, -0.082216] -1.0\n[5.286862, -2.358286] 1.0\n[6.080573, 0.418886] 1.0\n[6.543888, 0.433164] 1.0\n[1.966279, -1.840439] -1.0\n\n所输出的数据点即为支持向量。\n注：以上给出的仅是简化版SMO算法的实现，关于完整的SMO算法加速优化并应用核函数，请参照《机器学习实战》第 99 页。\n\n不足之处，欢迎指正。\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "1"}