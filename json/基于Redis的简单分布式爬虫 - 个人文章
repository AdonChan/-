{"title": "基于Redis的简单分布式爬虫 - 个人文章 ", "index": "redis,分布式爬虫,网页爬虫,python", "content": "Ugly-Distributed-Crawler\n建议先大概浏览一下项目结构\n项目介绍\n新手向，基于Redis构建的分布式爬虫。以爬取考研网的贴子为例，利用 PyQuery, lxml 进行解析，将符合要求的文章文本存入MySQ数据库中。\n结构简介\ncooperator\n协作模块，用于为Master&Worker模块提供代理IP支持\nmaster\n提取满足条件的文章url，并交给Worker进一步处理\nworker\n解析文章内容，将符合要求的存入数据库\n环境依赖\nsqlalchemy => 1.0.13  pyquery => 1.2.17  requests => 2.12.3  redis => 2.10.5  lxml => 3.6.0\n\n需要预先安装MySQL-server 和 Redis-server.\nMySQL中应有名为kybsrc的数据库，且该数据库包含一个名为posts的表，拥有num(INT AUTO_INCREMENT)和post(TEXT)两个字段。\n\n如何启动\n0. 先配置好各模块所引用的配置文件\n1. 为了更好地运行，cooperator/start.py 应提前开始并完成一次工作函数执行\n第一次执行完后，每五分钟运行一次工作函数\n2. 启动 master/start.py\n默认只执行一次\n3. 启动 worker/start.py\n默认循环监听是否有新的URL待解析\n核心点说明\n1. 通过Redis的集合类型进行代理IP和URL的传递\n# Summary Reference\n# ---------\n# 创建句柄\ndef make_redis_handler():\n    pool = redis.ConnectionPool(host=r_server['ip'], port=r_server['port'], password=r_server['passwd'])\n    return redis.Redis(connection_pool=pool)\n\n# 获得句柄\ndef make_proxy_handler():\n    return make_redis_handler()\n\n# 保存到指定的set下\ndef check_and_save(self, proxy):\n 'pass'\n   self.redis_handler.sadd(r_server['s_name'], proxy)\n2. 由于在验证代理IP和使用封装的get_url()函数的时候网络IO较多，所以使用多线程（效果还是很明显的）。\n#Summary Reference\n#---------\ndef save_proxy_ip(self):\n    'pass'\n    for proxy in self.proxy_ip:\n        Thread(target=self.check_and_save, args=(proxy,)).start()\n\ndef get_url(url):\n    'pass'\n    while True:\n    'pass'\n        resp = request('get', url, headers=headers, proxies={'http': proxy})\n    'pass'\n项目地址\nhttps://github.com/A101428020...\n有任何问题可以与我联系(微信：smartseer)\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "5"}