{"title": "学习极客学院多线程爬虫课程的收获 - veve的极客之路 ", "index": "网页爬虫,编码,python", "content": "昨天开始了极客学院《XPath与多线程爬虫》课程的学习，主要涉及到XPath和requests的使用，在测试过程中出现了很多问题，经过不断摸索以及前辈们的帮助，现将经验总结如下：1. Python3下面文本编码问题虽然Python3相对于2已经集成了很多编码方式，使我们不需要过多去关心和指定编码，但有时候在文本读取，写入的时候还是需要多小心，在测试过程中多次出现在写入文件时报告错误“UnicodeEncodeError: 'ascii' codec can't encode character '\\u56de' in position 0: ordinal not in range(128)”，这是由于我们在抓取网页的时候采用的是UTF-8编码，而存储时没有指定编码，在存储到文件的过程中就会报错。解决办法为：在读取文件时加入指定UTF-8编码的选项\nf = open('content.txt','a',encoding='UTF-8')\n\n另外需要注意的是使用requests获取到网页之后同样要指定编码\nhtml = requests.get(url)\nhtml = re.sub(r'charset=(/w*)', 'charset=UTF-8', html.text)\n    \n\n2. XPath的用法XPath可以很方便的解析XML文件的节点和属性，使用也很简单，相比于正则表达式来说，XPath的查询方式更加高效准确，它来自于lxml包内的etree，在使用之前应该声明\nfrom lxml import etree\n\n在使用XPath应该遵循“先抓大，再抓小”的原则，现定位到大的节点，获取到所有字节点再一层一层往下寻找，直到获取所需要的信息例如，我们想要抓取百度贴吧的网页每一个楼层的信息（包括作者，回帖时间，回帖内容等等），通过Chrome-Inspect element可以审查代码，得到某一个楼层的代码楼层最外层都有声明：\n<div class=\"l_post j_l_post l_post_bright  \" \n\n使用XPath先获取整个楼层的所有节点（Node）\ncontent_field = selector.xpath('//div[@class=\"l_post j_l_post l_post_bright  \"]')\n\n再往下寻找，发现我们要提取的内容位于\n<div class=\"d_post_content_main\">\n\n这一个节点以内，再继续往下挖掘：\ncontent =each.xpath('div[@class=\"d_post_content_main\"]/div/cc/div[@class=\"d_post_content j_d_post_content  clearfix\"]/text()')\n\n这样一步步得到想要的内容\n3.JSON格式网页中很多内容使用JSON来传输，我们要把内容还原出来需要使用json模块\nimport json\nreply_info = json.loads(each.xpath('@data-field')[0].replace('&quot',''))\n\n4.Python中的多线程多线程可以很大幅度提高软件的处理速度，可以充分利用计算机性能，不同的核处理不同的任务，并行执行，提高处理速度，使用方法如下：\nfrom multiprocessing.dummy import Pool as ThreadPool\npool = ThreadPool(8)\nresults = pool.map(spider,page)\npool.close()\npool.join()\n\nmap 这一小巧精致的函数是简捷实现 Python 程序并行化的关键。map 源于 Lisp 这类函数式编程语言。它可以通过一个序列实现两个函数之间的映射。上面的这两行代码将 page这一序列中的每个元素作为参数传递到 spyder 方法中，并将所有结果保存到 results 这一列表中。其结果大致相当于：\nresults = []\nfor page in pages: \n    results.append(spyder(page))\n\n上述代码中调用join之前，先调用close函数，否则会出错。执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束。\n全部代码：\n#-*-coding:utf8-*-\nfrom lxml import etree\nfrom multiprocessing.dummy import Pool as ThreadPool\nimport requests\nimport json\nimport re\nimport sys\n\n'''重新运行之前请删除content.txt，因为文件操作使用追加方式，会导致内容太多。'''\n\ndef towrite(contentdict):\n    #f=open(\"content.txt\",'wb')\n    f.writelines(u'回帖时间:' + str(contentdict['topic_reply_time']) + '\\n')\n    f.writelines(u'回帖内容:' + str(contentdict['topic_reply_content']) + '\\n')\n    f.writelines(u'回帖人:' + contentdict['user_name'] + '\\n\\n')\n    #f.close()\n\ndef spider(url):\n    html = requests.get(url)\n    #print(html.text)\n    html = re.sub(r'charset=(/w*)', 'charset=UTF-8', html.text)\n    selector = etree.HTML(html)\n    # print(selector)\n    #content_field = selector.xpath('//div[starts-with(@class,\"l_post l_post_bright\")]')p_content p_content_nameplate\n    #content_field = selector.xpath('//*[@id=\"j_p_postlist\"]')\n    content_field = selector.xpath('//div[@class=\"l_post j_l_post l_post_bright  \"]')\n    item = {}\n    for each in content_field:\n        reply_info = json.loads(each.xpath('@data-field')[0].replace('&quot',''))\n        author = reply_info['author']['user_name']\n        # content1 = each.xpath('//div[@class=\"d_post_content_main\"]')\n        content = each.xpath('div[@class=\"d_post_content_main\"]/div/cc/div[@class=\"d_post_content j_d_post_content  clearfix\"]/text()')\n        reply_time = reply_info['content']['date']\n        print(\"content:{0}\".format(content))\n        print(\"Reply_time:{0}\".format(reply_time))\n        print(\"Author:{0}\".format(author))\n        item['user_name'] = author\n        item['topic_reply_content'] = content\n        item['topic_reply_time'] = reply_time\n        towrite(item)\n\nif __name__ == '__main__':\n    pool = ThreadPool(8)\n    f = open('content.txt','a',encoding='UTF-8')\n    # f = open('content.txt','a')\n    page = []\n    for i in range(1,21):\n        newpage = 'http://tieba.baidu.com/p/3522395718?pn=' + str(i)\n        page.append(newpage)\n\n    results = pool.map(spider,page)\n    pool.close()\n    pool.join()\n    f.close()\n\n结果如下：\n\n回帖时间:2015-01-11 16:52\n回帖内容:['            6和plus纠结买哪款。还有 买完新机可以让他上色吗']\n回帖人:斗已转0\n\n回帖时间:2015-01-11 16:53\n回帖内容:['            我现在是以贴吧高级会员的身份帮你顶贴，请注意你的态度']\n回帖人:暑假干啥\n\n回帖时间:2015-01-11 16:57\n回帖内容:['            我去']\n回帖人:qw518287200\n\n回帖时间:2015-01-11 16:57\n回帖内容:['            能教我怎么看序列号或imei号麽，大神\\uf618']\n回帖人:花颜诱朕醉\n\n需要注意的是，极客学院附带资料的源代码是无法使用的，以上说到的几点就是我在调试过程中淌过的坑，要注意使用Chrome对要抓取的网页进行细心分析，修改xpath参数并不断试验。\n+++++++明日计划++++++++++++++++加入计时功能，测试单线程与多线程的性能差别尝试抓取网页中的图片并保存\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "8"}