{"title": "服务器上部署scrapy爬虫项目 - 个人文章 ", "index": "python", "content": "爬爬们，如果你已经开始部署项目了，那么你肯定也已经写好了完整的爬虫项目，恭喜你，你很优秀！**今天忙了小半天的服务器部署，跟大家分享一些心得～\n\n首先我们要有一台服务器，不好意思，这是废话，略过。。。。。\n\n安装python\n   # 下载安装包，好习惯可以自己创建文件夹/home/download/\n   $ wget https://www.python.org/ftp/python/3.4.1/Python-3.4.1.tgz\n   # 解压\n   $ tar zxvf Python-3.4.1.tgz /us\n   3进入解压后的目录，执行安装配置\n   $ ./configure\n   #或指定安装目录\n   $ ./configure --prefix=/opt/python3\n   $ make\n   $ make install\n   #安装后建立一个链接，这样我们可以用python3直接运行程序，和python2区别开来。\n   $ ln -s /opt/python3/bin/python3 /usr/bin/python3\n\n\n注意：1.在安装之前最好要安装各种依赖包\nyum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel\n若已经安装python3并改软链接了，请修改/usr/bin/yum文件中的第一行python后加2.？版本号，还有一个文件需要修改叫什么我忘记了，同理\n注意：2.pip3安装之前要建立pip3软连接\n$ ln -s /opt/python3/bin/pip3 /usr/bin/pip3\n3.安装scrapy\n在这里我们先安装scrapyd避免手动安装scrapy需要的插件\npip3 install scrapyd\npip3 install scrapy\n\n注意：若没有在第二步安装依赖环境 sqlite-devel，那么在启动scrapyd的时候会报错。安装各种依赖包最好创建虚拟环境，\n#安装\npip3 install virtualenv\n#建立软连接\n$ ln -s /opt/python3/bin/virtualenv /usr/bin/virtualenv\n#进入项目目录创建虚拟环境\n$ virtualenv venv\n# 在venv/bin目录下\nsource activate\n\n\n\n\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}