{"title": "python multiprocess 使用 yield - ddongjian0000 ", "index": "yield,python", "content": "python在处理数据的时候，memory-heavy 的数据往往会导致程序没办反运行或者运行期间服务器其他程序效率受到影响。这种情况往往会把数据集合变为通过genertor来遍历。\n\n但同时如我们所知，generoter看似只能被单进程消费，这样效率很低。\n\ngenerator 可以被pool.map消费。\n\n看一下pool.py的源码。\n\nfor i, task in enumerate(taskseq):\n     ...\n     try:\n         put(task)\n     except IOError:\n         debug('could not put task on queue')\n         break\n\n\n实际是先将generator全部消费掉放到queue中。然后通过map来并行。这样是解决了使用map来并行。\n\n但是依然没有解决占用内存的问题。这里有两步占用内存。\n\n\n第一步是全部消费掉的generator。\n第二步并行运算全部data。\n\n解决第一个问题，通过部分消费generator来达到。\n解决第二个问题，可以通过imap来达到.\n\n示例代码如下：\n\nimport multiprocessing as mp\nimport itertools\nimport time\n\n\ndef g():\n    for el in xrange(50):\n        print el\n        yield el\n\nimport os\n\ndef f(x):\n    time.sleep(1)\n    print str(os.getpid()) +\" \"+  str(x)\n    return x * x\n\nif __name__ == '__main__':\n    pool = mp.Pool(processes=4)              # start 4 worker processes\n    go = g()\n    result = []\n    N = 11\n    while True:\n        g2 = pool.imap(f, itertools.islice(go, N))\n        if g2:\n            for i in g2:\n                result.append(i)\n                time.sleep(1)\n        else:\n            break\n    print(result)\n\n\nps: 使用注意事项。在produce数据的时候，尽量少做操作，应为即使是map也是单线程的来消费数据。所以尽量把操作放到map中作。这样才能更好的利用多进程提高效率。\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "3"}