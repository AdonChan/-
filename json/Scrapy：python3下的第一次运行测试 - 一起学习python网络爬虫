{"title": "Scrapy：python3下的第一次运行测试 - 一起学习python网络爬虫 ", "index": "网页爬虫,scrapy,python", "content": "1，引言\n《Scrapy的架构初探》一文讲解了Scrapy的架构，本文就实际来安装运行一下Scrapy爬虫。本文以官网的tutorial作为例子，完整的代码可以在github上下载。\n2，运行环境配置\n\n本次测试的环境是：Windows10， Python3.4.3 32bit\n安装Scrapy ：   $ pip install Scrapy                 #实际安装时，由于服务器状态的不稳定，出现好几次中途退出的情况\n\n3，编写运行第一个Scrapy爬虫\n3.1. 生成一个新项目：tutorial\n$ scrapy startproject tutorial\n\n项目目录结构如下：\n3.2.  定义要抓取的item\n# -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# http://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\nclass DmozItem(scrapy.Item):\n    title = scrapy.Field()\n    link = scrapy.Field()\n    desc = scrapy.Field()\n\n3.3. 定义Spider\nimport scrapy\nfrom tutorial.items import DmozItem\n\nclass DmozSpider(scrapy.Spider):\n    name = \"dmoz\"\n    allowed_domains = [\"dmoz.org\"]\n    start_urls = [\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n    ]\n\n    def parse(self, response):\n        for sel in response.xpath('//ul/li'):\n            item = DmozItem()\n            item['title'] = sel.xpath('a/text()').extract()\n            item['link'] = sel.xpath('a/@href').extract()\n            item['desc'] = sel.xpath('text()').extract()\n            yield item\n\n3.4. 运行\n$ scrapy crawl dmoz -o item.json\n\n1) 结果报错：    A) ImportError: cannot import name '_win32stdio'   B) ImportError: No module named 'win32api'\n2) 查错过程：查看官方的FAQ和stackoverflow上的信息，原来是scrapy在python3上测试还不充分，还有小问题。\n3) 解决过程：   A) 需要手工去下载twisted/internet下的 _win32stdio 和 _pollingfile，存放到python目录的libsitepackagestwistedinternet下   B) 下载并安装pywin32\n再次运行，成功！在控制台上可以看到scrapy的输出信息，待运行完成退出后，到项目目录打开结果文件items.json， 可以看到里面以json格式存储的爬取结果。\n[\n{\"title\": [\"        About       \"], \"desc\": [\" \", \" \"], \"link\": [\"/docs/en/about.html\"]},\n{\"title\": [\"   Become an Editor \"], \"desc\": [\" \", \" \"], \"link\": [\"/docs/en/help/become.html\"]},\n{\"title\": [\"            Suggest a Site          \"], \"desc\": [\" \", \" \"], \"link\": [\"/docs/en/add.html\"]},\n{\"title\": [\" Help             \"], \"desc\": [\" \", \" \"], \"link\": [\"/docs/en/help/helpmain.html\"]},\n{\"title\": [\" Login                       \"], \"desc\": [\" \", \" \"], \"link\": [\"/editors/\"]},\n{\"title\": [], \"desc\": [\" \", \" Share via Facebook \"], \"link\": []},\n{\"title\": [], \"desc\": [\" \", \"  Share via Twitter  \"], \"link\": []},\n{\"title\": [], \"desc\": [\" \", \" Share via LinkedIn \"], \"link\": []},\n{\"title\": [], \"desc\": [\" \", \" Share via e-Mail   \"], \"link\": []},\n{\"title\": [], \"desc\": [\" \", \" \"], \"link\": []},\n{\"title\": [], \"desc\": [\" \", \"  \"], \"link\": []},\n{\"title\": [\"        About       \"], \"desc\": [\" \", \" \"], \"link\": [\"/docs/en/about.html\"]},\n{\"title\": [\"   Become an Editor \"], \"desc\": [\" \", \" \"], \"link\": [\"/docs/en/help/become.html\"]},\n{\"title\": [\"            Suggest a Site          \"], \"desc\": [\" \", \" \"], \"link\": [\"/docs/en/add.html\"]},\n{\"title\": [\" Help             \"], \"desc\": [\" \", \" \"], \"link\": [\"/docs/en/help/helpmain.html\"]},\n{\"title\": [\" Login                       \"], \"desc\": [\" \", \" \"], \"link\": [\"/editors/\"]},\n{\"title\": [], \"desc\": [\" \", \" Share via Facebook \"], \"link\": []},\n{\"title\": [], \"desc\": [\" \", \"  Share via Twitter  \"], \"link\": []},\n{\"title\": [], \"desc\": [\" \", \" Share via LinkedIn \"], \"link\": []},\n{\"title\": [], \"desc\": [\" \", \" Share via e-Mail   \"], \"link\": []},\n{\"title\": [], \"desc\": [\" \", \" \"], \"link\": []},\n{\"title\": [], \"desc\": [\" \", \"  \"], \"link\": []}\n]\n\n第一次运行scrapy的测试成功\n4，接下来的工作\n接下来，我们将使用GooSeeker API来实现网络爬虫，省掉对每个item人工去生成和测试xpath的工作量。目前有2个计划：\n\n在gsExtractor中封装一个方法：从xslt内容中自动提取每个item的xpath\n从gsExtractor的提取结果中自动提取每个item的结果\n\n具体选择哪个方案，将在接下来的实验中确定，并发布到gsExtractor新版本中。\n5，文档修改历史\n2016-06-15：V1.0，首次发布\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "2"}