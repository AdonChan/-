{"title": "Python抓取百度百科数据 - VoidKing ", "index": "网页爬虫,python", "content": "前言\n本文整理自慕课网《Python开发简单爬虫》，将会记录爬取百度百科“python”词条相关页面的整个过程。\n抓取策略\n确定目标：确定抓取哪个网站的哪些页面的哪部分数据。本实例抓取百度百科python词条页面以及python相关词条页面的标题和简介。分析目标：分析要抓取的url的格式，限定抓取范围。分析要抓取的数据的格式，本实例中就要分析标题和简介这两个数据所在的标签的格式。分析要抓取的页面编码的格式，在网页解析器部分，要指定网页编码，然后才能进行正确的解析。编写代码：在网页解析器部分，要使用到分析目标得到的结果。执行爬虫：进行数据抓取。\n分析目标\n1、url格式进入百度百科python词条页面，页面中相关词条的链接比较统一，大都是/view/xxx.htm。\n2、数据格式标题位于类lemmaWgt-lemmaTitle-title下的h1子标签，简介位于类lemma-summary下。\n3、编码格式查看页面编码格式，为utf-8。\n经过以上分析，得到结果如下：\n代码编写\n项目结构\n在sublime下，新建文件夹baike-spider，作为项目根目录。新建spider_main.py，作为爬虫总调度程序。新建url_manger.py，作为url管理器。新建html_downloader.py，作为html下载器。新建html_parser.py，作为html解析器。新建html_outputer.py，作为写出数据的工具。最终项目结构如下图：\nspider_main.py\n# coding:utf-8\nimport url_manager, html_downloader, html_parser, html_outputer\n\nclass SpiderMain(object):\n    def __init__(self):\n        self.urls = url_manager.UrlManager()\n        self.downloader = html_downloader.HtmlDownloader()\n        self.parser = html_parser.HtmlParser()\n        self.outputer = html_outputer.HtmlOutputer()\n\n    def craw(self, root_url):\n        count = 1\n        self.urls.add_new_url(root_url)\n        while self.urls.has_new_url():\n            try:\n                new_url = self.urls.get_new_url()\n                print('craw %d : %s' % (count, new_url))\n                html_cont = self.downloader.download(new_url)\n                new_urls, new_data = self.parser.parse(new_url, html_cont)\n                self.urls.add_new_urls(new_urls)\n                self.outputer.collect_data(new_data)\n\n                if count == 10:\n                    break\n\n                count = count + 1\n            except:\n                print('craw failed')\n\n        self.outputer.output_html()\n\n\nif __name__=='__main__':\n    root_url = 'http://baike.baidu.com/view/21087.htm'\n    obj_spider = SpiderMain()\n    obj_spider.craw(root_url)\nurl_manger.py\n# coding:utf-8\nclass UrlManager(object):\n    def __init__(self):\n        self.new_urls = set()\n        self.old_urls = set()\n\n    def add_new_url(self, url):\n        if url is None:\n            return\n        if url not in self.new_urls and url not in self.old_urls:\n            self.new_urls.add(url)\n\n    def add_new_urls(self, urls):\n        if urls is None or len(urls) == 0:\n            return\n        for url in urls:\n            self.add_new_url(url)\n\n    def has_new_url(self):\n        return len(self.new_urls) != 0\n\n    def get_new_url(self):\n        new_url = self.new_urls.pop()\n        self.old_urls.add(new_url)\n        return new_url\n\nhtml_downloader.py\n# coding:utf-8\nimport urllib.request\n\nclass HtmlDownloader(object):\n    def download(self, url):\n        if url is None:\n            return None\n        response = urllib.request.urlopen(url)\n        if response.getcode() != 200:\n            return None\n        return response.read()\nhtml_parser.py\n# coding:utf-8\nfrom bs4 import BeautifulSoup\nimport re\nfrom urllib.parse import urljoin\n\nclass HtmlParser(object):\n    def _get_new_urls(self, page_url, soup):\n        new_urls = set()\n        # /view/123.htm\n        links = soup.find_all('a', href=re.compile(r'/view/\\d+\\.htm'))\n        for link in links:\n            new_url = link['href']\n            new_full_url = urljoin(page_url, new_url)\n            # print(new_full_url)\n            new_urls.add(new_full_url)\n        #print(new_urls)\n        return new_urls\n\n    def _get_new_data(self, page_url, soup):\n        res_data = {}\n        # url\n        res_data['url'] = page_url\n        # <dd class=\"lemmaWgt-lemmaTitle-title\"> <h1>Python</h1>\n        title_node = soup.find('dd', class_='lemmaWgt-lemmaTitle-title').find('h1')\n        res_data['title'] = title_node.get_text()\n        # <div class=\"lemma-summary\" label-module=\"lemmaSummary\">\n        summary_node = soup.find('div', class_='lemma-summary')\n        res_data['summary'] = summary_node.get_text()\n        # print(res_data)\n        return res_data\n\n    def parse(self, page_url, html_cont):\n        if page_url is None or html_cont is None:\n            return\n        soup = BeautifulSoup(html_cont, 'html.parser')\n        # print(soup.prettify())\n        new_urls = self._get_new_urls(page_url, soup)\n        new_data = self._get_new_data(page_url, soup)\n        # print('mark')\n        return new_urls, new_data\nhtml_outputer.py\n# coding:utf-8\nclass HtmlOutputer(object):\n    def __init__(self):\n        self.datas = []\n\n    def collect_data(self, data):\n        if data is None:\n            return\n        self.datas.append(data)\n\n    def output_html(self):\n        fout = open('output.html','w', encoding='utf-8')\n\n        fout.write('<html>')\n        fout.write('<body>')\n        fout.write('<table>')\n\n        for data in self.datas:\n            fout.write('<tr>')\n            fout.write('<td>%s</td>' % data['url'])\n            fout.write('<td>%s</td>' % data['title'])\n            fout.write('<td>%s</td>' % data['summary'])\n            fout.write('</tr>')\n\n        fout.write('</table>')\n        fout.write('</body>')\n        fout.write('</html>')\n\n        fout.close()\n运行\n在命令行下，执行python spider_main.py。\n编码问题\n问题描述：UnicodeEncodeError: 'gbk' codec can't encode character 'xa0' in position ... \n使用Python写文件的时候，或者将网络数据流写入到本地文件的时候，大部分情况下会遇到这个问题。网络上有很多类似的文章讲述如何解决这个问题，但是无非就是encode，decode相关的，这是导致该问题出现的真正原因吗？不是的。很多时候，我们使用了decode和encode，试遍了各种编码，utf8，utf-8，gbk，gb2312等等，该有的编码都试遍了，可是仍然出现该错误，令人崩溃。\n在windows下面编写python脚本，编码问题很严重。将网络数据流写入文件时，我们会遇到几个编码：1、#encoding='XXX' 这里(也就是python文件第一行的内容)的编码是指该python脚本文件本身的编码，无关紧要。只要XXX和文件本身的编码相同就行了。 比如notepad++\"格式\"菜单里面里可以设置各种编码，这时需要保证该菜单里设置的编码和encoding XXX相同就行了，不同的话会报错。\n2、网络数据流的编码比如获取网页，那么网络数据流的编码就是网页的编码。需要使用decode解码成unicode编码。\n3、目标文件的编码 将网络数据流写入到新文件，写文件代码如下：\nfout = open('output.html','w')\nfout.write(str)\n在windows下面，新文件的默认编码是gbk，python解释器会用gbk编码去解析我们的网络数据流str，然而str是decode过的unicode编码，这样的话就会导致解析不了，出现上述问题。 解决的办法是改变目标文件的编码：\nfout = open('output.html','w', encoding='utf-8')\n运行结果\n\n源码分享\nhttps://github.com/voidking/b...\n书签\nPython开发简单爬虫http://www.imooc.com/learn/563\nThe Python Standard Libraryhttps://docs.python.org/3/lib...\nBeautiful Soup 4.2.0 文档https://www.crummy.com/softwa...\nPython词条http://baike.baidu.com/view/2...http://baike.baidu.com/item/P...\nPython3.x爬虫教程：爬网页、爬图片、自动登录http://www.2cto.com/kf/201507...\n使用python3进行优雅的爬虫（一）爬取图片http://www.jianshu.com/p/6969...\nPython UnicodeEncodeError: 'gbk' codec can't encode character 解决方法http://www.jb51.net/article/6...\nScrapy documentationhttps://doc.scrapy.org/en/lat...\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "5"}