{"title": "Python机器学习工具：Scikit-Learn介绍与实践 - 互联网业务风险控制 ", "index": "python,scikit-learn", "content": "\nScikit-learn 简介\n官方的解释很简单: Machine Learning in Python, 用python来玩机器学习。\n什么是机器学习\n机器学习关注的是: 计算机程序如何随着经验积累自动提高性能。而最大的吸引力在于，不需要写任何与问题相关的特定代码，泛型算法就能告诉你一些关于数据的秘密。\nScikit-learn的优点\n\n构建于现有的NumPy(基础n维数组包)，SciPy(科学计算基础包), matplotlib(全面的2D/3D画图)，IPython(加强的交互解释器)，Sympy(Symbolic mathematics)， Pandas(数据结构和分析)之上，做了易用性的封装。\n简单且高效的数据挖掘、数据分析的工具。\n对所有人开放，且在很多场景易于复用。\nBSD证书下开源。\n\nScikit-learn的生态\nPython\npython是一门简单易学的语言，语法要素不多，对于只关心机器学习本身非软件开发的人员，python语言层面的东西基本是不需要关心的。\nJupyter\nhttp://nbviewer.jupyter.org/ 提供了一种便利的方式去共享自己或是别人的计算成果，以一种之前单单共享代码不同的交互的方式，scikit-learn官网上面大量的例子也是以这种方式展示，使用者不仅看到了代码的使用方式，还看到了代码的结果，如果自己搭建了jupyter server的话，导入notebook还可以直接在浏览器中在其中上下文任意处修改，大大增加了学习效率。\nScikit-learn 的主要内容\nScikit-learn的算法地图\n\n按照上图 scikit-learn提供的主要功能主要关注与数据建模，而非加载、操作、总结数据, 这些任务可能NumPy、Pandas就已经足够了。为此scikit-learn 主要提供了以下功能:\n\n测试数据集，sklearn.datasets模块提供了乳腺癌、kddcup 99、iris、加州房价等诸多开源的数据集\n降维(Dimensionality Reduction): 为了特征筛选、统计可视化来减少属性的数量。\n特征提取(Feature extraction)： 定义文件或者图片中的属性。\n特征筛选(Feature selection)： 为了建立监督学习模型而识别出有真实关系的属性。\n按算法功能分类，分为监督学习：分类（classification）和回归（regression），以及非监督学习：聚类（clustering）。sklearn提供了很全面的算法实现，详细算法清单http://scikit-learn.org/stabl...。\n聚类(Clustring): 使用KMeans之类的算法去给未标记的数据分类。\n交叉验证(Cross Validation): 去评估监督学习模型的性能。\n参数调优(Parameter Tuning): 去调整监督学习模型的参数以获得最大效果。\n流型计算(Manifold Learning): 去统计和描绘多维度的数据\n\n常用算法的大致介绍\n分类 Classification\n\n适用范围： 用作训练预测已经标记的数据集的类别. 监督学习的代表。\n常用算法对比：\n\n\nDo we Need Hundreds of Classifiers to Solve Real World Classification Problems?，文章测试了179种分类模型在UCI所有的121个数据上的性能，发现Random Forests 和 SVM 性能最好。\n回归 Regression\n适用范围：\n回归是用于估计两种变量之间关系的统计过程，回归分析可以帮助我们理解当任意一个自变量变化，另一个自变量不变时，因变量变化的典型值。最常见的是，回归分析能在给定自变量的条件下估计出因变量的条件期望。 （举个例子，在二维的坐标系中，根据已有的坐标点去推导x、y轴的函数关系，既一元n次方程。)\n常用算法对比：\n\n优点：直接、快速；知名度高\n缺点：要求严格的假设；需要处理异常值\n\n集成算法 Ensemble Algorithms\n\n上图是单独用决策树来做回归任务去预测数据，但是反映了决策树虽然易于解释理解之外会有一些预测上的缺点，总结而言是趋向过拟合，可能或陷于局部最小值中、没有在线学习，所以下图引入了AdaBoost 集成算法来增加预测的可靠性，由此引出了集成算法的优点：\n\n集成方法是由多个较弱的模型集成模型组，其中的模型可以单独进行训练，并且它们的预测能以某种方式结合起来去做出一个总体预测。\n当先最先进的预测几乎都使用了算法集成。它比使用单个模型预测出来的结果要精确的多。\n\n但是如何找出可结合的弱模型、以及结合的方式又称为了繁重的维护工作。\n聚类 Clustering\n适用范围：\n是在没有标记的情况下去分类数据，使数据变得有意义， 如果已知分类分类的个数，Kmeans算法会更容易得出效果。\n常用算法对比：\n\n该图中颜色是聚类的结果，而非标记， 各算法的分类结果都可以根据输入参数调优，只是为了展示聚类的适用范围适合有特征的数据类型，对于最下一行的几乎均匀的数据几乎没有任何意义。\nScikit-learn进行计算的主要步骤\n\n数据获取、预处理。\n可选的降维过程.因为原始数据的维度比较大， 所以需要先找出真正跟预测目标相关的属性。\n学习以及预测的过程。\n反复学习的过程。增加样本、调优参数、换算法各种方式去提供预测的准确率。\n\nScikit-learn 的简单使用示例\n决策树示例:\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\n# 读取 iris 数据集\ndataset = datasets.load_iris()\n# 采用CART模型\nmodel = DecisionTreeClassifier()\nmodel.fit(dataset.data, dataset.target)\nprint(model)\n# 预测\nexpected = dataset.target\npredicted = model.predict(dataset.data)\n# 统计\nprint(metrics.classification_report(expected, predicted))\nprint(metrics.confusion_matrix(expected, predicted))\n\n输出:\n  precision    recall  f1-score   support\n     \n    0       1.00      1.00      1.00        50\n    1       1.00      1.00      1.00        50\n    2       1.00      1.00      1.00        50\n \n    avg / total       1.00      1.00      1.00       150\n    \n    [[50  0  0]\n    [ 0 50  0]\n    [ 0  0 50]]\n\n引用\n\nQuick Start Tutorial http://scikit-learn.org/stabl...\nUser Guide http://scikit-learn.org/stabl...\nAPI Reference http://scikit-learn.org/stabl...\nExample Gallery http://scikit-learn.org/stabl...\nScikit-learn: Machine Learning in Python \nAPI design for machine learning software: experiences from the scikit-learn project\n\n\n反爬虫文章来源：http://bigsec.com/\n\ntoyld 岂安科技搬运代码负责人 \n主导各处的挖坑工作，擅长挖坑于悄然不息，负责生命不息，挖坑不止。\n\n\n                ", "mainLikeNum": ["3 "], "mainBookmarkNum": "15"}