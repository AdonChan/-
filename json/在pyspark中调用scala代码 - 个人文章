{"title": "在pyspark中调用scala代码 - 个人文章 ", "index": "pyspark,scala,python,spark", "content": "在pyspark中调用scala代码\n情境说明\n问题\n我们这边是要使用Spark去并行一个自然语言处理的算法，其中使用到了LDA主题模型。由于使用的是天河二号，Spark版本是1.5.1，pyspark同样，所以获取主题时还不能使用describeTopics(在spark1.6中才开放对python的接口)，只能使用topicsMatrix的方法。\n本来凑合用topicsMatrix也行，但我们发现，这一个用来获取主题模型的函数，居然比Lda的训练还要慢！无论在我们自己的集群还是在天河二号的分区上，都是这一个情况。观察topicsMatrix的源代码，好像也没有什么复杂操作，只是把数据汇总collect而已：\n@Since(\"1.3.0\")\noverride lazy val topicsMatrix: Matrix = {\n  // Collect row-major topics\n  val termTopicCounts: Array[(Int, TopicCounts)] =\n    graph.vertices.filter(_._1 < 0).map { case (termIndex, cnts) =>\n    (index2term(termIndex), cnts)}.collect()\n  // Convert to Matrix\n  val brzTopics = BDM.zeros[Double](vocabSize, k)\n  termTopicCounts.foreach { case (term, cnts) =>\n    var j = 0\n    while (j < k) {\n      brzTopics(term, j) = cnts(j)\n      j += 1\n    }\n  }\n  Matrices.fromBreeze(brzTopics)\n}\n由于并不是算法中有一些复杂运算导致较慢，我们自然不希望在程序中有这样的情况。发现到在Spark1.5.1中，mllib中LdaModel已经实现了describeTopics，只是未在Python中开放，我们自然希望尝试使用describeTopics看看效果。\ndescribeTopics的源代码探索\n已知LDA.train()返回的是LdaModel的实例，于是乎，参考上篇博客，用以下方式去调用：\nmodel = LDA.train(rdd_data, k=num_topics, maxIterations=20)\ntopics = model.call('describeTopics', _py2java(sc, 10))\n执行速度特别快，然而返回的结果却不尽如人意，仅返回了一个长度k的列表，每个元素是一个key为'class'，value为'scala.Tuple2'的单元素字典。从结果来看，scala的代码应该是被成功执行了，然而返回结果却出了问题。查看callJavaFunc的内容，可以判断出，是describeTopics的返回结果没有被_java2py函数正常的转换。\n比对Spark1.5和Spark1.6的代码，LdaModel.describeTopics函数的内容是一致的，那么问题在哪儿呢？再去查看pyspark的LDA.train()调用的PythonMLLibAPI.trainLdaModel，发现在1.6中返回的不再是LdaModel而是它的子类LdaModelWrapper。查看这个类的方法，发现它重载了describeTopics来方便_java2py进行数据转换：\nprivate[python] class LDAModelWrapper(model: LDAModel) {\n\n  def topicsMatrix(): Matrix = model.topicsMatrix\n\n  def vocabSize(): Int = model.vocabSize\n\n  def describeTopics(): Array[Byte] = describeTopics(this.model.vocabSize)\n\n  def describeTopics(maxTermsPerTopic: Int): Array[Byte] = {\n    val topics = model.describeTopics(maxTermsPerTopic).map { case (terms, termWeights) =>\n      val jTerms = JavaConverters.seqAsJavaListConverter(terms).asJava\n      val jTermWeights = JavaConverters.seqAsJavaListConverter(termWeights).asJava\n      Array[Any](jTerms, jTermWeights)\n    }\n    SerDe.dumps(JavaConverters.seqAsJavaListConverter(topics).asJava)\n  }\n\n  def save(sc: SparkContext, path: String): Unit = model.save(sc, path)\n}\n找到这里，解决方法就油然而生了。只要我们把这一段scala代码在python中调用，并将describeTopics的Java对象传入，不就万事大吉了吗？\n在pyspark中调用scala代码\n也许还有别的方法，不过这里使用的方法也足够简单。将.scala文件打包成jar后，启动spark时加入参数--driver-class-path /path/to/xxx.jar，便可以将你的scala代码放入Spark运行的虚拟机JVM中，从而让python代码在运行中通过反射机制在SparkContext._jvm里动态获取到你的类与方法：\nfunc = sc._jvm.com.example.YourObject.func\n打包scala代码\n那么，现在的问题就是如何把scala代码打包成jar了。scala虽然也是基于JVM运行的语言，与java非常相似，但是其编译选项中并没有提供将其打包成jar的参数。这里我们用sbt打包它，sbt的下载与安装请自行查阅其他教程，这里就不提供了，官方网站。\n首先编写好你的scala代码，确认没有bug，并在文件开头用package关键字将其封装至包中。接着，请手动建立你的项目目录，并创建如下结构：\n在build.sbt中，请至少进行以下设置\n//项目名\nname := \"Project\"\n\n//项目版本\nversion := \"0.1\"\n\n//scala版本\nscalaVersion := \"2.10.5\"\n\n//jdk版本\njavacOptions ++= Seq(\"-source\", \"1.7\", \"-target\", \"1.7\")\n\n//主函数\nmainClass in Compile := Some(\"YourClass.func\")\n在plugins.sbt中，请加上这一句话，告诉sbt需要这个第三方插件，这是用来打包的\naddSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.5\")\n这些都准备完成后，在terminal里进入你的项目根目录下，输入\nsbt package\n等待打包完成，会有相应提示。更多的打包选项，以及sbt的更多用法，感兴趣可以自行查阅。\n解决我们的问题\n回到我们这里的问题，我们希望能在python中对describeTopics的返回值进行转换，那么我么只需要打包那一个重载的describeTopics就好了，这样可以避免打包Spark的第三方包。更改一下函数的返回值，并注释掉调用Spark的SerDe进行序列化的语句，最终的代码如下：\npackage com.sysu.sparkhelper\n\nimport java.util.List\nimport scala.collection.JavaConverters\n\nobject LdaHelper {\n    def convert(topics: Array[(Array[Int], Array[Double])]): List[Array[Any]] = {\n        val result = topics.map { case (terms, termWeights) =>\n          val jTerms = JavaConverters.seqAsJavaListConverter(terms).asJava\n          val jTermWeights = JavaConverters.seqAsJavaListConverter(termWeights).asJava\n          Array[Any](jTerms, jTermWeights)\n        }\n        return JavaConverters.seqAsJavaListConverter(result).asJava\n        // SerDe.dumps(JavaConverters.seqAsJavaListConverter(result).asJava)\n    }\n}\n用sbt打包完成后，使用--driver-class-path添加jar包，在python中相应代码为：\nlda_java_model = model._java_model\nfunc = getattr(model._java_model, 'describeTopics')\nresult = func(_py2java(sc, 10))\ntopics = _java2py(sc, sc._jvm.com.sysu.sparkhelper.LdaHelper.convert(result))\n总结\n这算是阅读源码的一次应用，可以说还是解决了遇到的问题，同时也加深了对Spark的了解。本来做并行化就是希望效率更高，pyspark却在调用scala代码，同时进行了很多数据转换。想要更好的使用Spark的话，使用scala去编程应该才是最好的。\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "1"}