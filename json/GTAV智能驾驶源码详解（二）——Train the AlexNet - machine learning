{"title": "GTAV智能驾驶源码详解（二）——Train the AlexNet - machine learning ", "index": "python", "content": "GTAV智能驾驶源码详解（二）——Train the AlexNet\n模型简介：\n本AI（ScooterV2）使用AlexNet进行图像分类（前进、左转、右转）。Alexnet是一个经典的卷积神经网络，有5个卷积层，其后为3个全连接层，最后的输出激活函数为分类函数softmax。其性能超群，在2012年ImageNet图像识别比赛上展露头角，是当时的冠军Model，由SuperVision团队开发，领头人物为AI教父Jeff Hinton。\n网络结构如图1所示：\n图1 AlexNet示意图\n定义模型：\n#导入依赖库（tflearn backended）\nimport tflearn\nfrom tflearn.layers.conv import conv_2d, max_pool_2d\nfrom tflearn.layers.core import input_data, dropout, fully_connected\nfrom tflearn.layers.estimator import regression\nfrom tflearn.layers.normalization import local_response_normalization\nfrom collections import Counter\nfrom numpy.random import shuffle\nimport numpy as np\nimport numpy as np\nimport pandas as pd\n\n#定义AlexNet模型\ndef alexnet(width, height, lr):\n    network = input_data(shape=[None, width, height, 1], name='input')\n    network = conv_2d(network, 96, 11, strides=4, activation='relu')\n    network = max_pool_2d(network, 3, strides=2)\n    network = local_response_normalization(network)\n    network = conv_2d(network, 256, 5, activation='relu')\n    network = max_pool_2d(network, 3, strides=2)\n    network = local_response_normalization(network)\n    network = conv_2d(network, 384, 3, activation='relu')\n    network = conv_2d(network, 384, 3, activation='relu')\n    network = conv_2d(network, 256, 3, activation='relu')\n    network = max_pool_2d(network, 3, strides=2)\n    network = local_response_normalization(network)\n    network = fully_connected(network, 4096, activation='tanh')\n    network = dropout(network, 0.5)\n    network = fully_connected(network, 4096, activation='tanh')\n    network = dropout(network, 0.5)\n    network = fully_connected(network, 3, activation='softmax')\n    network = regression(network, \n                         optimizer='momentum',\n                         loss='categorical_crossentropy',\n                         learning_rate=lr, \n                         name='targets')\n\n    model = tflearn.DNN(network, \n                        checkpoint_path='model_alexnet',\n                        max_checkpoints=1, \n                        tensorboard_verbose=2, \n                        tensorboard_dir='log')\n\n    return model\nAlexNet注释：\n\n模型传入参数为3个：图像的长度、宽度和梯度下降学习率；\n模型的Input Layer接受数据集中的图片作为输入，图像长度160，高度为90，channel数量为三，输入数据为m*160*90*3的张量，m为一次英处理的样本数量；\nInput Layer后跟着第一个卷积层，卷积核数量为96，卷积核尺寸为11*11，卷积步长为4，该卷积层使用ReLu作为激活函数；\n第一个卷积层后跟着第一个池化层，池化类型为MaxPooling，池化尺寸3*3，池化步长为2；\n池化之后的结果通向LRN层，jeff hinton的标注为模拟大脑的侧向抑制，对张量中的每个元素都用它和它相邻feature map的元素的平均值代替（虽然好像并没有什么用），具体原理如图2；\n之后是AlexNet的第二个卷积层，卷积核数量为256，卷积核尺寸为5*5，卷积步长默认为1，该卷积层依然使用ReLu作为激活函数；\n之后的池化层依旧为MaxPooling，池化尺寸3*3，池化步长为2；\n第二个LRN层，作用同上；\n之后是三个接连的卷积层，卷积核数量依次为384、384、256，卷积核尺寸都为3*3，都用ReLu作为激活函数；\n经过尺寸为3*3，步长为2的池化层和一个LRN层之后，卷积网络部分结束，通向3个全连接层。前两个全连接层都向后输出长度为4096的向量，使用tanh作为非线性激活函数，都有50%的dropout概率，神经元有二分之一的可能性被deactivate；第三个全连接层为输出层，输出3维向量，并使用softmax作为分类的激活函数。\n每一次前向传播完成后，使用交叉熵cross_entropy作为卷积网络的loss函数；整个神经网络使用momentum作为优化（梯度下降加上momentum过滤由于lr过高引起的振荡），个人觉得使用Adam也许效果会更好，收敛会更快。\n\n图2 Local_Response_Normolization示意图\n卷积网络的训练：\nWIDTH = 160\nHEIGHT = 90\nLR = 1e-3\nEPOCHS = 10\nMODEL_NAME = 'scooterv2.model'\nmodel = alexnet(WIDTH, HEIGHT, LR)\n\ntrain_data = np.load('training_data_after_shuffle.npy')\ntrain = train_data[:-1000]\ntest = train_data[-1000:]\n\nX = np.array([i[0] for i in train]).reshape(-1,WIDTH,HEIGHT,1)\nY = [i[1] for i in train]\n\ntest_x = np.array([i[0] for i in test]).reshape(-1,WIDTH,HEIGHT,1)\ntest_y = [i[1] for i in test]\n\nfor index in range(1,200):\n\n    \n    model.fit({'input': X}, \n              {'targets': Y}, \n              n_epoch=EPOCHS, \n              validation_set=({'input': test_x}, {'targets': test_y}), \n              snapshot_step=500, \n              show_metric=True, \n              run_id=MODEL_NAME)\n    model.save(MODEL_NAME)\n学习率为0.001，for循环中每一次迭代训练的epoch数量为10，mini_batch的样本数量使用默认值64；数据集的后1000个作为validation set，剩余的都作为测试集使用。跑一次一共训练了200*10=2000次，但实际上参数更新了20万次，每一次mini_batch都更新一次参数。\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "1"}