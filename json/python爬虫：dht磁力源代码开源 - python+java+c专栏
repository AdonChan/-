{"title": "python爬虫：dht磁力源代码开源 - python+java+c专栏 ", "index": "磁力链接,dht爬虫,python", "content": "之前我在写百度网盘爬虫，百度图片爬虫的时候答应网友说，抽时间要把ok搜搜的的源码公开，如今是时候兑现诺言了，下面就是爬虫的所有代码，完全，彻底的公开，你会不会写程序都可以使用，不过请先装个linux系统，具备公网条件，然后运行：\npython startCrawler.py\n有必要提醒你，数据库字段代码中都有，请你自己建张表格，这个太简单了，就不多说了。同时我也提供一下下载地址，源码都在：下载地址1 下载地址2\n\n#!/usr/bin/env python\n# encoding: utf-8\n\n\"\"\"\nauthor:haoning\ncreate time:2015.8.1\n\"\"\"\n\nimport hashlib\nimport os\nimport time\nimport datetime\nimport traceback\nimport sys\nimport random\nimport json\nimport socket\nimport threading\nfrom hashlib import sha1 #进行hash加密\nfrom random import randint\nfrom struct import unpack\nfrom socket import inet_ntoa\nfrom threading import Timer, Thread\nfrom time import sleep\nfrom collections import deque\nfrom Queue import Queue\n \nimport MySQLdb as mdb  #数据库连接器\n \nimport metautils\nimport downloadTorrent\nfrom bencode import bencode, bdecode\nimport pygeoip\n \nDB_HOST = '127.0.0.1'\nDB_USER = 'root'\nDB_PASS = 'root'\n \nBOOTSTRAP_NODES = (\n    (\"67.215.246.10\", 6881),\n    (\"82.221.103.244\", 6881),\n    (\"23.21.224.150\", 6881)\n)\nRATE = 1 #调控速率\nTID_LENGTH = 2\nRE_JOIN_DHT_INTERVAL = 3\nTOKEN_LENGTH = 2\nINFO_HASH_LEN = 500000 #50w数据很小，限制内存不至于消耗太大\nCACHE_LEN = 100 #更新数据库缓存\nWAIT_DOWNLOAD = 80\n \n \ngeoip = pygeoip.GeoIP('GeoIP.dat')\n \ndef is_ip_allowed(ip):\n    country = geoip.country_code_by_addr(ip)\n    if country in ('CN','TW','JP','HK', 'KR'):\n        return True\n    return False\n \ndef entropy(length):\n    return \"\".join(chr(randint(0, 255)) for _ in xrange(length))\n \ndef random_id():\n    h = sha1()\n    h.update(entropy(20))\n    return h.digest()\n \n \ndef decode_nodes(nodes):\n    n = []\n    length = len(nodes)\n    if (length % 26) != 0:\n        return n\n \n    for i in range(0, length, 26):\n        nid = nodes[i:i+20]\n        ip = inet_ntoa(nodes[i+20:i+24])\n        port = unpack(\"!H\", nodes[i+24:i+26])[0]\n        n.append((nid, ip, port))\n \n    return n\n \n \ndef timer(t, f):\n    Timer(t, f).start()\n \n \ndef get_neighbor(target, nid, end=10):\n    return target[:end]+nid[end:]\n \n \nclass KNode(object):\n \n    def __init__(self, nid, ip, port):\n        self.nid = nid\n        self.ip = ip\n        self.port = port\n \n \nclass DHTClient(Thread):\n \n    def __init__(self, max_node_qsize):\n        Thread.__init__(self)\n        self.setDaemon(True)\n        self.max_node_qsize = max_node_qsize\n        self.nid = random_id()\n        self.nodes = deque(maxlen=max_node_qsize)\n \n    def send_krpc(self, msg, address):\n        try:\n            self.ufd.sendto(bencode(msg), address)\n        except Exception:\n            pass\n \n    def send_find_node(self, address, nid=None):\n        nid = get_neighbor(nid, self.nid) if nid else self.nid\n        tid = entropy(TID_LENGTH)\n        msg = {\n            \"t\": tid,\n            \"y\": \"q\",\n            \"q\": \"find_node\",\n            \"a\": {\n                \"id\": nid,\n                \"target\": random_id()\n            }\n        }\n        self.send_krpc(msg, address)\n \n    def join_DHT(self):\n        for address in BOOTSTRAP_NODES:\n            self.send_find_node(address)\n \n    def re_join_DHT(self):\n        if len(self.nodes) == 0:\n            self.join_DHT()\n        timer(RE_JOIN_DHT_INTERVAL, self.re_join_DHT)\n \n    def auto_send_find_node(self):\n        wait = 1.0 / self.max_node_qsize\n        while True:\n            try:\n                node = self.nodes.popleft()\n                self.send_find_node((node.ip, node.port), node.nid)\n            except IndexError:\n                pass\n            try:\n                sleep(wait)\n            except KeyboardInterrupt:\n                os._exit(0)\n \n    def process_find_node_response(self, msg, address):\n        nodes = decode_nodes(msg[\"r\"][\"nodes\"])\n        for node in nodes:\n            (nid, ip, port) = node\n            if len(nid) != 20: continue\n            if ip == self.bind_ip: continue\n            n = KNode(nid, ip, port)\n            self.nodes.append(n)\n \n \nclass DHTServer(DHTClient): #获得info_hash\n \n    def __init__(self, master, bind_ip, bind_port, max_node_qsize):\n        DHTClient.__init__(self, max_node_qsize)\n \n        self.master = master\n        self.bind_ip = bind_ip\n        self.bind_port = bind_port\n        self.speed=0\n \n        self.process_request_actions = {\n            \"get_peers\": self.on_get_peers_request,\n            \"announce_peer\": self.on_announce_peer_request,\n        }\n \n        self.ufd = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)\n        self.ufd.bind((self.bind_ip, self.bind_port))\n \n        timer(RE_JOIN_DHT_INTERVAL, self.re_join_DHT)\n \n \n    def run(self):\n        self.re_join_DHT()\n        while True:\n            try:\n                (data, address) = self.ufd.recvfrom(65536)\n                msg = bdecode(data)\n                self.on_message(msg, address)\n            except Exception:\n                pass\n \n    def on_message(self, msg, address):\n        global RATE #设为全局量\n        try:\n            if msg[\"y\"] == \"r\":\n                if msg[\"r\"].has_key(\"nodes\"):\n                    self.process_find_node_response(msg, address) #发现节点\n            elif msg[\"y\"] == \"q\":\n                try:\n                    self.speed+=1\n                    if self.speed % 10000 ==0:\n                        RATE=random.randint(1,3)\n                        if RATE==2:\n                            RATE=1\n                        if RATE==3:\n                            RATE=10\n                        if self.speed>100000:\n                            self.speed=0\n                    if self.speed % RATE==0: #数据过多，占用cpu太多，划分限速,1,1,10\n                        self.process_request_actions[msg[\"q\"]](msg, address) #处理其他节点的请求，这个过程获取info_hash\n                    #self.process_request_actions[msg[\"q\"]](msg, address) #处理其他节点的请求，这个过程获取info_hash\n                except KeyError:\n                    self.play_dead(msg, address)\n        except KeyError:\n            pass\n \n    def on_get_peers_request(self, msg, address):\n        try:\n            infohash = msg[\"a\"][\"info_hash\"]\n            tid = msg[\"t\"]\n            nid = msg[\"a\"][\"id\"]\n            token = infohash[:TOKEN_LENGTH]\n            msg = {\n                \"t\": tid,\n                \"y\": \"r\",\n                \"r\": {\n                    \"id\": get_neighbor(infohash, self.nid),\n                    \"nodes\": \"\",\n                    \"token\": token\n                }\n            }\n            self.master.log(infohash, address)\n            self.send_krpc(msg, address)\n        except KeyError:\n            pass\n \n    def on_announce_peer_request(self, msg, address):\n        try:\n            infohash = msg[\"a\"][\"info_hash\"]\n            token = msg[\"a\"][\"token\"]\n            nid = msg[\"a\"][\"id\"]\n            tid = msg[\"t\"]\n \n            if infohash[:TOKEN_LENGTH] == token:\n                if msg[\"a\"].has_key(\"implied_port \") and msg[\"a\"][\"implied_port \"] != 0:\n                    port = address[1]\n                else:\n                    port = msg[\"a\"][\"port\"]\n                self.master.log_announce(infohash, (address[0], port))\n        except Exception:\n            print 'error'\n            pass\n        finally:\n            self.ok(msg, address)\n \n    def play_dead(self, msg, address):\n        try:\n            tid = msg[\"t\"]\n            msg = {\n                \"t\": tid,\n                \"y\": \"e\",\n                \"e\": [202, \"Server Error\"]\n            }\n            self.send_krpc(msg, address)\n        except KeyError:\n            pass\n \n    def ok(self, msg, address):\n        try:\n            tid = msg[\"t\"]\n            nid = msg[\"a\"][\"id\"]\n            msg = {\n                \"t\": tid,\n                \"y\": \"r\",\n                \"r\": {\n                    \"id\": get_neighbor(nid, self.nid)\n                }\n            }\n            self.send_krpc(msg, address)\n        except KeyError:\n            pass\n \n \nclass Master(Thread): #解析info_hash\n \n    def __init__(self):\n        Thread.__init__(self)\n        self.setDaemon(True)\n        self.queue = Queue()\n        self.cache = Queue()\n        self.count=0\n        self.mutex = threading.RLock() #可重入锁，使单线程可以再次获得已经获得的?\n        self.waitDownload = Queue()\n        self.metadata_queue = Queue()\n        self.dbconn = mdb.connect(DB_HOST, DB_USER, DB_PASS, 'oksousou', charset='utf8')\n        self.dbconn.autocommit(False)\n        self.dbcurr = self.dbconn.cursor()\n        self.dbcurr.execute('SET NAMES utf8')\n        self.visited = set()\n                 \n    def lock(self): #加锁\n        self.mutex.acquire()\n \n    def unlock(self): #解锁\n        self.mutex.release()\n         \n    def work(self,item):\n \n        print \"start thread\",item\n        while True:\n            self.prepare_download_metadata()\n            self.lock()\n            self.download_metadata()\n            self.unlock()\n \n            self.lock()\n            self.got_torrent()\n            self.unlock()\n                     \n    def start_work(self,max):\n     \n        for item in xrange(max):\n            t = threading.Thread(target=self.work, args=(item,))\n            t.setDaemon(True)\n            t.start()\n         \n    #入队的种子效率更高\n    def log_announce(self, binhash, address=None):\n        if self.queue.qsize() < INFO_HASH_LEN : #大于INFO_HASH_LEN就不要入队，否则后面来不及处理\n            if is_ip_allowed(address[0]):\n                self.queue.put([address, binhash]) #获得info_hash\n         \n    def log(self, infohash, address=None):\n        if self.queue.qsize() < INFO_HASH_LEN: #大于INFO_HASH_LEN/2就不要入队，否则后面来不及处理\n            if is_ip_allowed(address[0]):\n                self.queue.put([address, infohash])\n     \n    def prepare_download_metadata(self):\n         \n        if self.queue.qsize() == 0:\n            sleep(2)\n        #从queue中获得info_hash用来下载\n        address, binhash= self.queue.get() \n        if binhash in self.visited:\n            return\n        if len(self.visited) > 100000: #大于100000重置队列,认为已经访问过了\n            self.visited = set()\n        self.visited.add(binhash)\n        #跟新已经访问过的info_hash\n        info_hash = binhash.encode('hex')\n        utcnow = datetime.datetime.utcnow()\n         \n        self.cache.put((address,binhash,utcnow)) #装入缓存队列\n     \n    def download_metadata(self):\n     \n        if self.cache.qsize() > CACHE_LEN/2: #出队更新下载\n            while self.cache.qsize() > 0: #排空队列\n                address,binhash,utcnow = self.cache.get()\n                info_hash = binhash.encode('hex')\n                self.dbcurr.execute('SELECT id FROM search_hash WHERE info_hash=%s', (info_hash,))\n                y = self.dbcurr.fetchone()\n                if y:\n                # 更新最近发现时间，请求数\n                    self.dbcurr.execute('UPDATE search_hash SET last_seen=%s, requests=requests+1 WHERE info_hash=%s', (utcnow, info_hash))\n                else: \n                    self.waitDownload.put((address, binhash))\n            self.dbconn.commit()\n            if self.waitDownload.qsize() > WAIT_DOWNLOAD:\n                while self.waitDownload.qsize() > 0:\n                    address,binhash = self.waitDownload.get()\n                    t = threading.Thread(target=downloadTorrent.download_metadata, args=(address, binhash, self.metadata_queue))\n                    t.setDaemon(True)\n                    t.start()\n \n    def decode(self, s):\n        if type(s) is list:\n            s = ';'.join(s)\n        u = s\n        for x in (self.encoding, 'utf8', 'gbk', 'big5'):\n            try:\n                u = s.decode(x)\n                return u\n            except:\n                pass\n        return s.decode(self.encoding, 'ignore')\n \n    def decode_utf8(self, d, i):\n        if i+'.utf-8' in d:\n            return d[i+'.utf-8'].decode('utf8')\n        return self.decode(d[i])\n     \n    def parse_metadata(self, data): #解析种子\n        info = {}\n        self.encoding = 'utf8'\n        try:\n            torrent = bdecode(data) #编码后解析\n            if not torrent.get('name'):\n                return None\n        except:\n            return None\n        detail = torrent\n        info['name'] = self.decode_utf8(detail, 'name')\n        if 'files' in detail:\n            info['files'] = []\n            for x in detail['files']:\n                if 'path.utf-8' in x:\n                    v = {'path': self.decode('/'.join(x['path.utf-8'])), 'length': x['length']}\n                else:\n                    v = {'path': self.decode('/'.join(x['path'])), 'length': x['length']}\n                if 'filehash' in x:\n                    v['filehash'] = x['filehash'].encode('hex')\n                info['files'].append(v)\n            info['length'] = sum([x['length'] for x in info['files']])\n        else:\n            info['length'] = detail['length']\n        info['data_hash'] = hashlib.md5(detail['pieces']).hexdigest()\n        return info\n \n    def got_torrent(self):\n        if self.metadata_queue.qsize() == 0:\n            return\n        binhash, address, data,start_time = self.metadata_queue.get()\n        if not data:\n            return\n        try:\n            info = self.parse_metadata(data)\n            if not info:\n                return\n        except:\n            traceback.print_exc()\n            return\n \n        temp = time.time()\n        x = time.localtime(float(temp))\n        utcnow = time.strftime(\"%Y-%m-%d %H:%M:%S\",x) # get time now\n         \n        info_hash = binhash.encode('hex') #磁力\n        info['info_hash'] = info_hash\n        # need to build tags\n        info['tagged'] = False\n        info['classified'] = False\n        info['requests'] = 1\n        info['last_seen'] = utcnow\n        info['create_time'] = utcnow\n        info['source_ip'] = address[0]\n         \n        if info.get('files'):\n            files = [z for z in info['files'] if not z['path'].startswith('_')]\n            if not files:\n                files = info['files']\n        else:\n            files = [{'path': info['name'], 'length': info['length']}]\n        files.sort(key=lambda z:z['length'], reverse=True)\n        bigfname = files[0]['path']\n        info['extension'] = metautils.get_extension(bigfname).lower()\n        info['category'] = metautils.get_category(info['extension'])\n \n        try:\n            try:\n                print '\\n', 'Saved', info['info_hash'], info['name'], (time.time()-start_time), 's', address[0]\n            except:\n                print '\\n', 'Saved', info['info_hash']\n            ret = self.dbcurr.execute('INSERT INTO search_hash(info_hash,category,data_hash,name,extension,classified,source_ip,tagged,' + \n                'length,create_time,last_seen,requests) VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)',\n                (info['info_hash'], info['category'], info['data_hash'], info['name'], info['extension'], info['classified'],\n                info['source_ip'], info['tagged'], info['length'], info['create_time'], info['last_seen'], info['requests']))\n            if self.count %50 ==0:\n                self.dbconn.commit()\n                if self.count>100000:\n                    self.count=0\n        except:\n            print self.name, 'save error', self.name, info\n            traceback.print_exc()\n            return\n \nif __name__ == \"__main__\":\n     \n    #启动客户端\n    master = Master()\n    master.start_work(150)\n     \n    #启动服务器\n    dht = DHTServer(master, \"0.0.0.0\", 6881, max_node_qsize=200)\n    dht.start()\n    dht.auto_send_find_node()\n注意，上面的代码有一段代码需要下载种子，所以下面的这段代码十分重要：\n#!/usr/bin/env python\n# encoding: utf-8\n\n\"\"\"\nauthor:haoning\ncreate time:2015.8.1\n\"\"\"\n\nfrom hashlib import sha1\nimport math\nfrom socket import inet_ntoa\nimport socket\nfrom struct import pack, unpack\nfrom threading import Timer, Thread\nfrom time import sleep, time\n \nfrom bencode import bencode, bdecode \nfrom startCrawler import entropy\n \n \nBT_PROTOCOL = \"BitTorrent protocol\"\nBT_MSG_ID = 20\nEXT_HANDSHAKE_ID = 0\n \ndef random_id():\n    hash = sha1()\n    hash.update(entropy(20))\n    return hash.digest()\n \ndef send_packet(the_socket, msg):\n    the_socket.send(msg)\n \ndef send_message(the_socket, msg):\n    msg_len = pack(\">I\", len(msg))\n    send_packet(the_socket, msg_len + msg)\n \ndef send_handshake(the_socket, infohash):\n    bt_header = chr(len(BT_PROTOCOL)) + BT_PROTOCOL\n    ext_bytes = \"\\x00\\x00\\x00\\x00\\x00\\x10\\x00\\x00\"\n    peer_id = random_id()\n    packet = bt_header + ext_bytes + infohash + peer_id\n \n    send_packet(the_socket, packet)\n \ndef check_handshake(packet, self_infohash):\n    try:\n        bt_header_len, packet = ord(packet[:1]), packet[1:]\n        if bt_header_len != len(BT_PROTOCOL):\n            return False\n    except TypeError:\n        return False\n \n    bt_header, packet = packet[:bt_header_len], packet[bt_header_len:]\n    if bt_header != BT_PROTOCOL:\n        return False\n \n    packet = packet[8:]\n    infohash = packet[:20]\n    if infohash != self_infohash:\n        return False\n \n    return True\n \ndef send_ext_handshake(the_socket):\n    msg = chr(BT_MSG_ID) + chr(EXT_HANDSHAKE_ID) + bencode({\"m\":{\"ut_metadata\": 1}})\n    send_message(the_socket, msg)\n \ndef request_metadata(the_socket, ut_metadata, piece):\n    \"\"\"bep_0009\"\"\"\n    msg = chr(BT_MSG_ID) + chr(ut_metadata) + bencode({\"msg_type\": 0, \"piece\": piece})\n    send_message(the_socket, msg)\n \ndef get_ut_metadata(data):\n    ut_metadata = \"_metadata\"\n    index = data.index(ut_metadata)+len(ut_metadata) + 1\n    return int(data[index])\n \ndef get_metadata_size(data):\n    metadata_size = \"metadata_size\"\n    start = data.index(metadata_size) + len(metadata_size) + 1\n    data = data[start:]\n    return int(data[:data.index(\"e\")])\n \ndef recvall(the_socket, timeout=5):\n    the_socket.setblocking(0)\n    total_data = []\n    data = \"\"\n    begin = time()\n \n    while True:\n        sleep(0.05)\n        if total_data and time()-begin > timeout:\n            break\n        elif time()-begin > timeout*2:\n            break\n        try:\n            data = the_socket.recv(1024)\n            if data:\n                total_data.append(data)\n                begin = time()\n        except Exception:\n            pass\n    return \"\".join(total_data)\n \ndef download_metadata(address, infohash, metadata_queue, timeout=5):\n    metadata = None\n    start_time = time()\n    the_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try: \n        the_socket.settimeout(timeout)\n        the_socket.connect(address)\n \n        # handshake\n        send_handshake(the_socket, infohash)\n        packet = the_socket.recv(4096)\n \n        # handshake error\n        if not check_handshake(packet, infohash):\n            return\n \n        # ext handshake\n        send_ext_handshake(the_socket)\n        packet = the_socket.recv(4096)\n \n        # get ut_metadata and metadata_size\n        ut_metadata, metadata_size = get_ut_metadata(packet), get_metadata_size(packet)\n \n        # request each piece of metadata\n        metadata = []\n        for piece in range(int(math.ceil(metadata_size/(16.0*1024)))): #piece是个控制块，根据控制块下载数据\n            request_metadata(the_socket, ut_metadata, piece)\n            packet = recvall(the_socket, timeout) #the_socket.recv(1024*17)\n            metadata.append(packet[packet.index(\"ee\")+2:])        \n        metadata = \"\".join(metadata)\n \n    except socket.timeout:\n        pass\n    except Exception, e:\n        pass\n    finally:\n        #print \"metadata= %s\" %(metadata)\n        the_socket.close() #确保没回都关闭socket\n        if metadata != None: #只让不空的种子入?            \n            metadata_queue.put((infohash, address, metadata,start_time))\n其实下载种子还有一种方式就是借助libtorrent，但这个太耗费cpu了，所以我一般不用他，如下：\n#coding: utf8\nimport threading\nimport traceback\nimport random\nimport time\nimport os\nimport socket\n \nimport libtorrent as lt\n \nthreading.stack_size(200*1024)\nsocket.setdefaulttimeout(30)\n \ndef fetch_torrent(session, ih, timeout):\n    name = ih.upper()\n    url = 'magnet:?xt=urn:btih:%s' % (name,)\n    data = ''\n    params = {\n        'save_path': '/tmp/downloads/',\n        'storage_mode': lt.storage_mode_t(2),\n        'paused': False,\n        'auto_managed': False,\n        'duplicate_is_error': True}\n    try:\n        handle = lt.add_magnet_uri(session, url, params)\n    except:\n        return None\n    status = session.status()\n    handle.set_sequential_download(1)\n    meta = None\n    down_time = time.time()\n    down_path = None\n    for i in xrange(0, timeout):\n        if handle.has_metadata():\n            info = handle.get_torrent_info()\n            down_path = '/tmp/downloads/%s' % info.name()\n            #print 'status', 'p', status.num_peers, 'g', status.dht_global_nodes, 'ts', status.dht_torrents, 'u', status.total_upload, 'd', status.total_download\n            meta = info.metadata()\n            break\n        time.sleep(1)\n    if down_path and os.path.exists(down_path):\n        os.system('rm -rf \"%s\"' % down_path)\n    session.remove_torrent(handle)\n    return meta\n \n \ndef download_metadata(address, binhash, metadata_queue, timeout=20):\n    metadata = None\n    start_time = time.time()\n    try:\n        session = lt.session()\n        r = random.randrange(10000, 50000)\n        session.listen_on(r, r+10)\n        session.add_dht_router('router.bittorrent.com',6881)\n        session.add_dht_router('router.utorrent.com',6881)\n        session.add_dht_router('dht.transmission.com',6881)\n        session.add_dht_router('127.0.0.1',6881)\n        session.start_dht()\n        metadata = fetch_torrent(session, binhash.encode('hex'), timeout)\n        session = None\n    except:\n        traceback.print_exc()\n    finally:\n        metadata_queue.put((binhash, address, metadata,start_time))\n这个爬虫还是耗费了本人和其他网上高手的很多时间的，请看到这篇博客的朋友保持钻研精神，开源精神，多多交流，秉承分享。本人建立个qq群作为去转盘网的官方群，人数现在也不多，如果有兴趣的话来逛逛吧，多个粉丝去转盘多一份热闹，qq群号：512245829\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "15"}