{"title": "TensorFlow学习笔记（10）：读取文件 - 数据实验室 ", "index": "python,tensorflow", "content": "简介\nTensorFlow读取数据共有三种方法：\n\nFeeding：当TensorFlow运行每步计算的时候，从Python获取数据。在Graph的设计阶段，用placeholder占住Graph的位置，完成Graph的表达；当Graph传给Session后，在运算时再把需要的数据从Python传过来。\nPreloaded data：数据直接预加载到TensorFlow的Graph中，再把Graph传入Session运行。只适用于小数据。\nReading from file：在Graph中定义好文件读取的运算节点，把Graph传入Session运行时，执行读取文件的运算，这样可以避免在Python和TensorFlow C++执行环境之间反复传递数据。\n\n本文讲解Reading from file的代码。\n其他关于TensorFlow的学习笔记，请点击入门教程\n实现\n#!/usr/bin/env python\n# -*- coding=utf-8 -*-\n# @author: 陈水平\n# @date: 2017-02-19\n# @description: modified program to illustrate reading from file based on TF offitial tutorial\n# @ref: https://www.tensorflow.org/programmers_guide/reading_data\n\ndef read_my_file_format(filename_queue):\n  \"\"\"从文件名队列读取一行数据\n  \n  输入：\n  -----\n  filename_queue：文件名队列，举个例子，可以使用`tf.train.string_input_producer([\"file0.csv\", \"file1.csv\"])`方法创建一个包含两个CSV文件的队列\n  \n  输出：\n  -----\n  一个样本：`[features, label]`\n  \"\"\"\n  reader = tf.SomeReader()  # 创建Reader\n  key, record_string = reader.read(filename_queue)  # 读取一行记录\n  example, label = tf.some_decoder(record_string)  # 解析该行记录\n  processed_example = some_processing(example)  # 对特征进行预处理\n  return processed_example, label\n\ndef input_pipeline(filenames, batch_size, num_epochs=None):\n  \"\"\" 从一组文件中读取一个批次数据\n  \n  输入：\n  -----\n  filenames：文件名列表，如`[\"file0.csv\", \"file1.csv\"]`\n  batch_size：每次读取的样本数\n  num_epochs：每个文件的读取次数\n  \n  输出：\n  -----\n  一批样本，`[[example1, label1], [example2, label2], ...]`\n  \"\"\"\n  filename_queue = tf.train.string_input_producer(filenames, num_epochs=num_epochs, shuffle=True)  # 创建文件名队列\n  example, label = read_my_file_format(filename_queue)  # 读取一个样本\n  # 将样本放进样本队列，每次输出一个批次样本\n  #   - min_after_dequeue：定义输出样本后的队列最小样本数，越大随机性越强，但start up时间和内存占用越多\n  #   - capacity：队列大小，必须比min_after_dequeue大\n  min_after_dequeue = 10000\n  capacity = min_after_dqueue + 3 * batch_size\n  example_batch, label_batch = tf.train.shuffle_batch(\n    [example, label], batch_size=batch_size, capacity=capacity,\n    min_after_dequeue=min_after_dequeue)\n  return example_batch, label_batch\n  \ndef main(_):\n  x, y = input_pipeline(['file0.csv', 'file1.csv'], 1000, 5)\n  train_op = some_func(x, y)\n  init_op = tf.global_variables_initializer()\n  local_init_op = tf.local_variables_initializer()  # local variables like epoch_num, batch_size\n  sess = tf.Session()\n  \n  sess.run(init_op)\n  sess.run(local_init_op)\n  \n  # `QueueRunner`用于创建一系列线程，反复地执行`enqueue` op\n  # `Coordinator`用于让这些线程一起结束\n  # 典型应用场景：\n  #   - 多线程准备样本数据，执行enqueue将样本放进一个队列\n  #   - 一个训练线程从队列执行dequeu获取一批样本，执行training op\n  # `tf.train`的许多函数会在graph中添加`QueueRunner`对象，如`tf.train.string_input_producer`\n  # 在执行training op之前，需要保证Queue里有数据，因此需要先执行`start_queue_runners`\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n  \n  try:\n    while not coord.should_stop():\n      sess.run(train_op)\n  except tf.errors.OutOfRangeError:\n    print 'Done training -- epoch limit reached'\n  finally:\n    coord.request_stop()\n  \n  # Wait for threads to finish  \n  coord.join(threads)\n  sess.close()\n  \nif __name__ == '__main__':\n  tf.app.run()\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "2"}