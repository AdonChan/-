{"title": "深度感知＋深度学习，伯克利的机器人面对陌生目标也能成功取物 - 前端学习 ", "index": "人工智能,深度学习,python", "content": "\n编者按：关于训练机器人抓手的研究并不少，大多都是从计算机视觉的角度出发，训练机器人“看得清”、“抓得准”。本文同样如此，不过与以往观察彩色图片不同，伯克利的研究者们借助“深度图像”这个“利器”，提出了一种更加高效的方法，能让机器人成功抓起此前并未见过的物体。\n\n左：3D立方体。右：对应深度图像，距相机越近颜色越深。\n\n早在AlexNet诞生的两年前，微软就为X-Box推出了Kinect。随着深度学习加速了超参数函数的性能，这种低成本的深度感知器层出不穷，也使得深度学习在图像分类、语音识别和语言翻译中取得了惊人的效果。如今，深度学习在端到端的电子游戏、机器人操控等问题中也表现出大有前景的势头。\n在机器人感知方面，类似于VGG或ResNet的卷积神经网络成为了主流选择。在一些机器人或计算机视觉的任务中，常会用到这些框架，附带有经过与训练的权重，进行迁移学习或对具体数据进行微调。但是在某些任务中，只了解图像的颜色是很有限的。当你想训练机器人抓住一个陌生物体时，更重要的是让机器人了解周围环境的几何结构，而不仅仅是颜色和材质。对目标物体进行控制时的物理过程，即通过力量控制一个或多个物体，取决于目标的形状、摆放位置和其他和颜色无关的因素。例如，当你手中拿笔时，不用看就能改变手中笔的位置。于是，这里有一个问题：这在彩色图像上也能成立吗？\n与彩色图像相对应的是深度图像，它是只有 单个通道 的 灰度图像 ，可以测量到相机的深度值，让我们了解一幅图像中目标物体的除了颜色以外的特征。我们还可以用深度来“过滤”一定范围之外的点，这可以用来去除背景噪声（如文中开头的图像示例）。\n这篇文章中，我们将深度图像和深度学习结合起来，用在伯克利AUTOLab三个正在进行的项目中：用于机器人抓取的Dex-Net、复杂目标分割以及让机器人整理床铺。\n深度感知简介\n深度图像将物体表面到相机的距离进行编码，显示出了特殊的视角。在文章开头的案例图片里，左边的立方体3D结构图中有很多点都处于离相机不同的位置上。右边的深度图像中，颜色越深的地方表示距离相机越近。\n深度感知最近的成果\n在计算机视觉和深度学习不断进步的同时，深度感知领域也出现了许多成果。\n通常，深度感知会将两个不同相机生成的RGB图像结合在一起，然后利用生成的视差图获取物体在环境中的深度值。\n目前常用的深度传感器是结构光传感器，它可以用一种看不见的波长将一直物体的形状投射到某场景中，比如我们熟知的Kinect。另一种深度感知的方法就是LIDAR，这种技术此前常用于地形测绘，最近在一些自动驾驶汽车上也出现了它的身影。LIDAR比Kinect生成的深度映射质量更高，但是速度较慢、成本高昂，因为它需要扫描激光器。\n总的来说，Kinect属于消费级RGB-D系统，可以通过硬件直接捕捉到RGB图像，以及每个像素的深度值，比此前的很多方法更快更便宜。现在，很多用于研究或工业的机器人，例如AGV或人形辅助机器人，都含有类似的内置深度感知相机。未来用于机器人的深度感知设备很可能会进一步升级。\n相关研究\n针对机器人的深度感知，研究人员将这一技术用于实时导航、实时映射和追踪以及对室内环境的建模。由于深度感知能让机器人知道它们距离障碍物有多远，就能使其进行定位，在导航时避免碰撞。除此之外，深度图像还用于实时检测、辨别、定位人的身体部位等研究中。\n这都说明在某些任务中，深度图像可以蕴涵很多除了颜色之外的有用信息。接下来，我们研究了三种不同任务\n案例一：机器人抓取\n让机器人抓取从未见过的物体是目前一个重要的难题。虽然很多研究者使用RGB图像，但他们的系统需要让机器人训练好几个月的抓取动作。利用3D目标网格的关键有点就是，研究人员可以通过渲染技术精确地合成深度图像。\n我们的Dex-Net是AUTOLab正在进行的研究项目，它包括训练机器人抓取策略的算法、代码。以及用于训练抓取的数据集。Dex-Net提出在抓取状态下的域随机算法，目的是用简单的抓手抓取复杂目标物体。在BAIR此前的博文中，我们介绍了含有670万个样本的数据集，我们用它来训练抓取模型。\n数据集和深度图像\n上图展示了Dex-Net的数据集生成过程。首先，我们从多个来源中得到大量目标物的网格模型，并进行强化。每个模型都会被机械手抓起来进行采样。有了网格模型和被抓起后的图像，我们计算出它的鲁棒性，并生成模拟深度图像。通过计算摆放位置、摩擦力、质量、外力（例如重力）和蒙特卡罗积分法，计算出抓取成功地概率，从而对鲁棒性进行估计。上图右边，我们展示了正采样（抓取成功）和负采样（抓取失败）的例子。\n训练GQ-CNN\n有了模拟数据集后，它们将用来训练一个抓取质量卷积神经网络，来预测机器人抓取成功的概率。结构如图所示，一张图像经过处理后，调整了角度和抓取中心，同时对应的96×96的深度图像被当做输入，高度为z，用于预测抓取的成功概率。\n下图我们展示了Dex-Net用于在某个容器内，对多个目标物体进行抓取的模拟深度图像：\n上行：ABB Yumi机器人的摄像机捕捉到的真实深度图像；下行：Dex-Net的模拟深度图像，红色表示抓取的位置\n案例二：在箱子中分割物体\n实例分割就是判断图像中的像素属于哪个物体，同时也要将同一类别中的每个物体分开。实例分割在机器人感知中很常用。例如，想让机器人从装满物体的纸箱中选择目标物体，首先就要对图片进行分割，定位到目标物体，再进行抓取。\n先前的研究表明，Mask R-CNN可以用于训练对RGB图像的目标分割，但是这一训练需要大量经过手动标记的RGB图像数据集。除此之外，用于训练的图像必须是自然场景下包含有限的目标物体种类。所以，预训练Mask R-CNN网络可能不适用于仓库这种杂乱的场景。\n数据集和深度图像\n上图是数据集的生成过程。和Dex-Net类似，我们对3D目标物体进行采样，然后通过模拟，将这些物体堆放在一个盒子中。生成对应的深度图像，以及用于训练的目标物体掩码和标准评估图像。\n对于基于几何形状的分割，我们可以用模拟和渲染技术，自动收集大量用于训练的数据集和经过标记的深度图像。我们假设，这些深度图像可能含有足够的用于分割的信息，因为各物体之间的像素边界不连贯。最终我们收集了5万张深度图像组成了数据集，并通过PyBullet模拟器将它们汇聚到盒子里。利用这一数据集，我们训练了另一个版本的Mask R-CNN，我们称之为SD Mask R-CNN。\n实际分割结果*虽然没有在真实图像上训练，我们提出的SD Mask R-CNN的表现超过了点云分割和经过改进的Mask R-CNN。如上图所示，我们的模型可以准确进行分割。更重要的是，用于创造手动标签数据集的目标物体并不是从SD Mask R-CNN的训练分布中选择的，而是常见的家用物品，我们并没有它们的3D模型。所以，SD Mask R-CNN可以预测此前从未见过的物体掩码。\n总的来说，我们的分割方法有三大优点：\n深度信息在分离目标或者背景时，其中编码了很多有用信息；\n合成深度图像可以快速生成，用它们训练可以高效地转移到现实图像中；\n用深度图像训练过的网络对此前未见过的物体泛化结果更好\n案例三：让机器人整理床铺\n整理床铺可以运用于家庭机器人身上，因为它没有时间限制，并且可以允许出现小差错。在此前的文章中，我们研究了用RGB图像，将其看作是序列决策问题，实现更好的模拟学习。\n数据集和深度图像\n\n我们将整理床铺的任务看作是检测毯子的四个角，家庭机器人需要抓起毯子，并且把它的角和床对齐。我们最初的假设是深度图像含有足够的有关毯子的几何形状的信息。\n为了手机训练数据，我们使用的是白色的毯子，将四个角用红色标记，如上图所示。重复几次将毯子随意仍在床上，然后从机器人内置的RGB-D传感器中采集RGB图像和深度图像。\n接下来，我们训练一个深度卷积神经网络，只从深度图像中检测它的四个角。我们希望网络可以泛化到能检测出不同毯子的四角。我们的深度网络使用了YOLO中的与训练权重，之后添加了几个图层。结果表明，利用预训练权重是非常有效果的。\n毯子检测结果\n\n我们将训练策略实施之后，模型表现出了优秀的结果，超越了无学习的基准策略，几乎和人类完成的效果相当。虽然我们这里检测的标准是毯子是否最大程度地覆盖了床，不过这也说明，只有完成了精准的检测，才能实现高度覆盖。\n结语\n通过这三个项目的实践，我们的结果表明深度图像在进行物体抓取、图像分割和不规则物体顶点检测三方面，包含了许多有用的线索。我们认为，随着深度相机质量的提高，深度图像对机器人的应用越来越重要。有了深度图像，训练样本的合成更加简单，背景噪音也能更容易地过滤掉。\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "2"}