{"title": "爬虫学习之一个简单的网络爬虫 - sudo rm -rf ", "index": "python", "content": "概述\n这是一个网络爬虫学习的技术分享，主要通过一些实际的案例对爬虫的原理进行分析，达到对爬虫有个基本的认识，并且能够根据自己的需要爬到想要的数据。有了数据后可以做数据分析或者通过其他方式重新结构化展示。\n什么是网络爬虫\n网络爬虫（又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。via 百度百科网络爬虫\n网络蜘蛛（Web spider）也叫网络爬虫（Web crawler），蚂蚁（ant），自动检索工具（automatic indexer），或者（在FOAF软件概念中）网络疾走（WEB scutter），是一种“自动化浏览网络”的程序，或者说是一种网络机器人。它们被广泛用于互联网搜索引擎或其他类似网站，以获取或更新这些网站的内容和检索方式。它们可以自动采集所有其能够访问到的页面内容，以供搜索引擎做进一步处理（分检整理下载的页面），而使得用户能更快的检索到他们需要的信息。via 维基百科网络蜘蛛\n以上是百度百科和维基百科对网络爬虫的定义，简单来说爬虫就是抓取目标网站内容的工具，一般是根据定义的行为自动进行抓取，更智能的爬虫会自动分析目标网站结构类似与搜索引擎的爬虫，我们这里只讨论基本的爬虫原理。\n爬虫工作原理\n网络爬虫框架主要由控制器、解析器和索引库三大部分组成，而爬虫工作原理主要是解析器这个环节，解析器的主要工作是下载网页，进行页面的处理，主要是将一些JS脚本标签、CSS代码内容、空格字符、HTML标签等内容处理掉，爬虫的基本工作是由解析器完成。所以解析器的具体流程是：\n入口访问->下载内容->分析结构->提取内容\n分析爬虫目标结构\n这里我们通过分析一个网站[落网：http://luoo.net] 对网站内容进行提取来进一步了解！\n第一步 确定目的抓取目标网站的某一期所有音乐\n第二步 分析页面结构访问落网的某一期刊，通过Chrome的开发者模式查看播放列表中的歌曲，右侧用红色框线圈出来的是一些需要特别注意的语义结构，见下图所示：\n\n以上红色框线圈出的地方主要有歌曲名称，歌曲的编号等，这里并没有看到歌曲的实际文件地址，所以我们继续查看，点击某一个歌曲就会立即在浏览器中播放，这时我们可以看到在Chrome的开发者模式的Network中看到实际请求的播放文件，如下图所示：\n\n\n根据以上分析我们可以得到播放清单的位置和音乐文件的路径，接下来我们通过Python来实现这个目的。\n实现爬虫\nPython环境安装请自行Google\n主要依赖第三方库\n\nRequests（http://www.python-requests.org） 用来发起请求\nBeautifulSoup（bs4） 用来解析HTML结构并提取内容\nfaker（http://fake-factory.readthedocs.io/en/stable/）用来模拟请求UA（User-Agent）\n\n主要思路是分成两部分，第一部分用来发起请求分析出播放列表然后丢到队列中，第二部分在队列中逐条下载文件到本地，一般分析列表速度更快，下载速度比较慢可以借助多线程同时进行下载。\n主要代码如下：\n#-*- coding: utf-8 -*-\n'''by sudo rm -rf  http://imchenkun.com'''\nimport os\nimport requests\nfrom bs4 import BeautifulSoup\nimport random\nfrom faker import Factory\nimport Queue\nimport threading\n\nfake = Factory.create()\nluoo_site = 'http://www.luoo.net/music/'\nluoo_site_mp3 = 'http://luoo-mp3.kssws.ks-cdn.com/low/luoo/radio%s/%s.mp3'\n\nproxy_ips = [    '27.15.236.236'    ] # 替换自己的代理IP\nheaders = {\n    'Connection': 'keep-alive',\n    'User-Agent': fake.user_agent()\n    }\n\ndef random_proxies():\n    ip_index = random.randint(0, len(proxy_ips)-1)\n    res = { 'http': proxy_ips[ip_index] }\n    return res\n\ndef fix_characters(s):\n    for c in ['<', '>', ':', '\"', '/', '\\\\\\\\', '|', '?', '*']:\n        s = s.replace(c, '')\n    return s\n\n\nclass LuooSpider(threading.Thread):\n    def __init__(self, url, vols, queue=None):\n        threading.Thread.__init__(self)\n        print '[luoo spider]'\n        print '=' * 20\n        self.url = url\n        self.queue = queue\n        self.vol = '1'\n        self.vols = vols\n\n    def run(self):\n        for vol in self.vols:\n            self.spider(vol)\n        print '\\\\ncrawl end\\\\n\\\\n'\n        def spider(self, vol):\n        url = luoo_site + vol\n        print 'crawling: ' + url + '\\\\n'\n        res = requests.get(url, proxies=random_proxies())\n                soup = BeautifulSoup(res.content, 'html.parser')\n        title = soup.find('span', attrs={'class': 'vol-title'}).text\n        cover = soup.find('img', attrs={'class': 'vol-cover'})['src']\n        desc = soup.find('div', attrs={'class': 'vol-desc'})\n        track_names = soup.find_all('a', attrs={'class': 'trackname'})\n        track_count = len(track_names)\n        tracks = []\n        for track in track_names:\n            _id = str(int(track.text[:2])) if (int(vol) < 12) else track.text[:2]  # 12期前的音乐编号1~9是1位（如：1~9），之后的都是2位 1~9会在左边垫0（如：01~09）\n            _name = fix_characters(track.text[4:])\n            tracks.append({'id': _id, 'name': _name})\n            phases = {\n                'phase': vol,                         # 期刊编号\n                'title': title,                       # 期刊标题\n                 'cover': cover,                      # 期刊封面\n                 'desc': desc,                        # 期刊描述\n                 'track_count': track_count,          # 节目数\n                 'tracks': tracks                     # 节目清单(节目编号，节目名称)\n            }\n            self.queue.put(phases)\n\n\nclass LuooDownloader(threading.Thread):\n    def __init__(self, url, dist, queue=None):\n        threading.Thread.__init__(self)\n        self.url = url\n        self.queue = queue\n        self.dist = dist\n        self.__counter = 0       \n\n     def run(self):\n        while True:\n            if self.queue.qsize() <= 0:\n                pass\n            else:\n                phases = self.queue.get()\n                self.download(phases)\n\n    def download(self, phases):\n        for track in phases['tracks']:\n            file_url = self.url % (phases['phase'], track['id'])\n\n            local_file_dict = '%s/%s' % (self.dist, phases['phase'])\n            if not os.path.exists(local_file_dict):\n                os.makedirs(local_file_dict)              \n\n            local_file = '%s/%s.%s.mp3' % (local_file_dict, track['id'], track['name'])\n            if not os.path.isfile(local_file):\n                print 'downloading: ' + track['name']\n                res = requests.get(file_url, proxies=random_proxies(), headers=headers)\n                with open(local_file, 'wb') as f:\n                    f.write(res.content)\n                    f.close()\n                print 'done.\\\\n'\n            else:\n                print 'break: ' + track['name']\n\n\nif __name__ == '__main__':\n    spider_queue = Queue.Queue()\n\n    luoo = LuooSpider(luoo_site, vols=['680', '721', '725', '720'],queue=spider_queue)\n    luoo.setDaemon(True)\n    luoo.start()\n\n    downloader_count = 5\n    for i in range(downloader_count):\n        luoo_download = LuooDownloader(luoo_site_mp3, 'D:/luoo', queue=spider_queue)\n        luoo_download.setDaemon(True)\n        luoo_download.start()\n以上代码执行后结果如下图所示\n\nGithub地址：https://github.com/imchenkun/ick-spider/blob/master/luoospider.py\n总结\n通过本文我们基本了解了网络爬虫的知识，对网络爬虫工作原理认识的同时我们实现了一个真实的案例场景，这里主要是使用一些基础的第三方Python库来帮助我们实现爬虫，基本上演示了网络爬虫框架中基本的核心概念。通常工作中我们会使用一些比较优秀的爬虫框架来快速的实现需求，比如 scrapy框架，接下来我会通过使用Scrapy这类爬虫框架来实现一个新的爬虫来加深对网络爬虫的理解！\n特别申明：本文所提到的落网是我本人特别喜欢的一个音乐网站，本文只是拿来进行爬虫的技术交流学习，读者涉及到的所有侵权问题都与本人无关\n本文首发在sudo rm -rf 采用署名(BY)-非商业性使用(NC)-禁止演绎(ND) 转载请注明原作者\n--EOF--\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "29"}