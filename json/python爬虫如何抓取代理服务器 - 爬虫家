{"title": "python爬虫如何抓取代理服务器 - 爬虫家 ", "index": "python", "content": "一年前突然有个灵感，想搞个强大的网盘搜索引擎，但由于大学本科学习软件工程偏嵌入式方向，web方面的能力有点弱，不会jsp，不懂html，好久没有玩过sql，但就是趁着年轻人的这股不妥协的劲儿，硬是把以前没有学习的全部给学了一遍，现在感觉web原来也就那么回事。好了，废话就不说了，看到本文的读者，可以先看看我做的东西：\n去转盘网：www.quzhuanpan.com\nok搜搜：www.oksousou.com（这个是磁力，顺便拿出来给大伙观赏）\n言归正传，由于我要爬取百度网盘，而度娘你懂的的搞爬虫出生的，反爬虫的能力很牛掰。尤其像我用我的电脑去爬百度网盘，爬几天百度就盯上了我的机子，爬虫开始爬不出东西。之后网上东查，西查，发现可以通过代理来解决这个问题，所以又去爬代理。我爬的是这个网站：\nhttp://www.xicidaili.com/ 之后他貌似他开始反击，我又将魔爪指向了：http://www.kuaidaili.com。\n想必看这篇博文的多半是程序猿，所以还是先上代码(我会写注释的，放心，该爬虫以http://www.xicidaili.com/为目标):\n#coding:utf-8\nimport json\nimport sys\nimport urllib, urllib2\nimport datetime\nimport time\nreload(sys)\nsys.setdefaultencoding('utf-8') \nfrom Queue import Queue\nfrom bs4 import BeautifulSoup\nimport MySQLdb as mdb\nDB_HOST = '127.0.0.1'\nDB_USER = 'root'\nDB_PASS = 'root'\nID=0\nST=1000\nuk='3758096603'\nclassify=\"inha\"\nproxy = {u'https':u'118.99.66.106:8080'}\n\nclass ProxyServer:\n    def __init__(self): #这个就不说了，数据库初始化，我用的是mysql\n        self.dbconn = mdb.connect(DB_HOST, DB_USER, DB_PASS, 'ebook', charset='utf8')\n        self.dbconn.autocommit(False)\n        self.next_proxy_set = set()\n        self.chance=0\n        self.fail=0\n        self.count_errno=0\n        self.dbcurr = self.dbconn.cursor()\n        self.dbcurr.execute('SET NAMES utf8')\n        \n    def get_prxy(self,num): #这个函数用来爬取代理\n        while num>0:\n            global proxy,ID,uk,classify,ST\n            count=0\n            for page in range(1,718): #代理网站总页数，我给了个718页\n                if self.chance >0: #羊毛出在羊身上，如过爬取网站开始反击我，我就从他那爬下来的\n                代理伪装，这个self.chance表示我什么时候开始换代理\n                    if ST % 100==0:\n                        self.dbcurr.execute(\"select count(*) from proxy\")\n                        for r in self.dbcurr:\n                            count=r[0]\n                        if ST>count:\n                            ST=1000 #我是从数据库的第1000条开始换的，这段你可以改，搞个随机函数随机换，我写的很简单\n                    self.dbcurr.execute(\"select * from proxy where ID=%s\",(ST))\n                    results = self.dbcurr.fetchall()\n                    for r in results:\n                        protocol=r[1]\n                        ip=r[2]\n                        port=r[3]\n                        pro=(protocol,ip+\":\"+port)\n                        if pro not in self.next_proxy_set:\n                            self.next_proxy_set.add(pro)\n                    self.chance=0\n                    ST+=1\n                proxy_support = urllib2.ProxyHandler(proxy) #注册代理\n                # opener = urllib2.build_opener(proxy_support,urllib2.HTTPHandler(debuglevel=1))\n                opener = urllib2.build_opener(proxy_support)\n                urllib2.install_opener(opener)\n                #添加头信息，模仿浏览器抓取网页，对付返回403禁止访问的问题\n                # i_headers = {'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US; rv:1.9.1.6) Gecko/20091201 Firefox/3.5.6'}\n                i_headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.48'}\n                #url='http://www.kuaidaili.com/free/inha/' + str(page)\n                url='http://www.kuaidaili.com/free/'+classify+'/' + str(page)\n                html_doc=\"\"\n                try:\n                    req = urllib2.Request(url,headers=i_headers)\n                    response = urllib2.urlopen(req, None,5)\n                    html_doc = response.read() #这不就获取了要爬取的页面嘛？\n                except Exception as ex: #看抛出异常了，可能开始反击我，我开始换代理\n                    print \"ex=\",ex\n                    pass\n                    self.chance+=1\n                    if self.chance>0:\n                        if len(self.next_proxy_set)>0:\n                            protocol,socket=self.next_proxy_set.pop()\n                            proxy= {protocol:socket}\n                            print \"proxy\",proxy\n                            print \"change proxy success.\"\n                    continue\n                #html_doc = urllib2.urlopen('http://www.xici.net.co/nn/' + str(page)).read()\n                if html_doc !=\"\": #解析爬取的页面，用的beautifulSoup\n                    soup = BeautifulSoup(html_doc,from_encoding=\"utf8\")\n                    #print \"soup\",soup\n                    #trs = soup.find('table', id='ip_list').find_all('tr') #获得所有行\n                    trs = \"\"\n                    try:\n                        trs = soup.find('table').find_all('tr')\n                    except:\n                        print \"error\"\n                        continue\n                    for tr in trs[1:]:\n                        tds = tr.find_all('td')\n                        ip = tds[0].text.strip() #ip\n                        port = tds[1].text.strip() #端口\n                        protocol = tds[3].text.strip()\n                        #tds = tr.find_all('td')\n                        #ip = tds[2].text.strip()\n                        #port = tds[3].text.strip()\n                        #protocol = tds[6].text.strip()\n                        get_time= tds[6].text.strip()\n                        #get_time = \"20\"+get_time\n                        check_time = datetime.datetime.strptime(get_time,'%Y-%m-%d %H:%M:%S')\n                        temp = time.time()\n                        x = time.localtime(float(temp))\n                        time_now = time.strftime(\"%Y-%m-%d %H:%M:%S\",x) # get time now,入库时间\n                        http_ip = protocol+'://'+ip+':'+port\n                        if protocol == 'HTTP' or protocol == 'HTTPS': #只要http协议相关代理，其他一律不要\n                            content=\"\"\n                            try: #我就是不放心这个网站，所以爬下来后我又开始检测代理是否真的有效\n                                proxy_support=urllib2.ProxyHandler({protocol:http_ip})\n                                # proxy_support = urllib2.ProxyHandler({'http':'http://124.200.100.50:8080'})\n                                opener = urllib2.build_opener(proxy_support, urllib2.HTTPHandler)\n                                urllib2.install_opener(opener)\n                                if self.count_errno>50:\n                                    self.dbcurr.execute(\"select UID from visited where ID=%s\",(ID)) #这是我的数据库，我取了一个叫uk的东东，这个\n                                    你不用管，你想检测拿你要爬取的链接检测代理吧\n                                    for uid in self.dbcurr:\n                                        uk=str(uid[0])\n                                    ID+=1\n                                    if ID>50000:\n                                        ID=0\n                                    self.count_errno=0\n                                test_url=\"http://yun.baidu.com/pcloud/friend/getfanslist?start=0&query_uk=\"+uk+\"&limit=24\" #我用来检测的链接\n                                print \"download:\",http_ip+\">>\"+uk\n                                req1 = urllib2.Request(test_url,headers=i_headers)\n                                response1 = urllib2.urlopen(req1, None,5)\n                                content = response1.read()\n                            except Exception as ex: #抛异常后的处理\n                                #print \"ex2=\",ex\n                                pass\n                                self.fail+=1\n                                if self.fail>10:\n                                    self.fail=0\n                                    break\n                                continue\n                            if content!=\"\": \n                                json_body = json.loads(content)    \n                                errno = json_body['errno']  \n                                self.count_errno+=1 \n                                if errno!=-55: #检验该代理是有用的，因为content！=\"\" 并且度娘返回not -55\n                                    print \"success.\"\n                                    self.dbcurr.execute('select ID from proxy where IP=%s', (ip)) #开始入库了\n                                    y = self.dbcurr.fetchone()\n                                    if not y:\n                                        print 'add','%s//:%s:%s' % (protocol, ip, port)\n                                        self.dbcurr.execute('INSERT INTO proxy(PROTOCOL,IP,PORT,CHECK_TIME,ACQ_TIME) VALUES(%s,%s,%s,%s,%s)',(protocol,ip,port,check_time,time_now))\n                                        self.dbconn.commit()\n            num-=1\n            if num % 4 ==0:\n                classify=\"intr\" #这个是原来网站的那几个标签栏名称，我是一栏一栏的爬取的\n            if num % 4 ==1:\n                classify=\"outha\"\n            if num % 4 ==2:\n                classify=\"outtr\"\n            if num % 4 ==3:\n                classify=\"inha\"\n\nif name == '__main__':\n  proSer = ProxyServer()\n  proSer.get_prxy(10000) #爬10000次，单线程，爬个1两周没有问题\n  \n 以上就是本人的代理爬虫代码，有兴趣可以加我qq：3047689758，还可以去：www.quzhuanpan.com首页关注我们的微博（如果你没有看到微博，可能是新代码我没有推），谢谢阅读，欢迎转载。\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "11"}