{"title": "python爬虫入门（一） - python爬虫入门 ", "index": "python", "content": "一、爬虫的基本知识：\n1. 什么是爬虫\n爬虫的英文翻译为spider或者crawder,意为蜘蛛或者爬行者，从字面意思我们可以体会到：爬虫就是把自己当做蜘蛛或者爬行者，沿着既定路线，爬到指定节点，猎取食物获取目标。在这里我们的蜘蛛网即互联网，我们爬取的方法即为路径，我们所要获取的数据即为食物或目标。\n2. 爬虫的核心\n\n爬取网页\n解析数据\n难点：与反爬虫博弈（反爬虫： 资源的所有者，想要保护资源，避免被第三方爬虫程序批量的把资源下载下去。想办法区分爬虫程序和正常的用户。）\n\n3. 爬虫的语言\n\nphp:虽然是世界上最好的语言，但是天生不是干爬虫的命，PHP对多线程、异步支持不足，并发不足。爬虫是工具性程序，对速度和效率要求较高。\njava:生态圈完善，是Python最大对手。但是Java本身很笨重，代码量大。重构成本比较高，任何修改都会导致大量代码的变动。最要命的是爬虫需要经常修改部分代码。\nCC++:运行效率和性能几乎最强，但是学习成本非常高，代码成型较慢，能用C/C++写爬虫，说明能力很强，但是不是最正确的选择\nPython：语法优美、代码简介、开发效率高、三方模块多，调用其他接口也方便。有强大的爬虫Scrapy，以及成熟高效的scrapy-redis分布式策略。\n\n4. 爬虫分类\n\n通用爬虫   也就是百度、Google、360、搜狐、firefox等搜索引擎。特点是爬取网站所有内容、但不能根据客户需求给出特定内容。在这里，各家浏览器根据其pagerank分进行网站搜索的排名，同时还有竞价排名。\n聚焦爬虫   就是现在我们这些爬虫程序员所进行的工作，就是根据客户的需求，爬取指定网站的特定内容。\n\n二、如何学习爬虫\n1. 首先要理解什么是http/https协议\n\nhttp协议：菜鸟教程：http协议详解\n\nhttps协议：菜鸟教程：https协议详解\n\n\n2. python基础知识\n\nurllib.request\nurllib.parse\n正则表达式等等基础知识。\n\n3. 开发工具\n\npacharm:下载安装请参考菜鸟教程：pycharm下载安装教程\n\nsublime Text3:下载安装请参考菜鸟教程：sublime下载安装\n\n\n4. 抓包工具\n\nchrome浏览器的开发者模式，在这里可以检查网页的各种元素。\nfiddler：原本是为测试来测试网站的，后来成了爬虫工程师最喜爱的工具。这是一款开源软件，可以直接去官网下载安装https://www.telerik.com/downl...\n\npostman：可以根据请求头信息生成简单的网页爬取代码，界面相对于fiddler也要美观\nmotimproxy\n\n三、代码实现简单的爬虫实例\n\n\npython爬虫有多简单，下面我一行代码爬取百度首页，如同在浏览器输入网址一样简单\nimport urllib.request\nurllib.request.urlretrieve('http://www.baidu.com/', 'baidu.html')\n\n\n上面的代码只能简单抓取网页，一旦遇到需要请求头的反爬虫时就不能发挥作用了，所以一般常用的爬虫代码如下：\n\n    import urllib.request\n    url = 'http://www.baidu.com/'\n    \n    request = urllib.request.Request(url)\n    \n    response = urllib.request.urlopen(request)\n    \n    with open('baidu.html', 'wb') as f:\n        f.write(response.read())\n\n\n请求头反反爬虫：\nimport urllib.request\n#指定url\nurl = 'https://www.qiushibaike.com/'\n#定义请求头对象\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36'\n\n}\n#获取网页信息\nrequest = urllib.request.Request(url, headers=headers)\n\nresponse = urllib.request.urlopen(request)\n\nwith open('xiushi.html','wb') as f:\n    f.write(response.read())\n\n今天先写到这儿了，我争取做到每日分享，与大家一起学习，一起进步，为了美好的明天，艰苦奋斗。\n                                                     ————你与别人的差距在加班的时间。\n\n\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}