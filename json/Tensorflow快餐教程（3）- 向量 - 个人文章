{"title": "Tensorflow快餐教程（3）- 向量 - 个人文章 ", "index": "机器学习,深度学习,神经网络,python", "content": "摘要： Tensorflow向量操作\n向量\n向量在编程语言中就是最常用的一维数组。二维数组叫做矩阵，三维以上叫做张量。\n向量虽然简单，高效，且容易理解。但是与操作0维的标量数据毕竟还是不同的。比如向量经常用于表示一个序列，生成序列像标量一样一个一个手工写就不划算了。当然可以用循环来写。在向量中这样还好，如果是在矩阵或者是张量中就强烈建议不要用循环来做了。系统提供的函数一般都是经过高度优化的，而且可以使用GPU资源来进行加速。我们一方面尽可能地多使用系统的函数，另一方面也不要迷信它们，代码优化是一个实践的过程，可以实际比较测量一下。\n快速生成向量的方法\nrange函数生成等差数列tf.range函数用来快速生成一个等差数列。相当于之前我们讲numpy时的np.arange函数。\n原型：\ntf.range(start, limit, delta=1, dtype=None, name='range')\n\n例：\n>>> b11 = tf.range(1,100,1)\n>>> b11\n<tf.Tensor 'range:0' shape=(99,) dtype=int32>\n>>> sess.run(b11)\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n      dtype=int32)\n\nlinspace生成浮点等差数组\ntf.linspace与tf.range的区别在于，数据类型不同。\ntf.lin_space(\n    start,\n    stop,\n    num,\n    name=None\n)\n\n其中，start和stop必须是浮点数，且类型必须相同。num必须是整数。\n例：\n>>> a2 = tf.linspace(1.0,10.0,4)  \n>>> a2\n<tf.Tensor 'LinSpace_2:0' shape=(4,) dtype=float32>\n>>> sess.run(a2)\narray([ 1.,  4.,  7., 10.], dtype=float32)\n\n拼瓷砖\n就是将一段向量重复若干次。\n>>> a10 = tf.range(1,4,1)\n>>> sess.run(a10)\narray([1, 2, 3], dtype=int32)\n>>> a11 = tf.tile(a10,[3])\n>>> sess.run(a11)\narray([1, 2, 3, 1, 2, 3, 1, 2, 3], dtype=int32)\n\n向量操作\n将向量反序可以使用tf.reverse函数。原型：\ntf.reverse(\n    tensor,\n    axis,\n    name=None\n)\n\ntensor是向量，axis轴对于向量不重要，给个[-1]就可以了。折腾轴是张量时间的事情，暂时还用不到。\n>>> a2 = tf.linspace(1.0,10.0,4)\n>>> a3 = tf.reverse(a2,[-1])\n>>> sess.run(a3)\narray([10.,  7.,  4.,  1.], dtype=float32)\n\n切片\n切片也是向量的常用操作之一，就是取数组的一部分。\n例：\n>>> a5 = tf.linspace(1.0,100.0, 10)\n>>> sess.run(a5)\narray([  1.,  12.,  23.,  34.,  45.,  56.,  67.,  78.,  89., 100.],\n      dtype=float32)\n>>> a6 = tf.slice(a5, [2],[4])\n>>> sess.run(a6)\narray([23., 34., 45., 56.], dtype=float32)\n\n将来处理张量时，我们从一个矩阵切一块，或从一个张量中切一块，就好玩得多了。但是原理跟向量上是一样的。\n连接\ntf.concat也是需要给定轴信息的。对于两个线性的向量，我们给0或者-1就好。\n>>> a20 = tf.linspace(1.0,2.0,10)\n>>> sess.run(a20)\narray([1.       , 1.1111112, 1.2222222, 1.3333334, 1.4444444, 1.5555556,\n       1.6666667, 1.7777778, 1.8888888, 2.       ], dtype=float32)\n>>> a21 = tf.linspace(2.0,3.0,5)\n>>> sess.run(a22)\narray([1.       , 1.1111112, 1.2222222, 1.3333334, 1.4444444, 1.5555556,\n       1.6666667, 1.7777778, 1.8888888, 2.       , 2.       , 2.25     ,\n       2.5      , 2.75     , 3.       ], dtype=float32)\n>>> a23 = tf.concat([a20,a21],-1)\n>>> sess.run(a23)\narray([1.       , 1.1111112, 1.2222222, 1.3333334, 1.4444444, 1.5555556,\n       1.6666667, 1.7777778, 1.8888888, 2.       , 2.       , 2.25     ,\n       2.5      , 2.75     , 3.       ], dtype=float32)\n\n向量计算\n向量加减法同样长度的向量之间可以进行加减操作。\n例：\n>>> a40 = tf.constant([1,1])\n>>> a41 = tf.constant([2,2])\n>>> a42 = a40 + a41\n>>> sess.run(a42)\narray([3, 3], dtype=int32)\n>>> a43 = a40 - a41\n>>> sess.run(a43)\narray([-1, -1], dtype=int32)\n>>> a43\n<tf.Tensor 'sub:0' shape=(2,) dtype=int32>\n\n向量乘除标量向量乘除标量也非常好理解，就是针对向量中的每个数都做乘除法。\n例：\n>>> a44 = a40 * 2\n>>> sess.run(a44)\narray([2, 2], dtype=int32)\n>>> a45 = a44 / 2  \n>>> sess.run(a45)\narray([1., 1.])\n>>> a44\n<tf.Tensor 'mul:0' shape=(2,) dtype=int32>\n>>> a45\n<tf.Tensor 'truediv_1:0' shape=(2,) dtype=float64>\n\n广播运算如果针对向量和标量进行加减运算，也是会对向量中的每个数进行加减运算。这种操作称为广播操作。\n例：\n**>>> a46 = a40 + 1\nsess.run(a46)array([2, 2], dtype=int32)a46<tf.Tensor 'add_1:0' shape=(2,) dtype=int32>**\n小结\n从上面我们学习的函数我们可以看到，与普通语言中提供的函数多是为一维数组操作不同，Tensorflow中的切片、拼接等操作也是基于张量的。当我们后面学到张量遇到困难时，不妨回来看下这一节。不管后面张量多么复杂，其实也只是从一维向二维和多维推广而己。\n详情请阅读原文\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}