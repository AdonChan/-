{"title": "爬取博客园首页并定时发送到微信 - 个人文章 ", "index": "网页爬虫,python", "content": "应女朋友要求，为了能及时掌握技术动向，特意写了这个爬虫，每天定时爬取博客园首页并发送至微信。\n环境：\nPython3.4\n第三方库\n\nRequests:向服务器发送请求\nBeautifulSoup4：解析Html\nwxpy：微信接口\nSchedule：定时器\n\n代码\n# -*-coding:utf-8 -*-\n\nimport requests\nfrom requests import exceptions\nfrom bs4 import BeautifulSoup as bs\nimport re\nfrom wxpy import *\nimport  schedule\nimport  time\n\n\nbot=Bot(cache_path=True)\n\n#获取网页内容\ndef getHtml(pageIndex):\n    #定义请求头 伪装成浏览器\n    headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'}\n    #pageIndex代表页数\n    payload={'CategoryType': 'SiteHome', 'ParentCategoryId': '0', 'CategoryId': '808', 'PageIndex': pageIndex, 'TotalPostCount': '4000'}\n    try:\n        r=requests.post('https://www.cnblogs.com/mvc/AggSite/PostList.aspx',data=payload,headers=headers)\n        r.raise_for_status()\n        r.encoding=r.apparent_encoding\n        return r.text\n    except requests.RequestException as e:\n        return e.strerror\n#向微信文件传输助手发送消息\ndef sendblogmsg(content):\n    #搜索自己的好友\n    #my_friend = bot.friends().search('')[0]\n    my_friend=bot.file_helper\n    my_friend.send(content)\n\ndef job():\n    contents=''\n    #i表示当前页数\n    for i in range(1,3):\n        html=getHtml(i)\n        soup=bs(html,\"html.parser\")\n        blogs=soup.findAll(\"div\",{'class':'post_item_body'})\n        for blog in blogs:\n            title=blog.find('h3').get_text()\n            summary=blog.find('p',{'class':'post_item_summary'}).get_text()\n            link=blog.find('a',{'class':'titlelnk'})['href']\n            content='标题：'+title+'\\n链接：'+link+'\\n-----------\\n'\n            contents+=content\n        sendblogmsg(contents)\n#定时\nschedule.every().day.at(\"06:00\").do(job)\nwhile True:\n    schedule.run_pending()\n    time.sleep(1)\nbot.join()\n注意事项：\n\n不要进行恶意攻击行为\n尽量在空闲时间访问网站，控制访问频率，不要恶意消耗网站资源\n\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "0"}