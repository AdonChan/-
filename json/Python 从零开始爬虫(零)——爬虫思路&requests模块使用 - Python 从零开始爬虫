{"title": "Python 从零开始爬虫(零)——爬虫思路&requests模块使用 - Python 从零开始爬虫 ", "index": "网页爬虫,python", "content": "前言\n\npython 3.6 ！！\n本爬虫系列是面对有Python语法基础的读者写的，如果你没学过python，emmmm.....也没关系，你或许能从每篇文章中学到一些爬虫的思路；如果你的python基础语法已经应用自如，那是极好的。\n本系列的教程和实例均总结于笔者的自学经历，如有不足欢迎指正和讨论，问题我会尽量回复，也希望大家能有所收获。\n爬虫真好玩.jpg\n\n废话不多说，赶紧开始这条不归路吧\n爬虫思路\n无思路不成器，如果你怎么想都想不出爬虫的原理，不妨来看下我们平时是如何上网的，大概就是这样：   ①点开目标网址→→[可选：登录/回复]→→②浏览全页筛选出价值内容→→③如果很喜欢，还会拷贝下载回来\n所以爬虫归结起来也就这几步：\n\n构造目标网址（重点）\n发起请求（request），相当于点开网页。\n获取网页内容（坑）。\n定制筛选器，对内容进行筛选（重点）。\n把爬取结果保存到容器里。\n\n本节学习的requests模块将会实现二，三步，其他步骤和注意事项将会在后续文章中展示出来。\nF12审查元素\n这是浏览器自带的工具，提供抓包和检查网页源码的功能，供使用者分析网页。也是学爬虫必须要学会的工具，一个优秀的虫爸/虫妈应该花更多的时间在网页分析和debug上。\n使用非常简单，打开任一浏览器（笔者的是google chrome），按F12或鼠标右键检查。\n\n选择Element是查看网页源码，是树结构的html文档，里面有要爬取的内容。\n选择Network是查看本地和服务器端交互的包，可以从中获取目标网址和headers。\n\nrequests模块\n为什么选择requests，因为它能完全胜任python自带的urllib模块，简化了不必要的功能的同时让使用更加简单。\n安装\n非常简单，打开cmd，直接pip安装\npip install requests\n或pycharm中搜索requests安装\n简单使用\n首先呈上官方文档，有中文版，欢迎来啃。下面主要介绍两种方法：get和post\nget，就是本地向服务器索取的意思，服务器检查请求头（request headers）后，如果觉得没问题，就会返回信息给本地。\nr = requests.get(url,**args)#返回一个Response对象，我们可以从这个对象中获取所有我们想要的信息\npost，就是本地要向服务器提交一些数据的意思，服务器还是会检查请求头，如果提交的数据和请求头都没问题，就会返回信息给本地。\nr = requests.post(url,**args)#也是返回Response对象\n参数详解\nget和post方法中有许多参数可以使用，部分参数后面会详解。\n\n\nurl：就是目标网址，接收完整（带http）的地址字符串。\n\nheaders：请求头，存储本地信息如浏览器版本，是一个字典。\n\ndata：要提交的数据，字典。\n\ncookies：cookies，字典。\n\ntimeout：超时设置，如果服务器在指定秒数内没有应答，抛出异常，用于避免无响应连接，整形或浮点数。\n\nparams：为网址添加条件数据，字典。\n\npayload = {'key1': 'value1', 'key2': 'value2'}\nr = requests.get(\"http://httpbin.org/get\", params=payload)\n#相当于目标网址变成了http://httpbin.org/get?key2=value2&key1=value1\n\nproxies：ip代理时使用，字典。\nResponse对象使用\n从这个对象中获取所有我们想要的信息非常简单，毕竟爬虫要的数据主要就三种，html源码，图片二进制数据，json数据，Response对象一次性满足你三个愿望。\nr.encoding = 'ISO-8859-1'    #指定r.text返回的数据类型，写在r.text之前。\nr.text    #默认以unicode形式返回网页内容，也就是网页源码的字符串。\n\nr.content    #以二进制形式返回网页内容，下载图片时专用。\nr.json()    #把网页中的json数据转成字典并将其返回。\n\n#还有一些很少用到的方法。\nr.headers    #返回服务器端的headers，字典。\nr.status_code    #返回连接状态，200正常。\n\n小实例\nrequests 学完后就可以到处试试了，如果红了（抛出异常），那大概是服务器拒绝你了，毕竟伪装什么的还没提到，服务器知道你是虫子就把你踢掉了。\nimport requests\nr = requets.get('http://cn.python-requests.org/zh_CN/latest/')\nwith open('test.txt','w',encoding = 'utf-8') as file:#编码要对应\n    file.write(r.text)\n    #然后打开看看吧，是不是和F12看到的源码一样，只不过是把分支全展开了而已。\n小提示：并不是所有网站的F12源码和爬取源码是一致的，网站有动态的，也有静态的；有防爬虫的，也有敞开大门任意爬的。关于对策之后会讲到。\n第一次写文章，挺累的，我需要做(wan)几(ji)道(pan)数(you)学(xi)题放松一下自己才行。\n\n                ", "mainLikeNum": ["4 "], "mainBookmarkNum": "8"}