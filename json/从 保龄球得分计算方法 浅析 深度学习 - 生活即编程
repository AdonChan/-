{"title": "从 保龄球得分计算方法 浅析 深度学习 - 生活即编程 ", "index": "编程,生活,deeplearning,mxnet,python", "content": "起因\n周六被小伙伴拖去游泳，美名其曰：锻炼身体。其实某人就是去泡澡的，哈哈。说正题吧，游完泳在体育场里闲逛，里面很大，转着转着看到一个保龄球馆，怀着对未知事物的好奇，决定和某人去尝试一下。我和S同学一人买了一局，按照说明，每一局分为10次，每一次有两次机会扔球。最后的比分就不说了，反正玩的很爽，最后也在边上一个厉害的大叔指点下，学会了基本的扔球姿势。  \n看到这你以为这是一篇叙事文？那就错了，起因是从这里开始的，我们的次数用完后，留在里面打台球（这里也有台球桌），看到不断有穿着队服一类东西的人进来，应该是来比赛的，同时又看到了赛道上面的牌子，有一个写着：289分。那分数是怎么计算的呢，怀着好奇心搜索起保龄球的积分规则来。在了解之后，我就在想一个问题：__如果是让我开发一个保龄球的游戏，那么计分程序要怎么写呢?__今天我们就从这里说起。。。\n规则\n先简述一下保龄球的规则，这里引用百度知道的别人的回答，每一局比赛有10格，每格有两次击球机会，我们这里关注它的得分情况，这里分为两种情况:\n\n\n1-9格击球每一格有3种可能：\n\n第一次击球全部击倒：这种情况得分就是击倒的瓶数（10）+后两次击球击倒的总数\n两次击球全部击倒：这样得分为击倒的瓶数（10）+后一次击球击倒的总数\n两次击球没有全部击倒：得分为两次击倒总瓶数\n\n\n\n第10格击球这一格有两种可能：\n\n前两次未能将瓶全部击倒：得分为击倒总瓶数+第9格的得分\n前两次将瓶全部击倒，获得一次追加机会：得分为两次击倒总数（10）+追加时击倒的总瓶数+第9格分数\n\n\n\n程序\n规则也了解了，下面就到了写代码的时候了，为了方便，这里选择Python，版本为3.6  考虑到直观性，这里没有用交互式的程序，而是直接将击中情况抽象成矩阵（数组），算出最后总分。  输入的数据大概是这个样子：\n[[0, 3], [2, 6], [3, 6], [0, 3], [3, 0], [9, 1], [6, 3], [6, 2], [4, 6], [4, 2]]\n10x2的数组，代表前10格每格的击倒瓶数，如果一格内不需要第二次击球，也算作0。这里先写一个简单的数据生成函数。\nimport random\ndef top_10():\n    for i in range(10):\n        for j in range(2):\n            if j == 0 :\n                a[i][j] = random.randint(0,10)\n            else :\n                a[i][j] = random.randint(0,10-a[i][j-1]) \n    return a\n同时，我们注意到了，这个生成函数还少了点什么，没错，就是第十格的追加击球数。所以，这里再定义一个追加球生成函数  这里为了后面计算方便，也定义为[[x,y]]这种格式\ndef addto_num(a):\n    return [[random.randint(0,10),0]] if sum(a[9]) == 10 else [[0,0]]\n原始数据的生成我们完成了，接下来要定义计算函数了，计算总分数\ndef calc_total(top):\n    sums = 0\n    index = 0\n    for x in top:\n        if x[0] == 10:\n            sums += 10\n            if top[index+1][0] == 10:\n                sums += 10 + top[index+2][0]\n            else:\n                sums += sum(top[index+1])\n        elif sum(x) == 10:\n            sums += 10 + top[index+1][0]\n        else:\n            sums += sum(x)\n        index+=1\n        if index == 9:\n            break\n    sums += sum(top[8]+top[9]+top[10])\n    return sums   \n代码写的不是很好看，大家请谅解啊，不过整个完整的功能是做完了，我们可以写个方法测试下\ntmp1 = top_10()\nadd1 = addto_num(tmp1)\nc = calc_total(tmp1+add1)\nprint(c)\n78\n\n神经网络版\n想必大家也了解，当下最火的就是AI，而作为实现AI的其中一种手段，深度学习必不可少。最近也在学习这方面的知识（ps:给沐神疯狂打call，强烈推荐他的深度学习课程，链接大家自己去搜，就不做广告了），虽然说自己连入门都算不上，但还是想实现一下自己版本的。  \n于是就有了这个：\n深度学习版本的保龄球得分计算方法\n\n\n这里我们用到了mxnet这个深度学习框架，最基础的部分的两个库ndarray和autograd\n首先，我们是基于线性回归这个最简单也是最基础的神经网络实现的，模型看起来就像这样$$\\boldsymbol{\\hat{y}} = X \\boldsymbol{w} + b$$同时定义它的损失函数，也就是计算预测值和实际值的差距，这里用两个的平方误差来计算，模型是这样$$\\sum_{i=1}^n (\\hat{y}_i-y_i)^2.$$\n首先，我们要__创建数据集__  因为我们之前定义的是Python的list，所以在这里要转换成mxnet的内置数组ndarray  \n不过在此之前我们要先改进下我们的生成函数，之前是由两个函数组成，现在为了方便，我们合成一个。同时，计算方法改造成ndarray版本的。\nfrom mxnet import ndarray as nd\nfrom mxnet import autograd\n\ndef init_data():\n    for i in range(0,10):\n        for j in range(0,2):\n            if j == 0 :\n                a[i][j] = random.randint(0, 10)\n            else :\n                a[i][j] = random.randint(0,10-a[i][j-1]) \n    return a+[[random.randint(0,10),0]] if sum(a[9]) == 10 else a+[[0,0]]\ndef calc_total_nd(top):\n    sums = 0\n    index = 0\n    for x in top:\n        if x[0].asscalar() == 10:\n            sums += 10\n            if top[index+1][0].asscalar() == 10:\n                sums += 10 + top[index+2][0].asscalar()\n            else:\n                sums += nd.sum(top[index+1]).asscalar()\n        elif nd.sum(x).asscalar() == 10:\n            sums += 10 + top[index+1][0].asscalar()\n        else:\n            sums += nd.sum(x).asscalar()\n        index+=1\n        if index == 9:\n            break\n    sums += nd.sum(top[8]+top[9]+top[10]).asscalar()\n    return sums   \n\nnum_inputs = 22\nnum_examples = 1000\nX = nd.zeros(shape=(num_examples,11,2))\nfor i in X:\n    i[:] = nd.array(init_data())\ny = nd.array([calc_total_nd(i) for i in X])\n然后是定义 数据读取方法  目的是在后面训练时随机遍历我们的数据集，这里参考了沐神教程里的方法。\nimport random\nbatch_size = 10\ndef data_iter():\n    # 产生一个随机索引\n    idx = list(range(num_examples))\n    random.shuffle(idx)\n    for i in range(0, num_examples, batch_size):\n        j = nd.array(idx[i:min(i+batch_size,num_examples)])\n        yield nd.take(X, j), nd.take(y, j)\n尝试着读取一个\nfor data, label in data_iter():\n    print(data, label)\n    break\n\n[[[  2.   0.]\n  [  7.   0.]\n  [  1.   7.]\n  [  2.   2.]\n  [  6.   2.]\n  [  0.   5.]\n  [  0.   5.]\n  [  7.   1.]\n  [  6.   4.]\n  [  3.   0.]\n  [  0.   0.]]\n\n [[  6.   3.]\n  [  4.   2.]\n  [  2.   4.]\n  [  8.   2.]\n  [  4.   6.]\n  [  6.   3.]\n  [  2.   6.]\n  [  6.   3.]\n  [  2.   3.]\n  [  8.   2.]\n  [  7.   0.]]\n\n [[ 10.   0.]\n  [  8.   0.]\n  [  2.   2.]\n  [  8.   2.]\n  [  0.   3.]\n  [ 10.   0.]\n  [ 10.   0.]\n  [  6.   3.]\n  [ 10.   0.]\n  [  1.   7.]\n  [  0.   0.]]\n\n [[  5.   1.]\n  [  6.   2.]\n  [ 10.   0.]\n  [  3.   6.]\n  [  8.   2.]\n  [ 10.   0.]\n  [  4.   4.]\n  [  2.   4.]\n  [  2.   0.]\n  [  7.   3.]\n  [ 10.   0.]]\n\n [[  6.   2.]\n  [  8.   0.]\n  [  0.   0.]\n  [  9.   0.]\n  [  6.   4.]\n  [  5.   3.]\n  [  5.   0.]\n  [  1.   6.]\n  [  0.   1.]\n  [  4.   4.]\n  [  0.   0.]]\n\n [[  5.   5.]\n  [  6.   3.]\n  [  0.   7.]\n  [  2.   8.]\n  [ 10.   0.]\n  [  4.   0.]\n  [  1.   5.]\n  [  1.   2.]\n  [  1.   2.]\n  [  0.   2.]\n  [  0.   0.]]\n\n [[ 10.   0.]\n  [  0.   3.]\n  [  3.   7.]\n  [  3.   1.]\n  [  8.   1.]\n  [  4.   2.]\n  [  8.   1.]\n  [  6.   4.]\n  [ 10.   0.]\n  [  5.   0.]\n  [  0.   0.]]\n\n [[  8.   2.]\n  [ 10.   0.]\n  [  6.   0.]\n  [ 10.   0.]\n  [  1.   4.]\n  [  2.   6.]\n  [  9.   0.]\n  [  5.   5.]\n  [  7.   1.]\n  [  5.   1.]\n  [  0.   0.]]\n\n [[  9.   1.]\n  [  7.   1.]\n  [  6.   3.]\n  [  0.   5.]\n  [  7.   3.]\n  [  7.   1.]\n  [  6.   3.]\n  [  3.   1.]\n  [  3.   3.]\n  [ 10.   0.]\n  [  6.   0.]]\n\n [[  0.  10.]\n  [  4.   3.]\n  [  2.   6.]\n  [  2.   6.]\n  [  4.   1.]\n  [  8.   1.]\n  [  5.   4.]\n  [  3.   6.]\n  [  6.   4.]\n  [  4.   2.]\n  [  0.   0.]]]\n<NDArray 10x11x2 @cpu(0)> \n[  73.  104.  133.  118.   70.   87.  107.  118.  105.   99.]\n<NDArray 10 @cpu(0)>\n\n数据准备好了，现在要定义一个__初始化的模型参数__  这里随机生成一个就好了，后面我们会通过训练，慢慢学习完善这个参数，这也是深度学习的目的\nw = nd.random_normal(shape=(num_inputs, ))\nb = nd.random_normal(shape=(1,))\nparams = [w, b]\nprint(params)\n[\n[ 0.50869578 -0.16038011  0.91511744  0.84187603 -0.49177799 -1.00553632\n -1.55609238  3.13221502 -0.15748753 -0.4358989  -0.52664566 -0.49295077\n -0.17884982  1.43718672  0.43164727 -0.31814137  0.46760127 -0.16282491\n  0.17287086  0.6836102   0.76158988  1.61066961]\n<NDArray 22 @cpu(0)>, \n[  9.91063134e-05]\n<NDArray 1 @cpu(0)>]\n\n然后附上梯度，也就是让后面autograde可以对这个函数求导\nfor param in params:\n    param.attach_grad()\n定义模型和损失函数  \n这里要注意的是：我们的维度不是1，所以要把数组的维度reshape一下变成一维数组\ndef net(X):\n    return nd.dot(X.reshape((-1,num_inputs)), w) + b\ndef square_loss(yhat, y):\n    return (yhat - y.reshape(yhat.shape)) ** 2\n\n然后是优化方法，也就是学习方法，让函数去学习参数\ndef SGD(params, lr):\n    for param in params:\n        param[:] = param - lr * param.grad\n最后就是__训练__了\nepochs = 5\nlearning_rate = .0001\nfor e in range(epochs):\n    total_loss = 0\n    for data, label in data_iter():\n        with autograd.record():\n            output = net(data)\n            loss = square_loss(output, label)\n        loss.backward()\n        SGD(params, learning_rate/batch_size)\n        total_loss += nd.sum(loss).asscalar()\n    print(\"Epoch %d, average loss: %f\" % (e, total_loss/num_examples))\nEpoch 0, average loss: 82.049488\nEpoch 1, average loss: 82.009441\nEpoch 2, average loss: 81.810044\nEpoch 3, average loss: 82.243776\nEpoch 4, average loss: 82.023799\n\n最后来验证下我们的预测结果\nfor data, label in data_iter():\n        print(\"实际分数\")\n        print(label)\n        print(\"预测分数\")\n        print(net(data))\n        break\n实际分数\n\n[ 108.   77.  102.  115.   85.  110.   76.  124.   78.   87.]\n<NDArray 10 @cpu(0)>\n预测分数\n\n[ 107.43678284   86.52748871  101.92710114  116.50645447   90.5655899\n  115.31760406   80.10424805  118.94145203   84.49520111   95.17882538]\n<NDArray 10 @cpu(0)>\n\n参考:动手学深度学习\n\n                ", "mainLikeNum": ["5 "], "mainBookmarkNum": "2"}