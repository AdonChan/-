{"title": "Sparse Autoencoder - 个人文章 ", "index": "python", "content": "作者：chen_h微信号 & QQ：862251340微信公众号：coderpai简书地址：https://www.jianshu.com/p/5f3...\n\n\n自编码器 Autoencoder\n稀疏自编码器 Sparse Autoencoder\n降噪自编码器 Denoising Autoencoder\n堆叠自编码器 Stacked Autoencoder\n\n\n稀疏自编码器可以看做是自编码器的一个变种，它的作用是给隐藏神经元加入稀疏性限制，那么自编码神经网络即使在隐藏神经元数量较多的情况下任然可以返现输入数据中一些有趣的结构。\n稀疏性可以被简单地解释为：如果当神经元的输出接近于1的时候我们认为它被激活，而输出接近于0的时候认为它被抑制，那么使得神经元大部分的时间都是被抑制的限制则被称作稀疏性限制。这里我们假设的神经元的激活函数是 sigmoid 函数。如果你使用 tanh 作为激活函数的话，当神经元输出为-1的时候，我们认为神经元是被抑制的。\n稀疏自编码器网络结果还是和自编码器一样，如下：\n\n稀疏自编码器与自编码器的不同点在于损失函数的设计上面。稀疏编码是对网络的隐藏层的输出有了约束，即隐藏层神经元输出的平均值应尽量为0。也就是说，大部分的隐藏层神经元都处于非 activite 状态。因此，此时的 sparse autoencoder 损失函数表达式为：\n\n最后的一项表示KL散度，其具体表达式如下：\n\n隐藏层神经元 j 的平均活跃度计算如下：\n\n其中，p 是稀疏性参数，通常是一个接近于0的很小的值（比如 p = 0.05）。换句话说，我们想要让隐藏层神经元 j 的平均活跃度接近 0.05 。为了满足这一条件，隐藏层神经元的活跃度必须接近于 0 。为了实现这一限制，所以我们才设计了上面的KL散度。\n如果我们假设平均激活度 p = 0.2，那么我们就能得到下图的关系：\n\n从图中，可以看出，当值一旦偏离期望激活度 p 时，这种误差便会急剧增大，从而作为称发现个添加到目标函数，可以指导整个网络学习出稀疏的特征表示。\n实验代码如下：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\n\nN_INPUT = 4\nN_HIDDEN = 100\nN_OUTPUT = N_INPUT\nBETA = tf.constant(3.0)\nLAMBDA = tf.constant(.0001)\nEPSILON = .00001\nRHO = .1\n\n\ndef diff(input_data, output_data):\n    ans = tf.reduce_sum(tf.pow(tf.sub(output_data, input_data), 2))\n    return ans\n\ndef main(_):\n    \n    weights = {\n        'hidden': tf.Variable(tf.random_normal([N_INPUT, N_HIDDEN]), name = \"w_hidden\"),\n        'out': tf.Variable(tf.random_normal([N_HIDDEN, N_OUTPUT]), name = \"w_out\")\n    }\n\n    biases = {\n        'hidden': tf.Variable(tf.random_normal([N_HIDDEN]), name = \"b_hidden\"),\n        'out': tf.Variable(tf.random_normal([N_OUTPUT]), name = \"b_out\")\n    }\n\n    def KLD(p, q):\n        invrho = tf.sub(tf.constant(1.), p)\n        invrhohat = tf.sub(tf.constant(1.), q)\n        addrho = tf.add(tf.mul(p, tf.log(tf.div(p, q))), tf.mul(invrho, tf.log(tf.div(invrho, invrhohat))))\n        return tf.reduce_sum(addrho)\n\n    with tf.name_scope('input'):\n        # input placeholders\n        x = tf.placeholder(\"float\", [None, N_INPUT], name = \"x_input\")\n        #hidden = tf.placeholder(\"float\", [None, N_HIDDEN], name = \"hidden_activation\")\n\n    with tf.name_scope(\"hidden_layer\"):\n        # from input layer to hidden layer\n        hiddenlayer = tf.sigmoid(tf.add(tf.matmul(x, weights['hidden']), biases['hidden']))\n\n    with tf.name_scope(\"output_layer\"):\n        # from hidden layer to output layer\n        out = tf.nn.softmax(tf.add(tf.matmul(hiddenlayer, weights['out']), biases['out']))\n\n    with tf.name_scope(\"loss\"):\n        # loss items\n        cost_J = tf.reduce_sum(tf.pow(tf.sub(out, x), 2))\n\n    with tf.name_scope(\"cost_sparse\"):\n        # KL Divergence items\n        rho_hat = tf.div(tf.reduce_sum(hiddenlayer), N_HIDDEN)\n        cost_sparse = tf.mul(BETA, KLD(RHO, rho_hat))\n\n    with tf.name_scope(\"cost_reg\"):\n        # Regular items\n        cost_reg = tf.mul(LAMBDA, tf.add(tf.nn.l2_loss(weights['hidden']), tf.nn.l2_loss(weights['out'])))\n\n    with tf.name_scope(\"cost\"):\n        # cost function\n        cost = tf.add(tf.add(cost_J, cost_reg), cost_sparse)\n\n    optimizer = tf.train.AdamOptimizer().minimize(cost)\n\n    with tf.Session() as sess:\n\n        init = tf.initialize_all_variables()\n        sess.run(init)\n\n        input_data = np.array([[0,0,0,1],[0,0,1,0],[0,1,0,0],[1,0,0,0]], float)\n\n        for i in xrange(10000):\n            sess.run(optimizer, feed_dict = {x: input_data})\n            if i % 100 == 0:\n                tmp = sess.run(out, feed_dict = {x: input_data})\n                print i, sess.run(diff(tmp, input_data))\n\n        tmp = sess.run(out, feed_dict = {x: input_data})\n        print tmp\n\n\nif __name__ == '__main__':\n    tf.app.run()\n\n\nReference:\nStanford Lecture\nUFLDL\nSAE code\n\n作者：chen_h微信号 & QQ：862251340简书地址：https://www.jianshu.com/p/5f3...\nCoderPai 是一个专注于算法实战的平台，从基础的算法到人工智能算法都有设计。如果你对算法实战感兴趣，请快快关注我们吧。加入AI实战微信群，AI实战QQ群，ACM算法微信群，ACM算法QQ群。长按或者扫描如下二维码，关注 “CoderPai” 微信号（coderpai）\n\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}