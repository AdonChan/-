{"title": "Python 爬虫入门(一)——爬取糗百 - Python提高班 ", "index": "网页爬虫,python", "content": "爬取糗百内容\nGitHub 代码地址https://github.com/injetlee/Python/blob/master/qiubai_crawer.py\n微信公众号：【智能制造专栏】，欢迎关注。\n本文目标\n\n掌握爬虫的基本概念\nRequests 及 Beautiful Soup 两个 Python 库的基本使用\n通过以上知识完成糗百段子抓取\n\n爬虫基本概念\n爬虫也称网页蜘蛛，主要用于抓取网页上的特定信息。这在我们需要获取一些信息时非常有用，比如我们可以批量到美图网站下载图片，批量下载段子。省去手工操作的大量时间。爬虫程序一般是通过模拟浏览器对相应URL发出请求，获取数据，并通过正则等手段匹配出页面中我们所需的数据。\n在学习爬虫之前，最好到 w3school 去了解一下 HTML 标签的概念以及基本的 CSS 的概念。这会让我们更容易的理解如何获取页面中某个内容。\nRequests 库基本介绍\nRequests 是学习爬虫的一大利器。是一个优雅简单的 HTTP库。官网介绍如下：\nRequests: HTTP for Humans\n专门为人类使用的 HTTP 库。使用起来非常简单明了。我们平时浏览网页的步骤是输入网址，打开。在 Requests 中是如下这样的，我们可以在 Python 交互式解释器中输入以下代码：\nimport requests\nr = requests.get('https://www.qiushibaike.com/text/') # 打开网址，一般我们会设置 请求头，来更逼真的模拟浏览器，下文有介绍\nr.text\n\n我门看到下面一堆的代码，其实就是网页的源代码(也可以在浏览器上右键查看页面源代码)。通过这几行代码我们就拿到了页面的所有信息，剩下的就是从页面中找到我们所需要的信息。\nBeautiful Soup 库介绍\n拿到网页信息后，我们要解析页面，通常来说我们有以下几种方式来解析页面，获取我们所需的信息。\n\n\n正则表达式\n适用于简单数据的匹配，如果匹配内容较复杂，正则表达式写起来会很绕，同时页面内容稍微变化，正则就会失效\n\n\nLxml\nLxml 是专门用来解析 XML 格式文件的库，该模块用 C 语言编写，解析速度很快，和正则表达式速度差不多，但是提供了 XPath 和 CSS 选择器等定位元素的方法\n\n\nBeautiful Soup\n这是一个 Python 实现的解析库，相比较于前两种来说，语法会更简单明了一点，文档也比较详细。唯一的一点就是运行速度比前两种方式慢几倍，当数据量非常大时相差会更多。\n\n\n本文作为入门教程，就从 Beautiful Soup 入手，来学习一下匹配页面所需元素的方法。假如有以下 HTML 内容 example.html\n<html>\n<head>\n    <meta charset=\"utf-8\" />\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n    <title>Page Title</title>\n</head>\n<body>\n    <div class='main-page'>\n        <ul class='menu-list'>\n                <li>首页</li>\n                <li>新闻</li>\n                <li>影视</li>\n        </ul>\n    </div>\n</body>\n</html>\n我们通过 Beautiful Soup 来解析这个 html. 首先我们pip install beautifulsoup4安装这个库，并看一下简单使用。\n>>>from bs4 import BeautifulSoup\n>>>soup = BeautifulSoup('example.html', 'html.parser') #加载我们的html文件\n>>>soup.find('div') # 找到 div 标签\n'<div class=\"main-page\">\n<ul class=\"menu-list\">\n<li>首页</li>\n<li>新闻</li>\n<li>影视</li>\n</ul>\n</div>'\n\n>>>soup.find_all('li') # 找到所有 li 标签\n'[<li>首页</li>, <li>新闻</li>, <li>影视</li>]'\n\n>>>for i in li:\n    print(i.text)    #获取每个 li 标签的内容\n'\n首页\n新闻\n影视\n'\n详细的操作可以去看一下文档，文档非常详细，例子也很多，简单明了。\n糗百爬虫代码\n我们先爬取纯文本的内容 https://www.qiushibaike.com/t... 爬取这个链接下的内容。我们把页面结构截图如下，我们要获取的信息，我用红色的方框进行了标注。\n图一：\n图二：\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n\ndef download_page(url):\n    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0\"}\n    r = requests.get(url, headers=headers)  # 增加headers, 模拟浏览器\n    return r.text\n\n\ndef get_content(html, page):\n    output = \"\"\"第{}页 作者：{} 性别：{} 年龄：{} 点赞：{} 评论：{}\\n{}\\n------------\\n\"\"\" # 最终输出格式\n    soup = BeautifulSoup(html, 'html.parser')\n    con = soup.find(id='content-left')  # 如图一红色方框\n    con_list = con.find_all('div', class_=\"article\")  # 找到文章列表\n    for i in con_list:\n        author = i.find('h2').string  # 获取作者名字\n        content = i.find('div', class_='content').find('span').get_text()  # 获取内容\n        stats = i.find('div', class_='stats')\n        vote = stats.find('span', class_='stats-vote').find('i', class_='number').string\n        comment = stats.find('span', class_='stats-comments').find('i', class_='number').string\n        author_info = i.find('div', class_='articleGender')  # 获取作者 年龄，性别\n        if author_info is not None:  # 非匿名用户\n            class_list = author_info['class']\n            if \"womenIcon\" in class_list:\n                gender = '女'\n            elif \"manIcon\" in class_list:\n                gender = '男'\n            else:\n                gender = ''\n            age = author_info.string   # 获取年龄\n        else:  # 匿名用户\n            gender = ''\n            age = ''\n\n        save_txt(output.format(page, author, gender, age, vote, comment, content))\n\n\ndef save_txt(*args):\n    for i in args:\n        with open('qiubai.txt', 'a', encoding='utf-8') as f:\n            f.write(i)\n\n\ndef main():\n    # 我们点击下面链接，在页面下方可以看到共有13页，可以构造如下 url，\n    # 当然我们最好是用 Beautiful Soup找到页面底部有多少页。\n    for i in range(1, 14):\n        url = 'https://qiushibaike.com/text/page/{}'.format(i)\n        html = download_page(url)\n        get_content(html, i)\n\nif __name__ == '__main__':\n    main()\n运行代码后，我们会得到 'qiubai.txt'文件，打开后如下所示\n\n                ", "mainLikeNum": ["10 "], "mainBookmarkNum": "6"}