{"title": "scrapy_redis 和 docker 实现简单分布式爬虫 - Hardy 试验田 ", "index": "scrapy,docker,python", "content": "简介\n在使用 scrapy 爬取 IT桔子公司信息，用来进行分析，了解 IT 创业公司的一切情况，之前使用 scrapy 写了一个默认线程是10的单个实例，为了防止被 ban IP 设置了下载的速度，3万多个公司信息爬了1天多才完成，现在想到使用分布式爬虫来提高效率。\n源码githup\n技术工具：Python3.5 scrapy scrapy_redis redis docker1.12 docker-compose Kitematic mysql SQLAlchemy\n\n准备工作\n\n安装 Docker 点这里去了解、安装;\npip install scrapy scrapy_redis;\n\n代码编写\n\n分析页面信息：我需要获取的是每一个「公司」的详情页面链接 和 分页按钮链接；\n统一存储获取到的链接，提供给多个 spider 爬取；\n多个 spider 共享一个 redis list 中的链接；\n\n目录结构图\n\njuzi_spider.py\n# coding:utf-8\n\nfrom bs4 import BeautifulSoup\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\n\nfrom scrapy_redis.spiders import RedisCrawlSpider\nfrom itjuzi_dis.items import CompanyItem\n\n\nclass ITjuziSpider(RedisCrawlSpider):\n    name = 'itjuzi_dis'\n    allowed_domains = ['itjuzi.com']\n    # start_urls = ['http://www.itjuzi.com/company/157']\n    redis_key = 'itjuziCrawler:start_urls'\n    rules = [\n        # 获取每一页的链接\n        Rule(link_extractor=LinkExtractor(allow=('/company\\?page=\\d+'))),\n        # 获取每一个公司的详情\n        Rule(link_extractor=LinkExtractor(allow=('/company/\\d+')), callback='parse_item')\n    ]\n\n    def parse_item(self, response):\n        soup = BeautifulSoup(response.body, 'lxml')\n\n         .\n         .省略一些处理代码\n         .\n        return item\n说明：\n\nclass 继承了RedisCrawlSpider 而不是CrawlSpider\nstart_urls 改为一个自定义的 itjuziCrawler:start_urls,这里的itjuziCrawler:start_urls 就是作为所有链接存储到 redis 中的 key,scrapy_redis 里也是通过redis的 lpop方法弹出并删除链接的；\n\ndb_util.py\n使用 SQLAlchemy 作为 ORM 工具，当表结构不存在时，自动创建表结构\nmiddlewares.py\n增加了很多 User-Agent，每一个请求随机使用一个，防止防止网站通过 User-Agent 屏蔽爬虫\nsettings.py\n配置middlewares.py scrapy_redis redis 链接相关信息\n部署\n在上面的「目录结构图」中有，Dockerfile和docker-compose.yml\nDockerfile\nFROM python:3.5\nENV PATH /usr/local/bin:$PATH\nADD . /code\nWORKDIR /code\nRUN pip install -r requirements.txt\nCOPY spiders.py /usr/local/lib/python3.5/site-packages/scrapy_redis\nCMD /usr/local/bin/scrapy crawl itjuzi_dis\n\n说明：\n\n使用 python3.5作为基础镜像\n将/usr/local/bin设置环境变量\n映射 host 和 container 的目录\n安装 requirements.txt\n特别要说明的是COPY spiders.py /usr/local/lib/python3.5/site-packages/scrapy_redis，将 host 中的 spiders.py 拷贝到container 中的 scrapy_redis 安装目录中，因为 lpop 获取redis 的值在 python2中是 str 类型，而在 python3中是 bytes 类型，这个问题在 scrapy_reids 中需要修复，spiders.py 第84行需要修改；\n启动后立即执行爬行命令 scrapy crawl itjuzi_dis\n\ndocker-compose.yml\nversion: '2'\nservices:\n  spider:\n    build: .\n    volumes:\n     - .:/code\n    links:\n     - redis\n    depends_on:\n     - redis\n  redis:\n    image: redis\n    ports:\n    - \"6379:6379\"\n\n说明:\n\n使用第2版本的 compose 描述语言\n定义了 spider 和 redis 两个 service\nspider默认使用当前目录的 Dockerfile 来创建，redis使用 redis:latest 镜像创建，并都映射6379端口\n\n开始部署\n启动 container\ndocker-compose up #从 docker-compose.yml 中创建 `container` 们\ndocker-compose scale spider=4 #将 spider 这一个服务扩展到4个，还是同一个 redis\n\n可以在 Kitematic GUI 工具中观察创建和运行情况；\n\n在没有设置 start_urls 时，4个 container 中的爬虫都处于饥渴的等待状态\n\n现在给 redis 中放入 start_urls:\nlpush itjuziCrawler:start_urls http://www.itjuzi.com/company\n\n4个爬虫都动起来了，一直爬到start_urls为空\n以上です！ありがとうございました！\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "24"}