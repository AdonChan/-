{"title": "爬虫性能：NodeJs  VS  Python - QueenKing ", "index": "多线程,node.js,python,网页爬虫", "content": "前言\n早就听说Nodejs的异步策略是多么的好，I/O是多么的牛逼......反正就是各种好。今天我就准备给nodejs和python来做个比较。能体现异步策略和I/O优势的项目，我觉得莫过于爬虫了。那么就以一个爬虫项目来一较高下吧。\n爬虫项目\n众筹网-众筹中项目  http://www.zhongchou.com/brow...，我们就以这个网站为例，我们爬取它所有目前正在众筹中的项目，获得每一个项目详情页的URL，存入txt文件中。\n实战比较\npython原始版\n# -*- coding:utf-8 -*-\n'''\nCreated on 20160827\n@author: qiukang\n'''\nimport requests,time\nfrom BeautifulSoup import BeautifulSoup    # HTML\n\n#请求头\nheaders = {\n   'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n   'Accept-Encoding':'gzip, deflate, sdch',\n   'Accept-Language':'zh-CN,zh;q=0.8',\n   'Connection':'keep-alive',\n   'Host':'www.zhongchou.com',\n   'Upgrade-Insecure-Requests':1,\n   'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2593.0 Safari/537.36'\n}\n\n# 获得项目url列表\ndef getItems(allpage):\n    no = 0\n    items = open('pystandard.txt','a')\n    for page in range(allpage):\n        if page==0:\n            url = 'http://www.zhongchou.com/browse/di'\n        else:\n            url = 'http://www.zhongchou.com/browse/di-p'+str(page+1)\n        # print url #①\n        r1 = requests.get(url,headers=headers)\n        html = r1.text.encode('utf8')\n        soup = BeautifulSoup(html);\n        lists = soup.findAll(attrs={\"class\":\"ssCardItem\"})\n        for i in range(len(lists)):\n            href = lists[i].a['href']\n            items.write(href+\"\\n\")\n            no +=1\n    items.close()\n    return no\n    \nif __name__ == '__main__':\n    start = time.clock()\n    allpage = 30\n    no = getItems(allpage)\n    end = time.clock()\n    print('it takes %s Seconds to get %s items '%(end-start,no))\n\n实验5次的结果：\n it takes 48.1727159614 Seconds to get 720 items \n it takes 45.3397999415 Seconds to get 720 items  \n it takes 44.4811429862 Seconds to get 720 items \n it takes 44.4619293082 Seconds to get 720 items\n it takes 46.669706593 Seconds to get 720 items \n\npython多线程版\n# -*- coding:utf-8 -*-\n'''\nCreated on 20160827\n@author: qiukang\n'''\nimport requests,time,threading\nfrom BeautifulSoup import BeautifulSoup    # HTML\n\n#请求头\nheaders = {\n   'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n   'Accept-Encoding':'gzip, deflate, sdch',\n   'Accept-Language':'zh-CN,zh;q=0.8',\n   'Connection':'keep-alive',\n   'Host':'www.zhongchou.com',\n   'Upgrade-Insecure-Requests':1,\n   'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2593.0 Safari/537.36'\n}\n\nitems = open('pymulti.txt','a')\nno = 0\nlock = threading.Lock()\n\n# 获得项目url列表\ndef getItems(urllist):\n    # print urllist  #①\n    global items,no,lock\n    for url in urllist:\n        r1 = requests.get(url,headers=headers)\n        html = r1.text.encode('utf8')\n        soup = BeautifulSoup(html);\n        lists = soup.findAll(attrs={\"class\":\"ssCardItem\"})\n        for i in range(len(lists)):\n            href = lists[i].a['href']\n            lock.acquire()\n            items.write(href+\"\\n\")\n            no +=1\n            # print no\n            lock.release()\n    \nif __name__ == '__main__':\n    start = time.clock()\n    allpage = 30\n    allthread = 30\n    per = (int)(allpage/allthread)\n    urllist = []\n    ths = []\n    for page in range(allpage):\n        if page==0:\n            url = 'http://www.zhongchou.com/browse/di'\n        else:\n            url = 'http://www.zhongchou.com/browse/di-p'+str(page+1)\n        urllist.append(url)\n    for i in range(allthread):\n        # print urllist[i*(per):(i+1)*(per)]\n        th = threading.Thread(target = getItems,args= (urllist[i*(per):(i+1)*(per)],))\n        th.start()\n        th.join()\n    items.close()\n    end = time.clock()\n    print('it takes %s Seconds to get %s items '%(end-start,no))\n    \n实验5次的结果：\nit takes 45.5222291114 Seconds to get 720 items \nit takes 46.7097831417 Seconds to get 720 items\nit takes 45.5334646156 Seconds to get 720 items \nit takes 48.0242797553 Seconds to get 720 items\nit takes 44.804855018 Seconds to get 720 items  \n\n这个多线程并没有优势，经过 #① 的注释与否发现，这个所谓的多线程也是按照单线程运行的。\npython改进\n单线程\n首先我们把解析html的步骤改进一下，分析发现\nlists = soup.findAll('a',attrs={\"class\":\"siteCardICH3\"})\n比\nlists = soup.findAll(attrs={\"class\":\"ssCardItem\"})\n更好，因为它是直接找 a ，而不是先找 div 再找 div 下的 a改进后实验5次结果如下，可见有进步：\nit takes 41.0018861912 Seconds to get 720 items \nit takes 42.0260390497 Seconds to get 720 items\nit takes 42.249635988 Seconds to get 720 items \nit takes 41.295524133 Seconds to get 720 items \nit takes 42.9022894154 Seconds to get 720 items \n\n多线程\n修改 getItems(urllist) 为 getItems(urllist，thno)函数起止加入 print thno,\" begin at\",time.clock() 和 print thno,\" end at\",time.clock()。结果：\n0  begin at 0.00100631078628\n0  end at 1.28625832936\n1  begin at 1.28703230691\n1  end at 2.61739476075\n2  begin at 2.61801291642\n2  end at 3.92514717937\n3  begin at 3.9255829208\n3  end at 5.38870235361\n4  begin at 5.38921134066\n4  end at 6.670658786\n5  begin at 6.67125734731\n5  end at 8.01520989534\n6  begin at 8.01566383155\n6  end at 9.42006780585\n7  begin at 9.42053340537\n7  end at 11.0386755513\n8  begin at 11.0391565464\n8  end at 12.421359168\n9  begin at 12.4218294329\n9  end at 13.9932716671\n10  begin at 13.9939957256\n10  end at 15.3535799145\n11  begin at 15.3540870354\n11  end at 16.6968289314\n12  begin at 16.6972665389\n12  end at 17.9798803157\n13  begin at 17.9804714125\n13  end at 19.326706238\n14  begin at 19.3271438455\n14  end at 20.8744308886\n15  begin at 20.8751017624\n15  end at 22.5306500245\n16  begin at 22.5311450156\n16  end at 23.7781693541\n17  begin at 23.7787245279\n17  end at 25.1775114499\n18  begin at 25.178350742\n18  end at 26.5497330734\n19  begin at 26.5501776789\n19  end at 27.970799259\n20  begin at 27.9712727895\n20  end at 29.4595075375\n21  begin at 29.4599959972\n21  end at 30.9507299602\n22  begin at 30.9513989679\n22  end at 32.2762763982\n23  begin at 32.2767182045\n23  end at 33.6476256057\n24  begin at 33.648137392\n24  end at 35.1100517711\n25  begin at 35.1104907783\n25  end at 36.462657099\n26  begin at 36.4632234696\n26  end at 37.7908515759\n27  begin at 37.7912845182\n27  end at 39.4359928956\n28  begin at 39.436448698\n28  end at 40.9955021593\n29  begin at 40.9960871912\n29  end at 42.6425665264\nit takes 42.6435882327 Seconds to get 720 items \n\n\n可见这些线程是真的没有并发执行，而是顺序执行的，并没有达到多线程的目的。问题在哪里呢？原来我的循环中\nth.start()\nth.join()\n两行代码是紧接着的，所以新的线程会等待上一个线程执行完毕才会start，修改为\nfor i in range(allthread):\n    # print urllist[i*(per):(i+1)*(per)]\n    th = threading.Thread(target = getItems,args= (urllist[i*(per):(i+1)*(per)],i))\n    ths.append(th)\nfor th in ths:\n    th.start()\nfor th in ths:\n    th.join()\n结果：\n0  begin at 0.0010814225325\n1  begin at 0.00135201143191\n2  begin at 0.00191744892518\n3  begin at 0.0021311208492\n4  begin at 0.00247495536449\n5  begin at 0.0027334144167\n6  begin at 0.00320601192551\n7  begin at 0.00379011072218\n8  begin at 0.00425431064445\n9  begin at 0.00511692939449\n10  begin at 0.0132038052264\n11  begin at 0.0165926979253\n12  begin at 0.0170886220634\n13  begin at 0.0174665134574\n14  begin at 0.018348726576\n15  begin at 0.0189780790334\n16  begin at 0.0201896641572\n17  begin at 0.0220576606283\n18  begin at 0.0231484138125\n19  begin at 0.0238804034387\n20  begin at 0.0273901280772\n21  begin at 0.0300363009005\n22  begin at 0.0362878375422\n23  begin at 0.0395512329756\n24  begin at 0.0431556637289\n25  begin at 0.0459581249682\n26  begin at 0.0482254733323\n27  begin at 0.0535430117384\n28  begin at 0.0584971212607\n29  begin at 0.0598136762161\n16  end at 65.2657542222\n24  end at 66.2951247811\n21  end at 66.3849747583\n4  end at 66.6230160119\n5  end at 67.5501632164\n29  end at 67.7516992283\n23  end at 68.6985322418\n7  end at 69.1060433231\n22  end at 69.2743398214\n2  end at 69.5523713152\n14  end at 69.6454986837\n15  end at 69.8333400981\n12  end at 69.9508018062\n10  end at 70.2860348602\n26  end at 70.3670659719\n13  end at 70.3847232972\n27  end at 70.3941635841\n11  end at 70.5132838156\n1  end at 70.7272351926\n0  end at 70.9115253609\n6  end at 71.0876563409\n8  end at 71.112480539825\n  end at 71.1145248855\n3  end at 71.4606034226\n19  end at 71.6103622486\n18  end at 71.6674453096\n20  end at 71.725601862\n17  end at 71.7778992318\n9  end at 71.7847479301\n28  end at 71.7921004837\nit takes 71.7931912368 Seconds to get 720 items \n\n反思\n上面的的多线是并发了，可是比单线程运行时间长了太多......我还没找出来原因，猜想是不是beautifulsoup不支持多线程？请各位多多指教。为了验证这个想法，我准备不用beautifulsoup,直接使用字符串查找。首先还是从单线程的修改：\n# -*- coding:utf-8 -*-\n'''\nCreated on 20160827\n@author: qiukang\n'''\nimport requests,time\nfrom BeautifulSoup import BeautifulSoup    # HTML\n\n#请求头\nheaders = {\n   'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n   'Accept-Encoding':'gzip, deflate, sdch',\n   'Accept-Language':'zh-CN,zh;q=0.8',\n   'Connection':'keep-alive',\n   'Host':'www.zhongchou.com',\n   'Upgrade-Insecure-Requests':'1',\n   'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2593.0 Safari/537.36'\n}\n\n# 获得项目url列表\ndef getItems(allpage):\n    no = 0\n    data = set()\n    for page in range(allpage):\n        if page==0:\n            url = 'http://www.zhongchou.com/browse/di'\n        else:\n            url = 'http://www.zhongchou.com/browse/di-p'+str(page+1)\n        # print url #①\n        r1 = requests.get(url,headers=headers)\n        html = r1.text.encode('utf8')\n        start = 5000    \n        while  True:     \n            index = html.find(\"deal-show\", start)   \n            if index == -1:     \n                break    \n            # print \"http://www.zhongchou.com/deal-show/\"+html[index+10:index+19]+\"\\n\"\n            # time.sleep(100)\n            data.add(\"http://www.zhongchou.com/deal-show/\"+html[index+10:index+19]+\"\\n\")    \n            start = index  + 1000 \n    items = open('pystandard.txt','a')\n    items.write(\"\".join(data))\n    items.close()\n    return len(data)\n    \nif __name__ == '__main__':\n    start = time.clock()\n    allpage = 30\n    no = getItems(allpage)\n    end = time.clock()\n    print('it takes %s Seconds to get %s items '%(end-start,no))\n实验3次，结果：\nit takes 11.6800132309 Seconds to get 720 items\nit takes 11.3621804427 Seconds to get 720 items\nit takes 11.6811991567 Seconds to get 720 items  \n然后对多线程进行修改：\n# -*- coding:utf-8 -*-\n'''\nCreated on 20160827\n@author: qiukang\n'''\nimport requests,time,threading\nfrom BeautifulSoup import BeautifulSoup    # HTML\n\n#请求头\nheader = {\n   'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n   'Accept-Encoding':'gzip, deflate, sdch',\n   'Accept-Language':'zh-CN,zh;q=0.8',\n   'Connection':'keep-alive',\n   'Host':'www.zhongchou.com',\n   'Upgrade-Insecure-Requests':'1',\n   'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2593.0 Safari/537.36'\n}\n\ndata = set()\nno = 0\nlock = threading.Lock()\n\n# 获得项目url列表 \ndef getItems(urllist,thno):\n    # print urllist\n    # print thno,\" begin at\",time.clock()\n    global no,lock,data\n    for url in urllist:\n        r1 = requests.get(url,headers=header)\n        html = r1.text.encode('utf8')\n        start = 5000    \n        while  True:     \n            index = html.find(\"deal-show\", start)   \n            if index == -1:     \n                break\n            lock.acquire()\n            data.add(\"http://www.zhongchou.com/deal-show/\"+html[index+10:index+19]+\"\\n\")    \n            start = index  + 1000 \n            lock.release()\n        \n    # print thno,\" end at\",time.clock()\n    \nif __name__ == '__main__':\n    start = time.clock()\n    allpage = 30  #页数\n    allthread = 10 #线程数\n    per = (int)(allpage/allthread)\n    urllist = []\n    ths = []\n    for page in range(allpage):\n        if page==0:\n            url = 'http://www.zhongchou.com/browse/di'\n        else:\n            url = 'http://www.zhongchou.com/browse/di-p'+str(page+1)\n        urllist.append(url)\n    for i in range(allthread):\n        # print urllist[i*(per):(i+1)*(per)]\n        low = i*allpage/allthread#注意写法\n        high = (i+1)*allpage/allthread\n        # print low,' ',high\n        th = threading.Thread(target = getItems,args= (urllist[low:high],i))\n        ths.append(th)\n    for th in ths:\n        th.start()\n    for th in ths:\n        th.join()\n    items = open('pymulti.txt','a')\n    items.write(\"\".join(data))\n    items.close()\n    end = time.clock()\n    print('it takes %s Seconds to get %s items '%(end-start,len(data)))\n实验3次，结果：\nit takes 1.4781525123 Seconds to get 720 items \nit takes 1.44905954029 Seconds to get 720 items\nit takes 1.49297891786 Seconds to get 720 items\n\n可见多线程确实比单线程快好多倍。对于简单的爬取任务而言，用字符串的内置方法比用beautifulsoup解析html快很多。\nNodeJs\n// npm install request -g #貌似不行，要进入代码所在目录：npm install --save request\n// npm install cheerio -g  #npm install --save cheerio\n\nvar request = require(\"request\");\nvar cheerio = require('cheerio');\nvar fs = require('fs');\n\nvar t1 = new Date().getTime();\nvar allpage = 30;\nvar urllist = new Array()  \nvar urldata = \"\";\nvar mark = 0;\nvar no = 0;\nfor (var i=0; i<allpage; i++) {\n    if (i==0) \n        urllist[i] = 'http://www.zhongchou.com/browse/di'\n    else\n        urllist[i] = 'http://www.zhongchou.com/browse/di-p'+(i+1).toString();\n    request(urllist[i],function(error,resp,body){\n        if (!error && resp.statusCode==200) {\n            getUrl(body);\n        }\n    });\n} \n\nfunction getUrl(data) {\n    var $ = cheerio.load(data);  //cheerio解析data\n    var href = $(\"a.siteCardICH3\").toArray();\n    for (var i = href.length - 1; i >= 0; i--) {\n        // console.log(href[i].attribs[\"href\"]);\n        urldata += (href[i].attribs[\"href\"]+\"\\n\");\n        no += 1;\n    }    \n    mark += 1;\n    if (mark==allpage) {\n        // console.log(urldata);\n        fs.writeFile('./nodestandard.txt',urldata,function(err){\n                    if(err) throw err;\n        });\n        var t2 = new Date().getTime();\n        console.log(\"it takes \" + ((t2-t1)/1000).toString() + \" Seconds to get \" + no.toString() + \" items\");\n    }  \n}\n\n实验5次的结果：\nit takes 3.949 Seconds to get 720 items\nit takes 3.642 Seconds to get 720 items\nit takes 3.641 Seconds to get 720 items\nit takes 3.938 Seconds to get 720 items\nit takes 3.783 Seconds to get 720 items\n\n可见同样是用解析html的方法，nodejs速度完虐python。字符串查找呢？\nvar request = require(\"request\");\nvar cheerio = require('cheerio');\nvar fs = require('fs');\n\nvar t1 = new Date().getTime();\nvar allpage = 30;\nvar urllist = new Array()  ;\nvar urldata = new Array();\nvar mark = 0;\nvar no = 0;\nfor (var i=0; i<allpage; i++) {\n    if (i==0) \n        urllist[i] = 'http://www.zhongchou.com/browse/di'\n    else\n        urllist[i] = 'http://www.zhongchou.com/browse/di-p'+(i+1).toString();\n    // console.log(urllist[i]);\n    request(urllist[i],function(error,resp,body){\n        if (!error && resp.statusCode==200) {\n            getUrl(body);\n        }\n    });\n} \n\nfunction getUrl(data) {\n    mark += 1;\n    var start = 5000\n    while (true) {\n        var index1 = data.indexOf(\"deal-show\", start);\n        if (index1 == -1)     \n            break;\n        var url = \"http://www.zhongchou.com/deal-show/\"+data.substring(index1+10,index1+19)+\"\\n\";\n        // console.log(url);\n        if (urldata.indexOf(url)==-1) {\n            urldata.push(url);\n        }\n        start = index1 + 1000;\n    }\n    if (mark==allpage) {//所有页面执行完毕\n        // console.log(urldata);\n        no = urldata.length;\n        fs.writeFile('./nodestandard.txt',urldata.join(\"\"),function(err){\n                    if(err) throw err;\n        });\n        var t2 = new Date().getTime();\n        console.log(\"it takes \" + ((t2-t1)/1000).toString() + \" Seconds to get \" + no.toString() + \" items\");\n    }  \n}\n实验5次的结果：\nit takes 3.695 Seconds to get 720 items\nit takes 3.781 Seconds to get 720 items\nit takes 3.94 Seconds to get 720 items\nit takes 3.705 Seconds to get 720 items\nit takes 3.601 Seconds to get 720 items\n可见和解析起来的时间是差不多的。\n综上\n由我自己了解的知识和本实验而言，我的结论是：python用上多线程下载速度能够比过nodejs，但是解析网页这种事python没有nodejs快，毕竟js原生就是为了写网页，而且复杂的爬虫总不能都用字符串去找吧。\n2016.9.13-补充\n\n评论中提到的time.time()，感谢老司机指出我的错误,我在python多线程，字符串查找版本中使用了，实验3次过后依然是快于nodejs版本的平均用时2.3S，不知道是不是您和我的网络环境不一样导致？我准备换个教室试试......至于有没有误导人，我想读者会自己去尝试，得出自己的结论。\nPython的确有异步（twisted），nodejs也的确有多进程（child_process），我想追求极致的性能比较还需要对这两种语言有更深入的研究，这个我目前也是半知不解，我会尽快花时间了解，争取实现比较（这里不是追求编程方法的比较，就是单纯的想比较在同一台机器同一个网络下，两种语言能做到的极致。道阻且长啊。）\n还有解析方法，我这里用的是python自带的解析，官网说lxml的确比自带的快，但是我这里换了过后多线程依然没有体现出来优势，所以我还是很疑惑，是不是beautifulsoup不支持多线程？，我在官网没找到相关文档，请各位指教。另外from BeautifulSoup import BeautifulSoup的确是比from bs4 import BeautifulSoup 慢多了，这是BeautifulSoup的版本原因，感谢评论者指出。\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "20"}