{"title": "使用lxml代替beautifulsoup - 浮生若梦的编程 ", "index": "python", "content": "深入使用过lxml的都会深深地喜欢上它,虽然BeautifulSoup很流行,但是深入了解lxml后,你就再也不会使用bs了\n我觉得beautifulsoup不好用,速度也慢(虽然可以使用lxml作为解析器了).另外soup.find_all这种简直就是手工时代的操作(很多人使用find find_all这几个函数, 其实它们使用起来不是很愉快),手工操作的另一个特征是需要自己手写正则表达式(正则是一个小范围内的军刀,大范围用是不对的)\n多使用soup.select才是正确的选择, 多简单方便.\n爬虫对html页面做的操作,大部分时候在选取东西,find_all还要自己手工去写(比如手工写正则表达式, it's a hell).\n使用XPath以及CSS选择器才是明智的选择,这两者,浏览器都可以帮我们自动生成,何乐不为?\n另外,lxml用起来舒服多了,速度也不错.\n另外,lxml安装是一个难题,常有很多错误,令人望而生畏,这里提供两种方法\n\nWindows平台: 使用Anaconda集成安装包,无脑解决\nUbuntu平台: sudo apt-get install python-lxml无脑解决我从不觉得去自己解决lxml在这些平台的安装有什么很大价值,反而容易令人自我怀疑.(宝贵的时间为何要浪费在这种事情上面?)\n\n下面就来一个示例:\n#####################\n#  获取SF首页的标题\n#####################\nfrom lxml.etree import HTML\nimport requests\n\n\nurl = 'https://segmentfault.com/'\ncss_selector = '.title>a'  #这是利用浏览器自动获取的,我甚至都不用知道它是什么意思\n\ntext = requests.get(url).text\npage = HTML(text)\n\ntitles = []\nfor title in page.cssselect(css_selector):\n    titles.append(title.text)\n    \nprint titles\n\n# 这一段程序写下来,不用动脑筋(无脑写),不消耗心智\n利用浏览器获取XPath/CSS选择器的方法:\n1. 推荐使用Firefox, 比Chrome强大太多\n2. 右键元素-->copy XPath\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "3"}