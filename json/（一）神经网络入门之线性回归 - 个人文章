{"title": "（一）神经网络入门之线性回归 - 个人文章 ", "index": "python", "content": "作者：chen_h微信号 & QQ：862251340微信公众号：coderpai简书地址：https://www.jianshu.com/p/0da...\n\n这篇教程是翻译Peter Roelants写的神经网络教程，作者已经授权翻译，这是原文。\n该教程将介绍如何入门神经网络，一共包含五部分。你可以在以下链接找到完整内容。\n\n（一）神经网络入门之线性回归\nLogistic分类函数\n（二）神经网络入门之Logistic回归（分类问题）\n（三）神经网络入门之隐藏层设计\nSoftmax分类函数\n（四）神经网络入门之矢量化\n（五）神经网络入门之构建多层网络\n\n这篇教程中的代码是由 Python 2 IPython Notebook产生的，在教程的最后，我会给出全部代码的链接，帮助学习。神经网络中有关矩阵的运算我们采用NumPy来构建，画图使用Matplotlib来构建。如果你来没有按照这些软件，那么我强烈建议你使用Anaconda Python来安装，这个软件包中包含了运行这个教程的所有软件包，非常方便使用。\n我们先导入教程需要的软件包\nfrom __future__ import print_function\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n线性回归\n\n本教程主要包含三部分：\n\n一个非常简单的神经网络\n一些概念，比如目标函数，损失函数\n梯度下降\n\n首先我们来构建一个最简单的神经网络，这个神经网络只有一个输入，一个输出，用来构建一个线性回归模型，从输入的x来预测一个真实结果t。神经网络的模型结构为y = x * w ，其中x是输入参数，w是权重，y是预测结果。神经网络的模型可以被表示为下图：\n\n在常规的神经网络中，神经网络结构中有多个层，非线性激活函数和每个节点上面的偏差单元。在这个教程中，我们只使用一个只有一个权重w的层，并且没有激活函数和偏差单元。在简单线性回归中，权重w和偏差单元一般都写成一个参数向量β，其中偏差单元是y轴上面的截距，w是回归线的斜率。在线性回归中，我们一般使用最小二乘法来优化这些参数。\n在这篇教程中，我们的目的是最小化目标损失函数，使得实际输出的y和正确结果t尽可能的接近。损失函数我们定义为：\n对于损失函数的优化，我们采用梯度下降，这个方法是神经网络中常见的优化方法。\n定义目标函数\n在这个例子中，我们使用函数f来产生目标结果t，但是对目标结果加上一些高斯噪声N(0, 0.2)，其中N表示正态分布，均值是0，方差是0.2，f定义为f(x) = 2x，x是输入参数，回归线的斜率是2，截距是0。所以最后的t = f(x) + N(0, 0.2)。\n我们将产生20个均匀分布的数据作为数据样本x，然后设计目标结果t。下面的程序我们生成了x和t，以及画出了他们之间的线性关系。\n# Define the vector of input samples as x, with 20 values sampled from a uniform distribution\n# between 0 and 1\nx = np.random.uniform(0, 1, 20)\n\n# Generate the target values t from x with small gaussian noise so the estimation won't be perfect.\n# Define a function f that represents the line that generates t without noise\ndef f(x): return x * 2\n\n# Create the targets t with some gaussian noise\nnoise_variance = 0.2 # Variance of the gaussian noise\n# Gaussian noise error for each sample in x\nnoise = np.random.randn(x.shape[0]) * noise_variance\n# Create targets t\nt = f(x) + noise\n# Plot the target t versus the input x\nplt.plot(x, t, 'o', label='t')\n# Plot the initial line\nplt.plot([0, 1], [f(0), f(1)], 'b-', label='f(x)')\nplt.xlabel('$x$', fontsize=15)\nplt.ylabel('$t$', fontsize=15)\nplt.ylim([0,2])\nplt.title('inputs (x) vs targets (t)')\nplt.grid()\nplt.legend(loc=2)\nplt.show()\n\n定义损失函数\n我们将优化模型y = w * x中的参数w，使得对于训练集中的N个样本，损失函数达到最小。\n\n即，我们的优化目标是：\n\n从函数中，我们可以发现，我们将所有样本的误差都进行了累加，这就是所谓的批训练（batch training）。我们也可以在训练的时候，每次训练一个样本，这种方法在在线训练中非常常用。\n我们利用以下函数画出损失函数与权重的关系。从图中，我们可以看出损失函数的值达到最小时，w的值是2。这个值就是我们函数f(x)的斜率。这个损失函数是一个凸函数，并且只有一个全局最小值。\nnn(x, w)函数实现了神经网络模型，cost(y, t)函数实现了损失函数。\n# Define the neural network function y = x * w\ndef nn(x, w): return x*w\n\n# Define the cost function\ndef cost(y, t): return ((t - y) ** 2).sum()\n\n优化损失函数\n对于教程中简单的损失函数，可能你看一眼就能知道最佳的权重是什么。但是对于复杂的或者更高维度的损失函数，这就是我们为什么要使用各种优化方法的原因了。\n梯度下降\n在训练神经网络中，梯度下降算法是一种比较常用的优化算法。梯度下降算法的原理是损失函数对于每个参数进行求导，并且利用负梯度对参数进行更新。权重w通过循环进行更新：\n\n其中，w(k)表示权重w更新到第k步时的值，Δw为定义为：\n\n其中，μ是学习率，它的含义是在参数更新的时候，每一步的跨度大小。∂ξ/∂w 表示损失函数 ξ 对于 w 的梯度。对于每一个训练样本i，我们可以利用链式规则推导出对应的梯度，如下：\n\n其中，ξi是第i个样本的损失函数，因此，∂ξi/∂yi可以这样进行推导：\n\n因为y(i) = x(i) ∗ w，所以我们对于∂yi/∂w可以这样进行推导：\n\n因此，对于第i个训练样本，Δw的完整推导如下：\n\n在批处理过程中，我们将所有的梯度都进行累加：\n\n在进行梯度下降之前，我们需要对权重进行一个初始化，然后再使用梯度下降算法进行训练，最后直至算法收敛。学习率作为一个超参数，需要单独调试。\ngradient(w, x, t)函数实现了梯度∂ξ/∂w，delta_w(w_k, x, t, learning_rate)函数实现了Δw。\n# define the gradient function. Remember that y = nn(x, w) = x * w\ndef gradient(w, x, t):\n  return 2 * x * (nn(x, w) - t)\n\n# define the update function delta w\ndef delta_w(w_k, x, t, learning_rate):\n  return learning_rate * gradient(w_k, x, t).sum()\n\n# Set the initial weight parameter\nw = 0.1\n# Set the learning rate\nlearning_rate = 0.1\n\n# Start performing the gradient descent updates, and print the weights and cost:\nnb_of_iterations = 4 # number of gradient descent updates\nw_cost = [(w, cost(nn(x, w), t))] # List to store the weight, costs values\nfor i in range(nb_of_iterations):\n  dw = delta_w(w, x, t, learning_rate) # Get the delta w update\n  w = w - dw # Update the current weight parameter\n  w_cost.append((w, cost(nn(x, w), t))) # Add weight, cost to list\n\n# Print the final w, and cost\nfor i in range(0, len(w_cost)):\n  print('w({}): {:.4f} \\t cost: {:.4f}'.format(i, w_cost[i][0], w_cost[i][1]))\n\n# output\nw(0): 0.1000   cost: 23.3917\nw(1): 2.3556   cost: 1.0670\nw(2): 2.0795   cost: 0.7324\nw(3): 2.1133   cost: 0.7274\nw(4): 2.1091   cost: 0.7273\n\n从计算结果中，我们很容易的看出来了，梯度下降算法很快的收敛到了2.0左右，接下来可视化一下梯度下降过程。\n# Plot the first 2 gradient descent updates\nplt.plot(ws, cost_ws, 'r-')  # Plot the error curve\n# Plot the updates\nfor i in range(0, len(w_cost)-2):\n  w1, c1 = w_cost[i]\n  w2, c2 = w_cost[i+1]\n  plt.plot(w1, c1, 'bo')\n  plt.plot([w1, w2],[c1, c2], 'b-')\n  plt.text(w1, c1+0.5, '$w({})$'.format(i)) \n# Show figure\nplt.xlabel('$w$', fontsize=15)\nplt.ylabel('$\\\\xi$', fontsize=15)\nplt.title('Gradient descent updates plotted on cost function')\nplt.grid()\nplt.show()\n\n梯度更新\n上图展示了梯度下降的可视化过程。图中蓝色的点表示在第k轮中w(k)的值。从图中我们可以得知，w的值越来越收敛于2.0。该模型训练10次就能收敛，如下图所示。\nw = 0\n# Start performing the gradient descent updates\nnb_of_iterations = 10  # number of gradient descent updates\nfor i in range(nb_of_iterations):\n  dw = delta_w(w, x, t, learning_rate)  # get the delta w update\n  w = w - dw  # update the current weight parameter\n\n# Plot the fitted line agains the target line\n# Plot the target t versus the input x\nplt.plot(x, t, 'o', label='t')\n# Plot the initial line\nplt.plot([0, 1], [f(0), f(1)], 'b-', label='f(x)')\n# plot the fitted line\nplt.plot([0, 1], [0*w, 1*w], 'r-', label='fitted line')\nplt.xlabel('input x')\nplt.ylabel('target t')\nplt.ylim([0,2])\nplt.title('input vs. target')\nplt.grid()\nplt.legend(loc=2)\nplt.show()\n\n完整代码，点击这里\n\n作者：chen_h微信号 & QQ：862251340简书地址：https://www.jianshu.com/p/0da...\nCoderPai 是一个专注于算法实战的平台，从基础的算法到人工智能算法都有设计。如果你对算法实战感兴趣，请快快关注我们吧。加入AI实战微信群，AI实战QQ群，ACM算法微信群，ACM算法QQ群。长按或者扫描如下二维码，关注 “CoderPai” 微信号（coderpai）\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "0"}