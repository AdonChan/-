{"title": "Scrapy-Cluster结合Spiderkeeper管理分布式爬虫 - 个人文章 ", "index": "scrapy,python", "content": "Scrapy-cluster 建设\n\n基于Scrapy-cluster库的kafka-monitor可以实现分布式爬虫\nScrapyd+Spiderkeeper实现爬虫的可视化管理\n\n环境\n\n\nIP\nRole\n\n\n\n168.*.*.118\nScrapy-cluster,scrapyd,spiderkeeper\n\n\n168.*.*.119\nScrapy-cluster,scrapyd,kafka,redis,zookeeper\n\n\n\n# cat /etc/redhat-release \nCentOS Linux release 7.4.1708 (Core) \n# python -V\nPython 2.7.5\n# java -version\nopenjdk version \"1.8.0_181\"\nOpenJDK Runtime Environment (build 1.8.0_181-b13)\nOpenJDK 64-Bit Server VM (build 25.181-b13, mixed mode)\nZookeeper 单机配置\n下载并配置\n# wget http://mirror.bit.edu.cn/apache/zookeeper/zookeeper-3.4.13/zookeeper-3.4.13.tar.gz\n# tar -zxvf zookeeper-3.4.13.tar.gz\n# cd zookeeper-3.4.13/conf\n# cp zoo_sample.cfg zoo.cfg\n# cd ..\n# PATH=/opt/zookeeper-3.4.13/bin:$PATH\n# echo 'export PATH=/opt/zookeeper-3.4.13/bin:$PATH' > /etc/profile.d/zoo.sh\n单节点启动\n# zkServer.sh status\nZooKeeper JMX enabled by default\nUsing config: /opt/zookeeper-3.4.13/bin/../conf/zoo.cfg\nError contacting service. It is probably not running.\n\n# zkServer.sh start\nkafka 单机配置\n下载\n# wget http://mirrors.hust.edu.cn/apache/kafka/2.0.0/kafka_2.12-2.0.0.tgz\n# tar -zxvf kafka_2.12-2.0.0.tgz\n# cd kafka_2.12-2.0.0/\n配置\n# vim config/server.properties\n\n############################# Server Basics #############################\n\n# The id of the broker. This must be set to a unique integer for each broker.\nbroker.id=0                     # kafka的机器编号，\nhost.name = 168.*.*.119         # 绑定ip\nport=9092                        # 默认端口9092，\n# Switch to enable topic deletion or not, default value is false\ndelete.topic.enable=true\n############################# Zookeeper #############################\nzookeeper.connect=localhost:2181\n启动\nnohup bin/kafka-server-start.sh config/server.properties & \n停止命令bin/kafka-server-stop.sh config/server.properties\nredis 单机配置\n安装配置\n# yum -y install redis\n# vim /etc/redis.conf\nbind 168.*.*.119\n启动\n# systemctl start redis.service\nscrapy-cluster 单机配置\n# git clone https://github.com/istresearch/scrapy-cluster.git\n# cd scrapy-cluster\n# pip install -r requirements.txt\n离线运行单元测试,以确保一切似乎正常\n# ./run_offline_tests.sh\n修改配置\n# vim kafka-monitor/settings.py\n# vim redis-monitor/settings.py\n# vim crawlers/crawling/settings.py\n修改以下\n# Redis host configuration\nREDIS_HOST = '168.*.*.119'\nREDIS_PORT = 6379\nREDIS_DB = 0\n\nKAFKA_HOSTS = '168.*.*.119:9092'\nKAFKA_TOPIC_PREFIX = 'demo'\nKAFKA_CONN_TIMEOUT = 5\nKAFKA_APPID_TOPICS = False\nKAFKA_PRODUCER_BATCH_LINGER_MS = 25  # 25 ms before flush\nKAFKA_PRODUCER_BUFFER_BYTES = 4 * 1024 * 1024  # 4MB before blocking\n\n# Zookeeper Settings\nZOOKEEPER_ASSIGN_PATH = '/scrapy-cluster/crawler/'\nZOOKEEPER_ID = 'all'\nZOOKEEPER_HOSTS = '168.*.*.119:2181'\n启动监听\n# nohup python kafka_monitor.py run >> /root/scrapy-cluster/kafka-monitor/kafka_monitor.log 2>&1 &\n# nohup python redis_monitor.py >> /root/scrapy-cluster/redis-monitor/redis_monitor.log 2>&1 &\nscrapyd 爬虫管理工具配置\n安装\n# pip install scrapyd\n配置\n# sudo mkdir /etc/scrapyd\n# sudo vi /etc/scrapyd/scrapyd.conf\n[scrapyd]\neggs_dir    = eggs\nlogs_dir    = logs\nitems_dir   =\njobs_to_keep = 5\ndbs_dir     = dbs\nmax_proc    = 0\nmax_proc_per_cpu = 10\nfinished_to_keep = 100\npoll_interval = 5.0\nbind_address = 0.0.0.0\nhttp_port   = 6800\ndebug       = off\nrunner      = scrapyd.runner\napplication = scrapyd.app.application\nlauncher    = scrapyd.launcher.Launcher\nwebroot     = scrapyd.website.Root\n\n[services]\nschedule.json     = scrapyd.webservice.Schedule\ncancel.json       = scrapyd.webservice.Cancel\naddversion.json   = scrapyd.webservice.AddVersion\nlistprojects.json = scrapyd.webservice.ListProjects\nlistversions.json = scrapyd.webservice.ListVersions\nlistspiders.json  = scrapyd.webservice.ListSpiders\ndelproject.json   = scrapyd.webservice.DeleteProject\ndelversion.json   = scrapyd.webservice.DeleteVersion\nlistjobs.json     = scrapyd.webservice.ListJobs\ndaemonstatus.json = scrapyd.webservice.DaemonStatus\n启动\n# nohup scrapyd >> /root/scrapy-cluster/scrapyd.log 2>&1 &\n建议做Nginx反向代理\n启动异常\nFile \"/usr/local/lib/python3.6/site-packages/scrapyd-1.2.0-py3.6.egg/scrapyd/app.py\", line 2, in <module>\nfrom twisted.application.internet import TimerService, TCPServer\nFile \"/usr/local/lib64/python3.6/site-packages/twisted/application/internet.py\", line 54, in <module>\nfrom automat import MethodicalMachine\nFile \"/usr/local/lib/python3.6/site-packages/automat/__init__.py\", line 2, in <module>\nfrom ._methodical import MethodicalMachine\nFile \"/usr/local/lib/python3.6/site-packages/automat/_methodical.py\", line 210, in <module>\n    class MethodicalInput(object):\nFile \"/usr/local/lib/python3.6/site-packages/automat/_methodical.py\", line 220, in MethodicalInput\n    @argSpec.default\nbuiltins.TypeError: '_Nothing' object is not callable\n\n\nFailed to load application: '_Nothing' object is not callable\n解决：Automat降级\npip install Automat==0.6.0\nSpiderkeeper 爬虫管理界面配置\n安装\npip install SpiderKeeper\n启动\nmkdir /root/spiderkeeper/\nnohup spiderkeeper --server=http://168.*.*.118:6800 --username=admin --password=admin --database-url=sqlite:////root/spiderkeeper/SpiderKeeper.db >> /root/scrapy-cluster/spiderkeeper.log 2>&1 &\n浏览器访问http://168.*.*.118:5000\n\n使用Spiderkeeper 管理爬虫\n使用scrapyd-deploy部署爬虫项目\n修改scrapy.cfg配置\nvim /root/scrapy-cluster/crawler/scrapy.cfg\n[settings]\ndefault = crawling.settings\n\n[deploy]\nurl = http://168.*.*.118:6800/\nproject = crawling\n添加新的spider\ncd /root/scrapy-cluster/crawler/crawling/spider\n使用scrapyd-deploy部署项目\n# cd /root/scrapy-cluster/crawler\n# scrapyd-deploy \nPacking version 1536225989\nDeploying to project \"crawling\" in http://168.*.*.118:6800/addversion.json\nServer response (200):\n{\"status\": \"ok\", \"project\": \"crawling\", \"version\": \"1536225989\", \"spiders\": 3, \"node_name\": \"ambari\"}\nspiderkeeper 配置爬虫项目\n登录Spiderkeeper创建项目\n使用scrapy.cfg中配置的项目名\n\n创建后再Spiders->Dashboard中看到所有spider\n\nScrapy-cluster 分布式爬虫\nScrapy Cluster需要在不同的爬虫服务器之间进行协调，以确保最大的内容吞吐量，同时控制集群服务器爬取网站的速度。\nScrapy Cluster提供了两种主要策略来控制爬虫对不同域名的攻击速度。这由爬虫的类型与IP地址确定，但他们都作用于不同的域名队列。\nScrapy-cluster分布式爬虫，分发网址是基于IP地址。在不同的机器上启动集群，不同服务器上的每个爬虫去除队列中的所有链接。\n部署集群中第二个scrapy-cluster\n配置一台新的服务器参照scrapy-cluster 单机配置,同时使用第一台服务器配置kafka-monitor/settings.py redis-monitor/settings.py crawling/settings.py\nCurrent public ip 问题\n由于两台服务器同时部署在相同内网，spider运行后即获取相同Current public ip，导致scrapy-cluster调度器无法根据IP分发链接\n2018-09-07 16:08:29,684 [sc-crawler] DEBUG: Current public ip: b'110.*.*.1'\n参考代码/root/scrapy-cluster/crawler/crawling/distributed_scheduler.py第282行：\ntry:\n    obj = urllib.request.urlopen(settings.get('PUBLIC_IP_URL',\n                                  'http://ip.42.pl/raw'))\n    results = self.ip_regex.findall(obj.read())\n    if len(results) > 0:\n        # results[0] 获取IP地址即为110.90.122.1\n        self.my_ip = results[0]\n    else:\n        raise IOError(\"Could not get valid IP Address\")\n    obj.close()\n    self.logger.debug(\"Current public ip: {ip}\".format(ip=self.my_ip))\nexcept IOError:\n    self.logger.error(\"Could not reach out to get public ip\")\n    pass\n建议修改代码，获取本机IP\nself.my_ip = [(s.connect(('8.8.8.8', 53)), s.getsockname()[0], s.close()) \n                for s in [socket.socket(socket.AF_INET, socket.SOCK_DGRAM)]][0][1]\n运行分布式爬虫\n在两个scrapy-cluster中运行相同Spider\nexecute(['scrapy', 'runspider', 'crawling/spiders/link_spider.py'])\n使用python kafka_monitor.py feed投递多个链接，使用DEBUG即可观察到链接分配情况\n使用SpiderKeeper管理分布式爬虫\n配置scrapyd管理集群第二个scrapy-cluster\n在第二台scrapy-cluster服务器上安装配置scrapyd，参考scrapyd 爬虫管理工具配置并修改配置\n[settings]\ndefault = crawling.settings\n\n[deploy]\nurl = http://168.*.*.119:6800/\nproject = crawling\n启动scrapyd后使用scrapyd-deploy工具部署两个scrapy-cluster上的爬虫项目。\n使用Spiderkeeper连接多个scrapy-cluster\n重新启动spiderkeeper，对接两个scrapy-cluster的管理工具scrapyd。\nnohup spiderkeeper --server=http://168.*.*.118:6800 --server=http://168.*.*.119:6800 --username=admin --password=admin --database-url=sqlite:////root/spiderkeeper/SpiderKeeper.db >> /root/scrapy-cluster/spiderkeeper.log 2>&1 &\n注意：要使用spiderkeeper管理同一个集群，爬虫项目名称须一致，同时集群中scrapy-cluster配置相同spider任务\n浏览器访问http://168.*.*.118:5000 启动爬虫时即可看见两个scrapy-cluster集群配置，启动同名爬虫开始scrapy-cluster分布式爬虫\n\n启动分布式爬虫后状态\n\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "0"}