{"title": "利用 TensorFlow 入门 Word2Vec - 个人文章 ", "index": "python", "content": "作者：chen_h微信号 & QQ：862251340微信公众号：coderpai简书地址：https://www.jianshu.com/p/4e1...\n\n\n我认为学习算法的最好方法就是尝试去实现它，因此这个教程我们就来学习如何利用 TensorFlow 来实现词嵌入。\n这篇文章我们不会去过多的介绍一些词向量的内容，所以很多 king - man - woman - queue 的例子会被省去，直接进入编码实践过程。\n我们如何设计这些词嵌入？\n对于如何设计词嵌入有很多的技术，这里我们讨论一种非常有名的技术。与我们往常的认知不同，word2vec 并不是一个深层的网络，它只是一个三层的浅层网络。\n注意：word2vec 有很多的技术细节，但是我们会跳过这些细节，来使得更加容易理解。\nword2vec 如何工作？\nword2vec 算法的设计如下：\n\n它是一个三层的网络（一个输入层 + 一个隐藏层 + 一个输出层）。\n模型输入一个词，然后去预测它周围的词。\n移除最后一层（输出层），保留输入层和隐藏层。\n现在，输入一个词库中的词，然后隐藏层的输出就是输入词的词向量。\n\n就是这么简单，这个三层网络就可以得到一个还不错的词向量。\n接下来就让我们来实现这个模型。完整的代码可以点击 Github，但我建议你先不要看完整的代码，先一步一步学习。\n接下来，我们先定义我们要处理的原始文本：\nimport numpy as np\nimport tensorflow as tf\ncorpus_raw = 'He is the king . The king is royal . She is the royal  queen '\n\n# convert to lower case\ncorpus_raw = corpus_raw.lower()\n现在，我们需要将输入的原始文本数据转换成一个输入输出对，以便我们对输入的词，可以去预测它附近的词。比如，我们确定一个中心词， 窗口大小 window_size 设置为 n ，那么我们就是去预测中心词前面 n 个词和后面 n 个词。Chris McCormick 的这篇博客给出了比较详细的解释。\n\n注意：如果中心词是在句子的开头或者末尾，那么我们就忽略窗口无法获得的词。\n在做这个之前，我们需要创建一个字典，用来确定每个单词的索引，具体如下：\nwords = []\nfor word in corpus_raw.split():\n    if word != '.': # because we don't want to treat . as a word\n        words.append(word)\nwords = set(words) # so that all duplicate words are removed\nword2int = {}\nint2word = {}\nvocab_size = len(words) # gives the total number of unique words\nfor i,word in enumerate(words):\n    word2int[word] = i\n    int2word[i] = word\n这个字典的运行结果如下：\nprint(word2int['queen'])\n-> 42 (say)\n\nprint(int2word[42])\n-> 'queen'\n接下来，我们将我们的句子向量转换成单词列表，如下：\n# raw sentences is a list of sentences.\nraw_sentences = corpus_raw.split('.')\nsentences = []\nfor sentence in raw_sentences:\n    sentences.append(sentence.split())\n上面代码将帮助我们得到一个句子的列表，列表中的每一个元素是句子的单词列表，如下：\nprint(sentences)\n\n-> [['he', 'is', 'the', 'king'], ['the', 'king', 'is', 'royal'], ['she', 'is', 'the', 'royal', 'queen']]\n接下来，我们要产生我们的训练数据：\ndata = []\n\nWINDOW_SIZE = 2\n\nfor sentence in sentences:\n    for word_index, word in enumerate(sentence):\n        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] : \n            if nb_word != word:\n                data.append([word, nb_word])\n这个程序给出了单词输入输出对，我们将窗口的大小设置为 2。\nprint(data)\n[['he', 'is'],\n ['he', 'the'],\n ['is', 'he'],\n ['is', 'the'],\n ['is', 'king'],\n ['the', 'he'],\n ['the', 'is'], \n.\n.\n.\n]\n至此，我们有了我们的训练数据，但是我们需要将它转换成计算机可以理解的表示，即数字。也就是我们之前设计的 word2int 字典。\n我们再进一步表示，将这些数字转换成 0-1 向量。\ni.e., \nsay we have a vocabulary of 3 words : pen, pineapple, apple\nwhere \nword2int['pen'] -> 0 -> [1 0 0]\nword2int['pineapple'] -> 1 -> [0 1 0]\nword2int['apple'] -> 2 -> [0 0 1]\n那么为什么要表示成 0-1 向量呢？这个问题我们后续讨论。\n# function to convert numbers to one hot vectors\ndef to_one_hot(data_point_index, vocab_size):\n    temp = np.zeros(vocab_size)\n    temp[data_point_index] = 1\n    return temp\nx_train = [] # input word\ny_train = [] # output word\nfor data_word in data:\n    x_train.append(to_one_hot(word2int[ data_word[0] ], vocab_size))\n    y_train.append(to_one_hot(word2int[ data_word[1] ], vocab_size))\n# convert them to numpy arrays\nx_train = np.asarray(x_train)\ny_train = np.asarray(y_train)\n现在，我们有了 x_train 和 y_train 数据：\nprint(x_train)\n->\n[[ 0.  0.  0.  0.  0.  0.  1.]\n [ 0.  0.  0.  0.  0.  0.  1.]\n [ 0.  0.  0.  0.  0.  1.  0.]\n [ 0.  0.  0.  0.  0.  1.  0.]\n [ 0.  0.  0.  0.  0.  1.  0.]\n [ 0.  0.  0.  0.  1.  0.  0.]\n [ 0.  0.  0.  0.  1.  0.  0.]\n [ 0.  0.  0.  0.  1.  0.  0.]\n [ 0.  0.  0.  1.  0.  0.  0.]\n [ 0.  0.  0.  1.  0.  0.  0.]\n [ 0.  0.  0.  0.  1.  0.  0.]\n [ 0.  0.  0.  0.  1.  0.  0.]\n [ 0.  0.  0.  1.  0.  0.  0.]\n [ 0.  0.  0.  1.  0.  0.  0.]\n [ 0.  0.  0.  1.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  1.  0.]\n [ 0.  0.  0.  0.  0.  1.  0.]\n [ 0.  0.  0.  0.  0.  1.  0.]\n [ 0.  1.  0.  0.  0.  0.  0.]\n [ 0.  1.  0.  0.  0.  0.  0.]\n [ 0.  0.  1.  0.  0.  0.  0.]\n [ 0.  0.  1.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  1.  0.]\n [ 0.  0.  0.  0.  0.  1.  0.]\n [ 0.  0.  0.  0.  0.  1.  0.]\n [ 0.  0.  0.  0.  1.  0.  0.]\n [ 0.  0.  0.  0.  1.  0.  0.]\n [ 0.  0.  0.  0.  1.  0.  0.]\n [ 0.  0.  0.  0.  1.  0.  0.]\n [ 0.  1.  0.  0.  0.  0.  0.]\n [ 0.  1.  0.  0.  0.  0.  0.]\n [ 0.  1.  0.  0.  0.  0.  0.]\n [ 1.  0.  0.  0.  0.  0.  0.]\n [ 1.  0.  0.  0.  0.  0.  0.]]\n这两个数据的维度如下：\nprint(x_train.shape, y_train.shape)\n->\n(34, 7) (34, 7)\n# meaning 34 training points, where each point has 7 dimensions\n构造 TensorFlow 模型\n# making placeholders for x_train and y_train\n\nx = tf.placeholder(tf.float32, shape=(None, vocab_size))\ny_label = tf.placeholder(tf.float32, shape=(None, vocab_size))\n\n从上图中可以看出，我们将训练数据转换成了另一种向量表示。\nEMBEDDING_DIM = 5 # you can choose your own number\nW1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))\nb1 = tf.Variable(tf.random_normal([EMBEDDING_DIM])) #bias\nhidden_representation = tf.add(tf.matmul(x,W1), b1)\n接下来，我们对隐藏层的数据进行处理，并且对其附近的词进行预测。预测词的方法我们采用 softmax 方法。\n\nW2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\nb2 = tf.Variable(tf.random_normal([vocab_size]))\nprediction = tf.nn.softmax(tf.add( tf.matmul(hidden_representation, W2), b2))\n所以，完整的模型是：\n\ninput_one_hot  --->  embedded repr. ---> predicted_neighbour_prob\npredicted_prob will be compared against a one hot vector to correct it.\n现在，我们可以训练这个模型：\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init) #make sure you do this!\n# define the loss function:\ncross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), reduction_indices=[1]))\n# define the training step:\ntrain_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)\nn_iters = 10000\n# train for n_iter iterations\nfor _ in range(n_iters):\n    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n    print('loss is : ', sess.run(cross_entropy_loss, feed_dict={x: x_train, y_label: y_train}))\n在训练的过程中，你在控制台可以得到如下结果：\nloss is :  2.73213\nloss is :  2.30519\nloss is :  2.11106\nloss is :  1.9916\nloss is :  1.90923\nloss is :  1.84837\nloss is :  1.80133\nloss is :  1.76381\nloss is :  1.73312\nloss is :  1.70745\nloss is :  1.68556\nloss is :  1.66654\nloss is :  1.64975\nloss is :  1.63472\nloss is :  1.62112\nloss is :  1.6087\nloss is :  1.59725\nloss is :  1.58664\nloss is :  1.57676\nloss is :  1.56751\nloss is :  1.55882\nloss is :  1.55064\nloss is :  1.54291\nloss is :  1.53559\nloss is :  1.52865\nloss is :  1.52206\nloss is :  1.51578\nloss is :  1.50979\nloss is :  1.50408\nloss is :  1.49861\n.\n.\n.\n随着损失值的不断下降，最终会达到一个稳定值。即使我们无法获得很精确的结果，但是我们也不在乎，因为我们感兴趣的是 W1 和 b1 的值，即隐藏层的权重。\n让我们来看看这些权重，如下：\nprint(sess.run(W1))\nprint('----------')\nprint(sess.run(b1))\nprint('----------')\n\n->\n[[-0.85421133  1.70487809  0.481848   -0.40843448 -0.02236851]\n [-0.47163373  0.34260952 -2.06743765 -1.43854153 -0.14699034]\n [-1.06858993 -1.10739779  0.52600187  0.24079895 -0.46390489]\n [ 0.84426647  0.16476244 -0.72731972 -0.31994426 -0.33553854]\n [ 0.21508843 -1.21030915 -0.13006891 -0.24056002 -0.30445012]\n [ 0.17842589  2.08979321 -0.34172744 -1.8842833  -1.14538431]\n [ 1.61166084 -1.17404735 -0.26805425  0.74437028 -0.81183684]]\n----------\n[ 0.57727528 -0.83760375  0.19156453 -0.42394346  1.45631313]\n----------\n为什么采用 0-1 向量？\n\n当我们将一个 0-1 向量与 W1 相乘时，我们基本上可以将 W1 与 0-1 向量对应的那个 1 相乘的结果就是词向量。也就是说， W1 就是一个数据查询表。\n在我们的程序中，我们也添加了一个偏置项 b1 ，所以我们也需要将它加上。\nvectors = sess.run(W1 + b1)\n\n# if you work it out, you will see that it has the same effect as running the node hidden representation\nprint(vectors)\n->\n[[-0.74829113 -0.48964909  0.54267412  2.34831429 -2.03110814]\n [-0.92472583 -1.50792813 -1.61014366 -0.88273793 -2.12359881]\n [-0.69424796 -1.67628145  3.07313657 -1.14802659 -1.2207377 ]\n [-1.7077738  -0.60641652  2.25586247  1.34536338 -0.83848488]\n [-0.10080346 -0.90931684  2.8825531  -0.58769202 -1.19922316]\n [ 1.49428082 -2.55578995  2.01545811  0.31536022  1.52662396]\n [-1.02735448  0.72176981 -0.03772151 -0.60208392  1.53156447]]\n如果我们想得到 queen 的向量，我们可以用如下表示：\nprint(vectors[ word2int['queen'] ])\n# say here word2int['queen'] is 2\n-> \n[-0.69424796 -1.67628145  3.07313657 -1.14802659 -1.2207377 ]\n那么这些漂亮的向量有什么用呢？\n我们写一个如何去查找最相近向量的函数，当然这个写法是非常简单粗糙的。\ndef euclidean_dist(vec1, vec2):\n    return np.sqrt(np.sum((vec1-vec2)**2))\n\ndef find_closest(word_index, vectors):\n    min_dist = 10000 # to act like positive infinity\n    min_index = -1\n    query_vector = vectors[word_index]\n    for index, vector in enumerate(vectors):\n        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n            min_dist = euclidean_dist(vector, query_vector)\n            min_index = index\n    return min_index\n接下来，让我们来测试一下单词 king ，queen 和 royal 这些词。\nprint(int2word[find_closest(word2int['king'], vectors)])\nprint(int2word[find_closest(word2int['queen'], vectors)])\nprint(int2word[find_closest(word2int['royal'], vectors)])\n\n->\nqueen\nking\nhe\n我们可以得到如下有趣的结果。\nking is closest to queen\nqueen is closest to king\nroyal is closest to he\n第三个数据是我们根据大型语料库得出来的（看起来还不错）。语料库的数据更大，我们得到的结果会更好。（注意：由于权重是随机初始化的，所以我们可能会得到不同的结果，如果有需要，我们可以多运行几次。）\n让我们来画出这个向量相关图。\n首先，我们需要利用将为技术将维度从 5 减小到 2，所用的技术是：tSNE（teesnee！）\nfrom sklearn.manifold import TSNE\nmodel = TSNE(n_components=2, random_state=0)\nnp.set_printoptions(suppress=True)\nvectors = model.fit_transform(vectors)\n然后，我们需要对结果进行规范化，以便我们可以在 matplotlib 中更好的对它进行查看。\nfrom sklearn import preprocessing\nnormalizer = preprocessing.Normalizer()\nvectors =  normalizer.fit_transform(vectors, 'l2')\n最后，我们将绘制出图。\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\nfor word in words:\n    print(word, vectors[word2int[word]][1])\n    ax.annotate(word, (vectors[word2int[word]][0],vectors[word2int[word]][1] ))\nplt.show()\n\n从图中，我们可以看出。she 跟 queen 的距离非常接近，king 与 royal 的距离和 king 与 queen 的距离相同。如果我们有一个更大的语料库，我们可以得到更加复杂的关系图。\n为什么会发生这些？\n我们给神经网络的任务是预测单词的相邻词。但是我们还没有具体的分析神经网络是如何预测的。因此，神经网络找出单词的向量表示，用来帮助它预测相邻词这个任务。预测相邻词这本身不是一个有趣的任务，我们关心的是隐藏层的向量表示。\n为了得到这些表示，神经网络使用了上下文信息。在我们的语料库中，king 和 royal 是作为相邻词出现的，queen 和 royal 也是作为相邻词出现的。\n为什么把预测相邻词作为一个任务？\n其他的任务也可以用来训练这个词向量任务，比如利用 n-gram 就可以训练出很好的词向量！这里有一篇博客有详细解释。\n那么，我们为什么还要使用相邻词预测作为任务呢？因为有一个比较著名的模型称为 skip gram 模型。我们可以使用中间词的相邻单词作为输入，并要求神经网络去预测中间词。这被称为连续词袋模型。\n总结\n\n词向量是非常酷的一个工具。\n不要在实际生产环境中使用这个 TensorFlow 代码，我们这里只是为了理解才这样写。生产环境建议使用一些成熟的工具包，比如 gensim\n\n\n我希望这个简单教程可以帮助到一些人，可以更加深刻的理解什么是词向量。\n\n作者：chen_h微信号 & QQ：862251340简书地址：https://www.jianshu.com/p/4e1...\nCoderPai 是一个专注于算法实战的平台，从基础的算法到人工智能算法都有设计。如果你对算法实战感兴趣，请快快关注我们吧。加入AI实战微信群，AI实战QQ群，ACM算法微信群，ACM算法QQ群。长按或者扫描如下二维码，关注 “CoderPai” 微信号（coderpai）\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "1"}