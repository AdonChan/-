{"title": "如何构建一个分布式爬虫：基础篇 - 培名的Python世界 ", "index": "python", "content": "继上篇我们谈论了Celery的基本知识后，本篇继续讲解如何一步步使用Celery构建分布式爬虫。这次我们抓取的对象定为celery官方文档。\n首先，我们新建目录distributedspider，然后再在其中新建文件workers.py,里面内容如下\nfrom celery import Celery\napp = Celery('crawl_task', include=['tasks'], broker='redis://223.129.0.190:6379/1', backend='redis://223.129.0.190:6379/2')\n# 官方推荐使用json作为消息序列化方式\napp.conf.update(\n    CELERY_TIMEZONE='Asia/Shanghai',\n    CELERY_ENABLE_UTC=True,\n    CELERY_ACCEPT_CONTENT=['json'],\n    CELERY_TASK_SERIALIZER='json',\n    CELERY_RESULT_SERIALIZER='json',\n)\n上述代码主要是做Celery实例的初始化工作，include是在初始化celery app的时候需要引入的内容，主要就是注册为网络调用的函数所在的文件。然后我们再编写任务函数，新建文件tasks.py,内容如下\nimport requests\nfrom bs4 import BeautifulSoup\nfrom workers import app\n@app.task\ndef crawl(url):\n    print('正在抓取链接{}'.format(url))\n    resp_text = requests.get(url).text\n    soup = BeautifulSoup(resp_text, 'html.parser')\n    return soup.find('h1').text\n它的作用很简单，就是抓取指定的url，并且把标签为h1的元素提取出来\n最后，我们新建文件task_dispatcher.py，内容如下\nfrom workers import app\nurl_list = [\n    'http://docs.celeryproject.org/en/latest/getting-started/introduction.html',\n    'http://docs.celeryproject.org/en/latest/getting-started/brokers/index.html',\n    'http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html',\n    'http://docs.celeryproject.org/en/latest/getting-started/next-steps.html',\n    'http://docs.celeryproject.org/en/latest/getting-started/resources.html',\n    'http://docs.celeryproject.org/en/latest/userguide/application.html',\n    'http://docs.celeryproject.org/en/latest/userguide/tasks.html',\n    'http://docs.celeryproject.org/en/latest/userguide/canvas.html',\n    'http://docs.celeryproject.org/en/latest/userguide/workers.html',\n    'http://docs.celeryproject.org/en/latest/userguide/daemonizing.html',\n    'http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html'\n]\ndef manage_crawl_task(urls):\n    for url in urls:\n        app.send_task('tasks.crawl', args=(url,))\nif __name__ == '__main__':\n    manage_crawl_task(url_list)\n这段代码的作用主要就是给worker发送任务，任务是tasks.crawl，参数是url(元祖的形式)\n现在，让我们在节点A(hostname为resolvewang的主机)上启动worker\ncelery -A workers worker -c 2 -l info\n这里 -c指定了线程数为2， -l表示日志等级是info。我们把代码拷贝到节点B(节点名为wpm的主机)，同样以相同命令启动worker，便可以看到以下输出\n\n可以看到左边节点(A)先是all alone，表示只有一个节点；后来再节点B启动后，它便和B同步了\nsync with celery@wpm\n这个时候，我们运行给这两个worker节点发送抓取任务\npython task_dispatcher.py\n可以看到如下输出\n\n可以看到两个节点都在执行抓取任务，并且它们的任务不会重复。我们再在redis里看看结果\n\n可以看到一共有11条结果，说明 tasks.crawl中返回的数据都在db2(backend)中了，并且以json的形式存储了起来，除了返回的结果，还有执行是否成功等信息。\n到此，我们就实现了一个很基础的分布式网络爬虫，但是它还不具有很好的扩展性，而且貌似太简单了...下一篇我将以微博数据采集为例来演示如何构建一个稳健的分布式网络爬虫。\n\n对微博大规模数据采集感兴趣的同学可以关注一下分布式微博爬虫，用用也是极好的\n\n                ", "mainLikeNum": ["6 "], "mainBookmarkNum": "11"}