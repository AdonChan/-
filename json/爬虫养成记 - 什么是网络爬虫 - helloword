{"title": "爬虫养成记 - 什么是网络爬虫 - helloword ", "index": "python,网页爬虫", "content": "趁着春节，希望能写一个小小的网络爬虫框架。先定一个小目标，希望能比较优雅地将某个网站上的所有图片爬下来。暂时先将这个爬虫的名字叫做Squirrel，小松鼠吧。\n什么是爬虫\n爬虫其实是一种从互联网上获取信息，并且提取我们需要的信息并且储存的手段。互联网就像一张网，这种网是由一个个url相互连接的。一个url往往是对应着一张网页(Page). 各个页面通过url链接成了一个网状结构。 那么我们从一个页面出发，分析其中的url，然后再去访问该url对应的页面；再分析其中的url，并访问。如此重复并可以爬遍所有的页面。\n简单地捋了一下，我们如果需要实现一只爬虫，则需要实现如下几个功能模块：\n\nurl管理器   url管理器应该维护两个不重复的set。一个储存未爬过的url，一个储存已经爬过的url。如果我们要将一个url加入未爬过的url set，那么这个url必须都不在两组set中。\n网页下载器   接收url，将页面内容下来。\n网页解析器   接收页面内容，从中提取出结构化的数据和url。\n储存器   储存结构化的数据。\n\n下面我们一步一步来认识我们设置的几个模块需要实现什么功能。\nurl管理器\nurl管理器主要需要维护两个集合：\n\n已经抓取的url集合，我们叫做crawled_set\n未抓取的url集合，我们叫做uncrawled_set目的就是为了防止重复抓取和循环抓取。\n\n我们来分解url管理器需要实现的功能：\n\n判断一个url是否已经在容器中\n判断uncrawled_set中是否为空。为空则停止爬取。\n将一个url添加到容器的uncrawled_set中\n将一个url从uncrawled_set移动到crawled_set中。\n\nurl管理器的实现方式有多种\n\n将crawled_set和uncrawed_set存放在内存中。  Python支持set数据类型，可以建立两个set用来存放未爬和已爬url。\n关系型数据库。  可以定义一个表，定义两个字段 url和is_crawled.\n缓存数据库 redis\n\n网页下载器\n网页下载器就是更具url下载网页内容（html等）。常见的网页下载器有-urllib2 urllib2是python官方的基础模块。-request\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "2"}