{"title": "【学习摘录】机器学习特征选择 - machine_learning_or_learning_machine ", "index": "python,数据挖掘,scikit-learn", "content": "应用过机器学习进行数据挖掘的同学应该都知道特征选择对模型表现的重要性。本文基于网上经典特征选择相关文章整理出干货：常用方法分类以及调包侠该如何用sklearn快速上手，供大家参考。\n（一）预处理：\n1 无量纲化：\n1.1 区间缩放\nfrom sklearn.preprocessing \nimport MinMaxScaler #区间缩放，返回值为缩放到[0, 1]区间的数据\nMinMaxScaler().fit_transform(iris.data)\n1.2 标准化（特征值服需从正态分布）\nfrom sklearn.preprocessing import StandardScaler #标准化，返回值为标准化后的数据\nStandardScaler().fit_transform(iris.data)\n2 特征二值化：定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0\n3 特征哑变量\n4 缺失值计算:一般以均值填充\n5 数据变换：常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的\n（二）特征选择\n1 过滤\n1.1 基于方差\nfrom sklearn.feature_selection\nimport VarianceThreshold\n#方差选择法，返回值为特征选择后的数据 #参数threshold为方差的阈值\nVarianceThreshold(threshold=3).fit_transform(iris.data)\n1.2 相关系数\nfrom sklearn.feature_selection \nimport SelectKBest\nfrom scipy.stats import pearsonr\nSelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)\n1.3 卡方检验\nfrom sklearn.feature_selection\nimport SelectKBest\nfrom sklearn.feature_selection import chi2#选择K个最好的特征，返回选择特征后的数据\nSelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)\n1.4 互信息\n2 递归特征消除：递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练\n3 嵌入法\n3.1 基于惩罚项的特征选择法\n3.2 树模型中GBDT也可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合GBDT模型，来选择特征的代码如下：\nfrom sklearn.feature_selection \nimport SelectFromModel\nfrom sklearn.ensemble import GradientBoostingClassifier  \nSelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)\n（四）降维\n1 主成分分析\nfrom sklearn.decomposition \nimport PCA2 3 #主成分分析法，返回降维后的数据 #参数n_components为主成分数目 PCA(n_components=2).fit_transform(iris.data)\n2 线性判别分析\nfrom sklearn.lda \nimport LDA2 3 #线性判别分析法，返回降维后的数据 #参数n_components为降维后的维数 LDA(n_components=2).fit_transform(iris.data, iris.target)\n参考：\n\nhttp://note.youdao.com/notesh...（收藏自公众号数据挖掘入门与实战）\nhttps://www.zhihu.com/questio...\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}