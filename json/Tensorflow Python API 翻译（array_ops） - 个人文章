{"title": "Tensorflow Python API 翻译（array_ops） - 个人文章 ", "index": "python", "content": "作者：chen_h微信号 & QQ：862251340微信公众号：coderpai简书地址：https://www.jianshu.com/p/00a...\n\n计划现将 tensorflow 中的 Python API 做一个学习，这样方便以后的学习。原文链接\n\n该章介绍有关张量转换的API\n\n数据类型投射\nTensorflow提供了很多的数据类型投射操作，你能将数据类型投射到一个你想要的数据类型上去。\n\ntf.string_to_number(string_tensor, out_type = None, name = None)\n解释：这个函数是将一个string的Tensor转换成一个数字类型的Tensor。但是要注意一点，如果你想转换的数字类型是tf.float32，那么这个string去掉引号之后，里面的值必须是一个合法的浮点数，否则不能转换。如果你想转换的数字类型是tf.int32，那么这个string去掉引号之后，里面的值必须是一个合法的浮点数或者整型，否则不能转换。\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\nsess = tf.Session()\ndata = tf.constant('123')\nprint sess.run(data)\nd = tf.string_to_number(data)\nprint sess.run(d)\n输入参数：\n\n\nstring_tensor: 一个string类型的Tensor。\n\nout_type: 一个可选的数据类型tf.DType，默认的是tf.float32，但我们也可以选择tf.int32或者tf.float32。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor，数据类型是out_type，数据维度和string_tensor相同。\n\ntf.to_double(x, name = 'ToDouble')\n解释：这个函数是将一个Tensor的数据类型转换成float64。\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\nsess = tf.Session()\ndata = tf.constant(123)\nprint sess.run(data)\nd = tf.to_double(data)\nprint sess.run(d)\n输入参数：\n\n\nx: 一个Tensor或者是SparseTensor。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor或者SparseTensor，数据类型是float64，数据维度和x相同。\n提示：\n错误: 如果x是不能被转换成float64类型的，那么将报错。\n\ntf.to_float(x, name = 'ToFloat')\n解释：这个函数是将一个Tensor的数据类型转换成float32。\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\nsess = tf.Session()\ndata = tf.constant(123)\nprint sess.run(data)\nd = tf.to_float(data)\nprint sess.run(d)\n输入参数：\n\n\nx: 一个Tensor或者是SparseTensor。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor或者SparseTensor，数据类型是float32，数据维度和x相同。\n提示：\n错误: 如果x是不能被转换成float32类型的，那么将报错。\n\ntf.to_bfloat16(x, name = 'ToBFloat16')\n解释：这个函数是将一个Tensor的数据类型转换成bfloat16。\n译者注：这个API的作用不是很理解，但我测试了一下，输入的x必须是浮点型的，别的类型都不行。\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\nsess = tf.Session()\ndata = tf.constant([x for x in range(20)], tf.float32)\nprint sess.run(data)\nd = tf.to_bfloat16(data)\nprint sess.run(d)\n输入参数：\n\n\nx: 一个Tensor或者是SparseTensor。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor或者SparseTensor，数据类型是bfloat16，数据维度和x相同。\n提示：\n错误: 如果x是不能被转换成bfloat16类型的，那么将报错。\n\ntf.to_int32(x, name = 'ToInt32')\n解释：这个函数是将一个Tensor的数据类型转换成int32。\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\nsess = tf.Session()\ndata = tf.constant([x for x in range(20)], tf.float32)\nprint sess.run(data)\nd = tf.to_int32(data)\nprint sess.run(d)\n输入参数：\n\n\nx: 一个Tensor或者是SparseTensor。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor或者SparseTensor，数据类型是int32，数据维度和x相同。\n提示：\n错误: 如果x是不能被转换成int32类型的，那么将报错。\n\ntf.to_int64(x, name = 'ToInt64')\n解释：这个函数是将一个Tensor的数据类型转换成int64。\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\nsess = tf.Session()\ndata = tf.constant([x for x in range(20)], tf.float32)\nprint sess.run(data)\nd = tf.to_int64(data)\nprint sess.run(d)\n输入参数：\n\n\nx: 一个Tensor或者是SparseTensor。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor或者SparseTensor，数据类型是int64，数据维度和x相同。\n提示：\n错误: 如果x是不能被转换成int64类型的，那么将报错。\n\ntf.cast(x, dtype, name = None)\n解释：这个函数是将一个Tensor或者SparseTensor的数据类型转换成dtype。\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\nsess = tf.Session()\ndata = tf.constant([x for x in range(20)], tf.float32)\nprint sess.run(data)\nd = tf.cast(data, tf.int32)\nprint sess.run(d)\n输入参数：\n\n\nx: 一个Tensor或者是SparseTensor。\n\ndtype: 目标数据类型。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor或者SparseTensor，数据维度和x相同。\n提示：\n错误: 如果x是不能被转换成dtype类型的，那么将报错。\n\n数据维度转换\nTensorflow提供了很多的数据维度转换操作，你能改变数据的维度，将它变成你需要的维度。\n\ntf.shape(input, name = None)\n解释：这个函数是返回input的数据维度，返回的Tensor数据维度是一维的。\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\nsess = tf.Session()\ndata = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])\nprint sess.run(data)\nd = tf.shape(data)\nprint sess.run(d)\n输入参数：\n\n\ninput: 一个Tensor。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor，数据类型是int32。\n\ntf.size(input, name = None)\n解释：这个函数是返回input中一共有多少个元素。\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\nsess = tf.Session()\ndata = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])\nprint sess.run(data)\nd = tf.size(data)\nprint sess.run(d)\n输入参数：\n\n\ninput: 一个Tensor。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor，数据类型是int32。\n\ntf.rank(input, name = None)\n解释：这个函数是返回Tensor的秩。\n注意：Tensor的秩和矩阵的秩是不一样的，Tensor的秩指的是元素维度索引的数目，这个概念也被成为order, degree或者ndims。比如，一个Tensor的维度是[1, 28, 28, 1]，那么它的秩就是4。\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\nsess = tf.Session()\ndata = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])\nprint sess.run(data)\nd = tf.rank(data)\nprint sess.run(tf.shape(data))\nprint sess.run(d)\n输入参数：\n\n\ninput: 一个Tensor。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor，数据类型是int32。\n\ntf.reshape(tensor, shape, name = None)\n解释：这个函数的作用是对tensor的维度进行重新组合。给定一个tensor，这个函数会返回数据维度是shape的一个新的tensor，但是tensor里面的元素不变。如果shape是一个特殊值[-1]，那么tensor将会变成一个扁平的一维tensor。如果shape是一个一维或者更高的tensor，那么输入的tensor将按照这个shape进行重新组合，但是重新组合的tensor和原来的tensor的元素是必须相同的。\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\nsess = tf.Session()\ndata = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])\nprint sess.run(data)\nprint sess.run(tf.shape(data))\nd = tf.reshape(data, [-1])\nprint sess.run(d)\nd = tf.reshape(data, [3, 4])\nprint sess.run(d)\n输入参数：\n\n\ntensor: 一个Tensor。\n\nshape: 一个Tensor，数据类型是int32，定义输出数据的维度。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor，数据类型和输入数据相同。\n\ntf.squeeze(input, squeeze_dims = None, name = None)\n解释：这个函数的作用是将input中维度是1的那一维去掉。但是如果你不想把维度是1的全部去掉，那么你可以使用squeeze_dims参数，来指定需要去掉的位置。\n使用例子：\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\nsess = tf.Session()\ndata = tf.constant([[1, 2, 1], [3, 1, 1]])\nprint sess.run(tf.shape(data))\nd_1 = tf.expand_dims(data, 0)\nd_1 = tf.expand_dims(d_1, 2)\nd_1 = tf.expand_dims(d_1, -1)\nd_1 = tf.expand_dims(d_1, -1)\nprint sess.run(tf.shape(d_1))\nd_2 = d_1\nprint sess.run(tf.shape(tf.squeeze(d_1)))\nprint sess.run(tf.shape(tf.squeeze(d_2, [2, 4])))\n\n# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\n# shape(squeeze(t)) ==> [2, 3]\n\n# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]\n# shape(squeeze(t, [2, 4])) ==> [1, 2, 3, 1]\n\n输入参数：\n\n\ninput: 一个Tensor。\n\nsqueeze_dims: （可选）一个序列，索引从0开始，只移除该列表中对应位的tensor。默认下，是一个空序列[]。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor，数据类型和输入数据相同。\n\ntf.expand_dims(input, dim, name = None)\n解释：这个函数的作用是向input中插入维度是1的张量。我们可以指定插入的位置dim，dim的索引从0开始，dim的值也可以是负数，从尾部开始插入，符合 python 的语法。这个操作是非常有用的。举个例子，如果你有一张图片，数据维度是[height, width, channels]，你想要加入“批量”这个信息，那么你可以这样操作expand_dims(images, 0)，那么该图片的维度就变成了[1, height, width, channels]。\n这个操作要求：-1-input.dims() <= dim <= input.dims()\n这个操作是squeeze()函数的相反操作，可以一起灵活运用。\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\nsess = tf.Session()\ndata = tf.constant([[1, 2, 1], [3, 1, 1]])\nprint sess.run(tf.shape(data))\nd_1 = tf.expand_dims(data, 0)\nprint sess.run(tf.shape(d_1))\nd_1 = tf.expand_dims(d_1, 2)\nprint sess.run(tf.shape(d_1))\nd_1 = tf.expand_dims(d_1, -1)\nprint sess.run(tf.shape(d_1))\n输入参数：\n\n\ninput: 一个Tensor。\n\ndim: 一个Tensor，数据类型是int32，标量。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor，数据类型和输入数据相同，数据和input相同，但是维度增加了一维。\n\n数据抽取和结合\nTensorflow提供了很多的数据抽取和结合的方法。\n\ntf.slice(input_, begin, size, name = None)\n解释：这个函数的作用是从输入数据input中提取出一块切片，切片的尺寸是size，切片的开始位置是begin。切片的尺寸size表示输出tensor的数据维度，其中size[i]表示在第i维度上面的元素个数。开始位置begin表示切片相对于输入数据input_的每一个偏移量，比如数据input_是`[[[1, 1, 1], [2, 2, 2]], [[33, 3, 3], [4, 4, 4]],[[5, 5, 5], [6, 6, 6]]]`，begin为[1, 0, 0]，那么数据的开始位置是33。因为，第一维偏移了1，其余几位都没有偏移，所以开始位置是33。\n操作满足：size[i] = input.dim_size(i) - begin[i]0 <= begin[i] <= begin[i] + size[i] <= Di for i in [0, n]\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\nsess = tf.Session()\ninput = tf.constant([[[1, 1, 1], [2, 2, 2]],\n                    [[3, 3, 3], [4, 4, 4]],\n                    [[5, 5, 5], [6, 6, 6]]])\ndata = tf.slice(input, [1, 0, 0], [1, 1, 3])\nprint sess.run(data)\ndata = tf.slice(input, [1, 0, 0], [1, 2, 3])\nprint sess.run(data)\ndata = tf.slice(input, [1, 0, 0], [2, 1, 3])\nprint sess.run(data)\ndata = tf.slice(input, [1, 0, 0], [2, 2, 2])\nprint sess.run(data)\n输入参数：\n\n\ninput_: 一个Tensor。\n\nbegin: 一个Tensor，数据类型是int32或者int64。\n\nsize: 一个Tensor，数据类型是int32或者int64。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor，数据类型和input_相同。\n\ntf.split(split_dim, num_split, value, name = 'split')\n解释：这个函数的作用是，沿着split_dim维度将value切成num_split块。要求，num_split必须被value.shape[split_dim]整除，即value.shape[split_dim] % num_split == 0。\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\nsess = tf.Session()\ninput = tf.random_normal([5,30])\nprint sess.run(tf.shape(input))[0] / 5\nsplit0, split1, split2, split3, split4 = tf.split(0, 5, input)\nprint sess.run(tf.shape(split0))\n输入参数：\n\n\nsplit_dim: 一个0维的Tensor，数据类型是int32，该参数的作用是确定沿着哪个维度进行切割，参数范围 [0, rank(value))。\n\nnum_split: 一个0维的Tensor，数据类型是int32，切割的块数量。\n\nvalue: 一个需要切割的Tensor。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n从value中切割的num_split个Tensor。\n\ntf.tile(input, multiples, name = None)\n解释：这个函数的作用是通过给定的tensor去构造一个新的tensor。所使用的方法是将input复制multiples次，输出的tensor的第i维有input.dims(i) * multiples[i]个元素，input中的元素被复制multiples[i]次。比如，input = [a b c d], multiples = [2]，那么tile(input, multiples) = [a b c d a b c d]。\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\nsess = tf.Session()\ndata = tf.constant([[1, 2, 3, 4], [9, 8, 7, 6]])\nd = tf.tile(data, [2,3])\nprint sess.run(d)\n输入参数：\n\n\ninput_: 一个Tensor，数据维度是一维或者更高维度。\n\nmultiples: 一个Tensor，数据类型是int32，数据维度是一维，长度必须和input的维度一样。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor，数据类型和input相同。\n\ntf.pad(input, paddings, name = None)\n解释：这个函数的作用是向input中按照paddings的格式填充0。paddings是一个整型的Tensor，数据维度是[n, 2]，其中n是input的秩。对于input的中的每一维D，paddings[D, 0]表示增加多少个0在input之前，paddings[D, 1]表示增加多少个0在input之后。举个例子，假设paddings = [[1, 1], [2, 2]]和input的数据维度是[2,2]，那么最后填充完之后的数据维度如下：\n\n也就是说，最后的数据维度变成了[4,6]。\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\nsess = tf.Session()\nt = tf.constant([[[3,3,],[2,2]]])\nprint sess.run(tf.shape(t))\npaddings = tf.constant([[3,3],[1,1],[2,2]])\nprint sess.run(tf.pad(t, paddings)).shape\n输入参数：\n\n\ninput: 一个Tensor。\n\npaddings: 一个Tensor，数据类型是int32。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor，数据类型和input相同。\n\ntf.concat(concat_dim, value, name = 'concat')\n解释：这个函数的作用是沿着concat_dim维度，去重新串联value，组成一个新的tensor。\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \nimport numpy as np \n\nsess = tf.Session()\nt1 = tf.constant([[1, 2, 3], [4, 5, 6]])\nt2 = tf.constant([[7, 8, 9], [10, 11, 12]])\nd1 = tf.concat(0, [t1, t2])\nd2 = tf.concat(1, [t1, t2])\nprint sess.run(d1)\nprint sess.run(tf.shape(d1))\nprint sess.run(d2)\nprint sess.run(tf.shape(d2))\n\n# output\n[[ 1  2  3]\n [ 4  5  6]\n [ 7  8  9]\n [10 11 12]]\n\n[[ 1  2  3  7  8  9]\n [ 4  5  6 10 11 12]]\n\n# tips\n从直观上来看，我们取的concat_dim的那一维的元素个数肯定会增加。比如，上述例子中的d1的第0维增加了，而且d1.shape[0] = t1.shape[0]+t2.shape[0]。\n输入参数：\n\n\nconcat_dim: 一个零维度的Tensor，数据类型是int32。\n\nvalues: 一个Tensor列表，或者一个单独的Tensor。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个重新串联之后的Tensor。\n\ntf.pack(values, name = 'pack')\n解释：这个函数的作用是将秩为R的tensor打包成一个秩为R+1的tensor。具体的公式可以表示为：\ntf.pack([x, y, z]) = np.asqrray([x, y, z])\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nx = tf.constant([1,2,3])\ny = tf.constant([4,5,6])\nz = tf.constant([7,8,9])\n\np = tf.pack([x,y,z])\n\nsess = tf.Session()\nprint sess.run(tf.shape(p))\nprint sess.run(p)\n\n输入参数：\n\n\nvalues: 一个Tensor的列表，每个Tensor必须有相同的数据类型和数据维度。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n\noutput: 一个打包的Tensor，数据类型和values相同。\n\ntf.unpack(value, num = None, name = 'unpack')\n解释：这个函数的作用是将秩为R+1的tensor解压成一些秩为R的tensor。其中，num表示要解压出来的tensor的个数。如果，num没有被指定，那么num = value.shape[0]。如果，value.shape[0]无法得到，那么系统将抛出异常ValueError。具体的公式可以表示为：\ntf.unpack(x, n) = list(x)\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\nx = tf.constant([1,2,3])\ny = tf.constant([4,5,6])\nz = tf.constant([7,8,9])\n\np = tf.pack([x,y,z])\n\nsess = tf.Session()\nprint sess.run(tf.shape(p))\npp = tf.unpack(p,3)\nprint sess.run(pp)\n\n输入参数：\n\n\nvalue: 一个秩大于0的Tensor。\n\nnum: 一个整型，value的第一维度的值。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n从value中解压出来的一个Tensor数组。\n异常：\n\nValueError: 如果num没有被正确指定，那么将抛出异常。\n\ntf.reverse_sequence(input, seq_lengths, seq_dim, name = None)\n解释：将input中的值沿着第seq_dim维度进行翻转。\n这个操作先将input沿着第0维度切分，然后对于每个切片，将切片长度为seq_lengths[i]的值，沿着第seq_dim维度进行翻转。\n向量seq_lengths中的值必须满足seq_lengths[i] < input.dims[seq_dim]，并且其长度必须是input_dims(0)。\n对于每个切片i的输出，我们将第seq_dim维度的前seq_lengths[i]的数据进行翻转。\n比如：\n# Given this:\nseq_dim = 1\ninput.dims = (4, 10, ...)\nseq_lengths = [7, 2, 3, 5]\n\n# 因为input的第0维度是4，所以先将input切分成4个切片；\n# 因为seq_dim是1，所以我们按着第1维度进行翻转。\n# 因为seq_lengths[0] = 7，所以我们第一个切片只翻转前7个值，该切片的后面的值保持不变。\n# 因为seq_lengths[1] = 2，所以我们第一个切片只翻转前2个值，该切片的后面的值保持不变。\n# 因为seq_lengths[2] = 3，所以我们第一个切片只翻转前3个值，该切片的后面的值保持不变。\n# 因为seq_lengths[3] = 5，所以我们第一个切片只翻转前5个值，该切片的后面的值保持不变。\noutput[0, 0:7, :, ...] = input[0, 7:0:-1, :, ...]\noutput[1, 0:2, :, ...] = input[1, 2:0:-1, :, ...]\noutput[2, 0:3, :, ...] = input[2, 3:0:-1, :, ...]\noutput[3, 0:5, :, ...] = input[3, 5:0:-1, :, ...]\n\noutput[0, 7:, :, ...] = input[0, 7:, :, ...]\noutput[1, 2:, :, ...] = input[1, 2:, :, ...]\noutput[2, 3:, :, ...] = input[2, 3:, :, ...]\noutput[3, 2:, :, ...] = input[3, 2:, :, ...]\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \n\nsess = tf.Session()\ninput = tf.constant([[1, 2, 3, 4], [3, 4, 5, 6]], tf.int64)\nseq_lengths = tf.constant([3, 2], tf.int64)\nseq_dim = 1\noutput = tf.reverse_sequence(input, seq_lengths, seq_dim)\nprint sess.run(output)\nsess.close()\n\n# output\n[[3 2 1 4]\n [4 3 5 6]]\n输入参数：\n\n\ninput: 一个Tensor，需要反转的数据。\n\nseq_lengths: 一个Tensor，数据类型是int64，数据长度是input.dims(0)，并且max(seq_lengths) < input.dims(seq_dim)。\n\nseq_dim: 一个int，确定需要翻转的维度。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor，数据类型和input相同，数据维度和input相同。\n\ntf.reverse(tensor, dims, name = None)\n解释：将指定维度中的数据进行翻转。\n给定一个tensor和一个bool类型的dims，dims中的值为False或者True。如果dims[i] == True，那么就将tensor中这一维的数据进行翻转。\ntensor最多只能有8个维度，并且tensor的秩必须和dims的长度相同，即rank(tensor) == size(dims)。\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \n\nsess = tf.Session()\ninput_data = tf.constant([[\n[\n[ 0,  1,  2,  3],\n[ 4,  5,  6,  7],\n[ 8,  9, 10, 11]\n],\n[\n[12, 13, 14, 15],\n[16, 17, 18, 19],\n[20, 21, 22, 23]\n]\n]])\nprint 'input_data shape : ', sess.run(tf.shape(input_data))\ndims = tf.constant([False, False, False, True])\nprint sess.run(tf.reverse(input_data, dims))\nprint \"==========================\"\ndims = tf.constant([False, True, False, False])\nprint sess.run(tf.reverse(input_data, dims))\nprint \"==========================\"\ndims = tf.constant([False, False, True, False])\nprint sess.run(tf.reverse(input_data, dims))\nsess.close()\n输入参数：\n\n\ntensor: 一个Tensor，数据类型必须是以下之一：uint8，int8，int32，bool，float32或者float64，数据维度不超过8维。\n\ndims: 一个Tensor，数据类型是bool。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor，数据类型和tensor相同，数据维度和tensor相同。\n\ntf.transpose(a, perm = None, name = 'transpose')\n解释：将a进行转置，并且根据perm参数重新排列输出维度。\n输出数据tensor的第i维将根据perm[i]指定。比如，如果perm没有给定，那么默认是perm = [n-1, n-2, ..., 0]，其中rank(a) = n。默认情况下，对于二维输入数据，其实就是常规的矩阵转置操作。\n比如：\ninput_data.dims = (1, 4, 3)\nperm = [1, 2, 0]\n\n# 因为 output_data.dims[0] = input_data.dims[ perm[0] ]\n# 因为 output_data.dims[1] = input_data.dims[ perm[1] ]\n# 因为 output_data.dims[2] = input_data.dims[ perm[2] ]\n# 所以得到 output_data.dims = (4, 3, 1)\noutput_data.dims = (4, 3, 1)\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \n\nsess = tf.Session()\ninput_data = tf.constant([[1,2,3],[4,5,6]])\nprint sess.run(tf.transpose(input_data))\nprint sess.run(input_data)\nprint sess.run(tf.transpose(input_data, perm=[1,0]))\ninput_data = tf.constant([[[1,2,3],[4,5,6],[7,8,9],[10,11,12]]])\nprint 'input_data shape: ', sess.run(tf.shape(input_data))\noutput_data = tf.transpose(input_data, perm=[1, 2, 0])\nprint 'output_data shape: ', sess.run(tf.shape(output_data))\nprint sess.run(output_data)\nsess.close()\n输入参数：\n\n\na: 一个Tensor。\n\nperm: 一个对于a的维度的重排列组合。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个经过翻转的Tensor。\n\ntf.gather(params, indices, name = None)\n解释：根据indices索引，从params中取对应索引的值，然后返回。\nindices必须是一个整型的tensor，数据维度是常量或者一维。最后输出的数据维度是indices.shape + params.shape[1:]。\n比如：\n# Scalar indices\noutput[:, ..., :] = params[indices, :, ... :]\n\n# Vector indices\noutput[i, :, ..., :] = params[indices[i], :, ... :]\n\n# Higher rank indices\noutput[i, ..., j, :, ... :] = params[indices[i, ..., j], :, ..., :]\n如果indices是一个从0到params.shape[0]的排列，即len(indices) = params.shape[0]，那么这个操作将把params进行重排列。\n\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \n\nsess = tf.Session()\nparams = tf.constant([6, 3, 4, 1, 5, 9, 10])\nindices = tf.constant([2, 0, 2, 5])\noutput = tf.gather(params, indices)\nprint sess.run(output)\nsess.close()\n输入参数：\n\n\nparams: 一个Tensor。\n\nindices: 一个Tensor，数据类型必须是int32或者int64。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor，数据类型和params相同。\n\ntf.dynamic_partition(data, partitions, num_partitions, name = None)\n解释：根据从partitions中取得的索引，将data分割成num_partitions份。\n我们先从partitions.ndim 中取出一个元祖js，那么切片data[js, ...]将成为输出数据outputs[partitions[js]]的一部分。我们将js按照字典序排列，即js里面的值为(0, 0, ..., 1, 1, ..., 2, 2, ..., ..., num_partitions - 1, num_partitions - 1, ...)。我们将partitions[js] = i的值放入outputs[i]。outputs[i]中的第一维对应于partitions.values == i的位置。更多细节如下：\noutputs[i].shape = [sum(partitions == i)] + data.shape[partitions.ndim:]\n\noutputs[i] = pack([data[js, ...] for js if partitions[js] == i])\ndata.shape must start with partitions.shape这句话不是很明白，说说自己的理解。data.shape(0)必须和partitions.shape(0)相同，即data.shape[0] == partitions.shape[0]。\n比如：\n# Scalar partitions\npartitions = 1\nnum_partitions = 2\ndata = [10, 20]\noutputs[0] = []  # Empty with shape [0, 2]\noutputs[1] = [[10, 20]]\n\n# Vector partitions\npartitions = [0, 0, 1, 1, 0]\nnum_partitions = 2\ndata = [10, 20, 30, 40, 50]\noutputs[0] = [10, 20, 50]\noutputs[1] = [30, 40]\n\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \n\nsess = tf.Session()\nparams = tf.constant([6, 3, 4, 1, 5, 9, 10])\nindices = tf.constant([2, 0, 2, 5])\noutput = tf.gather(params, indices)\nprint sess.run(output)\nsess.close()\n输入参数：\n\n\ndata: 一个Tensor。\n\npartitions: 一个Tensor，数据类型必须是int32。任意数据维度，但其中的值必须是在范围[0, num_partitions)。\n\nnum_partitions: 一个int，其值必须不小于1。输出的切片个数。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个数组Tensor，数据类型和data相同。\n\ntf.dynamic_stitch(indices, data, name = None)\n解释：这是一个交错合并的操作，我们根据indices中的值，将data交错合并，并且返回一个合并之后的tensor。\n如下构建一个合并的tensor：\nmerged[indices[m][i, ..., j], ...] = data[m][i, ..., j, ...]\n其中，m是一个从0开始的索引。如果indices[m]是一个标量或者向量，那么我们可以得到更加具体的如下推导：\n# Scalar indices\nmerged[indices[m], ...] = data[m][...]\n\n# Vector indices\nmerged[indices[m][i], ...] = data[m][i, ...]\n从上式的推导，我们也可以看出最终合并的数据是按照索引从小到大排序的。那么会产生两个问题：1）假设如果一个索引同时存在indices[m][i]和indices[n][j]中，其中(m, i) < (n, j)。那么，data[n][j]将作为最后被合并的值。2）假设索引越界了，那么缺失的位上面的值将被随机值给填补。\n比如：\nindices[0] = 6\nindices[1] = [4, 1]\nindices[2] = [[5, 2], [0, 3]]\ndata[0] = [61, 62]\ndata[1] = [[41, 42], [11, 12]]\ndata[2] = [[[51, 52], [21, 22]], [[1, 2], [31, 32]]]\nmerged = [[1, 2], [11, 12], [21, 22], [31, 32], [41, 42],\n          [51, 52], [61, 62]]\n\n使用例子：\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf \n\nsess = tf.Session()\nindices = [6, [4, 1], [[5, 2], [0, 3]]]\ndata = [[61, 62], [[41, 42], [11, 12]], [[[51, 52], [21, 22]], [[1, 2], [31, 32]]]]\noutput = tf.dynamic_stitch(indices, data)\nprint sess.run(output)\n# 缺少了第6，第7的位置，索引最后合并的数据中，这两个位置的值会被用随机数代替\nindices = [8, [4, 1], [[5, 2], [0, 3]]]\noutput = tf.dynamic_stitch(indices, data)\n# 第一个2被覆盖了，最后合并的数据是第二个2所指的位置\nindices = [6, [4, 1], [[5, 2], [2, 3]]]\noutput = tf.dynamic_stitch(indices, data)\nprint sess.run(output)\nprint sess.run(output)\nsess.close()\n输入参数：\n\n\nindices: 一个列表，至少包含两Tensor，数据类型是int32。\n\ndata: 一个列表，里面Tensor的个数和indices相同，并且拥有相同的数据类型。\n\nname:（可选）为这个操作取一个名字。\n\n输出参数：\n一个Tensor，数据类型和data相同。\n\n作者：chen_h微信号 & QQ：862251340简书地址：https://www.jianshu.com/p/00a...\nCoderPai 是一个专注于算法实战的平台，从基础的算法到人工智能算法都有设计。如果你对算法实战感兴趣，请快快关注我们吧。加入AI实战微信群，AI实战QQ群，ACM算法微信群，ACM算法QQ群。长按或者扫描如下二维码，关注 “CoderPai” 微信号（coderpai）\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}