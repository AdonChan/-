{"title": "利用Python爬取百度贴吧图片 - Java-siben ", "index": "python", "content": "背景介绍\n我大一的时候学校就开设了 python，但是并没有好好学，基本等于是什么也不会，最近才开始看，所以本身也是摸着石头过河，见谅...\n心得\n讲真的，爬虫确实不像别人想象的那样简单，爬虫首先要静下心来，细心寻找目标网站的布局规律，最重要的是 url 的变化，这是一个考验耐心与细心的过程；爬虫没有一个固定的套路，我们得时刻周旋于各种反爬虫机制中，并深陷不能自拔(sorry，废话太多了)，我们只能学习这种爬虫的思想，一定要多加练习，掌握知识的最佳途径就是实践，不要眼高手低，跟着大神的脚步模仿、练习，大神都是这样一步一步走过来的，加油。。。\n遇到的问题\n首先我们要知道所谓的爬虫其实说到原理上就是:1、如何获取到想要的目标信息？2、如何存入本地？3、如何转换格式基于这三点我来简单说明一下1、我在爬取百度贴吧的图片时，刚开始也是一度彷徨，不知道该如何下手，于是我把教程看了好几遍，又反复琢磨百度贴吧的html格式，终于我发现了我想要的数据的规律，从变化中寻求规律，这才是我们爬虫的解决之道，于是我发现它的 url 每次前半截基本都不会发生变化，并且会跟着相应的翻页数进行变化，这是第一个规律2、细心查找之下发现，它的突变的标签的上下父子节点也基本是固定的，这也给我们带来了很大的便捷，于是，我就在虽然这个图片的地址是一直发生变化的，但是我们可以从它的父子节点入手从而可以拿到这个图片的url地址，最后拿着这个url去请求图片然后下载到本地这样不就行了嘛3、这样我们的关注点就变成了如何拿到它的相关的父子节点信息呢？我仔细的查找了相关的知识点，发现在python中，处理html数据的虽然有很多框架可以做到，但是有三大最流行的技术：正则表达式，这个相信不用我多说了，无论什么语言都有正则表达式，并且由于是各种语言内置的，因此配置简单，但是使用难度是最高的（果然任何事物都有其有利有弊的一面，无所例外）；第二个就是 xpath，这个相对于正则难度就要小点，需要自己熟悉一下它的写法，并且需要导入lxml这个库，在下载时需要注意（下载是比较麻烦的，我当时就是搞了好久，版本一值不对，考验耐心）；第三个就是 BeautifulSoup，这个是三个里面最简单的一个，比较容易上手，有专门的官方文档说明（飞机票：https://beautifulsoup.readthe...）4、我个人是比较推荐使用正则表达式的，什么效率方面的就不说了，推崇的是它的使用面最广，什么语言都可以直接使用（由于我本身还使用别的语言，比如java、scala、go、node、python等），所以这也是我特别青睐它的理由（正则基础一定要学好，慢慢练习，加油哦）5、接下来就是数据处理了，这里我是将这些图片直接保存到本项目下的一个文件夹下面了，后续可以配置数据库，直接写入数据中。好了，大致思路就介绍到这里了，毕竟需要动手做一遍才能理解，欢迎交流（ps：在源码中也给出了我的私人邮箱哦）\n源代码\n#! /usr/bin/env python\n# -*- coding:utf-8 -*-\n\n'''\n爬取百度贴吧的数据\nauthor : shiro.liang.yi@gmail.com\ndata : 2018-11-07 22:27\n'''\n\nimport urllib2\nimport urllib\n\ndef loadPage(url,filename):\n    \"\"\"\n        作用：根据 url 发送请求，获取服务器响应文件\n    :param url: 需要爬取的 url 地址\n    :return:\n    \"\"\"\n    print \"正在下载 \" + filename\n    request = urllib2.Request(url,headers={\"User-Agent\":\"Mozilla/5.0 (Windows NT 6.1; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\"})\n    response = urllib2.urlopen(request)\n    return response.read()\n\ndef writePage(html, filename):\n    \"\"\"\n    作用：将 html 内容写入到本地\n    :param html: 服务器响应文件内容\n    :return:\n    \"\"\"\n    print \"正在保存 \" + filename\n    # 文件写入\n    with open(filename.decode('utf-8'), \"w\") as f:\n        f.write(html)\n    print \"-\" * 30\n\n\ndef tiebaSpider(url, beginPage, endPage):\n    \"\"\"\n    作用：贴吧爬虫调度器，负责组合处理每个页面的 url\n    url : 贴吧 url 的前部分\n    beginPage：起始页\n    endPage：结束页\n    :return:\n    \"\"\"\n    for page in range(beginPage, endPage + 1):\n        pn = (page - 1) * 50\n        filename = \"第\" + str(page) + \"页.html\"\n        fullurl = url + \"&pn=\" + str(pn)\n        print fullurl\n        html = loadPage(fullurl,filename)\n        #print html\n        writePage(html,filename)\n\nif __name__ == \"__main__\":\n    kw = raw_input(\"请输入需要爬取的贴吧名:\")\n    beginPage = int(raw_input(\"请输入起始页:\"))\n    endPage = int(raw_input(\"请输入结束页:\"))\n\n    url = \"http://tieba.baidu.com/f?\"\n    key = urllib.urlencode({\"kw\" : kw})\n    fullurl = url + key\n    tiebaSpider(fullurl,beginPage,endPage)\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}