{"title": "TensorFlow学习笔记（8）：基于MNIST数据的循环神经网络RNN - 数据实验室 ", "index": "tensorflow,python", "content": "前言\n本文输入数据是MNIST，全称是Modified National Institute of Standards and Technology，是一组由这个机构搜集的手写数字扫描文件和每个文件对应标签的数据集，经过一定的修改使其适合机器学习算法读取。这个数据集可以从牛的不行的Yann LeCun教授的网站获取。\n本系列的其他文章已经根据TensorFlow的官方教程基于MNIST数据集采用了softmax regression和CNN进行建模。为了完整性，本文对MNIST数据应用RNN模型求解，具体使用的RNN为LSTM。\n关于RNN/LSTM的理论知识，可以参考这篇文章\n代码\n# coding: utf-8\n# @author: 陈水平\n# @date：2017-02-14\n# \n\n# In[1]:\n\nimport tensorflow as tf\nimport numpy as np\n\n\n# In[2]:\n\nsess = tf.InteractiveSession()\n\n\n# In[3]:\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets('mnist/', one_hot=True)\n\n\n# In[4]:\n\nlearning_rate = 0.001\nbatch_size = 128\n\nn_input = 28\nn_steps = 28\nn_hidden = 128\nn_classes = 10\n\nx = tf.placeholder(tf.float32, [None, n_steps, n_input])\ny = tf.placeholder(tf.float32, [None, n_classes])\n\n\n# In[5]:\n\ndef RNN(x, weight, biases):\n    # x shape: (batch_size, n_steps, n_input)\n    # desired shape: list of n_steps with element shape (batch_size, n_input)\n    x = tf.transpose(x, [1, 0, 2])\n    x = tf.reshape(x, [-1, n_input])\n    x = tf.split(0, n_steps, x)\n    outputs = list()\n    lstm = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n    state = (tf.zeros([n_steps, n_hidden]),)*2\n    sess.run(state)\n    with tf.variable_scope(\"myrnn2\") as scope:\n        for i in range(n_steps-1):\n            if i > 0:\n                scope.reuse_variables()\n            output, state = lstm(x[i], state)\n            outputs.append(output)\n    final = tf.matmul(outputs[-1], weight) + biases\n    return final\n\n\n# In[6]:\n\ndef RNN(x, n_steps, n_input, n_hidden, n_classes):\n    # Parameters:\n    # Input gate: input, previous output, and bias\n    ix = tf.Variable(tf.truncated_normal([n_input, n_hidden], -0.1, 0.1))\n    im = tf.Variable(tf.truncated_normal([n_hidden, n_hidden], -0.1, 0.1))\n    ib = tf.Variable(tf.zeros([1, n_hidden]))\n    # Forget gate: input, previous output, and bias\n    fx = tf.Variable(tf.truncated_normal([n_input, n_hidden], -0.1, 0.1))\n    fm = tf.Variable(tf.truncated_normal([n_hidden, n_hidden], -0.1, 0.1))\n    fb = tf.Variable(tf.zeros([1, n_hidden]))\n    # Memory cell: input, state, and bias\n    cx = tf.Variable(tf.truncated_normal([n_input, n_hidden], -0.1, 0.1))\n    cm = tf.Variable(tf.truncated_normal([n_hidden, n_hidden], -0.1, 0.1))\n    cb = tf.Variable(tf.zeros([1, n_hidden]))\n    # Output gate: input, previous output, and bias\n    ox = tf.Variable(tf.truncated_normal([n_input, n_hidden], -0.1, 0.1))\n    om = tf.Variable(tf.truncated_normal([n_hidden, n_hidden], -0.1, 0.1))\n    ob = tf.Variable(tf.zeros([1, n_hidden]))\n    # Classifier weights and biases\n    w = tf.Variable(tf.truncated_normal([n_hidden, n_classes]))\n    b = tf.Variable(tf.zeros([n_classes]))\n\n    # Definition of the cell computation\n    def lstm_cell(i, o, state):\n        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n        update = tf.tanh(tf.matmul(i, cx) + tf.matmul(o, cm) + cb)\n        state = forget_gate * state + input_gate * update\n        output_gate = tf.sigmoid(tf.matmul(i, ox) +  tf.matmul(o, om) + ob)\n        return output_gate * tf.tanh(state), state\n    \n    # Unrolled LSTM loop\n    outputs = list()\n    state = tf.Variable(tf.zeros([batch_size, n_hidden]))\n    output = tf.Variable(tf.zeros([batch_size, n_hidden]))\n    \n    # x shape: (batch_size, n_steps, n_input)\n    # desired shape: list of n_steps with element shape (batch_size, n_input)\n    x = tf.transpose(x, [1, 0, 2])\n    x = tf.reshape(x, [-1, n_input])\n    x = tf.split(0, n_steps, x)\n    for i in x:\n        output, state = lstm_cell(i, output, state)\n        outputs.append(output)\n    logits =tf.matmul(outputs[-1], w) + b\n    return logits\n\n\n# In[7]:\n\npred = RNN(x, n_steps, n_input, n_hidden, n_classes)\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\ncorrect_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n\n# In[8]:\n\n# Launch the graph\nsess.run(init)\nfor step in range(20000):\n    batch_x, batch_y = mnist.train.next_batch(batch_size)\n    batch_x = batch_x.reshape((batch_size, n_steps, n_input))\n    sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n\n    if step % 50 == 0:\n        acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n        loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n        print \"Iter \" + str(step) + \", Minibatch Loss= \" +               \"{:.6f}\".format(loss) + \", Training Accuracy= \" +               \"{:.5f}\".format(acc)\nprint \"Optimization Finished!\"\n\n\n# In[9]:\n\n# Calculate accuracy for 128 mnist test images\ntest_len = batch_size\ntest_data = mnist.test.images[:test_len].reshape((-1, n_steps, n_input))\ntest_label = mnist.test.labels[:test_len]\nprint \"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: test_data, y: test_label})\n\n输出如下：\nIter 0, Minibatch Loss= 2.540429, Training Accuracy= 0.07812\nIter 50, Minibatch Loss= 2.423611, Training Accuracy= 0.06250\nIter 100, Minibatch Loss= 2.318830, Training Accuracy= 0.13281\nIter 150, Minibatch Loss= 2.276640, Training Accuracy= 0.13281\nIter 200, Minibatch Loss= 2.276727, Training Accuracy= 0.12500\nIter 250, Minibatch Loss= 2.267064, Training Accuracy= 0.16406\nIter 300, Minibatch Loss= 2.234139, Training Accuracy= 0.19531\nIter 350, Minibatch Loss= 2.295060, Training Accuracy= 0.12500\nIter 400, Minibatch Loss= 2.261856, Training Accuracy= 0.16406\nIter 450, Minibatch Loss= 2.220284, Training Accuracy= 0.17969\nIter 500, Minibatch Loss= 2.276015, Training Accuracy= 0.13281\nIter 550, Minibatch Loss= 2.220499, Training Accuracy= 0.14062\nIter 600, Minibatch Loss= 2.219574, Training Accuracy= 0.11719\nIter 650, Minibatch Loss= 2.189177, Training Accuracy= 0.25781\nIter 700, Minibatch Loss= 2.195167, Training Accuracy= 0.19531\nIter 750, Minibatch Loss= 2.226459, Training Accuracy= 0.18750\nIter 800, Minibatch Loss= 2.148620, Training Accuracy= 0.23438\nIter 850, Minibatch Loss= 2.122925, Training Accuracy= 0.21875\nIter 900, Minibatch Loss= 2.065122, Training Accuracy= 0.24219\n...\nIter 19350, Minibatch Loss= 0.001304, Training Accuracy= 1.00000\nIter 19400, Minibatch Loss= 0.000144, Training Accuracy= 1.00000\nIter 19450, Minibatch Loss= 0.000907, Training Accuracy= 1.00000\nIter 19500, Minibatch Loss= 0.002555, Training Accuracy= 1.00000\nIter 19550, Minibatch Loss= 0.002018, Training Accuracy= 1.00000\nIter 19600, Minibatch Loss= 0.000853, Training Accuracy= 1.00000\nIter 19650, Minibatch Loss= 0.001035, Training Accuracy= 1.00000\nIter 19700, Minibatch Loss= 0.007034, Training Accuracy= 0.99219\nIter 19750, Minibatch Loss= 0.000608, Training Accuracy= 1.00000\nIter 19800, Minibatch Loss= 0.002913, Training Accuracy= 1.00000\nIter 19850, Minibatch Loss= 0.003484, Training Accuracy= 1.00000\nIter 19900, Minibatch Loss= 0.005693, Training Accuracy= 1.00000\nIter 19950, Minibatch Loss= 0.001904, Training Accuracy= 1.00000\nOptimization Finished!\n\nTesting Accuracy: 0.992188\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "2"}