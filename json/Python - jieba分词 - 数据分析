{"title": "Python - jieba分词 - 数据分析 ", "index": "python,jieba分词", "content": "1.分词\n1.1主要分词函数\n\njieba.cut(sentence, cut_all, HMM):sentence-需要分词的字符串；cut_all-控制是否采用全模式；HMM-控制是否使用HMM模型；jieba.cut()返回的结构是一个可迭代的 generator。\njieba.cut_for_search(sentence, HMM):sentence-需要分词的字符串；HMM-控制是否使用HMM模型；这种分词方法粒度比较细，成为搜索引擎模式；jieba.cut_for_search()返回的结构是一个可迭代的 generator。\njieba.lcut()以及jieba.lcut_for_search用法和上述一致，最终返回的结构是一个列表list。\n\n1.2示例\nimport jieba as jb\n\nseg_list = jb.cut(\"我来到北京清华大学\", cut_all=True)\nprint(\"全模式: \" + \"/ \".join(seg_list))  # 全模式\n\nseg_list = jb.cut(\"我来到北京清华大学\", cut_all=False)\nprint(\"精确模式: \" + \"/ \".join(seg_list))  # 精确模式\n\nseg_list = jb.cut(\"他来到了网易杭研大厦\")  \nprint(\"默认模式: \" + \"/ \".join(seg_list)) # 默认是精确模式\n\nseg_list = jb.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")  \nprint(\"搜索引擎模式: \" + \"/ \".join(seg_list)) # 搜索引擎模式\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}