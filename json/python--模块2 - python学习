{"title": "python--模块2 - python学习 ", "index": "python", "content": "hashlib模块\n1.Python的hashlib提供了常见的摘要算法，如MD5，SHA1等等。什么是摘要算法呢？摘要算法又称哈希算法、散列算法。它通过一个函数，把任意长度的数据转换为一个长度固定的数据串（通常用16进制的字符串表示）。\n摘要算法就是通过摘要函数f()对任意长度的数据data计算出固定长度的摘要digest，目的是为了发现原始数据是否被人篡改过。\n摘要算法之所以能指出数据是否被篡改过，就是因为摘要函数是一个单向函数，计算f(data)很容易，但通过digest反推data却非常困难。而且，对原始数据做一个bit的修改，都会导致计算出的摘要完全不同。\n我们以常见的摘要算法MD5为例，计算出一个字符串的MD5值：\n#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport hashlib\nmd5_obj = hashlib.md5()\nmd5_obj.update(\"123456\".encode(\"utf-8\"))\nprint(md5_obj.hexdigest())\n如果数据量很大，可以分块多次调用update()，最后计算的结果是一样的：\n#!/usr/bin/env python\n# -*- coding:utf-8 -*-\nimport hashlib\nmd5_obj = hashlib.md5()\nmd5_obj.update(\"123456\".encode(\"utf-8\"))\nmd5_obj.update(\"000000\".encode(\"utf-8\"))\nprint(md5_obj.hexdigest()) #42c23b08884a131940e1d12196ed935c\nmd5_obj = hashlib.md5()\nmd5_obj.update(\"123456000000\".encode(\"utf-8\"))\nprint(md5_obj.hexdigest()) #42c23b08884a131940e1d12196ed935c\n应用根据MD5值判断两个文件是否相同：\nimport os\nimport  hashlib\ndef get_md5(file,n=10240):\n    with open(file,mode=\"rb\") as f:\n        md5_obj = hashlib.md5()\n        file_size =  os.path.getsize(file)\n        while file_size > 0:\n            md5_obj.update(f.read(n))\n            file_size -= n\n        return md5_obj.hexdigest()\n\ndef comper(file1,file2):\n    return  get_md5(file1) == get_md5(file2)\nprint(comper(\"1.txt\",\"2.txt\"))\nconfigparser模块\n[DEFAULT]\nServerAliveInterval = 45\nCompression = yes\nCompressionLevel = 9\nForwardX11 = yes\n  \n[bitbucket.org]\nUser = hg\n  \n[topsecret.server.com]\nPort = 50022\nForwardX11 = no\n代码生成：\nimport configparser\nconfig = configparser.ConfigParser()\nconfig[\"DEFAULT\"] = {\n    \"ServerAliveInterval\":\"45\",\n    \"Compression\":\"yes\",\n    \"CompressionLevel\":\"9\",\n    \"ForwardX11\":\"yes\"\n}\nconfig[\"bitbucket.org\"] = {\"User\":\"hg\"}\nconfig[\"topsecret.server.com\"] = {\n    \"Port\":\"50022\",\n    \"ForwardX11\":\"no\"\n}\nwith open(\"examle.ini\",\"w\") as f:\n    config.write(f)\n查找文件：\nimport configparser\nconfig = configparser.ConfigParser()\nprint(config.sections()) #[]\n\nconfig.read(\"examle.ini\") #['bitbucket.org', 'topsecret.server.com']\nprint(config.sections())\n\nprint(\"bytebong.com\" in config) #False\nprint(\"bitbucket.org\" in config) #True\n\nprint(config[\"bitbucket.org\"][\"User\"]) #hg\nprint(config[\"DEFAULT\"][\"ForwardX11\"]) #yes\n\nprint(config[\"bitbucket.org\"]) #<Section: bitbucket.org>\n\nfor key in config[\"bitbucket.org\"]: #注意,有default会默认default的键\n    print(key)\n\n\nprint(config.options(\"bitbucket.org\")) #['user', 'serveraliveinterval', 'compression', 'compressionlevel', 'forwardx11']\n\nprint(config.items(\"bitbucket.org\")) #[('serveraliveinterval', '45'), ('compression', 'yes'), ('compressionlevel', '9'), ('forwardx11', 'yes'), ('user', 'hg')]\n\nprint(config.get(\"bitbucket.org\",\"Compression\")) #yes\n增删改操作\nimport configparser\nconfig = configparser.ConfigParser()\nconfig.read(\"examle.ini\")\nconfig.add_section(\"yuan\")\nconfig.remove_section('bitbucket.org')\nconfig.remove_option(\"topsecret.server.com\",\"port\")\n\nconfig.set(\"topsecret.server.com\",\"k1\",\"111\")\nconfig.set(\"yuan\",\"k2\",\"222\")\nconfig.write(open(\"examle.ini2\",\"w\"))\n\nconfig.set('topsecret.server.com','k1','11111')\nconfig.set('yuan','k2','22222')\n\nconfig.write(open('new2.ini', \"w\"))\nlogging模块\n1.函数简单配置\nimport logging\nlogging.debug('debug message')\nlogging.info('info message')\nlogging.warning('warning message')\nlogging.error('error message')\nlogging.critical('critical message')\n#默认屏幕打印warning以上级别的\n默认情况下Python的logging模块将日志打印到了标准输出中，且只显示了大于等于WARNING级别的日志，这说明默认的日志级别设置为WARNING（日志级别等级CRITICAL > ERROR > WARNING > INFO > DEBUG），默认的日志格式为日志级别：Logger名称：用户输出消息。2.默认情况下Python的logging模块将日志打印到了标准输出中，且只显示了大于等于WARNING级别的日志，这说明默认的日志级别设置为WARNING（日志级别等级CRITICAL > ERROR > WARNING > INFO > DEBUG），默认的日志格式为日志级别：Logger名称：用户输出消息。灵活配置日志级别，日志格式，输出位置:\nimport logging\n\nlogging.basicConfig(level=logging.DEBUG,\n                    format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s',\n                    datefmt='%a, %d %b %Y %H:%M:%S',\n                    filename='test.log',\n                    filemode='w')\n\nlogging.debug('debug message')\nlogging.info('info message')\nlogging.warning('warning message')\nlogging.error('error message')\nlogging.critical('critical message')\n配置参数：\nlogging.basicConfig()函数中可通过具体参数来更改logging模块默认行为，可用参数有：\n\nfilename：用指定的文件名创建FiledHandler，这样日志会被存储在指定的文件中。\nfilemode：文件打开方式，在指定了filename时使用这个参数，默认值为“a”还可指定为“w”。\nformat：指定handler使用的日志显示格式。\ndatefmt：指定日期时间格式。\nlevel：设置rootlogger（后边会讲解具体概念）的日志级别\nstream：用指定的stream创建StreamHandler。可以指定输出到sys.stderr,sys.stdout或者文件(f=open(‘test.log’,’w’))，默认为sys.stderr。若同时列出了filename和stream两个参数，则stream参数会被忽略。\n\nformat参数中可能用到的格式化串：\n%(name)s Logger的名字\n%(levelno)s 数字形式的日志级别\n%(levelname)s 文本形式的日志级别\n%(pathname)s 调用日志输出函数的模块的完整路径名，可能没有\n%(filename)s 调用日志输出函数的模块的文件名\n%(module)s 调用日志输出函数的模块名\n%(funcName)s 调用日志输出函数的函数名\n%(lineno)d 调用日志输出函数的语句所在的代码行\n%(created)f 当前时间，用UNIX标准的表示时间的浮 点数表示\n%(relativeCreated)d 输出日志信息时的，自Logger创建以 来的毫秒数\n%(asctime)s 字符串形式的当前时间。默认格式是 “2003-07-08 16:49:45,896”。逗号后面的是毫秒\n%(thread)d 线程ID。可能没有\n%(threadName)s 线程名。可能没有\n%(process)d 进程ID。可能没有\n%(message)s用户输出的消息\n3.\nimport logging\n\nlogger = logging.getLogger()\n# 创建一个handler，用于写入日志文件\nfh = logging.FileHandler('test.log',encoding='utf-8') \n\n# 再创建一个handler，用于输出到控制台 \nch = logging.StreamHandler() \nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nfh.setLevel(logging.DEBUG)\n\nfh.setFormatter(formatter) \nch.setFormatter(formatter) \nlogger.addHandler(fh) #logger对象可以添加多个fh和ch对象 \nlogger.addHandler(ch) \n\nlogger.debug('logger debug message') \nlogger.info('logger info message') \nlogger.warning('logger warning message') \nlogger.error('logger error message') \nlogger.critical('logger critical message')\n序列化模块\n什么叫序列化——将原本的字典、列表等内容转换成一个字符串的过程就叫做序列化。序列化的目的：1.以某种存储形式使自定义对象持久化；\n         2.将对象从一个地方传递到另一个地方。\n         3.使程序更具维护性。\n1.jsonJson模块提供了四个功能：dumps、dump、loads、load\nimport json\ndic = {'k1':'v1','k2':'v2','k3':'v3'}\nstr_dic = json.dumps(dic)  #序列化：将一个字典转换成一个字符串\nprint(str_dic) #{\"k1\": \"v1\", \"k2\": \"v2\", \"k3\": \"v3\"}3\", \"k1\": \"v1\", \"k2\": \"v2\"}\n#注意，json转换完的字符串类型的字典中的字符串是由\"\"表示的\nimport json\ndic = {'k1':'v1','k2':'v2','k3':'v3'}\nstr_dic = json.dumps(dic)  #序列化：将一个字典转换成一个字符串\nprint(str_dic) #{\"k1\": \"v1\", \"k2\": \"v2\", \"k3\": \"v3\"}\ndic2 = json.loads(str_dic)  #反序列化：将一个字符串格式的字典转换成一个字典，注意，要用json的loads功能处理的字符串类型的字典中的字符串必须由\"\"表示\nprint(dic2)  #{'k1': 'v1', 'k2': 'v2', 'k3': 'v3'}\n\nlist_dic = [1,['a','b','c'],3,{'k1':'v1','k2':'v2'}]\nstr_dic = json.dumps(list_dic) #也可以处理嵌套的数据类型\nprint(str_dic) #[1, [\"a\", \"b\", \"c\"], 3, {\"k1\": \"v1\", \"k2\": \"v2\"}]\nlist_dic2 = json.loads(str_dic)\nprint(list_dic2) #[1, ['a', 'b', 'c'], 3, {'k1': 'v1', 'k2': 'v2'}]\nimport json\nf = open(\"json_file\",\"w\")\ndic = {\"k1\":\"v1\"}\njson.dump(dic,f)    #dump方法接收一个文件句柄，直接将字典转换成json字符串写入文件\nf.close()\n\nf = open(\"json_file\",\"r\")\ndic2 = json.load(f)  #load方法接收一个文件句柄，直接将文件中的json字符串转换成数据结构返回\nf.close()\nprint(dic2) #{'k1': 'v1'}\nimport json\nf = open('file','w')\njson.dump({'国籍':'中国'},f)\nret = json.dumps({'国籍':'中国'})\nf.write(ret+'\\n')\njson.dump({'国籍':'美国'},f,ensure_ascii=False)\nret = json.dumps({'国籍':'美国'},ensure_ascii=False)\nf.write(ret+'\\n')\nf.close()\nSerialize obj to a JSON formatted str.(字符串表示的json对象) \nSkipkeys：默认值是False，如果dict的keys内的数据不是python的基本类型(str,unicode,int,long,float,bool,None)，设置为False时，就会报TypeError的错误。此时设置成True，则会跳过这类key \nensure_ascii:，当它为True的时候，所有非ASCII码字符显示为\\uXXXX序列，只需在dump时将ensure_ascii设置为False即可，此时存入json的中文即可正常显示。) \nIf check_circular is false, then the circular reference check for container types will be skipped and a circular reference will result in an OverflowError (or worse). \nIf allow_nan is false, then it will be a ValueError to serialize out of range float values (nan, inf, -inf) in strict compliance of the JSON specification, instead of using the JavaScript equivalents (NaN, Infinity, -Infinity). \nindent：应该是一个非负的整型，如果是0就是顶格分行显示，如果为空就是一行最紧凑显示，否则会换行且按照indent的数值显示前面的空白分行显示，这样打印出来的json数据也叫pretty-printed json \nseparators：分隔符，实际上是(item_separator, dict_separator)的一个元组，默认的就是(‘,’,’:’)；这表示dictionary内keys之间用“,”隔开，而KEY和value之间用“：”隔开。 \ndefault(obj) is a function that should return a serializable version of obj or raise TypeError. The default simply raises TypeError. \nsort_keys：将数据根据keys的值进行排序。 \nTo use a custom JSONEncoder subclass (e.g. one that overrides the .default() method to serialize additional types), specify it with the cls kwarg; otherwise JSONEncoder is used.\n#格式化输出\nimport json\ndata = {'username':['李华','二愣子'],'sex':'male','age':16}\njson_dic2 = json.dumps(data,sort_keys=True,indent=2,separators=(',',':'),ensure_ascii=False)\nprint(json_dic2)\njson，用于字符串 和 python数据类型间进行转换\n2.pickle用于python特有的类型 和 python的数据类型间进行转换\npickle模块提供了四个功能：dumps、dump(序列化，存）、loads（反序列化，读）、load  （不仅可以序列化字典，列表...可以把python中任意的数据类型序列化）\nimport pickle\ndic = {'k1':'v1','k2':'v2','k3':'v3'}\nstr_dic = pickle.dumps(dic)\nprint(str_dic)  #一串二进制内容b'\\x80\\x03}q\\x00(X\\x02\\x00\\\n\ndic2 = pickle.loads(str_dic)\nprint(dic2)    #字典{'k1':'v1','k2':'v2','k3':'v3'}\nimport pickle\nimport time\nstruct_time  = time.localtime(1000000000)\nprint(struct_time) #time.struct_time(tm_year=2001, tm_mon=9, tm_mday=9, tm_hour=9, tm_min=46, tm_sec=40, tm_wday=6, tm_yday=252, tm_isdst=0\nf = open('pickle_file','wb')\npickle.dump(struct_time,f)\nf.close()\n\nf = open('pickle_file','rb')\nstruct_time2 = pickle.load(f)\nprint(struct_time2) #time.struct_time(tm_year=2001, tm_mon=9, tm_mday=9, tm_hour=9, tm_min=46, tm_sec=40, tm_wday=6, tm_yday=252, tm_isdst=0\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}