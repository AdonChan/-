{"title": "TensorFlow2 - 数据分析 ", "index": "数据分析,python", "content": "机器学习基础\n\n线性回归\n逻辑回归\nSoftmax分类\n神经网络\n\n线性回归\n什么是回归？\n通俗地讲：给定X1, X2, ..., Xn，通过模型或算法预测数值Y，即是回归。如上图所示。例如，预测测试分数：\n\n\nx(hours)\ny(score)\n\n\n\n10\n90\n\n\n9\n80\n\n\n3\n50\n\n\n2\n30\n\n\n\n以下面的数据阐述什么是线性回归：\n\n\nx\ny\n\n\n\n1\n1\n\n\n2\n2\n\n\n3\n3\n\n\n\n1.如下图所示，我们把上述数据中的点(x, y)在坐标中描绘出来，可以发现(x,y)呈线性趋势。2.试图用一条直线H(x)=wx+b去拟合坐标中的观察值，例如图中的3条直线。\n\n那么，图中的3条直线哪个能更好地拟合观察值(x,y)呢？如下图所示：我们可以用观察值到直线的竖直距离的平方(H(x)-y)^2来衡量模型的拟合效果，如图中的损失函数：cost。\n\n让我们观察一下这个例子中的损失函数到底长什么样子。如下图所示：cost(W)为平滑可导的凸函数，在w=1处取得最小值。因此，我们可以通过梯度下降的方法求解使得损失函数达到最小值的W。\n\nTensorFlow实现线性回归\n1.添加线性节点H(x) = Wx + b\n# 训练数据集x,y\nx_train = [1, 2, 3]\ny_train = [1, 2, 3]\n\nW = tf.Variable(tf.random_normal([1]), name='weight')\nb = tf.Variable(tf.random_normal([1]), name='bias')\n# 线性假设 XW + b\nhypothesis = x_train * W + b\n2.计算损失函数\ncost = tf.reduce_mean(tf.square(hypothesis - y_train))\n3.梯度下降\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\ntrain = optimizer.minimize(cost)\n4.更新图并获取结果\n# 开启session会话\nless = tf.Session()\n# 初始化全局变量\nsess.run(tf.global_variables_initializer())\n\n# 拟合直线\nfor step in range(2001):\n    sess.run(train)\n    if step % 20 == 0:\n        print(step, sets.run(cost), sess.run(W), sets.run(b))\n完整代码：\nimport tensorflow as tf\n# 训练数据集x,y\nx_train = [1, 2, 3]\ny_train = [1, 2, 3]\n\nW = tf.Variable(tf.random_normal([1]), name='weight')\nb = tf.Variable(tf.random_normal([1]), name='bias')\n# 线性假设 XW + b\nhypothesis = x_train * W + b\n# 损失函数\ncost = tf.reduce_mean(tf.square(hypothesis - y_train))\n\n# 梯度下降\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\ntrain = optimizer.minimize(cost)\n\n# 开启session会话\nsess = tf.Session()\n# 初始化全局变量\nsess.run(tf.global_variables_initializer())\n\n# 拟合直线\nfor step in range(2001):\n    sess.run(train)\n    if step % 20 == 0:\n        print(step, sess.run(cost), sess.run(W), sess.run(b))\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}