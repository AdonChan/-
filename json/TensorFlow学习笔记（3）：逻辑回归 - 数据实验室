{"title": "TensorFlow学习笔记（3）：逻辑回归 - 数据实验室 ", "index": "tensorflow,python,scikit-learn", "content": "前言\n本文使用tensorflow训练逻辑回归模型，并将其与scikit-learn做比较。数据集来自Andrew Ng的网上公开课程Deep Learning\n代码\n#!/usr/bin/env python\n# -*- coding=utf-8 -*-\n# @author: 陈水平\n# @date: 2017-01-04\n# @description: compare the logistics regression of tensorflow with sklearn based on the exercise of deep learning course of Andrew Ng.\n# @ref: http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=DeepLearning&doc=exercises/ex4/ex4.html\n\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\n\n# Read x and y\nx_data = np.loadtxt(\"ex4x.dat\").astype(np.float32)\ny_data = np.loadtxt(\"ex4y.dat\").astype(np.float32)\n\nscaler = preprocessing.StandardScaler().fit(x_data)\nx_data_standard = scaler.transform(x_data)\n\n# We evaluate the x and y by sklearn to get a sense of the coefficients.\nreg = LogisticRegression(C=999999999, solver=\"newton-cg\")  # Set C as a large positive number to minimize the regularization effect\nreg.fit(x_data, y_data)\nprint \"Coefficients of sklearn: K=%s, b=%f\" % (reg.coef_, reg.intercept_)\n\n# Now we use tensorflow to get similar results.\nW = tf.Variable(tf.zeros([2, 1]))\nb = tf.Variable(tf.zeros([1, 1]))\ny = 1 / (1 + tf.exp(-tf.matmul(x_data_standard, W) + b))\nloss = tf.reduce_mean(- y_data.reshape(-1, 1) *  tf.log(y) - (1 - y_data.reshape(-1, 1)) * tf.log(1 - y))\n\noptimizer = tf.train.GradientDescentOptimizer(1.3)\ntrain = optimizer.minimize(loss)\n\ninit = tf.initialize_all_variables()\n\nsess = tf.Session()\nsess.run(init)\nfor step in range(100):\n    sess.run(train)\n    if step % 10 == 0:\n        print step, sess.run(W).flatten(), sess.run(b).flatten()\n\nprint \"Coefficients of tensorflow (input should be standardized): K=%s, b=%s\" % (sess.run(W).flatten(), sess.run(b).flatten())\nprint \"Coefficients of tensorflow (raw input): K=%s, b=%s\" % (sess.run(W).flatten() / scaler.scale_, sess.run(b).flatten() - np.dot(scaler.mean_ / scaler.scale_, sess.run(W)))\n\n\n# Problem solved and we are happy. But...\n# I'd like to implement the logistic regression from a multi-class viewpoint instead of binary.\n# In machine learning domain, it is called softmax regression\n# In economic and statistics domain, it is called multinomial logit (MNL) model, proposed by Daniel McFadden, who shared the 2000  Nobel Memorial Prize in Economic Sciences.\n\nprint \"------------------------------------------------\"\nprint \"We solve this binary classification problem again from the viewpoint of multinomial classification\"\nprint \"------------------------------------------------\"\n\n# As a tradition, sklearn first\nreg = LogisticRegression(C=9999999999, solver=\"newton-cg\", multi_class=\"multinomial\")\nreg.fit(x_data, y_data)\nprint \"Coefficients of sklearn: K=%s, b=%f\" % (reg.coef_, reg.intercept_)\nprint \"A little bit difference at first glance. What about multiply them with 2?\"\n\n# Then try tensorflow\nW = tf.Variable(tf.zeros([2, 2]))  # first 2 is feature number, second 2 is class number\nb = tf.Variable(tf.zeros([1, 2]))\nV = tf.matmul(x_data_standard, W) + b\ny = tf.nn.softmax(V)  # tensorflow provide a utility function to calculate the probability of observer n choose alternative i, you can replace it with `y = tf.exp(V) / tf.reduce_sum(tf.exp(V), keep_dims=True, reduction_indices=[1])`\n\n# Encode the y label in one-hot manner\nlb = preprocessing.LabelBinarizer()\nlb.fit(y_data)\ny_data_trans = lb.transform(y_data)\ny_data_trans = np.concatenate((1 - y_data_trans, y_data_trans), axis=1)  # Only necessary for binary class \n\nloss = tf.reduce_mean(-tf.reduce_sum(y_data_trans * tf.log(y), reduction_indices=[1]))\noptimizer = tf.train.GradientDescentOptimizer(1.3)\ntrain = optimizer.minimize(loss)\n\ninit = tf.initialize_all_variables()\n\nsess = tf.Session()\nsess.run(init)\nfor step in range(100):\n    sess.run(train)\n    if step % 10 == 0:\n        print step, sess.run(W).flatten(), sess.run(b).flatten()\n\nprint \"Coefficients of tensorflow (input should be standardized): K=%s, b=%s\" % (sess.run(W).flatten(), sess.run(b).flatten())\nprint \"Coefficients of tensorflow (raw input): K=%s, b=%s\" % ((sess.run(W) / scaler.scale_).flatten(),  sess.run(b).flatten() - np.dot(scaler.mean_ / scaler.scale_, sess.run(W)))\n输出如下：\nCoefficients of sklearn: K=[[ 0.14834077  0.15890845]], b=-16.378743\n0 [ 0.33699557  0.34786162] [ -4.84287721e-09]\n10 [ 1.15830743  1.22841871] [ 0.02142336]\n20 [ 1.3378191   1.42655993] [ 0.03946959]\n30 [ 1.40735555  1.50197577] [ 0.04853692]\n40 [ 1.43754184  1.53418231] [ 0.05283691]\n50 [ 1.45117068  1.54856908] [ 0.05484771]\n60 [ 1.45742035  1.55512536] [ 0.05578374]\n70 [ 1.46030474  1.55814099] [ 0.05621871]\n80 [ 1.46163988  1.55953443] [ 0.05642065]\n90 [ 1.46225858  1.56017959] [ 0.0565144]\nCoefficients of tensorflow (input should be standardized): K=[ 1.46252561  1.56045783], b=[ 0.05655487]\nCoefficients of tensorflow (raw input): K=[ 0.14831361  0.15888004], b=[-16.26265144]\n------------------------------------------------\nWe solve this binary classification problem again from the viewpoint of multinomial classification\n------------------------------------------------\nCoefficients of sklearn: K=[[ 0.07417039  0.07945423]], b=-8.189372\nA little bit difference at first glance. What about multiply them with 2?\n0 [-0.33699557  0.33699557 -0.34786162  0.34786162] [  6.05359674e-09  -6.05359674e-09]\n10 [-0.68416572  0.68416572 -0.72988117  0.72988123] [ 0.02157043 -0.02157041]\n20 [-0.72234094  0.72234106 -0.77087188  0.77087194] [ 0.02693938 -0.02693932]\n30 [-0.72958517  0.72958535 -0.7784785   0.77847856] [ 0.02802362 -0.02802352]\n40 [-0.73103166  0.73103184 -0.77998811  0.77998811] [ 0.02824244 -0.02824241]\n50 [-0.73132294  0.73132324 -0.78029168  0.78029174] [ 0.02828659 -0.02828649]\n60 [-0.73138171  0.73138207 -0.78035289  0.78035301] [ 0.02829553 -0.02829544]\n70 [-0.73139352  0.73139393 -0.78036523  0.78036535] [ 0.02829732 -0.0282972 ]\n80 [-0.73139596  0.73139632 -0.78036767  0.78036791] [ 0.02829764 -0.02829755]\n90 [-0.73139644  0.73139679 -0.78036815  0.78036839] [ 0.02829781 -0.02829765]\nCoefficients of tensorflow (input should be standardized): K=[-0.7313965   0.73139679 -0.78036827  0.78036839], b=[ 0.02829777 -0.02829769]\nCoefficients of tensorflow (raw input): K=[-0.07417037  0.07446811 -0.07913655  0.07945422], b=[ 8.1893692  -8.18937111]\n思考\n\n对于逻辑回归，损失函数比线性回归模型复杂了一些。首先需要通过sigmoid函数，将线性回归的结果转化为0至1之间的概率值。然后写出每个样本的发生概率（似然），那么所有样本的发生概率就是每个样本发生概率的乘积。为了求导方便，我们对所有样本的发生概率取对数，保持其单调性的同时，可以将连乘变为求和（加法的求导公式比乘法的求导公式简单很多）。对数极大似然估计方法的目标函数是最大化所有样本的发生概率；机器学习习惯将目标函数称为损失，所以将损失定义为对数似然的相反数，以转化为极小值问题。\n我们提到逻辑回归时，一般指的是二分类问题；然而这套思想是可以很轻松就拓展为多分类问题的，在机器学习领域一般称为softmax回归模型。本文的作者是统计学与计量经济学背景，因此一般将其称为MNL模型。\n\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "5"}