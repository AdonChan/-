{"title": "Windows Theano GPU 版配置 - 苏州谷歌开发者社区 ", "index": "人工智能,python", "content": "因为自己在上Coursera的Advanced Machine Learning, 里面第四周的Assignment要用到PYMC3，然后这个似乎是基于theano后端的。然而CPU版TMD太慢了，跑个马尔科夫蒙特卡洛要10个小时，简直不能忍了。所以妥妥换gpu版。\n为了不把环境搞坏，我在Anaconda里面新建了一个环境。(关于Anaconda，可以看我之前翻译的文章)\nConda Create -n theano-gpu python=3.4\n（theano GPU版貌似不支持最新版，保险起见装了旧版）\nconda install theano pygpu\n这里面会涉及很多依赖，应该conda会给你搞好，缺什么的话自己按官方文档去装。\n然后至于Cuda和Cudnn的安装，可以看我写的关于TF安装的教程\n和TF不同的是，Theano不分gpu和cpu版，用哪个看配置文件设置，这一点是翻博客了解到的：配置好Theano环境之后，只要 C:Users你的用户名 的路径下添加 .theanorc.txt 文件。\n.theanorc.txt 文件内容:\n[global]\n\nopenmp=False\n\ndevice = cuda\n\nfloatX = float32\n\nbase_compiler = C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\bin\n\nallow_input_downcast=True \n\n[lib]\n\ncnmem = 0.75\n\n[blas]\n\nldflags=\n\n[gcc]\n\ncxxflags=-IC:\\Users\\lyh\\Anaconda2\\MinGW\n\n[nvcc]\n\nfastmath = True\n\nflags = -LC:\\Users\\lyh\\Anaconda2\\libs\n\ncompiler_bindir = C:\\Program Files (x86)\\Microsoft Visual Studio 12.0\\VC\\bin\n\nflags =  -arch=sm_30\n注意在新版本中，声明用gpu从device=gpu改为device=cuda\n然后测试是否成功：\nfrom theano import function, config, shared, tensor\nimport numpy\nimport time\n\nvlen = 10 * 30 * 768  # 10 x #cores x # threads per core\niters = 1000\n\nrng = numpy.random.RandomState(22)\nx = shared(numpy.asarray(rng.rand(vlen), config.floatX))\nf = function([], tensor.exp(x))\nprint(f.maker.fgraph.toposort())\nt0 = time.time()\nfor i in range(iters):\n    r = f()\nt1 = time.time()\nprint(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\nprint(\"Result is %s\" % (r,))\nif numpy.any([isinstance(x.op, tensor.Elemwise) and\n              ('Gpu' not in type(x.op).__name__)\n              for x in f.maker.fgraph.toposort()]):\n    print('Used the cpu')\nelse:\n    print('Used the gpu')\n输出：\n[GpuElemwise{exp,no_inplace}(<GpuArrayType<None>(float32, vector)>), HostFromGpu(gpuarray)(GpuElemwise{exp,no_inplace}.0)]\nLooping 1000 times took 0.377000 seconds\nResult is [ 1.23178029  1.61879349  1.52278066 ...,  2.20771813  2.29967761\n  1.62323296]\nUsed the gpu\n到这里就算配好了\n然后在作业里面，显示Quadro卡启用\n\n但是还是有个warning\nWARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n这个真不知道怎么处理\n然后后面运行到：\nwith pm.Model() as logistic_model:\n    # Since it is unlikely that the dependency between the age and salary is linear, we will include age squared\n    # into features so that we can model dependency that favors certain ages.\n    # Train Bayesian logistic regression model on the following features: sex, age, age^2, educ, hours\n    # Use pm.sample to run MCMC to train this model.\n    # To specify the particular sampler method (Metropolis-Hastings) to pm.sample,\n    # use `pm.Metropolis`.\n    # Train your model for 400 samples.\n    # Save the output of pm.sample to a variable: this is the trace of the sampling procedure and will be used\n    # to estimate the statistics of the posterior distribution.\n    \n    #### YOUR CODE HERE ####\n    \n    pm.glm.GLM.from_formula('income_more_50K ~  sex+age + age_square + educ + hours', data, family=pm.glm.families.Binomial())\n    with logistic_model:\n        trace = pm.sample(400, step=[pm.Metropolis()]) #nchains=1 works for gpu model\n        \n    ### END OF YOUR CODE ###\n这里出现的报错：\nGpuArrayException: cuMemcpyDtoHAsync(dst, src->ptr + srcoff, sz, ctx->mem_s): CUDA_ERROR_INVALID_VALUE: invalid argument\n这个问题最后github大神解决了：So njobs will spawn multiple chains to run in parallel. If the model uses the GPU there will be a conflict. We recently added nchains where you can still run multiple chains. So I think running pm.sample(niter, nchains=4, njobs=1) should give you what you want.我把：\ntrace = pm.sample(400, step=[pm.Metropolis()]) #nchains=1 works for gpu model\n加上nchains就好了，应该是并行方面的问题\ntrace = pm.sample(400, step=[pm.Metropolis()],nchains=1, njobs=1) #nchains=1 works for gpu model\n另外\nplot_traces(trace, burnin=200)\n出现pm.df_summary报错，把pm.df_summary 换成 pm.summary就好了，也是github搜出来的。\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}