{"title": "Python 查找重复文件升级版 - photodup - Python测试开发 ", "index": "python", "content": "之前写了一个简化版的使用Python查找目录中的重复文件，现在升级了一下，我们来提供一个友好的网页界面。\n思路\n上一个版本我们非常简单粗暴地将所有文件的hash扫描后保存到一个字典中，字典结构大概是这样的：\nfiles = [{'hash1':['file/path...','file/path...']},\n         {'hash2':['file/path...','file/path...','file/path...']},\n         {'hash3':['file/path...']}]\n然后通过一个循环找出hash值对应的数组长度大于1的数组，现在我们把这个扫描结果保存到数据库中，之后只要查询数据库即可找到重复的文件。\n步骤\n我们大致需要几个步骤就可以让程序跑起来：\ngit clone https://github.com/tobyqin/photodup.git # 克隆代码\n\ncd photodup\npip install -r requirements.txt # 安装必要的依赖包\n\npython db.py # 创建DB表结构\n表结构不需要太复杂：\n\n\nid\nhash\nname\npath\nExisted\n\n\n\n1\nab3d\nDCS_001.JPG\npath/to/DSC_001.JPG\n1\n\n\n2\n1d2c\nDCS_002.JPG\npath/to/DSC_002.JPG\n2\n\n\n\n然后开始扫描你要检查的目录。\npython scan.py dir1 dir2\n你可以传入一个或者多个目录，默认只检索jpg文件，也可以修改config.py里的配置项来自定义。扫描结束后，启动web服务即可。\npython web.py\n顺利的话用浏览器打开 http://127.0.0.1:5001 就可以看到一个友好的网页，可以通过文件hash或者文件名来清理重复文件，可以预览图片文件。\n\n\n原理&总结\n升级后的重复文件清理工具总共也不过两三百行代码，但是已经算是一个比较完整的程序，使用起来也方便了很多。升级过程中用到了前后端数据库各方面的知识，不管你的想法多简单，真正动手去实现才会有收获。\n\n项目地址：https://github.com/tobyqin/ph...\n\n技术栈：Python, SQL, Flask, Bootstrap, Jquery, CSS.\n\n\n关于作者：Toby Qin, Python 技术爱好者，目前从事测试开发相关工作，转载请注明原文出处。\n欢迎关注我的博客 https://betacat.online，你可以到我的公众号中去当吃瓜群众。\n\n\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "2"}