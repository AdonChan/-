{"title": "Softmax分类函数 - 个人文章 ", "index": "python", "content": "作者：chen_h微信号 & QQ：862251340微信公众号：coderpai简书地址：https://www.jianshu.com/p/8eb...\n\n这篇教程是翻译Peter Roelants写的神经网络教程，作者已经授权翻译，这是原文。\n该教程将介绍如何入门神经网络，一共包含五部分。你可以在以下链接找到完整内容。\n\n（一）神经网络入门之线性回归\nLogistic分类函数\n（二）神经网络入门之Logistic回归（分类问题）\n（三）神经网络入门之隐藏层设计\nSoftmax分类函数\n（四）神经网络入门之矢量化\n（五）神经网络入门之构建多层网络\n\nsoftmax分类函数\n\n这部分教程将介绍两部分：\n\nsoftmax函数\n交叉熵损失函数\n\n在先前的教程中，我们已经使用学习了如何使用Logistic函数来实现二分类问题。对于多分类问题，我们可以使用多项Logistic回归，该方法也被称之为softmax函数。接下来，我们来解释什么事softmax函数，以及怎么得到它。\n我们先导入教程需要使用的软件包。\nimport numpy as np \nimport matplotlib.pyplot as plt  \nfrom matplotlib.colors import colorConverter, ListedColormap \nfrom mpl_toolkits.mplot3d import Axes3D  \nfrom matplotlib import cm \nSoftmax函数\n在之前的教程中，我们已经知道了Logistic函数只能被使用在二分类问题中，但是它的多项式回归，即softmax函数，可以解决多分类问题。假设softmax函数ς的输入数据是C维度的向量z，那么softmax函数的数据也是一个C维度的向量y，里面的值是0到1之间。softmax函数其实就是一个归一化的指数函数，定义如下：\n\n式子中的分母充当了正则项的作用，可以使得\n\n作为神经网络的输出层，softmax函数中的值可以用C个神经元来表示。\n对于给定的输入z，我们可以得到每个分类的概率t = c for c = 1 ... C可以表示为：\n\n其中，P(t=c|z)表示，在给定输入z时，该输入数据是c分类的概率。\n下图展示了在一个二分类(t = 1, t = 2)中，输入向量是z = [z1, z2]，那么输出概率P(t=1|z)如下图所示。\n# Define the softmax function\ndef softmax(z):\n    return np.exp(z) / np.sum(np.exp(z))\n# Plot the softmax output for 2 dimensions for both classes\n# Plot the output in function of the weights\n# Define a vector of weights for which we want to plot the ooutput\nnb_of_zs = 200\nzs = np.linspace(-10, 10, num=nb_of_zs) # input \nzs_1, zs_2 = np.meshgrid(zs, zs) # generate grid\ny = np.zeros((nb_of_zs, nb_of_zs, 2)) # initialize output\n# Fill the output matrix for each combination of input z's\nfor i in range(nb_of_zs):\n    for j in range(nb_of_zs):\n        y[i,j,:] = softmax(np.asarray([zs_1[i,j], zs_2[i,j]]))\n# Plot the cost function surfaces for both classes\nfig = plt.figure()\n# Plot the cost function surface for t=1\nax = fig.gca(projection='3d')\nsurf = ax.plot_surface(zs_1, zs_2, y[:,:,0], linewidth=0, cmap=cm.coolwarm)\nax.view_init(elev=30, azim=70)\ncbar = fig.colorbar(surf)\nax.set_xlabel('$z_1$', fontsize=15)\nax.set_ylabel('$z_2$', fontsize=15)\nax.set_zlabel('$y_1$', fontsize=15)\nax.set_title ('$P(t=1|\\mathbf{z})$')\ncbar.ax.set_ylabel('$P(t=1|\\mathbf{z})$', fontsize=15)\nplt.grid()\nplt.show()\n\nsoftmax函数的导数\n在神经网络中，使用softmax函数，我们需要知道softmax函数的导数。如果我们定义：\n\n那么可以得到：\n\n因此，softmax函数的输出结果y对于它的输入数据z的导数∂yi/∂zj可以定义为：\n\n注意，当i = j时，softmax函数的倒数推导结果和Logistic函数一样。\nsoftmax函数的交叉熵损失函数\n在学习softmax函数的损失函数之前，我们先从学习它的最大似然函数开始。给定模型的参数组θ，利用这个参数组，我们可以得到输入样本的正确预测，正如在Logistic损失函数推导中，我们可以仿照写出这个的最大似然估计：\n\n根据联合概率，我们可以将似然函数改写成：P(t,z|θ)，根据条件分布，我们最终可以得到如下公式：\n\n因为我们不关心z的概率，所以公式又可以改写为：L(θ|t,z)=P(t|z,θ)。而且，P(t|z, θ)可以被写成P(t|z)，如果θ会一个定值。因为，每一个ti都是依赖于整个z，而且只有其中一个t将会被激活，所以我们可以得到下式：\n\n正如我们在Logistic函数中推导损失函数的导数一样，最大化似然函数就是最小化它的负对数释然函数：\n\n其中，ξ表示交叉熵误差函数。在二分类问题中，我们将t2定义为t2=1−t1。同理，在softmax函数中，我们也可以定义为：\n\n在n个样本的批处理中，交叉熵误差函数可以这样计算：\n\n其中，当且仅当tic是1，那么样本i是属于类别c，yic是样本i属于类别c的概率。\nsoftmax函数的交叉熵损失函数的推导\n损失函数对于zi的导数∂ξ/∂zi求解如下：\n\n上式已经求解了当i=j和i≠j的两种情况。\n最终的结果为∂ξ/∂zi=yi−ti for all i ∈ C，这个求导结果和Logistic函数的交叉熵损失函数求导是一样的，再次证明softmax函数是Logistic函数的一个扩展板。\n完整代码，点击这里\n\n作者：chen_h微信号 & QQ：862251340简书地址：https://www.jianshu.com/p/8eb...\nCoderPai 是一个专注于算法实战的平台，从基础的算法到人工智能算法都有设计。如果你对算法实战感兴趣，请快快关注我们吧。加入AI实战微信群，AI实战QQ群，ACM算法微信群，ACM算法QQ群。长按或者扫描如下二维码，关注 “CoderPai” 微信号（coderpai）\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}