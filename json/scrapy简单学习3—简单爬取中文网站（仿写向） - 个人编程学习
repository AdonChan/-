{"title": "scrapy简单学习3—简单爬取中文网站（仿写向） - 个人编程学习 ", "index": "python,scrapy", "content": "仿写原创——单页面爬取爬取网站：联合早报网左侧的标题，连接，内容1.item.py定义爬取内容\nimport scrapy\n\n\nclass MaiziItem(scrapy.Item):\n    title = scrapy.Field()\n    link=scrapy.Field()\n    desc =scrapy.Field()\n2.spider文件编写\n# -*- coding: utf-8 -*-\n#encoding=utf-8\nimport scrapy\nfrom LianHeZaoBao.items import LianhezaobaoItem\nreload(__import__('sys')).setdefaultencoding('utf-8') \n\nclass MaimaiSpider(scrapy.Spider):\n    name = \"lianhe\"\n    allowed_domains = [\"http://www.zaobao.com/news/china//\"]\n    start_urls = (\n        'http://www.zaobao.com/news/china//',\n    )\n\n    def parse(self, response):\n        \n        for li in response.xpath('//*[@id=\"l_title\"]/ul/li'):\n            item = LianhezaobaoItem()\n            item['title'] = li.xpath('a[1]/p/text()').extract()\n            item['link']=li.xpath('a[1]/@href').extract()\n            item['desc'] = li.xpath('a[2]/p/text()').extract()\n            \n            yield item\n3.保存文件:命令scrapy crawl lianhe -o lianhe.csv备注：excel打开出现乱码，用记事本转换成ANSI编码，excel打开中文可正常。4.完成样式：\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "5"}