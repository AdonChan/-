{"title": "利用 TensorFlow 实现卷积自编码器 - 个人文章 ", "index": "python", "content": "作者：chen_h微信号 & QQ：862251340微信公众号：coderpai简书地址：https://www.jianshu.com/p/250...\n\n介绍和概念\n自动编码器（Auto-encoders）是神经网络的一种形式，它的输入数据与输出数据是相同的。他们通过将输入数据压缩到一个潜在表示空间里面，然后再根据这个表示空间将数据进行重构得到最后的输出数据。\n\n自编码器的一个非常受欢迎的使用场景是图像处理。其中使用到的小技巧是用卷积层来替换全连接层。这个转变方法是将一个非常宽的，非常瘦的（比如 100*100 的像素点，3 通道，RGB）图像转换成一个非常窄的，非常厚的图像。这种方法非常有助于帮助我们从图像中提取出视觉特征，从而得到更准确的潜在表示空间。最后我们的图像重构过程采用上采样和卷积。\n这个自编码器就称之为卷积自编码器（Convolutional Autoencoder，CAE）\n使用卷积自编码器\n卷积自编码器可以用于图像的重构工作。例如，他们可以学习从图片中去除噪声，或者重构图片缺失的部分。\n为了实现上述提到的效果，我们一般不使用相同的输入数据和输出数据，取而代之的是，使用含有噪声的图片作为输入数据，然后输出数据是一个干净的图片。卷积自编码器就会通过学习，去去除图片中的噪声，或者去填补图片中的空缺部分。\n接下来，让我们来看一下 CAE 是如何来填充图中眼睛上的十字架。我们假设图片的眼睛上面存在一个十字架黑影，我们需要删除这个十字架噪声。首先，我们需要来手动创建这个数据库，当然，这个动作非常方便。\n\n现在我们的卷积自编码器就可以开始训练了，我们可以用它去除我们从未见过的眼睛照片上面的十字线！\n利用 TensorFlow 来实现这个卷积自编码器\n看我们利用 MNIST 数据集来看看这个网络是如何实现的，完整的代码可以在 Github 上面下载。\n网络架构\n卷积自编码器的编码部分将是一个典型的卷积过程。每一个卷积层之后都会加上一个池化层，主要是为了减少数据的维度。解码器需要从一个非常窄的数据空间中重构出一个宽的图像。\n一般情况下，你会看到我们后面是采用反卷积层来增加我们图像的宽度和高度。它们的工作原理和卷积层的工作原理几乎完全一样，但是作用方向相反。比如，你有一个 33 的卷积核，那么在编码器中我们是将该区域的图像编码成一个元素点，但是在解码器中，也就是反卷积中，我们是把一个元素点解码成 33 个元素点。TensorFlow API 为我们提供了这个功能，参考 tf.nn.conv2d_transpose\n自动编码器只需要在噪声的图像上进行训练，就可以非常成功的进行图片去燥。比如，我们可以在训练图片中添加入高斯噪声来创建包含噪声的图像，然后将这些像素值裁剪在 0 到 1 之间。我们将噪声图像作为输入数据，最原始的感觉图像作为输出数据，也就是我们的目标值。\n模型定义\nlearning_rate = 0.001\ninputs_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='inputs')\ntargets_ = tf.placeholder(tf.float32, (None, 28, 28, 1), name='targets')\n### Encoder\nconv1 = tf.layers.conv2d(inputs=inputs_, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu)\n# Now 28x28x32\nmaxpool1 = tf.layers.max_pooling2d(conv1, pool_size=(2,2), strides=(2,2), padding='same')\n# Now 14x14x32\nconv2 = tf.layers.conv2d(inputs=maxpool1, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu)\n# Now 14x14x32\nmaxpool2 = tf.layers.max_pooling2d(conv2, pool_size=(2,2), strides=(2,2), padding='same')\n# Now 7x7x32\nconv3 = tf.layers.conv2d(inputs=maxpool2, filters=16, kernel_size=(3,3), padding='same', activation=tf.nn.relu)\n# Now 7x7x16\nencoded = tf.layers.max_pooling2d(conv3, pool_size=(2,2), strides=(2,2), padding='same')\n# Now 4x4x16\n### Decoder\nupsample1 = tf.image.resize_images(encoded, size=(7,7), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n# Now 7x7x16\nconv4 = tf.layers.conv2d(inputs=upsample1, filters=16, kernel_size=(3,3), padding='same', activation=tf.nn.relu)\n# Now 7x7x16\nupsample2 = tf.image.resize_images(conv4, size=(14,14), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n# Now 14x14x16\nconv5 = tf.layers.conv2d(inputs=upsample2, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu)\n# Now 14x14x32\nupsample3 = tf.image.resize_images(conv5, size=(28,28), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n# Now 28x28x32\nconv6 = tf.layers.conv2d(inputs=upsample3, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu)\n# Now 28x28x32\nlogits = tf.layers.conv2d(inputs=conv6, filters=1, kernel_size=(3,3), padding='same', activation=None)\n#Now 28x28x1\n# Pass logits through sigmoid to get reconstructed image\ndecoded = tf.nn.sigmoid(logits)\n# Pass logits through sigmoid and calculate the cross-entropy loss\nloss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits)\n# Get cost and define the optimizer\ncost = tf.reduce_mean(loss)\nopt = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n训练过程：\nsess = tf.Session()\nepochs = 100\nbatch_size = 200\n# Set's how much noise we're adding to the MNIST images\nnoise_factor = 0.5\nsess.run(tf.global_variables_initializer())\nfor e in range(epochs):\n    for ii in range(mnist.train.num_examples//batch_size):\n        batch = mnist.train.next_batch(batch_size)\n        # Get images from the batch\n        imgs = batch[0].reshape((-1, 28, 28, 1))\n        \n        # Add random noise to the input images\n        noisy_imgs = imgs + noise_factor * np.random.randn(*imgs.shape)\n        # Clip the images to be between 0 and 1\n        noisy_imgs = np.clip(noisy_imgs, 0., 1.)\n        \n        # Noisy images as inputs, original images as targets\n        batch_cost, _ = sess.run([cost, opt], feed_dict={inputs_: noisy_imgs,\n                                                         targets_: imgs})\nprint(\"Epoch: {}/{}...\".format(e+1, epochs),\n              \"Training loss: {:.4f}\".format(batch_cost))\n\n作者：chen_h微信号 & QQ：862251340简书地址：https://www.jianshu.com/p/250...\nCoderPai 是一个专注于算法实战的平台，从基础的算法到人工智能算法都有设计。如果你对算法实战感兴趣，请快快关注我们吧。加入AI实战微信群，AI实战QQ群，ACM算法微信群，ACM算法QQ群。长按或者扫描如下二维码，关注 “CoderPai” 微信号（coderpai）\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}