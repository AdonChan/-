{"title": "Scrapy爬虫 - 获取知乎用户数据 - 连城博客 ", "index": "网页爬虫,scrapy,python", "content": "2016-04-10\nScrapy爬虫 - 获取知乎用户数据\n安装Scrapy爬虫框架\n关于如何安装Python以及Scrapy框架，这里不做介绍，请自行网上搜索。\n初始化\n安装好Scrapy后，执行 scrapy startproject myspider接下来你会看到 myspider 文件夹，目录结构如下：\n\nscrapy.cfg\n\nmyspider\n\nitems.py\npipelines.py\nsettings.py\n__init__.py\n\nspiders\n__init__.py\n\n\n\n\n编写爬虫文件\n在spiders目录下新建 users.py\n# -*- coding: utf-8 -*-\nimport scrapy\nimport os\nimport time\nfrom zhihu.items import UserItem\nfrom zhihu.myconfig import UsersConfig # 爬虫配置\n\nclass UsersSpider(scrapy.Spider):\n    name = 'users'\n    domain = 'https://www.zhihu.com'\n    login_url = 'https://www.zhihu.com/login/email'\n    headers = {\n        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n        \"Accept-Language\": \"zh-CN,zh;q=0.8\",\n        \"Connection\": \"keep-alive\",\n        \"Host\": \"www.zhihu.com\",\n        \"Upgrade-Insecure-Requests\": \"1\",\n        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.109 Safari/537.36\"\n    }\n\n    def __init__(self, url = None):\n        self.user_url = url\n\n    def start_requests(self):\n        yield scrapy.Request(\n            url = self.domain,\n            headers = self.headers,\n            meta = {\n                'proxy': UsersConfig['proxy'],\n                'cookiejar': 1\n            },\n            callback = self.request_captcha\n        )\n\n    def request_captcha(self, response):\n        # 获取_xsrf值\n        _xsrf = response.css('input[name=\"_xsrf\"]::attr(value)').extract()[0]\n        # 获取验证码地址\n        captcha_url = 'http://www.zhihu.com/captcha.gif?r=' + str(time.time() * 1000)\n        # 准备下载验证码\n        yield scrapy.Request(\n            url = captcha_url,\n            headers = self.headers,\n            meta = {\n                'proxy': UsersConfig['proxy'],\n                'cookiejar': response.meta['cookiejar'],\n                '_xsrf': _xsrf\n            },\n            callback = self.download_captcha\n        )\n\n    def download_captcha(self, response):\n        # 下载验证码\n        with open('captcha.gif', 'wb') as fp:\n            fp.write(response.body)\n        # 用软件打开验证码图片\n        os.system('start captcha.gif')\n        # 输入验证码\n        print 'Please enter captcha: '\n        captcha = raw_input()\n\n        yield scrapy.FormRequest(\n            url = self.login_url,\n            headers = self.headers,\n            formdata = {\n                'email': UsersConfig['email'],\n                'password': UsersConfig['password'],\n                '_xsrf': response.meta['_xsrf'],\n                'remember_me': 'true',\n                'captcha': captcha\n            },\n            meta = {\n                'proxy': UsersConfig['proxy'],\n                'cookiejar': response.meta['cookiejar']\n            },\n            callback = self.request_zhihu\n        )\n\n    def request_zhihu(self, response):\n        yield scrapy.Request(\n            url = self.user_url + '/about',\n            headers = self.headers,\n            meta = {\n                'proxy': UsersConfig['proxy'],\n                'cookiejar': response.meta['cookiejar'],\n                'from': {\n                    'sign': 'else',\n                    'data': {}\n                }\n            },\n            callback = self.user_item,\n            dont_filter = True\n        )\n\n        yield scrapy.Request(\n            url = self.user_url + '/followees',\n            headers = self.headers,\n            meta = {\n                'proxy': UsersConfig['proxy'],\n                'cookiejar': response.meta['cookiejar'],\n                'from': {\n                    'sign': 'else',\n                    'data': {}\n                }\n            },\n            callback = self.user_start,\n            dont_filter = True\n        )\n\n        yield scrapy.Request(\n            url = self.user_url + '/followers',\n            headers = self.headers,\n            meta = {\n                'proxy': UsersConfig['proxy'],\n                'cookiejar': response.meta['cookiejar'],\n                'from': {\n                    'sign': 'else',\n                    'data': {}\n                }\n            },\n            callback = self.user_start,\n            dont_filter = True\n        )\n\n    def user_start(self, response):\n        sel_root = response.xpath('//h2[@class=\"zm-list-content-title\"]')\n        # 判断关注列表是否为空\n        if len(sel_root):\n            for sel in sel_root:\n                people_url = sel.xpath('a/@href').extract()[0]\n\n                yield scrapy.Request(\n                    url = people_url + '/about',\n                    headers = self.headers,\n                    meta = {\n                        'proxy': UsersConfig['proxy'],\n                        'cookiejar': response.meta['cookiejar'],\n                        'from': {\n                            'sign': 'else',\n                            'data': {}\n                        }\n                    },\n                    callback = self.user_item,\n                    dont_filter = True\n                )\n\n                yield scrapy.Request(\n                    url = people_url + '/followees',\n                    headers = self.headers,\n                    meta = {\n                        'proxy': UsersConfig['proxy'],\n                        'cookiejar': response.meta['cookiejar'],\n                        'from': {\n                            'sign': 'else',\n                            'data': {}\n                        }\n                    },\n                    callback = self.user_start,\n                    dont_filter = True\n                )\n\n                yield scrapy.Request(\n                    url = people_url + '/followers',\n                    headers = self.headers,\n                    meta = {\n                        'proxy': UsersConfig['proxy'],\n                        'cookiejar': response.meta['cookiejar'],\n                        'from': {\n                            'sign': 'else',\n                            'data': {}\n                        }\n                    },\n                    callback = self.user_start,\n                    dont_filter = True\n                )\n\n    def user_item(self, response):\n        def value(list):\n            return list[0] if len(list) else ''\n\n        sel = response.xpath('//div[@class=\"zm-profile-header ProfileCard\"]')\n\n        item = UserItem()\n        item['url'] = response.url[:-6]\n        item['name'] = sel.xpath('//a[@class=\"name\"]/text()').extract()[0].encode('utf-8')\n        item['bio'] = value(sel.xpath('//span[@class=\"bio\"]/@title').extract()).encode('utf-8')\n        item['location'] = value(sel.xpath('//span[contains(@class, \"location\")]/@title').extract()).encode('utf-8')\n        item['business'] = value(sel.xpath('//span[contains(@class, \"business\")]/@title').extract()).encode('utf-8')\n        item['gender'] = 0 if sel.xpath('//i[contains(@class, \"icon-profile-female\")]') else 1\n        item['avatar'] = value(sel.xpath('//img[@class=\"Avatar Avatar--l\"]/@src').extract())\n        item['education'] = value(sel.xpath('//span[contains(@class, \"education\")]/@title').extract()).encode('utf-8')\n        item['major'] = value(sel.xpath('//span[contains(@class, \"education-extra\")]/@title').extract()).encode('utf-8')\n        item['employment'] = value(sel.xpath('//span[contains(@class, \"employment\")]/@title').extract()).encode('utf-8')\n        item['position'] = value(sel.xpath('//span[contains(@class, \"position\")]/@title').extract()).encode('utf-8')\n        item['content'] = value(sel.xpath('//span[@class=\"content\"]/text()').extract()).strip().encode('utf-8')\n        item['ask'] = int(sel.xpath('//div[contains(@class, \"profile-navbar\")]/a[2]/span[@class=\"num\"]/text()').extract()[0])\n        item['answer'] = int(sel.xpath('//div[contains(@class, \"profile-navbar\")]/a[3]/span[@class=\"num\"]/text()').extract()[0])\n        item['agree'] = int(sel.xpath('//span[@class=\"zm-profile-header-user-agree\"]/strong/text()').extract()[0])\n        item['thanks'] = int(sel.xpath('//span[@class=\"zm-profile-header-user-thanks\"]/strong/text()').extract()[0])\n\n        yield item\n添加爬虫配置文件\n在myspider目录下新建myconfig.py，并添加以下内容，将你的配置信息填入相应位置\n# -*- coding: utf-8 -*-\nUsersConfig = {\n    # 代理\n    'proxy': '',\n\n    # 知乎用户名和密码\n    'email': 'your email',\n    'password': 'your password',\n}\n\nDbConfig = {\n    # db config\n    'user': 'db user',\n    'passwd': 'db password',\n    'db': 'db name',\n    'host': 'db host',\n}\n修改items.py\n# -*- coding: utf-8 -*-\nimport scrapy\n\nclass UserItem(scrapy.Item):\n    # define the fields for your item here like:\n    url = scrapy.Field()\n    name = scrapy.Field()\n    bio = scrapy.Field()\n    location = scrapy.Field()\n    business = scrapy.Field()\n    gender = scrapy.Field()\n    avatar = scrapy.Field()\n    education = scrapy.Field()\n    major = scrapy.Field()\n    employment = scrapy.Field()\n    position = scrapy.Field()\n    content = scrapy.Field()\n    ask = scrapy.Field()\n    answer = scrapy.Field()\n    agree = scrapy.Field()\n    thanks = scrapy.Field()\n将用户数据存入mysql数据库\n修改pipelines.py\n# -*- coding: utf-8 -*-\nimport MySQLdb\nimport datetime\nfrom zhihu.myconfig import DbConfig\n\nclass UserPipeline(object):\n    def __init__(self):\n        self.conn = MySQLdb.connect(user = DbConfig['user'], passwd = DbConfig['passwd'], db = DbConfig['db'], host = DbConfig['host'], charset = 'utf8', use_unicode = True)\n        self.cursor = self.conn.cursor()\n        # 清空表\n        # self.cursor.execute('truncate table weather;')\n        # self.conn.commit()\n\n    def process_item(self, item, spider):\n        curTime = datetime.datetime.now()\n        try:\n            self.cursor.execute(\n                \"\"\"INSERT IGNORE INTO users (url, name, bio, location, business, gender, avatar, education, major, employment, position, content, ask, answer, agree, thanks, create_at)\n                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\"\",\n                (\n                    item['url'],\n                    item['name'],\n                    item['bio'],\n                    item['location'],\n                    item['business'],\n                    item['gender'],\n                    item['avatar'],\n                    item['education'],\n                    item['major'],\n                    item['employment'],\n                    item['position'],\n                    item['content'],\n                    item['ask'],\n                    item['answer'],\n                    item['agree'],\n                    item['thanks'],\n                    curTime\n                )\n            )\n            self.conn.commit()\n        except MySQLdb.Error, e:\n            print 'Error %d %s' % (e.args[0], e.args[1])\n\n        return item\n修改settings.py\n找到 ITEM_PIPELINES，改为：\nITEM_PIPELINES = {\n   'myspider.pipelines.UserPipeline': 300,\n}\n在末尾添加，设置爬虫的深度\nDEPTH_LIMIT=10\n爬取知乎用户数据\n确保MySQL已经打开，在项目根目录下打开终端，执行 scrapy crawl users -a url=https://www.zhihu.com/people/<user>，其中user为爬虫的第一个用户，之后会根据该用户关注的人和被关注的人进行爬取数据接下来会下载验证码图片，若未自动打开，请到根目录下打开 captcha.gif，在终端输入验证码数据爬取Loading...\n源码\n源码可以在这里找到 github\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "21"}