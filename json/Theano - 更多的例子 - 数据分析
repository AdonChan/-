{"title": "Theano - 更多的例子 - 数据分析 ", "index": "python3.x,python", "content": "Logistic函数\nimport theano\nimport theano.tensor as T\nx = T.dmatrix('x')\ns = 1 / (1 + T.exp(-x))\nlogistic = theano.function([x], s)\nlogistic([[0, 1], [-1, -2]])\n# s(x) = 1/(1+exp(-x)) = (1+tanh(x/2))/2\ns2 = (1 + T.tanh(x / 2)) / 2\nlogistic2 = theano.function([x], s2)\nlogistic2([[0, 1], [-1, -2]])\n同时执行多种计算任务\nTheano支持多种输出的函数。例如，我们可以同时计算两个矩阵a,b相应元素之间的差、绝对差、平方差。当我们调用函数f是，返回三个变量：\nimport theano\nimport theano.tensor as T\na, b = T.dmatrices('a', 'b')\ndiff = a - b\nabs_diff = abs(diff)\ndiff_squared = diff ** 2\nf = theano.function([a, b], [diff, abs_diff, diff_squared])\nf([[1, 1], [1, 1]], [[0, 1], [2, 3]])\n为参数设置默认值\n假设我们要定义一个实现两个数字加法的函数。如果你仅仅提供一个数字，另一个数字假设(默认)为1,就可以这么做：\nfrom theano import In, function\nimport theano.tensor as T\nx, y = T.dscalars('x', 'y')\nz = x + y\nf = function([x, In(y, value=1)], z)\nf(33)\nf(33, 2)\n含有默认值的输入必须位于不含默认值的输入之后（和python的函数类似）。允许多个输入含有默认值，这些参数可以通过位置设定，也可以通过名字进行设定。\nx, y, w = T.dscalars('x', 'y', 'w')\nz = (x + y) * w\nf = function([x, In(y, value=1), In(w, value=2, name='w_by_name')], z)\nf(33)\nf(33, 2)\nf(33, 0, 1)\nf(33, w_by_name=1)\nf(33, w_by_name=1, y=0)\nIn 不知道通过参数传递的局部变量x,y的名字。符号变量对象拥有名字（name）属性（在上本例中通过dscalars进行设置），这也是我们构建函数function关键字参数的名字。通过In(y, value=1)这一机制实现。在In(w, value=2, name='w_by_name')中，我们重写了符号变量的名字属性。所有当我们通过f(x=33, y=0, w=1)的形式调用函数时，就会出错。w应该改为w_by_name.\n使用共享变量\n我们也可以构建一个含有内状态（internal state）的函数。例如,假设我们要构造一个累加函数（accumulator）:初始状态设置为0。接着，每次调用函数，状态就会通过函数的参数自动增加。\n# 首先，我们定义一个累加函数。它将自己的内状态加上它的参数，然后返回旧状态的值。\nimport theano\nimport theano.tensor as T\nfrom theano import shared\nstate = shared(0)\ninc = T.iscalar('inc')\naccumulator = function([inc], state, updates=[(state, state+inc)])\n\n# state的值可以通过.get_value()和.set_value()惊行获取和修改\nstate.get_value()\naccumulator(1)\nstate.get_value()\naccumulator(300)\nstate.get_value()\n\nstate.set_value(-1)\naccumulator(3)\nstate.get_value()\n\n# 我们可以构造多个函数，使用相同共享变量，这些函数都可以更新状态的值\ndecrementor = function([inc], state, updates=[(state, state-inc)])\ndecrementor(2)\nstate.get_value()\n\n# 可能你会使用一个共享变量表达多个公式，但是你并不想使用共享变量的值。\n# 这种情况下，你可以使用function中的givens参数。\nfn_of_state = state * 2 + inc\nfoo = T.scalar(dtype=state.dtype)   # foo的类型必须和将要通过givens取代的共享变量的类型保持一致\nskip_shared = function([inc, foo], fn_of_state, givens=[(state, foo)])\nskip_shared(1, 3)   # 我们正在使用3作为state,并非state.value\nstate.get_value()   # 旧的状态(state)一直存在，但是我们使用它。\n复制函数（copying functions)\nTheano中的函数可以被复制，被用于构造相似的函数（拥有不同的共享变量和更新），这可以通过function中的copy()实现。让我们从以上定义的累加函数(accumulator)开始：\nimport theano\nimport theano.tensor as T\nstate = theano.shared(0)\ninc = T.iscalar('inc')\naccumulator = function([inc], state, updates=[(state, state+inc)])\n# 我们可以像平常一样增加它的状态（state）\naccumulator(10)\nstate.get_value()\n# 我们可以用copy()创建一个相似的累加器(accumulator)，但是可以通过swap参数拥有自己的内状态,\n# swap参数是将要交换的共享参数字典\nnew_state = theano.shared(0)\nnew_accumulator = accumulator.copy(swap={state:new_state})\nnew_accumulator(100)\nnew_state.get_value()\nstate.get_value()\n\n# 现在我们创建一个复制，但是使用delete_updates参数移除更新，此时，默认为False\n# 此时，共享状态将不会再更新。\nnull_accumulator = accumulator.copy(delete_updates=True)\nnull_accumulator(9000)\nstate.get_value()\n使用随机数（Using Random Numbers)\n简洁的例子\nfrom theano.tensor.shared_randomstreams import RandomStreams\nfrom theano import function\nsrng = RandomStreams(seed=324)\nrv_u = srng.uniform((2,2))\nrv_n = srng.normal((2,2))\nf = function([], rv_u)\ng = function([], rv_n, no_default_updates=True) # 不更新rv_n.rng\nnearly_zeros = function([], rv_u + rv_u - 2 * rv_u)\n\n# rv_u表示服从均匀分布的2*2随机数矩阵\n# rv_n表示服从正太分布的2*2随机数矩阵\n# 现在我们来调用这些对象。如果调用f()，我们将会得到随机均匀分布数。\n# 随机数产生器的内状态将会自动更新，所以我们每次调用f()时将会得到不同的随机数\nf_val0 = f()\nf_val1 = f()\n\n# 当我们添加额外的参数no_default_updates=True（在函数g中）后，随机数产生器的状态将不会受调用函数的影响。\n# 例如：多次调用g()将会返回相同的随机数,g_val0和g_val1相同。\ng_val0 = g()\ng_val1 = g()\n\n# 一个重要的观点是：一个随机变量在一次调用函数期中最多只能构建一次。\n# 所以nearly_zeros函数保证了输出近似为0，尽管rv_u随机变量在输出表达式中出现了3次。\nnearly_zeros()\n种子流（Seeding Streams）\n随机变量可以单独也可以共同产生，你可以通过对.rng属性进行seeding或者使用.rng.set_value()对.rng进行赋值产生一个随机变量。\nrng_val = rv_u.rng.get_value(borrow=True)   # 获取rv_u的rng(随机数生成器)\nrng_val.seed(89234)                         # 对generator(生成器）进行seeds（播种）\nrv_u.rng.set_value(rng_val, borrow=True)    # 对rng进行赋值\n\n# 你可以seed由RandomStreams对象分配的所有随机变量。\nsrng.seed(902340)\n函数之间共享流（Sharing Streams Between Functions）\n像共享变量一样，随机变量使用的随机数生成器在不同函数之间是相同的。所以我们的nearly_zeros函数将会更新f函数使用的生成器的状态。例如：\nstate_after_v0 = rv_u.rng.get_value().get_state()\nnearly_zeros()  # 这将会影响rv_u的生成器\nv1 = f()\nrng = rv_u.rng.get_value(borrow=True)\nrng.set_state(state_after_v0)\nrv_u.rng.set_value(rng, borrow=True)\nv2 = f()    # v2 != v1\nv3 = f()    # v3 == v1\n在Theano Graphs之间复制随机状态\n在很多应用场景中，使用者可能想把一个theano graph（图：g1,内置函数：f1）中的所有随机数生成器的状态传递给第二个theano graph（图：g2,内置函数：f2)。\n例如：如果你试图从之前储存模型的参数中，初始化一个模型的状态，将会出现上述需要。theano.tensor.shared_randomstreams.RandomStreams和theano.sandbox.rng_mrg.MRG_RandomStreams这些在state_updates参数的复制元素可以实现。\n每一次从RandomStreams对象中生成一个随机变量，将会有一个元组添加到state_update列表中。 第一个元素是共享变量：它表示和特定变量相关的随机数生成器的状态。第二个元素表示和随机数生成过程相对应的theano graph。\n下面的例子展示了：随机状态(random states)如何从一个theano function 传递给另一个theano function中的。\nimport theano\nimport numpy\nimport theano.tensor as T\nfrom theano.sandbox.rng_mrg import MRG_RandomStreams\nfrom theano.tensor.shared_randomstreams import RandomStreams\n\n\nclass Graph:\n    def __init__(self, seed=123):\n        self.rng = RandomStreams(seed)\n        self.y = self.rng.uniform(size=(1,))\n\n\ng1 = Graph(seed=123)\nf1 = theano.function([], g1.y)\n\ng2 = Graph(seed=987)\nf2 = theano.function([], g2.y)\n\n# 默认情况下，两个函数f1,f2不同步\nf1()\nf2()\n\n\ndef copy_random_state(g1, g2):\n    if isinstance(g1.rng, MRG_RandomStreams):\n        g2.rng.rstate = g1.rng.rstate\n    for (su1, su2) in zip(g1.rng.state_updates, g2.rng.state_updates):\n        su2[0].set_value(su1[0].get_value())\n\n\n# 现在我们赋值theano随机数生成器的状态\ncopy_random_state(g1, g2)\nf1()\nf2()\n一个真实的例子：逻辑回归\nimport numpy\nimport theano\nimport theano.tensor as T\nrng = numpy.random\n\nN = 400                         # training sample size\nfeats = 784                     # number of input variables\n\n# generate a data set: D = (input_values, target_class)\nD = (rng.rand(N, feats), rng.randint(size=N, low=0, high=2))\ntraining_steps = 10000\n\n# Declare Theano symbolic variables\nx = T.dmatrix('x')\ny = T.dvector('y')\n\n# initialize the weight vector w randomly\n#\n# this and the following bias variable b\n# are shared so they keep their values\n# between training iterations (updates)\nw = theano.shared(rng.randn(feats), name='w')\n\n# initialize the bias term\nb = theano.shared(0., name='b')\n\nprint('Initial model:')\nprint(w.get_value())\nprint(b.get_value())\n\n# Construct Theano expression graph\np_1 = 1 / (1 + T.exp(-T.dot(x, w) - b))         # Probability that target = 1\nprediction = p_1 > 0.5                          # The prediction thresholded\nxent = -y * T.log(p_1) - (1-y) * T.log(1-p_1)   # Cross-entropy loss function\ncost = xent.mean() + 0.01 * (w ** 2).sum()      # The cost to minimize\ngw, gb = T.grad(cost, [w, b])                   # Compute the gradient of the cost\n\n\n# Compile\ntrain = theano.function(\n    inputs=[x,y],\n    outputs=[prediction, xent],\n    updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb))\n)\n\npredict = theano.function(inputs=[x], outputs=prediction)\n\n# Train\nfor i in range(training_steps):\n    pred, err = train(D[0], D[1])\n\nprint('Final model:')\nprint(w.get_value())\nprint(b.get_value())\nprint('target values for D:')\nprint(D[1])\nprint('prediction on D:')\nprint(predict(D[0]))\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}