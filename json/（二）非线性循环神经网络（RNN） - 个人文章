{"title": "（二）非线性循环神经网络（RNN） - 个人文章 ", "index": "python", "content": "作者：chen_h微信号 & QQ：862251340微信公众号：coderpai简书地址：https://www.jianshu.com/p/9a1...\n\n这篇教程是翻译Peter Roelants写的循环神经网络教程，作者已经授权翻译，这是原文。\n该教程将介绍如何实现一个循环神经网络（RNN），一共包含两部分。你可以在以下链接找到完整内容。\n\n（一）线性循环神经网络（RNN）\n（二）非线性循环神经网络（RNN）\n\n非线性循环神经网络应用于二进制相加\n\n本教程主要包含两部分：\n\n利用张量存储数据\n利用弹性反向传播和动量方法进行优化\n\n在第一部分中，我们已经学习了一个简单的线性循环神经网络。在这一部分中，我们将利用非线性函数来设计一个非线性的循环神经网络，并且实现一个二进制相加的功能。\n我们先导入教程需要的软件包：\nimport itertools\nimport numpy as np \nimport matplotlib\nimport matplotlib.pyplot as plt\n定义数据集\n在这个教程中，我们训练的数据集是2000个数据，在程序中用 create_dataset 函数产生。每个训练样本都是有两部分 (Xi1, Xi2)组成，每一部分是一个7位的二进制表示，分别由6位的二进制和最右边一位的0组成（最右边的0是为了防止二进制相加溢出）。预测目标 ti 也是一个7位的二进制表示，即 ti = Xi1 + Xi2。我们之所以从左到右编码二进制，是因为我们的RNN网络是从左到右进行计算的。\n输入数据和预测结果都是被存储在三维张量里，比如下图表示了我们的训练数据 (X_train, T_train)的维度表示。第一维度表示一共有多少组数据（我们第一维度的值是 2000），第二维度表示的是每个时间步长上面的取值，一共7个时间步长，第三维度表示RNN输入单元神经元的个数（该教程设置的是2个神经元）。下图就是输入张量 X_train 的可视化展示：\n\n下面代码定义了输入数据集：\n# Create dataset\nnb_train = 2000  # Number of training samples\n# Addition of 2 n-bit numbers can result in a n+1 bit number\nsequence_len = 7  # Length of the binary sequence\n\ndef create_dataset(nb_samples, sequence_len):\n    \"\"\"Create a dataset for binary addition and return as input, targets.\"\"\"\n    max_int = 2**(sequence_len-1) # Maximum integer that can be added\n    format_str = '{:0' + str(sequence_len) + 'b}' # Transform integer in binary format\n    nb_inputs = 2  # Add 2 binary numbers\n    nb_outputs = 1  # Result is 1 binary number\n    X = np.zeros((nb_samples, sequence_len, nb_inputs))  # Input samples\n    T = np.zeros((nb_samples, sequence_len, nb_outputs))  # Target samples\n    # Fill up the input and target matrix\n    for i in xrange(nb_samples):\n        # Generate random numbers to add\n        nb1 = np.random.randint(0, max_int)\n        nb2 = np.random.randint(0, max_int)\n        # Fill current input and target row.\n        # Note that binary numbers are added from right to left, but our RNN reads \n        #  from left to right, so reverse the sequence.\n        X[i,:,0] = list(reversed([int(b) for b in format_str.format(nb1)]))\n        X[i,:,1] = list(reversed([int(b) for b in format_str.format(nb2)]))\n        T[i,:,0] = list(reversed([int(b) for b in format_str.format(nb1+nb2)]))\n    return X, T\n\n# Create training samples\nX_train, T_train = create_dataset(nb_train, sequence_len)\nprint('X_train shape: {0}'.format(X_train.shape))\nprint('T_train shape: {0}'.format(T_train.shape))\nX_train shape: (2000, 7, 2)T_train shape: (2000, 7, 1)\n\n二进制相加\n如果需要理解循环神经网络从输入数据流到输出数据流的一整个形式，那么二进制相加将是一个很好的例子。循环神经网络需要学习两件事：第一，怎么去将上一次的运算状态传递到下一次的运算中去；第二，根据输入数据和上一步的输入状态值（也就是记忆），去判断什么时候应该输出0，什么时候应该输出1。\n下面代码将二进制相加结果做了一个可视化：\n# Show an example input and target\ndef printSample(x1, x2, t, y=None):\n    \"\"\"Print a sample in a more visual way.\"\"\"\n    x1 = ''.join([str(int(d)) for d in x1])\n    x2 = ''.join([str(int(d)) for d in x2])\n    t = ''.join([str(int(d[0])) for d in t])\n    if not y is None:\n        y = ''.join([str(int(d[0])) for d in y])\n    print('x1:   {:s}   {:2d}'.format(x1, int(''.join(reversed(x1)), 2)))\n    print('x2: + {:s}   {:2d} '.format(x2, int(''.join(reversed(x2)), 2)))\n    print('      -------   --')\n    print('t:  = {:s}   {:2d}'.format(t, int(''.join(reversed(t)), 2)))\n    if not y is None:\n        print('y:  = {:s}'.format(t))\n    \n# Print the first sample\nprintSample(X_train[0,:,0], X_train[0,:,1], T_train[0,:,:])\nx1:    1010010   37x2: + 1101010   43   --------------------t:  = 0000101   80\n循环神经网络架构\n在这个教程中，对于每个时间点，我们设计的循环神经网络有2个输入神经元，之后将它们转换成状态值，最后输出一个单独的预测概率值。当前的时间点是1（而不是0）。由输入数据转换成的状态值，它的作用是记住一部分信息，以便于网络与预测下一步应该输出什么。\n网上有很多的方法，可以将我们设计的RNN进行可视化展示，我们还是利用在第一部分中的展示方法，将我们的RNN架构进行战术，如下图：\n\n或者，我们还能将完整的输入数据，完整的状态值和完整的预测结果进行可视化，输入数据张量可以被并行映射到状态值张量，状态值张量也可以被并行映射到每一个时间点的预测值张量。如下图：\n\n在代码中，每一个映射过程被抽象成了一个类。在每一个类中，都有一个 forward 函数用来计算BP算法中的前向传播，backward 函数来计算BP算法中的反向传播。\nRNN的计算过程\n线性转换\n在神经网络中，将输入数据映射到下一层的常用方法是矩阵相乘并且加上偏差项，最后利用一个非线性函数进行激活操作。在这篇教程中，二维的输入数据 (Xik1, Xik2) ，通过一个 2*3 的链接矩阵和长度是3的偏差向量映射到状态层，即下一层。在状态反馈之前，三维的状态值，将会通过一个 3*1 的链接矩阵和长度是1的偏差向量映射到输出层，即得到输出概率。\n因为我们想在一个计算步骤中，将训练样本中的每个样本在每个时间点都进行状态映射，所以我们将使用 Numpy 中的 tensordot 函数，去实现这个相乘的操作。这个函数需要输入2个张量和指定需要累加的轴。比如，输入数据为 shape(X) = (2000, 7, 2)，状态值为 shape(S) = (2000, 7, 3)，链接矩阵为 shape(W) = (2, 3)，那么我们可以得到公式 S = tensordot(X, W, axes = ((-1), (0)) 。这个公式会把 X 的最后一维 (-1) 和 W 的第零维度进行相乘累加，最后得到一个维度是 (2000, 7, 3) 的张量。\n这个线性转换可以用在输入数据 X 到状态层 S 的映射，也可以用在状态层 S 到输出层 Y 的映射。在代码的 TensorLinear 类中，实现了这个线性转换，还实现了它的梯度。根据Xavier Glorot的建议，权重初始值是一个均匀分布，数据范围是：\n\nLogitstic分类\nLogistic分类函数将被在输出层使用，用来得到输出的概率值，在 LogisticClassifier 函数中实现了它的损失函数和梯度。\n# Define the linear tensor transformation layer\nclass TensorLinear(object):\n    \"\"\"The linear tensor layer applies a linear tensor dot product and a bias to its input.\"\"\"\n    def __init__(self, n_in, n_out, tensor_order, W=None, b=None):\n        \"\"\"Initialse the weight W and bias b parameters.\"\"\"\n        a = np.sqrt(6.0 / (n_in + n_out))\n        self.W = (np.random.uniform(-a, a, (n_in, n_out)) if W is None else W)\n        self.b = (np.zeros((n_out)) if b is None else b)  # Bias paramters\n        self.bpAxes = tuple(range(tensor_order-1))  # Axes summed over in backprop\n\n    def forward(self, X):\n        \"\"\"Perform forward step transformation with the help of a tensor product.\"\"\"\n        # Same as: Y[i,j,:] = np.dot(X[i,j,:], self.W) + self.b (for i,j in X.shape[0:1])\n        # Same as: Y = np.einsum('ijk,kl->ijl', X, self.W) + self.b\n        return np.tensordot(X, self.W, axes=((-1),(0))) + self.b\n\n    def backward(self, X, gY):\n        \"\"\"Return the gradient of the parmeters and the inputs of this layer.\"\"\"\n        # Same as: gW = np.einsum('ijk,ijl->kl', X, gY)\n        # Same as: gW += np.dot(X[:,j,:].T, gY[:,j,:]) (for i,j in X.shape[0:1])\n        gW = np.tensordot(X, gY, axes=(self.bpAxes, self.bpAxes))\n        gB = np.sum(gY, axis=self.bpAxes)\n        # Same as: gX = np.einsum('ijk,kl->ijl', gY, self.W.T)\n        # Same as: gX[i,j,:] = np.dot(gY[i,j,:], self.W.T) (for i,j in gY.shape[0:1])\n        gX = np.tensordot(gY, self.W.T, axes=((-1),(0)))  \n        return gX, gW, gB\n# Define the logistic classifier layer\nclass LogisticClassifier(object):\n    \"\"\"The logistic layer applies the logistic function to its inputs.\"\"\"\n   \n    def forward(self, X):\n        \"\"\"Perform the forward step transformation.\"\"\"\n        return 1 / (1 + np.exp(-X))\n    \n    def backward(self, Y, T):\n        \"\"\"Return the gradient with respect to the cost function at the inputs of this layer.\"\"\"\n        # Normalise of the number of samples and sequence length.\n        return (Y - T) / (Y.shape[0] * Y.shape[1])\n    \n    def cost(self, Y, T):\n        \"\"\"Compute the cost at the output.\"\"\"\n        # Normalise of the number of samples and sequence length.\n        # Add a small number (1e-99) because Y can become 0 if the network learns\n        #  to perfectly predict the output. log(0) is undefined.\n        return - np.sum(np.multiply(T, np.log(Y+1e-99)) + np.multiply((1-T), np.log(1-Y+1e-99))) / (Y.shape[0] * Y.shape[1])\n展开循环神经网络的中间状态\n在上一部分教程中，我们知道随着时间步长，我们需要把循环状态进行展开处理。在代码中，RecurrentStateUnfold 类实现了这个展开的BPTT算法。这个类包含了前一状态层到当前状态层的权重，偏差单元，当然也实现了权重初始化和优化函数。\n在 RecurrentStateUnfold 类中，forward 函数实现了随着时间步长，状态函数的迭代更新。backward 函数实现了每个输出状态值的梯度。在每个时间点 k 上，输出层 Y 的梯度还需要加上上一状态的梯度之和。权重项和偏差项的梯度需要将所有时间点上面的权重项和偏差项的梯度都进行累加，因为在每一个时间点它们的值都是共享的。在时间点 k = 0，最后状态的梯度需要去优化初始的状态 S0，因为初始状体的梯度是 ∂ξ/∂S0 。\nRecurrentStateUnfold 类需要使用 RecurrentStateUpdate 类。这个类中的 forward 方法实现了将 k-1 的状态和 input 输入进行联合计算得到 k 时刻的状态值。backward 方法实现了BPTT算法。在 RecurrentStateUpdate 类中实现的非线性激活函数是hyperbolic tangent (tanh)函数，这个函数的取值范围是从 -1 到 +1。这个函数在 TanH 类中实现了。\n# Define tanh layer\nclass TanH(object):\n    \"\"\"TanH applies the tanh function to its inputs.\"\"\"\n    \n    def forward(self, X):\n        \"\"\"Perform the forward step transformation.\"\"\"\n        return np.tanh(X) \n    \n    def backward(self, Y, output_grad):\n        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n        gTanh = 1.0 - np.power(Y,2)\n        return np.multiply(gTanh, output_grad)\n# Define internal state update layer\nclass RecurrentStateUpdate(object):\n    \"\"\"Update a given state.\"\"\"\n    def __init__(self, nbStates, W, b):\n        \"\"\"Initialse the linear transformation and tanh transfer function.\"\"\"\n        self.linear = TensorLinear(nbStates, nbStates, 2, W, b)\n        self.tanh = TanH()\n\n    def forward(self, Xk, Sk):\n        \"\"\"Return state k+1 from input and state k.\"\"\"\n        return self.tanh.forward(Xk + self.linear.forward(Sk))\n    \n    def backward(self, Sk0, Sk1, output_grad):\n        \"\"\"Return the gradient of the parmeters and the inputs of this layer.\"\"\"\n        gZ = self.tanh.backward(Sk1, output_grad)\n        gSk0, gW, gB = self.linear.backward(Sk0, gZ)\n        return gZ, gSk0, gW, gB\n# Define layer that unfolds the states over time\nclass RecurrentStateUnfold(object):\n    \"\"\"Unfold the recurrent states.\"\"\"\n    def __init__(self, nbStates, nbTimesteps):\n        \" Initialse the shared parameters, the inital state and state update function.\"\n        a = np.sqrt(6.0 / (nbStates * 2))\n        self.W = np.random.uniform(-a, a, (nbStates, nbStates))\n        self.b = np.zeros((self.W.shape[0]))  # Shared bias\n        self.S0 = np.zeros(nbStates)  # Initial state\n        self.nbTimesteps = nbTimesteps  # Timesteps to unfold\n        self.stateUpdate = RecurrentStateUpdate(nbStates, self.W, self.b)  # State update function\n        \n    def forward(self, X):\n        \"\"\"Iteratively apply forward step to all states.\"\"\"\n        S = np.zeros((X.shape[0], X.shape[1]+1, self.W.shape[0]))  # State tensor\n        S[:,0,:] = self.S0  # Set initial state\n        for k in range(self.nbTimesteps):\n            # Update the states iteratively\n            S[:,k+1,:] = self.stateUpdate.forward(X[:,k,:], S[:,k,:])\n        return S\n    \n    def backward(self, X, S, gY):\n        \"\"\"Return the gradient of the parmeters and the inputs of this layer.\"\"\"\n        gSk = np.zeros_like(gY[:,self.nbTimesteps-1,:])  # Initialise gradient of state outputs\n        gZ = np.zeros_like(X)  # Initialse gradient tensor for state inputs\n        gWSum = np.zeros_like(self.W)  # Initialise weight gradients\n        gBSum = np.zeros_like(self.b)  # Initialse bias gradients\n        # Propagate the gradients iteratively\n        for k in range(self.nbTimesteps-1, -1, -1):\n            # Gradient at state output is gradient from previous state plus gradient from output\n            gSk += gY[:,k,:]\n            # Propgate the gradient back through one state\n            gZ[:,k,:], gSk, gW, gB = self.stateUpdate.backward(S[:,k,:], S[:,k+1,:], gSk)\n            gWSum += gW  # Update total weight gradient\n            gBSum += gB  # Update total bias gradient\n        gS0 = np.sum(gSk, axis=0)  # Get gradient of initial state over all samples\n        return gZ, gWSum, gBSum, gS0\n整个网络\n在 RnnBinaryAdder 类中，实现了整个二进制相加的网络过程。它在创建的时候，同时初始化了所有的网络参数。forward 方法实现了整个网络的前向传播过程，backward 方法实现了整个网络的梯度更新和反向传播过程。getParamGrads 方法计算了每一个参数的梯度，并且作为一个列表进行返回。get_params_iter 方法是将参数做一个索引排序，使得参数的梯度按照一定的顺序返回。\n# Define the full network\nclass RnnBinaryAdder(object):\n    \"\"\"RNN to perform binary addition of 2 numbers.\"\"\"\n    def __init__(self, nb_of_inputs, nb_of_outputs, nb_of_states, sequence_len):\n        \"\"\"Initialse the network layers.\"\"\"\n        self.tensorInput = TensorLinear(nb_of_inputs, nb_of_states, 3)  # Input layer\n        self.rnnUnfold = RecurrentStateUnfold(nb_of_states, sequence_len)  # Recurrent layer\n        self.tensorOutput = TensorLinear(nb_of_states, nb_of_outputs, 3)  # Linear output transform\n        self.classifier = LogisticClassifier()  # Classification output\n        \n    def forward(self, X):\n        \"\"\"Perform the forward propagation of input X through all layers.\"\"\"\n        recIn = self.tensorInput.forward(X)  # Linear input transformation\n        # Forward propagate through time and return states\n        S = self.rnnUnfold.forward(recIn)\n        Z = self.tensorOutput.forward(S[:,1:sequence_len+1,:])  # Linear output transformation\n        Y = self.classifier.forward(Z)  # Get classification probabilities\n        # Return: input to recurrent layer, states, input to classifier, output\n        return recIn, S, Z, Y\n    \n    def backward(self, X, Y, recIn, S, T):\n        \"\"\"Perform the backward propagation through all layers.\n        Input: input samples, network output, intput to recurrent layer, states, targets.\"\"\"\n        gZ = self.classifier.backward(Y, T)  # Get output gradient\n        gRecOut, gWout, gBout = self.tensorOutput.backward(S[:,1:sequence_len+1,:], gZ)\n        # Propagate gradient backwards through time\n        gRnnIn, gWrec, gBrec, gS0 = self.rnnUnfold.backward(recIn, S, gRecOut)\n        gX, gWin, gBin = self.tensorInput.backward(X, gRnnIn)\n        # Return the parameter gradients of: linear output weights, linear output bias,\n        #  recursive weights, recursive bias, linear input weights, linear input bias, initial state.\n        return gWout, gBout, gWrec, gBrec, gWin, gBin, gS0\n    \n    def getOutput(self, X):\n        \"\"\"Get the output probabilities of input X.\"\"\"\n        recIn, S, Z, Y = self.forward(X)\n        return Y  # Only return the output.\n    \n    def getBinaryOutput(self, X):\n        \"\"\"Get the binary output of input X.\"\"\"\n        return np.around(self.getOutput(X))\n    \n    def getParamGrads(self, X, T):\n        \"\"\"Return the gradients with respect to input X and target T as a list.\n        The list has the same order as the get_params_iter iterator.\"\"\"\n        recIn, S, Z, Y = self.forward(X)\n        gWout, gBout, gWrec, gBrec, gWin, gBin, gS0 = self.backward(X, Y, recIn, S, T)\n        return [g for g in itertools.chain(\n                np.nditer(gS0),\n                np.nditer(gWin),\n                np.nditer(gBin),\n                np.nditer(gWrec),\n                np.nditer(gBrec),\n                np.nditer(gWout),\n                np.nditer(gBout))]\n    \n    def cost(self, Y, T):\n        \"\"\"Return the cost of input X w.r.t. targets T.\"\"\"\n        return self.classifier.cost(Y, T)\n    \n    def get_params_iter(self):\n        \"\"\"Return an iterator over the parameters.\n        The iterator has the same order as get_params_grad.\n        The elements returned by the iterator are editable in-place.\"\"\"\n        return itertools.chain(\n            np.nditer(self.rnnUnfold.S0, op_flags=['readwrite']),\n            np.nditer(self.tensorInput.W, op_flags=['readwrite']),\n            np.nditer(self.tensorInput.b, op_flags=['readwrite']),\n            np.nditer(self.rnnUnfold.W, op_flags=['readwrite']),\n            np.nditer(self.rnnUnfold.b, op_flags=['readwrite']),\n            np.nditer(self.tensorOutput.W, op_flags=['readwrite']), \n            np.nditer(self.tensorOutput.b, op_flags=['readwrite']))\n梯度检查\n我们需要将网络求得的梯度和进行数值计算得到的梯度进行比较，从而判断梯度是否计算正确，我们在这篇博客中已经详细介绍了如何进行梯度检查，如果还有不明白，可以查看这篇博客。\n# Do gradient checking\n# Define an RNN to test\nRNN = RnnBinaryAdder(2, 1, 3, sequence_len)\n# Get the gradients of the parameters from a subset of the data\nbackprop_grads = RNN.getParamGrads(X_train[0:100,:,:], T_train[0:100,:,:])\n\neps = 1e-7  # Set the small change to compute the numerical gradient\n# Compute the numerical gradients of the parameters in all layers.\nfor p_idx, param in enumerate(RNN.get_params_iter()):\n    grad_backprop = backprop_grads[p_idx]\n    # + eps\n    param += eps\n    plus_cost = RNN.cost(RNN.getOutput(X_train[0:100,:,:]), T_train[0:100,:,:])\n    # - eps\n    param -= 2 * eps\n    min_cost = RNN.cost(RNN.getOutput(X_train[0:100,:,:]), T_train[0:100,:,:])\n    # reset param value\n    param += eps\n    # calculate numerical gradient\n    grad_num = (plus_cost - min_cost)/(2*eps)\n    # Raise error if the numerical grade is not close to the backprop gradient\n    if not np.isclose(grad_num, grad_backprop):\n        raise ValueError('Numerical gradient of {:.6f} is not close to the backpropagation gradient of {:.6f}!'.format(float(grad_num), float(grad_backprop)))\nprint('No gradient errors found')\nNo gradient errors found\n使用动量方法优化Rmsprop\n在上一部分中，我们使用弹性反向传播算法去优化我们的网络。在这个博客中，我们将使用动量方法来优化Rmsprop。我们将原来的 Rprop 算法替换为 Rmsprop 算法，是因为 Rprop 算法在处理小批量数据上的效果并不是很好，可能会发生梯度翻转的情况。\nRmsprop 算法是从 Rprop 算法中得到灵感的，它保留了对于每一个参数 θ 的平方梯度的平均移动，如下：\n\n其中，λ 是一个平均移动参数。\n这时候，梯度已经被归一化了，如下：\n\n之后，这个归一化的梯度被用于参数的更新。\n注意，这个梯度不是直接被使用在参数的更新上面，而是用在每个参数的速度参数（Vs）上面的更新。这个参数和这篇博客中的动量部分中的速度参数很像，但是在使用的方法上面又有一点差异。Nesterov 的加速梯度和一般的动量方法是不同的，主要体现在更新迭代方面。常规的动量算法在每一次迭代的开始就计算梯度，并且更新速度参数。但是 Nesterov 的加速梯度算法是根据较少速度来计算梯度的值，然后再更新速度，最后再根据局部梯度进行移动。这种处理方法有一个优点就是梯度在进行局部更新时将得到更多的信息，即使当前速度进行了一个错误的更新，该算法也能使梯度进行正确的计算。Nesterov 的更新可以如下计算：\n\n其中，∇(θ) 是一个在关于参数 θ 的局部梯度。比如，当前是第 i 次循环，那么式子可以被表示为如下图：\n\n注意，我们不能保证一定会收敛到全局最小值，即 cost = 0。因为如果你在参数更新的开始取的位置不是很好，那么最后的优化可能会取到局部最小值。而且训练过程对参数 lmbd，learning_rate，mementum_term，eps 都很敏感。你可以尝试一下以下代码，看看运行多久可以达到收敛。\n# Set hyper-parameters\nlmbd = 0.5  # Rmsprop lambda\nlearning_rate = 0.05  # Learning rate\nmomentum_term = 0.80  # Momentum term\neps = 1e-6  # Numerical stability term to prevent division by zero\nmb_size = 100  # Size of the minibatches (number of samples)\n\n# Create the network\nnb_of_states = 3  # Number of states in the recurrent layer\nRNN = RnnBinaryAdder(2, 1, nb_of_states, sequence_len)\n# Set the initial parameters\nnbParameters =  sum(1 for _ in RNN.get_params_iter())  # Number of parameters in the network\nmaSquare = [0.0 for _ in range(nbParameters)]  # Rmsprop moving average\nVs = [0.0 for _ in range(nbParameters)]  # Velocity\n\n# Create a list of minibatch costs to be plotted\nls_of_costs = [RNN.cost(RNN.getOutput(X_train[0:100,:,:]), T_train[0:100,:,:])]\n# Iterate over some iterations\nfor i in range(5):\n    # Iterate over all the minibatches\n    for mb in range(nb_train/mb_size):\n        X_mb = X_train[mb:mb+mb_size,:,:]  # Input minibatch\n        T_mb = T_train[mb:mb+mb_size,:,:]  # Target minibatch\n        V_tmp = [v * momentum_term for v in Vs]\n        # Update each parameters according to previous gradient\n        for pIdx, P in enumerate(RNN.get_params_iter()):\n            P += V_tmp[pIdx]\n        # Get gradients after following old velocity\n        backprop_grads = RNN.getParamGrads(X_mb, T_mb)  # Get the parameter gradients    \n        # Update each parameter seperately\n        for pIdx, P in enumerate(RNN.get_params_iter()):\n            # Update the Rmsprop moving averages\n            maSquare[pIdx] = lmbd * maSquare[pIdx] + (1-lmbd) * backprop_grads[pIdx]**2\n            # Calculate the Rmsprop normalised gradient\n            pGradNorm = learning_rate * backprop_grads[pIdx] / np.sqrt(maSquare[pIdx] + eps)\n            # Update the momentum velocity\n            Vs[pIdx] = V_tmp[pIdx] - pGradNorm     \n            P -= pGradNorm   # Update the parameter\n        ls_of_costs.append(RNN.cost(RNN.getOutput(X_mb), T_mb))  # Add cost to list to plot\n# Plot the cost over the iterations\nplt.plot(ls_of_costs, 'b-')\nplt.xlabel('minibatch iteration')\nplt.ylabel('$\\\\xi$', fontsize=15)\nplt.title('Decrease of cost over backprop iteration')\nplt.grid()\nplt.show()\n\n测试网络\n下面代码对我们上述设计的循环神经网络进行了二进制相加的测试，具体结果如下：\n# Create test samples\nnb_test = 5\nXtest, Ttest = create_dataset(nb_test, sequence_len)\n# Push test data through network\nY = RNN.getBinaryOutput(Xtest)\nYf = RNN.getOutput(Xtest)\n\n# Print out all test examples\nfor i in range(Xtest.shape[0]):\n    printSample(Xtest[i,:,0], Xtest[i,:,1], Ttest[i,:,:], Y[i,:,:])\n    print ''\nx1:   0100010   34x2: + 1100100   19\n  \\-------   --\nt:  = 1010110   53y:  = 1010110\nx1:   1010100   21x2: + 1110100   23\n  \\-------   --\nt:  = 0011010   44y:  = 0011010\nx1:   1111010   47x2: + 0000000    0\n  \\-------   --\nt:  = 1111010   47y:  = 1111010\nx1:   1000000    1x2: + 1111110   63\n  \\-------   --\nt:  = 0000001   64y:  = 0000001\nx1:   1010100   21x2: + 1010100   21\n  \\-------   --\nt:  = 0101010   42y:  = 0101010\n完整代码，点击这里\n\n作者：chen_h微信号 & QQ：862251340简书地址：https://www.jianshu.com/p/9a1...\nCoderPai 是一个专注于算法实战的平台，从基础的算法到人工智能算法都有设计。如果你对算法实战感兴趣，请快快关注我们吧。加入AI实战微信群，AI实战QQ群，ACM算法微信群，ACM算法QQ群。长按或者扫描如下二维码，关注 “CoderPai” 微信号（coderpai）\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}