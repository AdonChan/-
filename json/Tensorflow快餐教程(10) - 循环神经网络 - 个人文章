{"title": "Tensorflow快餐教程(10) - 循环神经网络 - 个人文章 ", "index": "神经网络,python", "content": "摘要： 循环神经网络：LSTM和GRU\n循环神经网络上节介绍了在图像和语音领域里大放异彩引发革命的CNN。但是，还有一类问题是CNN所不擅长的。这类问题的特点是上下文相关序列，比如理解文字。这时需要一种带有记忆的结构，于是，深度学习中的另一法宝RNN横空出世了。\n大家还记得第8节中我们讲的人工神经网络的第二次复兴吗？没错，第二次复兴的标志正是1984年加州理工学院的物理学家霍普菲尔德实现了他于两年前提出的一种循环神经网络模型。这种网络被称为Hopfield网络。当时因为硬件条件的限制，Hopfield网络并没有得到广泛应用。而两年扣BP网络被重新发明，全连接前馈神经网络成为主流。RNN正是在Hopfield网络的基础上发展起来的。\n\nRNN的图片都取自：https://colah.github.io/posts...\n从图中我们可以看到，一个典型的循环神经网络神经元的结构，是在输入Xt之外，A与自己也有一个连接。\n我们将其展开的话可能看得更清楚一些：\n\n也就是前一次输出的结果是对下一次输出有影响。\nLSTM\nRNN中增加了对于之前状态的记忆项，不能直接使用之前BP网络的梯度下降的方法。但是基于该方法将循环项的输入都考虑进来，这个改进方法叫做BPTT算法（Back-Propagation Through Time）。\n但是这种方法有个隐患，就是输入序列过长时会出现梯度消散问题（the vanishing gradient problem）。\n于是一个改进算法LSTM(Long short-term memory)就增加了一个遗忘的机制。\nLSTM的细节我们放到后面详细讲。我们先看看在Tensorflow中如何实现一个LSTM模型：\n  def RNN(x, weights, biases):\n        x = tf.unstack(x, timesteps, 1)\n    \n        lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n    \n        outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n    \n        return tf.matmul(outputs[-1], weights['out']) + biases['out']\n\n第一步准备数据，第二步创建一个LSTMCell，第三步连成一个RNN网络，第四步矩阵乘输出。\n下面我们还是以第1讲的例子来用LSTM来处理MNIST分类问题，第一时间有个可以运行的代码：\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n\n# 训练参数\nlearning_rate = 0.001\ntraining_steps = 10000\nbatch_size = 128\ndisplay_step = 200\n\n# 网络参数\nnum_input = 28 # MNIST data input (img shape: 28*28)\ntimesteps = 28 # timesteps\nnum_hidden = 128 # hidden layer num of features\nnum_classes = 10 # MNIST total classes (0-9 digits)\n\nX = tf.placeholder(\"float\", [None, timesteps, num_input])\nY = tf.placeholder(\"float\", [None, num_classes])\n\n# 初始权值\nweights = {\n    'out': tf.Variable(tf.random_normal([num_hidden, num_classes]))\n}\nbiases = {\n    'out': tf.Variable(tf.random_normal([num_classes]))\n}\n\ndef RNN(x, weights, biases):\n    x = tf.unstack(x, timesteps, 1)\n\n    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n\n    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n\n    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n\nlogits = RNN(X, weights, biases)\nprediction = tf.nn.softmax(logits)\n\n# 定义损失和优化函数\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=logits, labels=Y))\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\ntrain_op = optimizer.minimize(loss_op)\n\ncorrect_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n\n    sess.run(init)\n\n    for step in range(1, training_steps+1):\n        batch_x, batch_y = mnist.train.next_batch(batch_size)\n        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n        if step % display_step == 0 or step == 1:\n            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n                                                                 Y: batch_y})\n            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n                  \"{:.3f}\".format(acc))\n\n    print(\"Optimization Finished!\")\n\n    test_len = 128\n    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n    test_label = mnist.test.labels[:test_len]\n    print(\"Testing Accuracy:\", \\\n        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))\n\n门控循环单元GRU(Gated Recurrent Unit)\nLSTM所使用的技术属于门控RNN（Gated RNN）技术。除了LSTM之外，还有一种应用广泛的门控RNN叫做GRU(Gated Recurrent Unit).\n不同于1997年就发明的LSTM，GRU的技术比较新，提出在2014年。GRU与LSTM的不同在于，GRU同时可以控制『更新』门和『复位』门。在Tensorflow中，使用tf.contrib.rnn.GRUCell来表示GRU单元。\n到Tensorflow 1.8版本，一共支持5种单元，其中4种是LSTM单元，1种是GRU单元：\ntf.contrib.rnn.BasicRNNCell\ntf.contrib.rnn.BasicLSTMCell\ntf.contrib.rnn.GRUCell\ntf.contrib.rnn.LSTMCell\ntf.contrib.rnn.LayerNormBasicLSTMCell\n\n双向循环神经网络\n从前面的LSTM的结构我们可以看到，它是有方向的。GRU是在LSTM基础上的改良，也是如此。就像一个链表一样。\n那么，我们如果想同时支持两个方向该怎么办？这就是双向循环神经网络。\n我们还是先看核心代码：\ndef BiRNN(x, weights, biases):\n    x = tf.unstack(x, timesteps, 1)\n\n    lstm_fw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n    lstm_bw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n\n    try:\n        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n                                              dtype=tf.float32)\n    except Exception: \n        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n                                        dtype=tf.float32)\n\n    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n别的没什么变化，就是前向和后向各需要一个单元，然后调用static_bidirectional_rnn来运行网络。\n\n最后是双向RNN训练MNIST的完整代码：\n\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\nfrom tensorflow.contrib import rnn\nimport numpy as np\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n\nlearning_rate = 0.001\ntraining_steps = 10000\nbatch_size = 128\ndisplay_step = 200\n\nnum_input = 28 # MNIST data input (img shape: 28*28)\ntimesteps = 28 # timesteps\nnum_hidden = 128 # hidden layer num of features\nnum_classes = 10 # MNIST total classes (0-9 digits)\n\nX = tf.placeholder(\"float\", [None, timesteps, num_input])\nY = tf.placeholder(\"float\", [None, num_classes])\n\nweights = {\n     'out': tf.Variable(tf.random_normal([2*num_hidden, num_classes]))\n}\nbiases = {\n    'out': tf.Variable(tf.random_normal([num_classes]))\n}\n\ndef BiRNN(x, weights, biases):\n\n    x = tf.unstack(x, timesteps, 1)\n\n    lstm_fw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n    lstm_bw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n\n    try:\n        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n                                              dtype=tf.float32)\n    except Exception: # Old TensorFlow version only returns outputs not states\n        outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,\n                                        dtype=tf.float32)\n\n    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n\nlogits = BiRNN(X, weights, biases)\nprediction = tf.nn.softmax(logits)\n\nloss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits=logits, labels=Y))\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\ntrain_op = optimizer.minimize(loss_op)\n\ncorrect_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n\n    sess.run(init)\n\n    for step in range(1, training_steps+1):\n        batch_x, batch_y = mnist.train.next_batch(batch_size)\n        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n        if step % display_step == 0 or step == 1:\n            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n                                                                 Y: batch_y})\n            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n                  \"{:.3f}\".format(acc))\n\n    print(\"Optimization Finished!\")\n\n    test_len = 128\n    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n    test_label = mnist.test.labels[:test_len]\n    print(\"Testing Accuracy:\", \\\n        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))\n\n本文作者：lusing\n原文链接\n本文为云栖社区原创内容，未经允许不得转载。\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "1"}