{"title": "文章内容提取库 goose 简介 - Crossin的编程教室 ", "index": "python", "content": "爬虫抓取数据有两个头疼的点，写过爬虫的小伙伴们一定都深有体会：\n\n网站的 防抓取 机制。你要尽可能将自己伪装成“一个人”，骗过对方的服务器反爬验证。\n网站的 内容提取 。每个网站都需要你做不同的处理，而且网站一旦改版，你的代码也得跟着更新。\n\n第一点没什么捷径可走，套路见得多了，也就有经验了。关于第二点，今天咱们就来介绍一个小工具，在某些需求场景下，或许可以给你省不少事。\nGoose\nGoose 是一个 文章内容提取器 ，可以从任意资讯文章类的网页中提取 文章主体 ，并提取 标题、标签、摘要、图片、视频 等信息，且 支持中文 网页。它最初是由 http://Gravity.com 用 Java 编写的。python-goose 是用 Python 重写的版本。\n有了这个库，你从网上爬下来的网页可以直接获取正文内容，无需再用 bs4 或正则表达式一个个去处理文本。\n项目地址：  （py2） https://github.com/grangier/python-goose  （py3） https://github.com/goose3/goose3\n安装\n网上大多数教程提到的 python-goose 项目目前只支持到 python 2.7。可以通过 pip 安装：\n\n\n pip install goose-extractor\n\n\n或者安装官网上的方法从源代码安装：\n\n\nmkvirtualenv --no-site-packages goose\ngit clone https://github.com/grangier/python-goose.git\ncd python-goose\npip install -r requirements.txt\npython setup.py install\n\n\n我找到一个 python 3 的版本 goose3 ：\n\n\npip install goose3\n\n\n经过我一些简单的测试，未发现两个版本在结果上有太大的差异。\n快速上手\n这里使用 goose3，而 python-goose 只要把其中的 goose3 改成 goose 即可，接口都是一样的。以我之前发过的一篇文章 如何用Python抓抖音上的小姐姐 为抓取目标来做个演示。\n\n\nfrom goose3 import Goose\nfrom goose3.text import StopWordsChinese\n# 初始化，设置中文分词\ng = Goose({'stopwords_class': StopWordsChinese})\n# 文章地址\nurl = 'http://zhuanlan.zhihu.com/p/46396868'\n# 获取文章内容\narticle = g.extract(url=url)\n# 标题\nprint('标题：', article.title)\n# 显示正文\nprint(article.cleaned_text)\n\n\n输出：\n\n除了标题 title 和正文 cleaned_text 外，还可以获取一些额外的信息，比如：\n\n\nmeta_description ：摘要\n\nmeta_keywords ：关键词\n\ntags ：标签\n\ntop_image ：主要图片\n\ninfos ：包含所有信息的 dict\n\nraw_html ：原始 HTML 文本\n\n如有有些网站限制了程序抓取，也可以根据需要添加 user-agent 信息：\n\n\n g = Goose({'browser_user_agent': 'Version/5.1.2 Safari/534.52.7'})\n\n\n如果是 goose3，因为使用了 requests 库作为请求模块，因此还可以以相似方式配置 headers、proxies 等属性。\n在上述示例中使用到的 StopWordsChinese 为中文分词器，可一定程度上提高中文文章的识别准确率，但更耗时。\n其他说明\n\nGoose 虽然方便，但并不能保证每个网站都能精确获取，因此 适合大规模文章的采集 ，如热点追踪、舆情分析等。它只能从概率上保证大多数网站可以相对准确地抓取。我经过一些尝试后发现，抓取英文网站优于中文网站，主流网站优于小众网站，文本的提取优于图片的提取。\n\n从项目中的 requirements.txt 文件可以看出，goose 中使用到了 Pillow、lxml、cssselect、jieba、beautifulsoup、nltk ，goose3 还用到了 requests ，我们之前很多文章和项目中都有所涉及：\n这个男人让你的爬虫开发效率提升8倍  【编程课堂】jieba-中文分词利器\n\n如果你是使用基于 python2 的 goose，有可能会遇到 编码 上的问题（尤其是 windows 上）。这方面可以在公众号对话里回复关键词 编码 ，我们有过相关的讲解。\n\n除了 goose 外，还有其他的正文提取库可以尝试，比如 python-boilerpipe、python-readability 等。\n实例\n最后，我们来用 goose3 写小一段代码，自动抓取 爱范儿、雷锋网、DoNews 上的新闻文章：\n\n\n from goose3 import Goose\nfrom goose3.text import StopWordsChinese\nfrom bs4 import BeautifulSoup\n\ng = Goose({'stopwords_class': StopWordsChinese})\nurls = [\n    'https://www.ifanr.com/',\n    'https://www.leiphone.com/',\n    'http://www.donews.com/'\n]\nurl_articles = []\nfor url in urls:\n    page = g.extract(url=url)\n    soup = BeautifulSoup(page.raw_html, 'lxml')\n    links = soup.find_all('a')\n    for l in links:\n        link = l.get('href')\n        if link and link.startswith('http') and any(c.isdigit() for c in link if c) and link not in url_articles:\n            url_articles.append(link)\n            print(link)\n\nfor url in url_articles:\n    try:\n        article = g.extract(url=url)\n        content = article.cleaned_text\n        if len(content) > 200:\n            title = article.title\n            print(title)\n            with open('homework/goose/' + title + '.txt', 'w') as f:\n                f.write(content)\n    except:\n        pass\n\n\n这段程序所做的事情就是：\n\n抓取网站首页\n从页面上提取地址中带有数字的链接（因为文章页基本带数字，这里为了演示简单以此判断）\n抓取这些链接，提取正文。如果结果超过 200 个字，就保存成文件\n\n效果：\n\n在此基础上，你可以继续改进这个程序，让它不停地去寻找新的地址并抓取文章，并对获取到的文章进行词频统计、生成词云等后续操作。类似我们之前的分析案例 数据分析：当赵雷唱民谣时他唱些什么？。进一步完善，相信你能做出更有意思的项目。\n相关代码已上传，获取地址请在公众号（ Crossin的编程教室 ）里回复关键字 goose\n════  其他文章及回答：  如何自学Python | 新手引导 | 精选Python问答 | 如何debug？ | Python单词表 | 知乎下载器 | 人工智能 | 嘻哈 | 爬虫 | 我用Python | 高考 | requests | AI平台\n欢迎微信搜索及关注： Crossin的编程教室\n\n\n                ", "mainLikeNum": ["10 "], "mainBookmarkNum": "5"}