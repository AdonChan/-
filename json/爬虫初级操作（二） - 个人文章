{"title": "爬虫初级操作（二） - 个人文章 ", "index": "mongodb,mysql,python", "content": "本篇内容为 python 网络爬虫初级操作的简单介绍，内容主要有以下 2 部分：\n\n解析网页\n数据库\n\n\n解析网页\n一般来说，解析网页有三种方式：正则表达式、BeautifulSoup、lxml。其中正则表达式较难，BeautifulSoup 适合初学者，可以快速掌握提取网页中数据的方法。\n正则表达式\n常见的正则字符和含义如下：\n.    匹配任意字符，除了换行符\n*     匹配前一个字符 0 次或多次\n+    匹配前一个字符 1 次或多次\n?    匹配前一个字符 0 次或 1 次\n\n^    匹配字符串开头\n$    匹配字符串末尾\n\n()    匹配括号内表示式，也表示一个组\n\n\\s    匹配空白字符\n\\S    匹配任何非空白字符\n\n\\d    匹配数字，等价于[0-9]\n\\D    匹配任何非数字，等价于[^0-9]\n\n\\w    匹配字母数字，等价于[A-Za-z0-9]\n\\W    匹配非字母数字，等价于[^A-Za-z0-9]\n\n[]    用来表示一组字符\nPython 正则表达式有以下 3 种方法：re.match 方法：从字符串起始位置匹配一个模式，如果从起始位置匹配了，match()就返回none。语法 re.match(pattern, string, flags=0)pattern：正则表达式string：要匹配的字符串flags：控制正则表达式的匹配方式，如 是否区分大小写、多行匹配等\nre.search方法：只能从字符串的起始位置进行匹配。\nfind_all方法：可以找到所有的匹配。\nBeautifulSoup\nBeautifulSoup 从 HTML 或 XML 文件中提取数据。首先需要使用命令行来进行安装：\npip install bs4\n在使用的时候需要导入：\nfrom bs4 import BeautifulSoup\n例如使用 BeautifulSoup 获取博客主页文章的标题，代码和注释如下：\nimport requests\nfrom bs4 import BeautifulSoup\n\nlink = 'http://www.santostang.com/'\nheaders = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'}\nr = requests.get(link, headers=headers)\n# 将网页响应体的字符串转换为soup对象\nsoup = BeautifulSoup(r.text, 'html.parser')\nfirst_title = soup.find('h1', class_='post-title').a.text.strip()\nprint('第一篇文章的标题是：', first_title)\n\ntitle_list = soup.find_all('h1', class_='post-title')\n\nfor i in range(len(title_list)):\n    title = title_list[i].a.text.strip()\n\n    print('第 %s 篇文章的标题是： %s' % (i+1, title))\n\n\n\n运行得到结果：\n成功抓取到所需内容。\n关于 BeautifulSoup， 我们最后再来看一个实战项目：爬取北京二手房价格。代码如下：\nimport requests\nfrom bs4 import BeautifulSoup\nimport time\n\nheaders = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'}\n\nfor i in range(1,11):\n    link = 'https://beijing.anjuke.com/sale/'\n    r = requests.get(link, headers=headers)\n    print('第', i, '页')\n\n    soup = BeautifulSoup(r.text, 'lxml')\n    house_list = soup.find_all('li', class_='list-item')\n\n    for house in house_list:\n        name = house.find('div', class_='house-title').a.text.strip()\n        price = house.find('span', class_='price-det').text.strip()\n        price_area = house.find('span', class_='unit-price').text.strip()\n\n        no_room = house.find('div', class_='details-item').span.text\n        area = house.find('div', class_='details-item').contents[3].text\n        floor = house.find('div', class_='details-item').contents[5].text\n        year = house.find('div', class_='details-item').contents[7].text\n        broker = house.find('span', class_='brokername').text\n        broker = broker[1:]\n        address = house.find('span', class_='comm-address').text.strip()\n        address = address.replace('\\xa0\\xa0\\n                 ', '    ')\n        tag_list = house.find_all('span', class_='item-tags')\n        tags = [i.text for i in tag_list]\n        print(name, price, price_area, no_room, area, floor, year, broker, address, tags)\n    time.sleep(5)\n这样就成功爬取了安居客上前 10 页的北京二手房价格。\n数据库\n数据存储分为两种，存储在文件（TXT和CSV）中和存储在数据库（MySQL关系数据库和MongoDB数据库）中。\nCSV (Comma-Separated Values)是逗号分隔值的文件格式，其文件以纯文本的形式存储表格数据（数字和文本）。 CSV 文件的每一行都用换行符分隔，列与列之间用逗号分隔。\nMySQL是一种关系数据库管理系统，所使用的是SQL语言，是访问数据库最常用的标准化语言。关系数据库（建立在关系模型基础上的数据库）将数据保存在不同的表中，而不是将所有数据放在一个大仓库内，这样就增加了写入和提取的速度，数据的存储也比较灵活。\n关于存储在文件的方法这里不再赘述，下面首先介绍如何存储至MySQL数据库。\n你需要先到官网下载并安装 MySQL数据库，博主用的是 macOS Sierra 系统，在安装完成后，打开系统偏好设置，如下图：\n最下方出现 MySQL的图标，打开并连接，如下图所示：\n打开终端，在终端中输入添加MySQL路径的命令：\nPATH=\"$PATH\":/usr/local/mysql/bin\n继续输入登录到MySQL的命令：mysql -u root -p，然后输入密码即可成功登录。成功登录界面如下：\n接下来介绍MySQL的基本操作：\n创建数据库\n\n创建数据表\n创建数据表必须指明每一列数据的名称(column_name)和类别(column_type)。在上图中，创建了 4 个变量：id, url, content, created_time。其中id的类别是整数INT，属性为自己增加(AUTO_INCREMENT)。新添加数据的数值会自动加 1。PRIMARY KEY的关键字用于将id定义为主键。\nurl和content的类别是可变长度的字符串VARCHAR,括号中的数字代表长度的最大值，NOT NULL表示url和content不能为空。created_time为该数据添加时间，不需要设置，因为有时间戳，它会自动根据当时的时间填入。\n创建数据表后，可查看数据表的结构：\n在数据表中插入数据\n\n这里插入了url和content两个属性，id是自动递增的，created_time是数据加入的时间戳，这两个变量会自动填入。\n从数据表中提取数据\n\n由上图，我们可以看到提取数据有 3 种方法：（1）将id等于 1 的数据行提取出来；（2）提取只看部分字段的数据；（3）提取包含部分内容的数据。\n删除数据\n\n⚠️注意，如果没有指定 WHERE 子句，用DELETE FROM urls将会导致MySQL表中的所有记录被删除，即误删除整张表。\n修改数据\n\n由于id和created_time是数据库自动填入的，因此这一行数据的id为 2。\n更多操作可参考菜鸟教程。\n下面介绍使用Python操作MySQL数据库，依次使用下令命令安装mysqlclient：\nbrew install mysql\nexport PATH=$PATH:/usr/local/mysql/bin\npip install MySQL-Python\npip3 install mysqlclient\n出现下列文字即安装成功：\n用Python操作MySQL数据库，我们以在博客爬取标题和 url 地址为例，具体代码和解释如下：\n#coding=UTF-8\nimport MySQLdb\nimport requests\nfrom bs4 import BeautifulSoup\n\n# connect() 方法用于创建数据库的连接，里面可以指定参数：用户名，密码，主机等信息\n#这只是连接到了数据库，要想操作数据库需要创建游标\nconn = MySQLdb.connect(host='localhost', user='root', passwd='your_password', db='MyScraping', charset='utf8')\n# 通过获取到的数据库连接conn下的cursor()方法来创建游标。\ncur=conn.cursor()\n\nlink = 'http://www.santostang.com/'\nheaders = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'}\nr = requests.get(link, headers=headers)\n\nsoup = BeautifulSoup(r.text, 'html.parser')\ntitle_list = soup.find_all('h1', class_='post-title')\nfor eachone in title_list:\n    url = eachone.a['href']\n    title = eachone.a.text.strip()\n    # 创建数据表,通过游标cur 操作execute()方法可以写入纯sql语句。通过execute()方法中写如sql语句来对数据进行操作\n    cur.execute('INSERT INTO urls (url, content) VALUES (%s, %s)', (url, title))\n\ncur.close()\nconn.commit()\nconn.close()\n最后，我们来介绍如何存储至MongoDB数据库。\n首先要知道NoSQL泛指非关系型数据库，数据之间无关系，具有非常高的读写性能，而MongoDB是其中非常流行的一种数据库。它是一种关系数据库管理系统，所使用的是SQL语言，是访问数据库最常用的标准化语言。\n下面仍以上述的在博客爬取标题和 url 地址为例。\n第一步连接 MongoDB客户端，然后连接数据库blog_database，再选择该数据的集合blog。如果它们不存在，就会自动创建一个，代码示例如下：\nfrom pymongo import MongoClient\n\nclient = MongoClient('localhost', 27017)\ndb = client.blog_database\ncollection = db.blog\n第二步爬取博客主页的所有文章标题存储至MongoDB数据库，代码如下：\nimport requests\nimport datetime\nfrom bs4 import BeautifulSoup\nfrom pymongo import MongoClient\n\nclient = MongoClient('localhost', 27017)\ndb = client.blog_database\ncollection = db.blog\n\nlink = 'http://www.santostang.com/'\nheaders = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'}\nr = requests.get(link, headers=headers)\n\nsoup = BeautifulSoup(r.text, 'html.parser')\ntitle_list = soup.find_all('h1', class_='post-title')\nfor eachone in title_list:\n    url = eachone.a['href']\n    title = eachone.a.text.strip()\n    post = {'url': url,\n            'title': title,\n            'date': datetime.datetime.utcnow()\n\n    }\n    collection.insert_one(post)\n重点在最后一部分的代码，首先将爬虫获取的数据存入post的字典中，然后使用insert_one加入集合collection中。\n最后，启动MongoDB查看结果。\n打开终端输入：\nsudo mongod --config /usr/local/etc/mongod.conf\n确认权限后，保持当前终端不关，新建终端依次输入：\nmongod\n\nmongo\n出现以下文字表示连接成功：\n然后，输入：\nuse blog_database\n\ndb.blog.find().pretty()\n就查询到数据集合的数据了，如下图所示：\n同时注意到，它是JSON格式的。\n更多使用 Python 来操作 MongoDB 数据库的知识，可以参考 PyMongo官网。\n\n本文为崔庆才博客和唐松的《Python网络爬虫从入门到实践》学习记录与总结，具体内容可参考二者。博主在学习过程中的练习代码也已上传至 GitHub。\n不足之处，欢迎指正。\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "1"}