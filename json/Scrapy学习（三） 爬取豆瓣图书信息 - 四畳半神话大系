{"title": "Scrapy学习（三） 爬取豆瓣图书信息 - 四畳半神话大系 ", "index": "scrapy,网页爬虫,python", "content": "前言\n\nScrapy学习（一） 安装\nScrapy学习（二） 入门\n\n有了前两篇的基础，就可以开始互联网上爬取我们感兴趣的信息了。因为暂时还没有学到如何模拟登陆，所以我就先抓像豆瓣这样不需要登陆的网站上的内容。我的开发环境是 Win7 + PyChram + Python3.5 + MongoDB爬虫的目标是豆瓣的日本文学标签下的所有书籍基本信息\n创建项目\nscrapy startproject douban\n接着移动到douban目录下\nscrapy genspider book book.douban.com\n在spider目录下生成相应的BookSpider模板\n编写Item\n在items.py中编写我们需要的数据模型\nclass BookItem(scrapy.Item):\n    book_name = scrapy.Field()\n    book_star = scrapy.Field()\n    book_pl = scrapy.Field()\n    book_author = scrapy.Field()\n    book_publish = scrapy.Field()\n    book_date = scrapy.Field()\n    book_price = scrapy.Field()\n编写Spider\n访问豆瓣的日本文学标签,将url的值写到start_urls中。接着在Chrome的帮助下，可以看到每本图书是在ul#subject-list > li.subject-item\nclass BookSpider(scrapy.Spider):\n    ...\n    def parse(self, response):\n        sel = Selector(response)\n        book_list = sel.css('#subject_list > ul > li')\n        for book in book_list:\n            item = BookItem()\n            item['book_name'] = book.xpath('div[@class=\"info\"]/h2/a/text()').extract()[0].strip()\n            item['book_star'] = book.xpath(\"div[@class='info']/div[2]/span[@class='rating_nums']/text()\").extract()[\n                0].strip()\n            item['book_pl'] = book.xpath(\"div[@class='info']/div[2]/span[@class='pl']/text()\").extract()[0].strip()\n            pub = book.xpath('div[@class=\"info\"]/div[@class=\"pub\"]/text()').extract()[0].strip().split('/')\n            item['book_price'] = pub.pop()\n            item['book_date'] = pub.pop()\n            item['book_publish'] = pub.pop()\n            item['book_author'] = '/'.join(pub)\n            yield item\n测试一下代码是否有问题\nscrapy crawl book -o items.json\n奇怪的发现，items.json内并没有数据，后头看控制台中的DEBUG信息\n2017-02-04 16:15:38 [scrapy.core.engine] INFO: Spider opened2017-02-04 16:15:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0  pages/min), scraped 0 items (at 0 items/min)2017-02-04 16:15:38 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:60232017-02-04 16:15:39 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://book.douban.com/robot... (referer: None)2017-02-04 16:15:39 [scrapy.core.engine] DEBUG: Crawled (403) <GET https://book.douban.com/tag/%... (referer: None)\n爬取网页时状态码是403。这是因为服务器判断出爬虫程序，拒绝我们访问。我们可以在settings中设定USER_AGENT的值，伪装成浏览器访问页面。\nUSER_AGENT = \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\"\n再试一次，就发现items.json有值了。但仔细只有第一页的数据，如果我们想要爬取所有的数据，就需要爬完当前页后自动获得下一页的url，以此类推爬完所有数据。所以我们对spider进行改造。\n    ...\n    def parse(self, response):\n        sel = Selector(response)\n        book_list = sel.css('#subject_list > ul > li')\n        for book in book_list:\n            item = BookItem()\n            try:\n                item['book_name'] = book.xpath('div[@class=\"info\"]/h2/a/text()').extract()[0].strip()\n                item['book_star'] = book.xpath(\"div[@class='info']/div[2]/span[@class='rating_nums']/text()\").extract()[0].strip()\n                item['book_pl'] = book.xpath(\"div[@class='info']/div[2]/span[@class='pl']/text()\").extract()[0].strip()\n                pub = book.xpath('div[@class=\"info\"]/div[@class=\"pub\"]/text()').extract()[0].strip().split('/')\n                item['book_price'] = pub.pop()\n                item['book_date'] = pub.pop()\n                item['book_publish'] = pub.pop()\n                item['book_author'] = '/'.join(pub)\n                yield item\n            except:\n                pass\n        nextPage = sel.xpath('//div[@id=\"subject_list\"]/div[@class=\"paginator\"]/span[@class=\"next\"]/a/@href').extract()[0].strip()\n        if nextPage:\n            next_url = 'https://book.douban.com'+nextPage\n            yield scrapy.http.Request(next_url,callback=self.parse)\n其中scrapy.http.Request会回调parse函数，用try...catch是因为豆瓣图书并不是格式一致的。遇到有问题的数据，就抛弃不用。\n突破反爬虫\n一般来说，如果爬虫速度过快。会导致网站拒绝我们的访问，所以我们需要在settings设置爬虫的间隔时间，并关掉COOKIES\nDOWNLOAD_DELAY = 2COOKIES_ENABLED = False\n或者，我们可以设置不同的浏览器UA或者IP地址来回避网站的屏蔽下面用更改UA来作为例子。在middlewares.py,编写一个随机替换UA的中间件，每个request都会经过middleware。其中process_request，返回None，Scrapy将继续到其他的middleware进行处理。\nclass RandomUserAgent(object):\n    def __init__(self,agents):\n        self.agents = agents\n    @classmethod\n    def from_crawler(cls,crawler):\n        return cls(crawler.settings.getlist('USER_AGENTS'))\n    def process_request(self,request,spider):\n        request.headers.setdefault('User-Agent',random.choice(self.agents))\n接着道settings中设置\nDOWNLOADER_MIDDLEWARES = {\n'douban.middlewares.RandomUserAgent': 1,\n}\n...\nUSER_AGENTS = [\n    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n    ...\n]\n再次运行程序，显然速度快了不少。\n保存到MongoDB\n接下来我们要将数据保存到数据库做持久化处理(这里用MongoDB举例，保存到其他数据库同理)。这部分处理是写在pipelines中。在此之前我们还要先安装连接数据库的驱动。\npip install pymongo\n我们在settings写下配置\n# MONGODB configure\nMONGODB_SERVER = 'localhost'\nMONGODB_PORT = 27017\nMONGODB_DB = 'douban'\nMONGODB_COLLECTION = \"book\"\nclass MongoDBPipeline(object):\n    def __init__(self):\n        connection = MongoClient(\n            host=settings['MONGODB_SERVER'],\n            port=settings['MONGODB_PORT']\n        )\n        db = connection[settings['MONGODB_DB']]\n        self.collection = db[settings['MONGODB_COLLECTION']]\n\n    def process_item(self, item, spider):\n        self.collection.insert(dict(item))\n        log.msg(\"Book  added to MongoDB database!\",\n                level=log.DEBUG, spider=spider)\n        return item\n其他\n将运行项目的时候控制台中输出的DEBUG信息保存到log文件中。只需要在settings中设置\nLOG_FILE = \"logs/book.log\"\n项目代码地址：豆瓣图书爬虫\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "8"}