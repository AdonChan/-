{"title": "scrapy入门 - 小翰学习周记 ", "index": "scrapy,python爬虫,python", "content": "因为公司项目需求，需要做一个爬虫。所以我一个python小白就被拉去做了爬虫。花了两周时间，拼拼凑凑总算赶出来了。所以写个blog做个记录。\n快速入门\n首先，初步要做的就是快速构建一个爬虫。\n配置环境\nMac下安装\n1) 直接从官网下载 python下载官网\n2) 是通过brew安装首先安装xcode可以选择在App Store安装或者使用xcode-select --install这个命令安装\n接着安装brew\nusr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n把这个命令输入终端运行就可以安装了brew官网\n接着利用brew安装python3brew install python3\n安装完python3以后，我们来装scrapy。因为python3是自带pip3的，所以如果找不到该命令的话，可以去看看python3的安装路径。brew是将其安装在了\\usr\\local\\bin这个目录下。\n如果pip没安装上的话，不要慌。我们还可以通过命令来安装\ncurl -O https://bootstrap.pypa.io/get-pip.py\npython3 get-pip.py\n接下来安装scrapy输入命令pip3 install Scrapy就可以了\nwindows下安装\n首先从官网上下载msi文件在安装选项中勾选上需要pip\n然后在cmd中输入pip3 install Scrapy\n完成\nLinux安装\nsudo get-apt install python36 python36-devel gcc\nsudo pip3 install Scrapy\n两条命令就搞定了。因为我的爬虫是跑在docker上，有些镜像可能没有gcc。所以需要devel和gcc，否则有些包会安不上。切记\nRedhat系的话，只需要把get-apt改成yum就可以了\n\n快速开始\n\n建立项目 scrapy startproject demo\n\n建立爬虫 scrapy genspider demo_spider www.google.com\n\n启动爬虫 scrapy crwal demo_spider\n\n\n当你建立完项目的时候，scrapy会帮你生成一堆文件。\n目录结构是这样的\n在你的demo项目中，会有个scrapy.cfg的配置文件和一个demo的文件夹\nscrapy.cfg这个文件我们先暂时不去关心。我们来关心一下demo文件夹下的东西分别是items.py,middlewares.py,pipelines.py,settings.py和一个spiders文件夹。\n接着我们去spiders目录下去创建一个爬虫scrapy genspider demo_spider www.google.com\n\nOK，爬虫生成功了。\n\nspider初步解析\n我们来初步解析一下这个爬虫。有一个DemoSpiderSpider的类。很明显，这个是我们刚才生成爬虫的名字为demo_spider然后后面又添加了一个Spider。\n接着往下看，有个name的属性，这个属性很重要，我们到时候启动爬虫的时候，就要通过这个name来告知scarpy启动的是哪个爬虫\nallowed_demains是用来指定，我们只爬取哪些域名下的。比如说，我在爬取google的搜索结果的时候，会爬到很多别的网站，这个属性就是用来告知，除了www.google.com以外的，我都不爬取。\nstart_urls是用来做启动url，看命名就知道了。可以把Scrapy的爬取过程看成一个广度搜索。所以它会先迅速把start_urls下的所有url都爬取一遍。然后把结果加入到一个队列中。也是因为这个原因，所以在做并发爬虫时，会让不同的爬虫的start_urls不一样。当然还有重复的问题要解决（笑）\n接下来就是parse方法了，我们对页面的爬取也是在这个parse中解决\n\n向外走\n初步理解了spider，我们往外层走。\n我们首先来假想几个问题，如果我是Scrapy框架的设计者，我会这么设计这个框架。既然是通用的爬虫框架，那用户是不是应该可以操作header之类的，让我的每一个爬虫都经过代理，或者是设置不同的cookie。\n当当当，所以就有了中间件。middlewares.py我们来看看Scrapy生成的是什么样的。\n\n\n既然是入门，我们肯定是先关心我们需要的。看第二张图。有这么几个干方法process_request,process_response,process_exception处理request,response,exception。很好，我们就要这几个东西。\n我们可以这么理解，我们的爬虫爬到的每一个页面，都会经过这些中间件。\n来看看架构图\n所以我们的数据是经过每一个中间件。然后中间件来决定去留。\n然后我们来想想具体process_request下的情况。\n\n第一种，我修改了代理，但是我只是改了代理，我还需要把这个数据继续下去。返回None\n\n第二种，这个已经处理好了，现在我需要想直接把数据发给spider了，这个时候，我们就需要返回一个response了。\n第三种，我想要重新调度我的request.这时候只要返回request。调度器会终止process_request，然后用这个request重新开始。\n第四种，这个数据我直接丢弃了，不想用了。直接raise一个IgnoreRequest，也就是，如果你不处理这个异常，这异常就直接扔了。当然，你还可以选择在process_exception去处理这个异常\n\nPython爬虫从入门到放弃（十七）之 Scrapy框架中Download Middleware用法\n\n初步理解了中间件，我们来说说items在spider处理完数据以后，写入item中，然后就来到了这儿。scrapy生成的代码\n\n怎么用呢。举个例子，我在spider中需要存储url。于是我在这个文件中就写入url = scrapy.Field接着在spider中生成这个item。然后将item['url'] = url，在处理完了以后。yield item完成\n这个数据呢，就会交给pipelines来处理\n\n接着，我们来看看pipelines\n\n我们和刚才一样，思考一下，数据拿过来有几种可能。\n\n数据不是我的，我不处理。OK，我直接返回itme\n数据是我的，但是数据错了。扔掉。raise一个DropItem就可以了\n\n这边呢，如果要写入数据库，玩玩写入的时间特别长，所以推荐使用Twisted来做一个异步写入\n\n最后。我们来看看settings\n\n配置文件，比如是否遵守robots.txt之类的。当然，你刚才写的所有middlewares，pipelines。都要在这儿注册！！！！随便给它一个不重复的数字就好了\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}