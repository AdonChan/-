{"title": "使用python爬虫工具Scrapy统计简书文章阅读量 - liuwill的技术之路 ", "index": "python,网页爬虫", "content": "\n突然发现多年来一直断断续续在学习使用的python，拥有着广泛的使用场景，从开源硬件、服务器运维、自动化测试，到数学计算，人工智能，都有python的一席之地，在各个领域python有丰富的框架和工具。\n听闻python的Scrapy爬虫已久，最近正好想对去年一年在简书写作的文章做一下统计。作为软件工程师，肯定不能去按计算器，正好找到一个可以使用scrapy的场景，直接用爬虫来抓取简书页面，然后提取数据作统计。\n工作原理和步骤\n\n分析要抓取的页面结构，找到有价值的信息；\n通过发起http请求，获得要抓取页面的html正文；\n分析html页面的标签结构，根据xml层级结构或者css选择器语法，读取到需要的document元素；\n设计数据结构和计算规则，进行统计，然后将结果输出\n\n抓取网页的话使用urllib库其实就可以完成，css选择器的话，python也有pyquery提供类似jquery的功能。剩下就是简单的数据统计了。\n使用Scrapy动手操作\n完整的项目还没有整理好，暂时不提供代码，本身核心业务逻辑的代码量并不多。\n\n安装python和scrapy\n#安装pip\napt-get install -y build-essential libffi-dev libxml2-dev libxslt1-dev libevent-dev libssl-dev zlib1g-dev  python-dev python-pip\npip install -U pip\n\n#安装scrapy\npip install scrapy\n本次应用中使用pip来安装scrapy，详细安装可以参看官网，安装pip的同时，需要安装一些linux开发相关的工具。\n需要注意的事项有两点，首先如果是在mac上安装的话，由于操作系统以来sis这个库，而scrapy需要更高版本的sis，无法升级，所以可能有坑，笔者直接使用docker搭建的容器进行开发，所以不存在问题；另外一个是pip源，在国内可能下载速度会比较慢，可以使用国内的源或者翻墙下载。\nimport sys\nimport scrapy\nimport csv\n\nclass MyJianShuSpider(scrapy.Spider):\n    name = 'my_jisnshu_spider'\n    page = 1\n    userid = 'yourid'\n    base_url = 'http://www.jianshu.com/u/yourid?order_by=shared_at&page='\n    start_urls = ['http://www.jianshu.com/u/yourid?order_by=shared_at&page=1']\n    headers = {\n      \"Host\": \"www.jianshu.com\",\n      \"User-Agent\": \"your agent\",\n      \"Accept\": \"*/*\"\n    }\n\n    def start_requests(self):\n        url = self.base_url + str(self.page)\n        yield scrapy.Request(url, headers=self.headers, callback=self.parse)\n\n    def parse(self, response):\n        for item in response.css('ul.note-list li'):\n            print publishTimeText = item.css('.author .name > .time ::attr(data-shared-at)').extract_first()\nScrapy基本工作原理\nscrapy提供了Spider类，该类通过start_requests接口被scrapy执行器调用，运行scrapy执行器，就会从入口点start_requests开始执行爬虫的代码；使用过javascript的es2015应该会比较容易理解yield，感觉是js也借鉴了很多python的理念。\n在Spider执行过程中，任何时候都可以通过调用scrapy.Request来发起一个爬虫抓取操作，callback参数是抓取的网络请求成功之后的回调函数，url是请求的链接地址，headers是http协议的头部，可以根据需要设置。\n简书页面的相关结构\n\n查看简书列表页面的document结构，很容易找到#list-container下面的ul列表就是包含我们要统计的信息的节点，因此可以直接使用scrapy提供的css选择器来获取。执行回调函数response.css('ul.note-list li')就获取到了一个列表，对列表做循环处理，取出每一个需要的元素。\n#发表时间\npublishTimeText = item.css('.author .name > .time ::attr(data-shared-at)').extract_first()\n# 摘要\nabstractText = item.css('p.abstract ::text').extract_first()\n# 文章标题\ntitleText = item.css('a.title ::text').extract_first()\n# 文章的链接\narticleLink = item.css('a.title ::attr(href)').extract_first()\n\n#统计信息\nmetaList = item.css('.meta > a ::text').re(r' ([0-9]*)\\n')\n最后的metaList返回的是一个数组，下标0-2分别是阅读、评论和点赞的数目。\n在实际获取到的列表页面中，页面的文章是通过异步加载，每次获取一页数据的，所以在抓取过程中，需要多次抓取每次抓取一页，直到没有更多文章为止。\n简书实际上提供了分页的文章列表页，可以分页获取数据，页面中页可以找到是否还有更多文章的标记。为了不破坏读者探索的乐趣，文章里就不详细介绍了。\n\n文／liuwill（简书作者）原文链接：使用python爬虫工具Scrapy统计简书文章阅读量著作权归作者所有，转载请联系作者获得授权，并标注“简书作者”。\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "7"}