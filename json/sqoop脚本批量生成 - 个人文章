{"title": "sqoop脚本批量生成 - 个人文章 ", "index": "python", "content": "通过all_tab_columnss字典表生成hive的建表语句\n\ncreate or replace view create_sql as--通过all_tab_columnss字典表生成hive的建表语句select owner,table_name, case\n     when nm = 1 then\n      'create table ' || owner || '.' || TABLE_NAME || ' (' ||\n      COLUMN_NAME || ' ' || DATA_TYPE || ','\n     when np = 1 then\n      COLUMN_NAME || ' ' || DATA_TYPE || ') partitioned by (dt string);'\n     else\n      COLUMN_NAME || ' ' || DATA_TYPE || ','\n   end create_sql\nfrom (\n    SELECT OWNER,\n            TABLE_NAME,\n            COLUMN_NAME,\n            CASE\n              WHEN DATA_TYPE IN ('VARCHAR2','LONG','CLOB','CHAR','NCHAR','BLOB','VARCHAR2') THEN\n               'STRING'\n              WHEN DATA_TYPE IN ('NUMBER') then\n               'DECIMAL' || '(' || DATA_PRECISION || ',' || DATA_SCALE || ')'\n              WHEN DATA_TYPE IN ('DATE', 'TIMESTAMP(6)') THEN\n               'STRING'\n              ELSE\n               DATA_TYPE||'(' || DATA_PRECISION || ',' || DATA_SCALE || ')'\n            END DATA_TYPE,\n            COLUMN_ID,\n            NM,\n            NP\n      FROM (select t.*,\n                    row_number() over(partition by owner, TABLE_NAME order by COLUMN_ID) nm,\n                    row_number() over(partition by owner, TABLE_NAME order by COLUMN_ID desc) np\n             \n               from all_tab_columns t\n              ))\nORDER BY OWNER, TABLE_NAME, COLUMN_ID;\n\n二、生成sqoop任务的配置表 selectowner||table_name        job_name,                  --任务名称''                       table_type,                --表类型dim 维表，fact_i 只增，Fact_iud 增删改''                       partition,                 --hive表是否是分区表 0 非，1 是'BS'                     source_db,                 --来源业务系统，一般用源表用户名或库名owner||'.'||table_name   source_table,              --源表名称''                       datatime_cloumn,           --增量时间戳字段'APPEND'                 incremental,               --增量接入方式，append:增量接入，overwrite:全量接入''                       SPLIT_BY,                  --并行字段，选择重复数据较少的字段'APP_NO'                 row_key,                   --主键字段'fz_bs'                  hive_db,                   --指定接入的hive库table_name               hive_table,                --指定接入的hive表，必须是无数据库名的存表名''                       check_column,              --指定接入的源表字段columns from (select owner,table_name,wm_concat(column_name)over(partition by owner,table_name order by column_id) columns,rn from (select owner,table_name,column_name,column_id,row_number()over(partition by owner,table_name order by column_id desc) rn from  all_tab_columnwhere owner||'_'||table_name in('BS_S_FAULT_RPT',         'BS_ARC_S_FAULT_RPT','HIS_ARC_S_FAULT_RPT','BS_S_FAULT_HANDLE',      'BS_ARC_S_FAULT_HANDLE','HIS_ARC_S_FAULT_HANDLE','BS_S_RETVISIT','BS_ARC_S_RETVISIT','HIS_ARC_S_RETVISIT','BS_S_95598_WKST_RELA','HIS_S_95598_WKST_RELA')order by owner,table_name,column_id))where rn=1order by owner,table_name;\n\n下面是自动生成sqoop配置任务的的python脚本\n\nhive任务\n!/usr/bin/python3\ncoding=utf-8\nimport json,osdef json_make(input_file='./tablelist'):\n#input_file ='./sqoopjob/tablelist'\nlines = open(input_file, \"r\",encoding=\"utf_8_sig\").readlines()\n[lines.remove(i) for i in lines if i in ['', '\\n']]\nlines = [line.strip() for line in lines]\n\n# 获取键值\nkeys = lines[0].split('\\t')\nline_num = 1\ntotal_lines = len(lines)\nparsed_datas = []\nwhile line_num < total_lines:\n        values = lines[line_num].split(\"\\t\")\n        parsed_datas.append(dict(zip(keys, values)))\n        line_num = line_num + 1\njson_str = json.dumps(parsed_datas, ensure_ascii=False, indent=4)\noutput_file = input_file+'.json'\n\n# write to the file\nf = open(output_file, \"w\", encoding=\"utf-8\")\nf.write(json_str)\nf.close()\nprint('json格式转换结束！详见%s'%(output_file))\n\n\ndef create_job(tablelist):\nif os.path.exists('sqoopjob'):\n    os.system('rm -fr sqoopjob/*')\n    os.system('mkdir  sqoopjob/hive-bin')\n    os.system('mkdir  sqoopjob/impala-bin')\n    os.system('mkdir  sqoopjob/partition-bin')\n    os.system('mkdir  sqoopjob/job-bin')\nelse:\n    os.mkdir('sqoopjob')\nsqoopmeta='jdbc:hsqldb:hsql://localhost:16000/sqoop'\njdbc='jdbc:oracle:thin:@10.90.87.35:11521:bdccdb2 --username sqoopuser --password Bigdata_2016'\nsjf=open('sqoopjob/createjob.sh', \"w\")\nhcf = open('./sqoopjob/hql_corntab.cron', 'w')\nimcf=open('./sqoopjob/imql_corntab.cron', 'w')\ncrontabf=open('./sqoopjob/crontab.cron', 'w')\ndf=open('./sqoopjob/deletejob.sh', 'w')\n#scmd=open('./sqoopjob/sqoopcmd.sh', 'w')\n#scmd.write('''yesdate=`date -d last-day +%Y-%m-%d`;todday=`date +%Y-%m-%d`''')\n#cf=open('./sqoopjob/cron.sh', 'w')\nkerboros='kinit -kt /keytab/sqoopdba.keytab sqoopdba \\n'\n\nwith open(tablelist, 'r') as load_f:\n    load_dict = json.load(load_f)\n    for job in load_dict:\n        if job['table_type'].lower()=='dim' and job['partition']=='0':\n            #处理档案表\n            hivetruncate='''hive -e\"use {hive_db};truncate table {hive_table}_new\"\\n'''.format(hive_db=job['hive_db'],hive_table=job['hive_table'])\n            createjob = '''sqoop  job --create {job_name} --meta-connect {sqoopmeta} \\\\\n                    -- import --connect {jdbc} \\\\\n                    --table {source_table} \\\\\n                    --incremental append \\\\\n                    --split-by {split_by} \\\\\n                    --hive-import \\\\\n                    --hive-drop-import-delims \\\\\n                    --hive-overwrite \\\\\n                    --hive-table {hive_db}.{hive_table}_NEW \\\\\n                    --check-column  {check_column} \\\\\n                    --last-value '2011-01-01 11:00:00' \\\\\n                    -m 2;\\n'''.format(\n                    job_name=job['job_name'].lower(),\n                    sqoopmeta=sqoopmeta,\n                    jdbc=jdbc,\n                    hive_db=job['hive_db'],\n                    source_table=job['source_table'].upper(),\n                    split_by=job['split_by'].upper(),\n                    hive_table=job['hive_table'].upper(),\n                    check_column=job['datatime_cloumn'].upper()\n                    ).replace('    ','')\n\n            execjob = '''sqoop  job --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop \\\n                    --exec {job_name};\\n'''.format(job_name=job['job_name'].lower()).replace('    ','')\n\n            deledrop = '''sqoop  job --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop \\\n                    --delete {job_name};\\n'''.format(job_name=job['job_name'].lower() ).replace('    ','')\n\n            hql = '''hive -e \"use {hive_db};\n                create table {tmp_table} as\n                select {columns} from\n                (select t.*,row_number()over(partition by {pattition_by} ORDER BY  t.{order_by} desc) as rn\n                from (select * from {table_name}_new union all select * from {table_name}) t\n                ) tm\n                where rn =1;\n                drop table {table_name};\n                alter table {table_name}_TMP rename to {table_name};\n                \"\\n'''.format(\n                                    hive_db=job['hive_db'],\n                                    tmp_table=job['source_table'].replace('.', '_')+'_tmp',\n                                    columns=job['columns'],\n                                    pattition_by=','.join(['t.' + cl for cl in job['pattition_by'].split(',')]),\n                                    order_by=job['datatime_cloumn'],\n                                    table_name=job['source_table'].replace('.', '_'),\n                                    ).replace('    ','')\n            imql = '''impala-shell -i bigdata-w-001 -q \"invalidate metadata;use {hive_db};\n                create table {tmp_table} as\n                select {columns} from\n                (select t.*,row_number()over(partition by {pattition_by} ORDER BY  t.{order_by} desc) as rn\n                from (select * from {table_name}_new union all select * from {table_name}) t\n                ) tm\n                where rn =1;\n                drop table {table_name};\n                alter table {table_name}_TMP rename to {table_name};\n                \"\\n'''.format(\n                                    hive_db=job['hive_db'],\n                                    tmp_table=job['source_table'].replace('.', '_') + '_tmp',\n                                    columns=job['columns'],\n                                    pattition_by=','.join(['t.' + cl for cl in job['pattition_by'].split(',')]),\n                                    order_by=job['datatime_cloumn'],\n                                    table_name=job['source_table'].replace('.', '_'),\n                                ).replace('    ','')\n            #print(sjf)\n            sjf.write(hivetruncate+createjob)\n            df.write(deledrop)\n            open('./sqoopjob/hive-bin/' + job['job_name'].lower() + '_hql.sh', 'w').write(kerboros + execjob  + hql)\n            open('./sqoopjob/impala-bin/' + job['job_name'].lower() + '_imql.sh', 'w').write(kerboros + execjob  + imql)\n            hcf.write('''30 02 * * 6 cd /root/hive_import/bin&& ./{job_name}_hql.sh >>../logs/{job_name}_hql.out 2>&1\\n'''.format(\n                job_name=job['job_name'].lower()\n                    )\n                    )\n            imcf.write(\n            '''30 02 * * 6 cd /root/hive_import/bin&& ./{job_name}_imql.sh >>../logs/{job_name}_imql.out 2>&1\\n'''.format(\n                job_name=job['job_name'].lower()))\n            execjob = '''sqoop  job --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop \\\n                    --exec {job_name};\\n'''.format(\n                job_name=job['job_name'].lower()).replace('    ','')\n            open('./sqoopjob/exec_run.sh', 'a').write(execjob)\n        elif job['table_type'].lower()=='fact_iud'and job['partition']=='0':\n            # 处理增量事实表,有增删改查的事实表\n            hivetruncate = '''hive -e\"use {hive_db};truncate table {hive_table}\"\\n'''.format(\n                hive_db=job['hive_db'], hive_table=job['hive_table'])\n            createjob = '''sqoop  job --create {job_name} --meta-connect {sqoopmeta} \\\\\n                    -- import --connect {jdbc} \\\\\n                    --table {source_table} \\\\\n                    --split-by {split_by} \\\\\n                    --hive-import \\\\\n                    --hive-drop-import-delims \\\\\n                    --hive-table {hive_db}.{hive_table} \\\\\n                    --delete-target-dir \\\\\n                    -m 2;\\n'''.format(\n                                            job_name=job['job_name'].lower(),\n                                            sqoopmeta=sqoopmeta,\n                                            jdbc=jdbc,\n                                            hive_db=job['hive_db'],\n                                            source_table=job['source_table'].upper(),\n                                            split_by=job['split_by'].upper(),\n                                            hive_table=job['hive_table'].upper(),\n                                        ).replace('    ','')\n            sjf.write(hivetruncate+createjob)\n            execjob = '''sqoop  job --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop \\\n                                    --exec {job_name};\\n'''.format(job_name=job['job_name'].lower()).replace('    ',\n                                                                                                             '')\n            open('./sqoopjob/job-bin/' + job['job_name'].lower() + '.sh', 'w').write(kerboros + execjob)\n            open('./sqoopjob/exec_run.sh', 'a').write(execjob)\n            crontabf.write('''30 02 * * 6 cd /root/hive_import/bin&& ./{job_name}.sh >>../logs/{job_name}.out 2>&1\\n'''.format(\n                    job_name=job['job_name'].lower()))\n        elif job['table_type'].lower()=='fact_i'and job['partition']=='0':\n            # 处理在线事实表,只有写入事务的事实表\n            hivetruncate = '''hive -e\"use {hive_db};truncate table {hive_table}\"\\n'''.format(\n                hive_db=job['hive_db'], hive_table=job['hive_table'])\n            createjob = '''sqoop  job --create {job_name} --meta-connect {sqoopmeta} \\\\\n                                    -- import --connect {jdbc} \\\\\n                                    --table {source_table} \\\\\n                                    --incremental append \\\\\n                                    --split-by {split_by} \\\\\n                                    --hive-import \\\\\n                                    --hive-drop-import-delims \\\\\n                                    --hive-table {hive_db}.{hive_table} \\\\\n                                    --check-column  {check_column} \\\\\n                                    --last-value '2011-01-01 11:00:00' \\\\\n                                    -m 2;\\n'''.format(\n                                                                job_name=job['job_name'].lower(),\n                                                                sqoopmeta=sqoopmeta,\n                                                                jdbc=jdbc,\n                                                                hive_db=job['hive_db'],\n                                                                source_table=job['source_table'].upper(),\n                                                                split_by=job['split_by'].upper(),\n                                                                hive_table=job['hive_table'].upper(),\n                                                                check_column=job['datatime_cloumn'].upper()\n                                                            ).replace('    ', '')\n            sjf.write(hivetruncate+createjob)\n            execjob = '''sqoop  job --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop \\\n                    --exec {job_name};\\n'''.format(job_name=job['job_name'].lower()).replace('    ', '')\n            open('./sqoopjob/job-bin/' + job['job_name'].lower() + '.sh', 'w').write(kerboros + execjob)\n            open('./sqoopjob/exec_run.sh', 'a').write(execjob)\n            crontabf.write('''30 02 * * 6 cd /root/hive_import/bin&& ./{job_name}.sh >>../logs/{job_name}.out 2>&1\\n'''.format(\n                    job_name=job['job_name'].lower()))\n\n\n        elif job['partition']=='1' and job['table_type'] in ['fact_i','fact_iud']:\n            #处理带有where条件查询数据\n            shell_cmd='''if [ $# -gt 1 ]; then\n                        yesdate=$1\n                        today=$2\n                        var_len1=`echo ${yesdate} |wc -L`\n                        var_len2=`echo ${today} |wc -L`\n                        if [[ ${var_len1} != 10 || ${var_len2} != 10 ]];then\n                            echo 'vars is wrong'\n                            echo 'var input like:2017-01-01 2017-01-21'\n                            exit\n                        fi\n                    else\n                        yesdate=`date -d \"today -1 day \" +%Y-%m-%d`\n                        today=`date -d today +%Y-%m-%d`\n                    fi\n\n                echo \"data:${yesdate}  ${today}\"\\n'''.replace('    ',' ')\n            createjob = '''hive -e\"use {hive_db};alter table {hive_table} drop if  exists  partition (dt='$yesdate');\n                    alter table {hive_table} add partition(dt='$yesdate') \"\n                    sqoop  import --connect {jdbc} \\\\\n                    --table {source_table} \\\\\n                    --where \"{where} >= date'$yesdate' and {where}<date'$today' \" \\\\\n                    --split-by {split_by} \\\\\n                    --hive-import \\\\\n                    --hive-partition-key dt  \\\\\n                    --hive-partition-value  $yesdate \\\\\n                    --hive-drop-import-delims \\\\\n                    --hive-table {hive_db}.{hive_table} \\\\\n                    --delete-target-dir \\\\\n                    -m 2;\\n'''.format(\n                                            job_name=job['job_name'].lower(),\n                                            hive_db=job['hive_db'],\n                                            sqoopmeta=sqoopmeta,\n                                            jdbc=jdbc,\n                                            source_table=job['source_table'].upper(),\n                                            where=job['datatime_cloumn'],\n                                            split_by=job['split_by'].upper(),\n                                            hive_table=job['hive_table'].upper(),\n                                        ).replace('    ','')\n            #scmd.write(createjob)\n            open('./sqoopjob/partition-bin/'+job['job_name'].lower()+'.sh', 'w').write(shell_cmd+createjob)\n            open('./sqoopjob/exec_run.sh', 'a').write(shell_cmd+createjob)\n            crontabf.write(\n                '''30 02 * * 6 cd /root/hive_import/bin&& ./{job_name}.sh >>../logs/{job_name}.out 2>&1\\n'''.format(\n                    job_name=job['job_name'].lower()))\n        elif job['partition'] == '1' and job['table_type'] in ['dim']:\n            # 处理带有where条件查询数据\n            shell_cmd = '''if [ $# -gt 1 ]; then\n                    yesdate=$1\n                    today=$2\n                    var_len1=`echo ${yesdate} |wc -L`\n                    var_len2=`echo ${today} |wc -L`\n                    if [[ ${var_len1} != 10 || ${var_len2} != 10 ]];then\n                        echo 'vars is wrong'\n                        echo 'var input like:2017-01-01 2017-01-21'\n                        exit\n                    fi\n                else\n                    yesdate=`date -d \"today -1 day \" +%Y-%m-%d`\n                    today=`date -d today +%Y-%m-%d`\n                fi\n\n            echo \"data:${yesdate}  ${today}\"\\n'''.replace('    ',' ')\n            createjob = '''hive -e\"use {hive_db};alter table {hive_table} drop if  exists  partition (dt='$yesdate');\n                       alter table {hive_table} add partition(dt='$yesdate') \"\n                       sqoop  import --connect {jdbc} \\\\\n                       --table {source_table} \\\\\n                       --split-by {split_by} \\\\\n                       --hive-import \\\\\n                       --hive-partition-key dt  \\\\\n                       --hive-partition-value  $yesdate \\\\\n                       --hive-drop-import-delims \\\\\n                       --hive-table {hive_db}.{hive_table} \\\\\n                       --delete-target-dir \\\\\n                       -m 2;\\n'''.format(\n                                                job_name=job['job_name'].lower(),\n                                                hive_db=job['hive_db'],\n                                                sqoopmeta=sqoopmeta,\n                                                jdbc=jdbc,\n                                                source_table=job['source_table'].upper(),\n                                                where=job['datatime_cloumn'],\n                                                split_by=job['split_by'].upper(),\n                                                hive_table=job['hive_table'].upper(),\n                                                ).replace('    ', '')\n            #scmd.write(createjob)\n            open('./sqoopjob/partition-bin/' + job['job_name'].lower() + '.sh', 'w').write(shell_cmd+createjob)\n            open('./sqoopjob/exec_run.sh', 'a').write(shell_cmd+createjob)\n            crontabf.write('''30 02 * * 6 cd /root/hive_import/bin&& ./{job_name}.sh >>../logs/{job_name}.out 2>&1\\n'''.format(\n                    job_name=job['job_name'].lower()))\nsjf.close()\nhcf.close()\nimcf.close()\ndf.close()\n#cf.close()\nprint('脚本生成结束,详见./sqoopjob/*')\n\nif __name__=='__main__':\n#生成json文件\njson_make(input_file='./tablelist')\n#生成sqoop脚本\ncreate_job(tablelist='tablelist.json')\n\n\n\n\n下面是生成hbase任务的脚本\n\n!/usr/bin/python3\ncoding=utf-8\nimport json,osdef json_make(input_file='./hbase_tablelist'):\n#input_file ='./sqoopjob/tablelist'\nlines = open(input_file, \"r\",encoding=\"utf_8_sig\").readlines()\n[lines.remove(i) for i in lines if i in ['', '\\n']]\nlines = [line.strip() for line in lines]\n\n# 获取键值\nkeys = lines[0].split('\\t')\nline_num = 1\ntotal_lines = len(lines)\nparsed_datas = []\nwhile line_num < total_lines:\n        values = lines[line_num].split(\"\\t\")\n        parsed_datas.append(dict(zip(keys, values)))\n        line_num = line_num + 1\njson_str = json.dumps(parsed_datas, ensure_ascii=False, indent=4)\noutput_file = input_file+'.json'\n\n# write to the file\nf = open(output_file, \"w\", encoding=\"utf-8\")\nf.write(json_str)\nf.close()\nprint('json格式转换结束！详见%s'%(output_file))\n\n\ndef create_job(tablelist):\nif os.path.exists('sqoopjob'):\n    os.system('rm -fr sqoopjob/*')\n    os.system('mkdir  sqoopjob/hbase-bin')\nelse:\n    os.mkdir('sqoopjob')\nsqoopmeta='jdbc:hsqldb:hsql://localhost:16000/sqoop'\njdbc='jdbc:oracle:thin:@10.90.87.35:11521:bdccdb2 --username sqoopuser --password Bigdata_2016'\nsjf=open('sqoopjob/createjob.sh', \"w\")\ncrontabf=open('./sqoopjob/crontab.cron', 'w')\ndf=open('./sqoopjob/deletejob.sh', 'w')\nkerboros='kinit -kt /keytab/sqoopdba.keytab sqoopdba \\n'\n\nwith open(tablelist, 'r') as load_f:\n    load_dict = json.load(load_f)\n    for job in load_dict:\n        if job['table_type'].lower()=='dim' and job['partition']=='0':\n            #处理档案表\n            createjob = '''sqoop  job --create {job_name} --meta-connect {sqoopmeta} \\\\\n                    -- import --connect {jdbc} \\\\\n                    --table {source_table} \\\\\n                    --incremental append \\\\\n                    --split-by {split_by} \\\\\n                    --hbase-create-table \\\\\n                    --hbase-table {hive_table} \\\\\n                    --check-column  {check_column} \\\\\n                    --last-value '2011-01-01 11:00:00' \\\\\n                    --hbase-row-key {row_key} \\\\\n                    --column-family cf \\\\\n                    -m 2;\\n'''.format(\n                    job_name=job['job_name'].lower(),\n                    sqoopmeta=sqoopmeta,\n                    jdbc=jdbc,\n                    source_table=job['source_table'].upper(),\n                    split_by=job['split_by'].upper(),\n                    hive_table=job['hive_table'].upper(),\n                    check_column=job['datatime_cloumn'].upper(),\n                    row_key=job['row_key']\n                    ).replace('    ','')\n\n            execjob = '''sqoop  job --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop \\\n                    --exec {job_name};\\n'''.format(job_name=job['job_name'].lower()).replace('    ','')\n\n            deledrop = '''sqoop  job --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop \\\n                    --delete {job_name};\\n'''.format(job_name=job['job_name'].lower() ).replace('    ','')\n\n            createtable=''' CREATE EXTERNAL TABLE {hive_table}({columns})\nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'WITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,cf:LAST_ANALYZED,cf:SAMPLE_SIZE,cf:CHARACTER_SET_NAME\")TBLPROPERTIES(\"hbase.table.name\" = \"{hive_table}\", \"hbase.mapred.output.outputtable\" = \"{hive_table}\");'''.format(hive_table=job['hive_table'],\n       columns=job['colums'].split(',').join())\n            #print(sjf)\n            sjf.write(createjob)\n\n            df.write(deledrop)\n            open('./sqoopjob/hbase-bin/' + job['job_name'].lower() + '.sh', 'w').write(execjob)\n            crontabf.write('''30 02 * * 6 cd /root/hive_import/bin&& ./{job_name}_hql.sh >>../logs/{job_name}_hql.out 2>&1\\n'''.format(\n                job_name=job['job_name'].lower()))\n            execjob = '''sqoop  job --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop \\\n                    --exec {job_name};\\n'''.format(job_name=job['job_name'].lower()).replace('    ','')\n            open('./sqoopjob/exec_run.sh', 'a').write(execjob)\n        else :pass\n\nsjf.close()\ndf.close()\n#cf.close()\nprint('脚本生成结束,详见./sqoopjob/*')\n\nif __name__=='__main__':\n#生成json文件\njson_make(input_file='./hbase_tablelist')\n#生成sqoop脚本\ncreate_job(tablelist='./hbase_tablelist.json')\n\n\n\nhbase任务\n!/usr/bin/python3\ncoding=utf-8\nimport json,osdef json_make(input_file='./hbase_tablelist'):\n#input_file ='./sqoopjob/tablelist'\nlines = open(input_file, \"r\",encoding=\"utf_8_sig\").readlines()\n[lines.remove(i) for i in lines if i in ['', '\\n']]\nlines = [line.strip() for line in lines]\n\n# 获取键值\nkeys = lines[0].split('\\t')\nline_num = 1\ntotal_lines = len(lines)\nparsed_datas = []\nwhile line_num < total_lines:\n        values = lines[line_num].split(\"\\t\")\n        parsed_datas.append(dict(zip(keys, values)))\n        line_num = line_num + 1\njson_str = json.dumps(parsed_datas, ensure_ascii=False, indent=4)\noutput_file = input_file+'.json'\n\n# write to the file\nf = open(output_file, \"w\", encoding=\"utf-8\")\nf.write(json_str)\nf.close()\nprint('json格式转换结束！详见%s'%(output_file))\n\n\ndef create_job(tablelist):\nif os.path.exists('sqoopjob'):\n    os.system('rm -fr sqoopjob/*')\n    os.system('mkdir  sqoopjob/hbase-bin')\nelse:\n    os.mkdir('sqoopjob')\nsqoopmeta='jdbc:hsqldb:hsql://localhost:16000/sqoop'\njdbc='jdbc:oracle:thin:@10.90.87.35:11521:bdccdb2 --username sqoopuser --password Bigdata_2016'\nsjf=open('sqoopjob/createjob.sh', \"w\")\ncrontabf=open('./sqoopjob/crontab.cron', 'w')\ndf=open('./sqoopjob/deletejob.sh', 'w')\ncreatetablef=open('./sqoopjob/createtable.sh', 'w')\nkerboros='kinit -kt /keytab/sqoopdba.keytab sqoopdba \\n'\n\nwith open(tablelist, 'r') as load_f:\n    load_dict = json.load(load_f)\n    for job in load_dict:\n        if job['table_type'].lower()=='dim' and job['partition']=='0':\n            #处理档案表\n            createjob = '''sqoop  job --create {job_name} --meta-connect {sqoopmeta} \\\\\n                    -- import --connect {jdbc} \\\\\n                    --table {source_table} \\\\\n                    --incremental append \\\\\n                    --split-by {split_by} \\\\\n                    --hbase-create-table \\\\\n                    --hbase-table {hive_table} \\\\\n                    --check-column  {check_column} \\\\\n                    --last-value '2011-01-01 11:00:00' \\\\\n                    --hbase-row-key {row_key} \\\\\n                    --column-family cf \\\\\n                    -m 2;\\n'''.format(\n                    job_name=job['job_name'].lower(),\n                    sqoopmeta=sqoopmeta,\n                    jdbc=jdbc,\n                    source_table=job['source_table'].upper(),\n                    split_by=job['split_by'].upper(),\n                    hive_table=job['hive_table'].upper(),\n                    check_column=job['datatime_cloumn'].upper(),\n                    row_key=job['row_key']\n                    ).replace('    ','')\n\n            execjob = '''sqoop  job --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop \\\n                    --exec {job_name};\\n'''.format(job_name=job['job_name'].lower()).replace('    ','')\n\n            deledrop = '''sqoop  job --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop \\\n                    --delete {job_name};\\n'''.format(job_name=job['job_name'].lower() ).replace('    ','')\n\n            createtable='''hive -e \"use{hive_db};CREATE EXTERNAL TABLE {hive_table}_external(key string,{columns_hive})\n                        STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n                        WITH SERDEPROPERTIES ('hbase.columns.mapping' = '{columns_hbase}')\n                        TBLPROPERTIES('hbase.table.name' = '{hive_table}',\n                        'hbase.mapred.output.outputtable' = '{hive_table}')\";\\n '''.format(\n                                    hive_table=job['hive_table'],\n                                    hive_db=job['hive_db'],\n                                    columns_hive=' string,'.join(job['columns'].split(','))+' string',\n                                    columns_hbase=':key,cf:'+',cf:'.join(job['columns'].split(','))\n                                    ).replace('    ','')\n            sjf.write(createjob)\n            createtablef.write(createtable)\n            df.write(deledrop)\n            open('./sqoopjob/hbase-bin/' + job['job_name'].lower() + '.sh', 'w').write(execjob)\n            crontabf.write('''30 02 * * 6 cd /root/hive_import/bin&& ./{job_name}_hql.sh >>../logs/{job_name}_hql.out 2>&1\\n'''.format(\n                job_name=job['job_name'].lower()))\n            execjob = '''sqoop  job --meta-connect jdbc:hsqldb:hsql://localhost:16000/sqoop \\\n                    --exec {job_name};\\n'''.format(job_name=job['job_name'].lower()).replace('    ','')\n            open('./sqoopjob/exec_run.sh', 'a').write(execjob)\n        else :pass\n\nsjf.close()\ndf.close()\n#cf.close()\nprint('脚本生成结束,详见./sqoopjob/*')\n\nif __name__=='__main__':\n#生成json文件\njson_make(input_file='./hbase_tablelist')\n#生成sqoop脚本\ncreate_job(tablelist='./hbase_tablelist.json')\n\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}