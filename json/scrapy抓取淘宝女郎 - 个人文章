{"title": "scrapy抓取淘宝女郎 - 个人文章 ", "index": "python,scrapy", "content": "scrapy抓取淘宝女郎\n准备工作\n\n\n首先在淘宝女郎的首页这里查看，当然想要爬取更多的话，当然这里要查看翻页的url,不过这操蛋的地方就是这里的翻页是使用javascript加载的，这个就有点尴尬了，找了好久没有找到，这里如果有朋友知道怎样翻页的话，麻烦告诉我一声，谢谢了...，不过就这样坐以待毙了吗，所以就在chrome上搜，结果看到有人直接使用的这个网页,我当时一看感觉神奇的样子，这就是简化版的首页啊，只需要改变page的数字就可以实现遍历了，不过还是有点小失落，为什么人家就能找到呢，这个我还是希望知道的朋友能够分享一下，我也会查看相关的资料，把这个空缺不上的，好了，现在开我们的工作了\n我们的目的是抓取册以及相关的信息，所以我们需要随便打开一个淘女郎的相册页面，然后随便进入一个相册即可，很显然这里的相册是异步加载的，因此我们需要抓包，这里我抓到了含有相册的url以及相关信息的json数据，如下图：\n\n\n\n然后我们查看它的url为https://mm.taobao.com/album/j...\n通过我尝试之后这条url可以简化为:\nhttps://mm.taobao.com/album/j...{0}&album_id={1}&top_pic_id=0&page={2}&_ksTS=1493654931946_405\n其中user_id是每一个女郎对的id,ablum_id时每一个相册的id,这里一个女郎有多个相册，因此这个id是不同的，但是page就是要翻页的作用了,可以看到我去掉了callback=json155这一项，因为如果加上这一项，返回的数据就不是json类型的数据，其中page是在抓包的时候点击翻页才会在下面出现的，可以看到同一个相册返回的除了page不同之外，其他的都是相同的，因此这里通过page来实现翻页的数据\n上面分析了每一个相册的url数据的由来，可以看到我们下面需要找到user_id,ablum_id这两个数据.\nuser_id的获取：我们打开首页,然后打开chrome的调试工具，可以看到每一个 女郎的url中都包含user_id这一项，因此我们只需要通过这个实现翻页然后获取每一个女郎的url,之后用正则将user_id匹配出来即可,代码如下\n\n\nps=response.xpath('//p[@class=\"top\"]')\n        for p in ps:\n            item=JrtItem()\n            href=p.xpath('a/@href').extract()   #这个得到的是一个数组url\n            if href:\n                item['user_id']=self.pattern_user_id.findall(href[0])[0]   #用则正匹配出user_id,其中的正则为  pattern_user_id=re.compile(r'user_id=(\\d+)')\n\n\nablum_id的获取：想要获取ablum_id当然要在相册的页面查找，于是我们在相册页面抓包获得了如下图的页面\n\n\n通过上图我们清晰的知道每一个相册的里面包含多少相册，但最令人开心的是在这个页面中不是动态加载，因此我们可以查看它的源码，当我们查看源码的时候，我们可以看到和user_id一样，这里的ablum_id包含在了href中，因此我们只需要找到每一张相册的url，然后用正则匹配处来即可，其中这个页面的url简化为:\nhttps://mm.taobao.com/self/al...{0}&page={1}\n所以我们可以通过上面得到的user_id构成请求即可,代码如下：\n\n\n ## 解析得到ablum_id，根据ablum_id解析请求出每一个相册的json数据\n    def parse_ablum_id(self,response):\n        if response.status==200:\n            print response.url\n            item = response.meta['item']\n            soup = BeautifulSoup(response.text, 'lxml')\n            divs = soup.find_all('div', class_='mm-photo-cell-middle')\n            for div in divs:\n                href = div.find('h4').find('a').get('href')\n                items = self.pattern_ablum_id.findall(href)    #这里就得到了ablum_id\n上面已经获得了user_id和ablum_id，那么现在就可以请求每一个相册的json数据了，这个就不用多说了，详情请看源代码\nMongoDB存储\n安装方式\n\nWindows下安装请看我的MogoDB干货篇\nubantu直接用sudo apt-get install安装即可\n安装对应python的包：pip install pymongo\n安装完成以后就可以连接了，下面贴出我的连接代码\n\nfrom pymongo import MongoClient\n\nclass MongoDBPipelines(object):\n    collection_name = 'taobao'\n    def open_spider(self,spider):\n        self.client = MongoClient('localhost', 27017)   #连接，这里的27017是默认的额端口号\n        self.db = self.client['python']               #python是自己的数据库，当然这里不用提前建好也可以\n\n    def close_spider(self, spider):            \n        self.client.close()\n\n    def process_item(self,item,spider):\n        self.db[self.collection_name].update({'picId':item['picId']}, {'$set': dict(item)}, True)\n        return item\n现在这就算好了，当然这里还有很多东西需要优化的，像代理池。。。最后本人想在暑期找一个地方实习，但是一直没有好的地方，希望有实习的地方推荐的可以联系我，在这里先谢谢了\n推荐\n\n最后推荐博主的一些源码\n\nscrapy爬取妹子网站实现图片的存储，这个只是一个演示怎样存储图片的实例，因此不想过多的投入时间去爬，因此写的不是很详细\nscrapy爬取知乎用户的详细信息\n\n\n更多文章请一步本人博客\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "2"}