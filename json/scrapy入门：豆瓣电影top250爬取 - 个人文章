{"title": "scrapy入门：豆瓣电影top250爬取 - 个人文章 ", "index": "python,scrapy", "content": "本文内容\n\n爬取豆瓣电影Top250页面内容，字段包含：排名，片名，导演，一句话描述 有的为空，评分，评价人数，上映时间，上映国家，类别\n抓取数据存储\n\nscrapy介绍\nScrapy爬虫框架教程（一）-- Scrapy入门\n创建项目\nscrapy startproject dbmovie\n创建爬虫\ncd dbmoive\nscarpy genspider dbmovie_spider movie.douban.com/top250\n注意，爬虫名不能和项目名一样\n应对反爬策略的配置\n\n\n打开settings.py文件，将ROBOTSTXT_OBEY修改为False。\nROBOTSTXT_OBEY = False\n\n\n修改User-Agent\nDEFAULT_REQUEST_HEADERS = {\n    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n    'Accept-Language': 'en',\n    'Accept-Encoding' :  'gzip, deflate, br',\n    'Cache-Control' :  'max-age=0',\n    'Connection' :  'keep-alive',\n    'Host' :  'movie.douban.com',\n    'Upgrade-Insecure-Requests' :  '1',\n    'User-Agent' :  'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36',\n}\n\n\n运行爬虫\nscrapy crawl dbmovie_spider\n定义item\n根据前面的分析，我们需要抓取一共十个字段的信息，现在在items.py文件中定义item\nimport scrapy\n\nclass DoubanItem(scrapy.Item):\n    # 排名\n    ranking = scrapy.Field()\n    # 篇名 \n    title = scrapy.Field()\n    # 导演\n    director = scrapy.Field()\n    # 一句话描述 有的为空\n    movie_desc = scrapy.Field()\n    # 评分\n    rating_num = scrapy.Field()\n    # 评价人数\n    people_count = scrapy.Field()\n    # 上映时间\n    online_date = scrapy.Field()\n    # 上映国家\n    country = scrapy.Field()\n    # 类别\n    category = scrapy.Field()\n字段提取\n这里需要用到xpath相关知识，偷了个懒，直接用chrome插件获取Chrome浏览器获取XPATH的方法----通过开发者工具获取\ndef parse(self, response):\n    item = DoubanItem()\n    movies = response.xpath('//div[@class=\"item\"]')\n    for movie in movies:\n        # 名次\n        item['ranking'] = movie.xpath('div[@class=\"pic\"]/em/text()').extract()[0]\n        # 片名 提取多个片名\n        titles = movie.xpath('div[@class=\"info\"]/div[1]/a/span/text()').extract()[0]\n        item['title'] = titles\n        # 获取导演信息\n        info_director = movie.xpath('div[2]/div[2]/p[1]/text()[1]').extract()[0].replace(\"\\n\", \"\").replace(\" \", \"\").split('\\xa0')[0]\n        item['director'] = info_director\n        # 上映日期\n        online_date = movie.xpath('div[2]/div[2]/p[1]/text()[2]').extract()[0].replace(\"\\n\", \"\").replace('\\xa0', '').split(\"/\")[0].replace(\" \", \"\")\n        # 制片国家\n        country = movie.xpath('div[2]/div[2]/p[1]/text()[2]').extract()[0].replace(\"\\n\", \"\").split(\"/\")[1].replace('\\xa0', '')\n        # 影片类型\n        category = movie.xpath('div[2]/div[2]/p[1]/text()[2]').extract()[0].replace(\"\\n\", \"\").split(\"/\")[2].replace('\\xa0', '').replace(\" \", \"\")\n        item['online_date'] = online_date\n        item['country'] = country\n        item['category'] = category\n        movie_desc = movie.xpath('div[@class=\"info\"]/div[@class=\"bd\"]/p[@class=\"quote\"]/span/text()').extract()\n        if len(movie_desc) != 0:  # 判断info的值是否为空，不进行这一步有的电影信息并没有会报错或数据不全\n            item['movie_desc'] = movie_desc\n        else:\n            item['movie_desc'] = ' '\n\n        item['rating_num'] = movie.xpath('div[@class=\"info\"]/div[@class=\"bd\"]/div[@class=\"star\"]/span[@class=\"rating_num\"]/text()').extract()[0]\n        item['people_count'] = movie.xpath('div[@class=\"info\"]/div[@class=\"bd\"]/div[@class=\"star\"]/span[4]/text()').extract()[0]\n        yield item\n    # 获取下一页\n    next_url = response.xpath('//span[@class=\"next\"]/a/@href').extract()\n    \n    if next_url:\n        next_url = 'https://movie.douban.com/top250' + next_url[0]\n        yield scrapy.Request(next_url, callback=self.parse, dont_filter=True)\n存储数据，mysql\n注意1064错误，表中字段包含mysql关键字导致Scrapy入门教程之写入数据库\nimport pymysql\n\ndef dbHandle():\n    conn = pymysql.connect(\n        host='localhost',\n        user='root',\n        passwd='pwd',\n        db=\"dbmovie\",\n        charset='utf8',\n        use_unicode=False\n    )\n    return conn\n\nclass DoubanPipeline(object):\n    def process_item(self, item, spider):\n        dbObject = dbHandle()\n        cursor = dbObject.cursor()\n        sql = \"insert into db_info(ranking,title,director,movie_desc,rating_num,people_count,online_date,country,category) values(%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n\n        try:\n            cursor.execute(sql, (item['ranking'], item['title'], item['director'], item['movie_desc'], item['rating_num'], item['people_count'], item['online_date'], item['country'], item['category']))\n            dbObject.commit()\n        except Exception as e:\n            print(e)\n            dbObject.rollback()\n\n        return item\n简单应对爬虫策略\nScrapy爬虫——突破反爬虫最全策略解析\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}