{"title": "Scrapy+Chromium+代理+selenium - 小翰学习周记 ", "index": "代理,chromium,scrapy,python", "content": "上周说到scrapy的基本入门。这周来写写其中遇到的代理和js渲染的坑。\njs渲染\njs是爬虫中毕竟麻烦处理的一块。通常的解决办法是通过抓包，然后查看request信息，接着捕获ajax返回的消息。但是，如果遇到一些js渲染特别复杂的情况，这种办法就非常非常的麻烦。所以我们采用了selenium这个包，用它来调用chromium完成js渲染的问题。\n安装\n\n安装selenium\n\n安装chromium\n\n安装chromium-drive\n\ntip:为什么选择chromium而不是chrome。我之前装的就是chrome。但是安装chrome之后还需要安装chrome-drive，而很多linux发行版的包管理没有现成的chrome包和chrome-drive包，自己去找的话很容易出现chrome-drive和chrome版本不一致而导致不能使用。\n为了减少因为安装环境所带来的烦恼。我们这边用docker来解决。Dockerfile\nFROM alpine:3.8\nCOPY requirements.txt /tmp\nRUN apk update \\\n    && apk add --no-cache xvfb python3 python3-dev curl libxml2-dev libxslt-dev libffi-dev gcc musl-dev \\\n    && apk add --no-cache libgcc openssl-dev chromium=68.0.3440.75-r0 libexif udev chromium-chromedriver=68.0.3440.75-r0 \\\n    && curl https://bootstrap.pypa.io/get-pip.py | python3 \\\n    && adduser -g chromegroup -D chrome \\\n    && pip3 install -r /tmp/requirements.txt && rm /tmp/requirements.txt\nUSER chrome\ntip：这边还有一个坑，chrome和chromium都不能在root模式下运行，而且也不安全。所以最好是创建一个用户来运行。使用docker的时候，run时候需要加--privileged参数\n\n如果你需要了解如何在root用户下运行chrome，请阅读这篇博文Ubuntu16.04安装Chrome浏览器及解决root不能打开的问题\nrequirements.txt\nScrapy\nselenium\nTwisted\nPyMysql\npyvirtualdisplay\n把requirements.txt和Dockerfile放在一起。并在目录下使用docker命令docker build -t \"chromium-scrapy-image\" .\n至于为什么要安装xvfb和pyvirtualdisplay。因为chromium的headless模式下不能处理带账号密码的问题。待会就会说到了。\n\nRedhat和Debian可以去包仓库找一下最新的chromium和对应的chromium-drive下载安装就可以了。版本一定要是对应的！这边使用chromium=68.0.3440.75-r0和chromium-chromedriver=68.0.3440.75-r0。\n\n修改Scrapy的Middleware\n\n使用了chromium之后，我们在middlewares.py文件修改一下。我们的设想是让chromium来替代掉request请求。所以我们修改了DownloaderMiddleware\n#DownloaderMiddleware\nclass DemoDownloaderMiddleware(object):\n    def __init__(self):\n        chrome_options = webdriver.ChromeOptions()\n        # 启用headless模式\n        chrome_options.add_argument('--headless')\n        # 关闭gpu\n        chrome_options.add_argument('--disable-gpu')\n        # 关闭图像显示\n        chrome_options.add_argument('--blink-settings=imagesEnabled=false') \n        self.driver = webdriver.Chrome(chrome_options=chrome_options)\n        \n    def __del__(self):\n        self.driver.quit()\n        \n    @classmethod\n    def from_crawler(cls, crawler):\n        s = cls()\n        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n        return s\n        \n    def process_request(self, request, spider):\n        # chromium处理\n        # ...\n        return HtmlResponse(url=request.url, \n        body=self.driver.page_source, \n        request=request, \n        encoding='utf-8', \n        status=200)\n        \n    def process_response(self, request, response, spider):\n        # Called with the response returned from the downloader.\n\n        # Must either;\n        # - return a Response object\n        # - return a Request object\n        # - or raise IgnoreRequest\n        return response\n\n    def process_exception(self, request, exception, spider):\n        # Called when a download handler or a process_request()\n        # (from other downloader middleware) raises an exception.\n\n        # Must either:\n        # - return None: continue processing this exception\n        # - return a Response object: stops process_exception() chain\n        # - return a Request object: stops process_exception() chain\n        pass\n\n    def spider_opened(self, spider):\n        spider.logger.info('Spider opened: %s' % spider.name)\n\ntip：这边我们只有一个中间件来处理request。也就是说，所有的逻辑都要经过这儿。所以直接返回了response。\n这就解决了selenium和chromium的安装问题。\n\nchromium不支持headless问题\n如果你安装的chromium版本太老，不支持headless，不着急。之前我们安装的xvfb和pyvirtualdisplay就派上用场了。\nfrom pyvirtualdisplay import Display\n...\n>>>\nchrome_options.add_argument('--headless')\n\n<<<\n# chrome_options.add_argument('--headless')\ndisplay=Display(visible=0,size=(800,800))\ndisplay.start()\n...\n\n>>>\nself.driver.quit()\n\n<<<\nself.driver.quit()\ndisplay.stop()\n...\n我们模拟出了一个显示界面，这个时候，不管chromium开不开启headless，都能在我们的服务器上运行了。\n代理\n因为我们已经用chromium替换了request。所以我们做的代理也不能在Scrapy中来处理。我们需要直接用chromium来处理IP代理问题。\n这是不使用chromium之前使用代理的办法\nclass DemoProxyMiddleware(object):\n    # overwrite process request\n\n    def process_request(self, request, spider):\n        # Set the location of the proxy\n        request.meta['proxy'] = \"https://proxy.com:8080\"\n\n        # Use the following lines if your proxy requires authentication\n        \n        proxy_user_pass = \"username:password\"\n        encoded_user_pass = base64.b64encode(proxy_user_pass.encode('utf-8'))\n\n        # setup basic authentication for the proxy\n        request.headers['Proxy-Authorization'] = 'Basic ' + str(encoded_user_pass, encoding=\"utf-8\")\n\n如果你的IP代理不需要账号密码的话，只需要把后面三行删除了就可以了。\n根据上面这段代码，我们也不难猜出chromium解决代理的方法了。\nchrome_options.add_argument('--proxy=proxy.com:8080')\n只需要加一段argument就可以了。\n那解决带账号密码的办法呢？\n解决chromium下带账号密码的代理问题\n先创建一个py文件\nimport string\nimport zipfile\n\n\ndef create_proxyauth_extension(proxy_host, proxy_port,\n                               proxy_username, proxy_password,\n                               scheme='http', plugin_path=None):\n    \"\"\"代理认证插件\n\n    args:\n        proxy_host (str): 你的代理地址或者域名（str类型）\n        proxy_port (int): 代理端口号（int类型）\n        proxy_username (str):用户名（字符串）\n        proxy_password (str): 密码 （字符串）\n    kwargs:\n        scheme (str): 代理方式 默认http\n        plugin_path (str): 扩展的绝对路径\n\n    return str -> plugin_path\n    \"\"\"\n\n    if plugin_path is None:\n        plugin_path = 'vimm_chrome_proxyauth_plugin.zip'\n\n    manifest_json = \"\"\"\n    {\n        \"version\": \"1.0.0\",\n        \"manifest_version\": 2,\n        \"name\": \"Chrome Proxy\",\n        \"permissions\": [\n            \"proxy\",\n            \"tabs\",\n            \"unlimitedStorage\",\n            \"storage\",\n            \"<all_urls>\",\n            \"webRequest\",\n            \"webRequestBlocking\"\n        ],\n        \"background\": {\n            \"scripts\": [\"background.js\"]\n        },\n        \"minimum_chrome_version\":\"22.0.0\"\n    }\n    \"\"\"\n\n    background_js = string.Template(\n        \"\"\"\n        var config = {\n                mode: \"fixed_servers\",\n                rules: {\n                  singleProxy: {\n                    scheme: \"${scheme}\",\n                    host: \"${host}\",\n                    port: parseInt(${port})\n                  },\n                  bypassList: [\"foobar.com\"]\n                }\n              };\n    \n        chrome.proxy.settings.set({value: config, scope: \"regular\"}, function() {});\n    \n        function callbackFn(details) {\n            return {\n                authCredentials: {\n                    username: \"${username}\",\n                    password: \"${password}\"\n                }\n            };\n        }\n    \n        chrome.webRequest.onAuthRequired.addListener(\n                    callbackFn,\n                    {urls: [\"<all_urls>\"]},\n                    ['blocking']\n        );\n        \"\"\"\n    ).substitute(\n        host=proxy_host,\n        port=proxy_port,\n        username=proxy_username,\n        password=proxy_password,\n        scheme=scheme,\n    )\n    with zipfile.ZipFile(plugin_path, 'w') as zp:\n        zp.writestr(\"manifest.json\", manifest_json)\n        zp.writestr(\"background.js\", background_js)\n\n    return plugin_path\n使用方式\n    proxyauth_plugin_path = create_proxyauth_extension(\n        proxy_host=\"host\",\n        proxy_port=port,\n        proxy_username=\"user\",\n        proxy_password=\"pwd\")\n    chrome_options.add_extension(proxyauth_plugin_path)\n这样就完成了chromium的代理了。但是，如果你开启了headless模式，这个方法会提示错误。所以解决办法就是，关闭headless模式。至于怎么在没有gui的情况下使用chromium。在之前已经提到过，使用xvfb和pyvirtualdisplay就可以了。\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}