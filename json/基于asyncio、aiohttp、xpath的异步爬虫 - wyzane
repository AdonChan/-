{"title": "基于asyncio、aiohttp、xpath的异步爬虫 - wyzane ", "index": "xpath,asyncio,python", "content": "今天介绍一下基于asyncio和aiohttp的异步爬虫的编写，解析html用的是xpath。\n该爬虫实现了以下功能:1.读取csv文件中的爬取规则，根据规则爬取数据；代码中添加了对3个网站的不同提取规则，如有需要，还可以继续添加；2.将爬取到的数据保存到mysql数据库中。\n通过输入问题，该爬虫能爬取关于健康方面的数据。\n具体代码如下:\n# coding:utf-8\n\n\n\"\"\"\nasync-apiser xpath\n\"\"\"\n\n\nfrom lxml import etree\nimport csv\nimport re\nimport os\nimport asyncio\nimport aiohttp\nimport aiomysql\nfrom datetime import datetime\n\nfrom config import Config\n\n\nclass HealthSpider(object):\n\n    def __init__(self, user_id, keyword, url, hrule, drule, count, trule):\n        self.user_id = user_id\n        self.keyword = keyword\n        self.url = url\n        self.hrule = hrule\n        self.drule = drule\n        self.count = count\n        self.trule = trule\n        self.headers = ''\n        self.urls_done = []\n        self.urls_will = []\n        self.spider_data = {}\n\n    @staticmethod\n    def handle_flag(str):\n        \"\"\"\n        去除字符串中的style样式标签\n        :param html:\n        :return:\n        \"\"\"\n        pattern = re.compile(r' style=\".*?;\"', re.S)\n        return pattern.sub('', str)\n\n    async def get_html(self, url, session):\n        \"\"\"\n        根据url，返回html\n        :param url:\n        :return:\n        \"\"\"\n        try:\n            async with session.get(url, headers=self.headers, timeout=5) as resp:\n                if resp.status in [200, 201]:\n                    data = await resp.text()\n                    return data\n        except Exception as e:\n            raise Exception(\"数据搜索错误\")\n\n    def get_url(self, resp):\n        \"\"\"\n        根据html获取每条数据的url\n        :param resp:\n        :return:\n        \"\"\"\n        # 保存爬取的数据\n        root = etree.HTML(str(resp))\n        items = root.xpath(self.hrule)\n        # html结构不同，组织url的方式也不同\n        if 5 == self.count:\n            self.urls_will = ['https://dxy.com' + i for i in items[:5]]\n        elif 3 == self.count:\n            self.urls_will = [i for i in items[:3]]\n        elif 2 == self.count:\n            self.urls_will = [i for i in items[:2]]\n\n    async def get_data(self, url, session, pool):\n        \"\"\"\n        根据url获取具体数据\n        :return:\n        \"\"\"\n        # 根据url解析出htm\n        html = await self.get_html(url, session)\n        # 保存爬取的数据\n        root = etree.HTML(str(html))\n        html_data = ''\n        try:\n            title = root.xpath(self.trule)\n            title = ''.join(title)\n        except Exception as e:\n            title = ''\n        try:\n            data = root.xpath(self.drule)\n            if data:\n                # html结构不同，获取数据的方式也不同\n                if 3 == self.count:\n                    html_data = ''.join(map(etree.tounicode, data))\n                    # 去除结果中的style标签\n                    html_data = HealthSpider.handle_flag(html_data)\n                else:\n                    html_data = etree.tounicode(data[0])\n                    html_data = HealthSpider.handle_flag(html_data)\n        except Exception as e:\n            html_data = []\n\n        self.urls_done.append(url)\n        # 数据入库,保存：用户id, 关键词, 日期, 主url, 子url, html数据\n        if html_data:\n            self.spider_data[\"data\"].append({\"title\": title, \"html_data\": html_data})\n            spide_date = datetime.now()\n            data = (self.user_id, self.keyword, spide_date, self.url, url, title, html_data)\n            stmt = \"INSERT INTO spider_data (user_id, keyword, spide_date,  main_url, sub_url, title, html_data) \" \\\n                   \"VALUES (%s, %s, %s, %s, %s, %s, %s)\"\n            try:\n                async with pool.acquire() as conn:\n                    async with conn.cursor() as cur:\n                        await cur.execute(stmt, data)\n            except Exception as e:\n                pass\n\n    async def start_spider(self, pool):\n        \"\"\"\n        开始爬取数据\n        :return:\n        \"\"\"\n        async with aiohttp.ClientSession() as session:\n            self.spider_data[\"user_id\"] = self.user_id\n            self.spider_data[\"keyword\"] = self.keyword\n            self.spider_data[\"data\"] = []\n            while True:\n                # 待爬取url队列为空或者已经爬取3条数据,则停止爬取\n                if (len(self.urls_will) == 0) or len(self.spider_data[\"data\"]) == self.count:\n                    break\n                # 获取待爬url\n                url = self.urls_will.pop()\n                # 开始爬取数据\n                if url not in self.urls_done:\n                    await self.get_data(url, session, pool)\n            return self.spider_data\n\n    async def main(self, loop):\n        # 请求头\n        self.headers = {'Accept': 'text/html, application/xhtml+xml, application/xml;q=0.9,*/*;q=0.8',\n                        'Accept-Encoding': 'gzip, deflate',\n                        'Accept-Language': 'zh-Hans-CN, zh-Hans; q=0.5',\n                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n                                      '(KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36 Edge/15.15063'\n                        }\n\n        # 连接mysql数据库\n        pool = await aiomysql.create_pool(host=Config.DB_HOST, port=Config.DB_PORT,\n                                          user=Config.DB_USER, password=Config.DB_PASSWORD,\n                                          db=Config.DB_NAME, loop=loop, charset=\"utf8\", autocommit=True)\n        async with aiohttp.ClientSession() as session:\n            # 首次获取html\n            html = await self.get_html(self.url, session)\n            # 获取url\n            self.get_url(html)\n        data = await self.start_spider(pool)\n        return data\n        # asyncio.ensure_future(self.start_spider(pool))\n\n\ndef get_rules(keyword):\n    \"\"\"\n    获取csv中的xpath规则\n    :return:\n    \"\"\"\n    csv_dict = []\n    path = os.path.join(os.path.dirname(__file__), 'rules.csv')\n    with open(path, 'rU') as f:\n        reader = csv.DictReader(f)\n        for line in reader:\n            url = line['url'].format(keyword)\n            hrule = line['hrule']\n            drule = line['drule']\n            count = int(line['count'])\n            title = line['trule']\n            csv_dict.append({\"url\": url, \"hrule\": hrule, \"drule\": drule, \"count\": count, \"trule\": title})\n    return csv_dict\n\n\ndef start_spider(keyword):\n    \"\"\"\n    爬取数据\n    :param user_id:\n    :param keyword:\n    :return:\n    \"\"\"\n    try:\n        data_list = get_rules(keyword)\n    except Exception as e:\n        raise Exception(\"搜索规则获取失败\")\n    spider_data = []\n    tasks = []\n    loop = asyncio.get_event_loop()\n    for i in data_list:\n        spider = HealthSpider(1, keyword, i['url'], i['hrule'], i['drule'], i['count'], i['trule'])\n        # 任务列表\n        tasks.append(asyncio.ensure_future(spider.main(loop)))\n        # 添加到loop\n        loop.run_until_complete(asyncio.wait(tasks))\n\n    try:\n        for task in tasks:\n            for i in range(len(task.result()[\"data\"])):\n                spider_data.append(task.result()[\"data\"][i])\n    except Exception as e:\n        pass\n    # 延时以等待底层打开的连接关闭\n    loop.run_until_complete(asyncio.sleep(0.250))\n    loop.close()\n    return spider_data\n\n\nif __name__ == '__main__':\n    # 爬取感冒了怎么办相关内容\n    start_spider(\"感冒了怎么办\")\n    \n    \n下面讲一下代码中某些方法的作用:1.handle_flag()方法用于去掉html字符串中的style样式标签，保留html中的其他标签，便于前端的展示；2.get_data()方法用于爬取具体数据，并使用aiomysql将爬取道德数据保存到数据库；数据库的配置文件config.py:\n# coding=utf-8\n\n\nclass Config(object):\n    DB_ENGINE = 'mysql'\n    DB_HOST = '127.0.0.1'\n    DB_PORT = 3306\n    DB_USER = 'root'\n    DB_PASSWORD = 'wyzane'\n    DB_NAME = 'db_tornado'\n    DB_OPTIONS = {\n        'init_command': \"SET sql_mode='STRICT_TRANS_TABLES'\",\n        'charset': 'utf8mb4',\n    }\n3.get_rules()方法用于从rules.csv文件中读取爬取的规则。因为这里同时爬取了3个不同的网站，由于每个网站解析html的xpath规则不同，并且每个网站提取的数据条数不同，所以把这些规则写到了rules.csv文件(就是一个excel文件)中。先读取规则，再爬取数据。\n以上就是基于asyncio的异步爬虫的代码，如有错误，欢迎交流指正！\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "1"}