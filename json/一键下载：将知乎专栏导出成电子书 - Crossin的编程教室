{"title": "一键下载：将知乎专栏导出成电子书 - Crossin的编程教室 ", "index": "python", "content": "老是有同学问，学了 Python 基础后不知道可以做点什么来提高。今天就再用个小例子，给大家讲讲，通过 Python 和 爬虫 ，可以完成怎样的小工具。  \n在知乎上，你一定关注了一些不错的专栏（比如 Crossin的编程教室）。但万一有那么一天，你喜欢的答主在网上被人喷了，一怒之下删帖停更，这些好内容可就都看不到了。尽管这是小概率事件（可也不是没发生过），但未雨绸缪，你可以把关注的专栏导出成电子书，这样既可以离线阅读，又不怕意外删帖了。\n只是需要工具和源码的可以拉到文章底部获取代码。\n【最终效果】\n运行程序，输入 专栏的 id ，也就是网页地址上的路径：\n\n之后程序便会自动抓取专栏中的文章，并按发布时间合并导出为 pdf 文件。\n\n【实现思路】\n这个程序主要分为三个部分：\n\n抓取专栏文章地址 列表\n\n抓取每一篇文章的 详细内容\n\n导出 PDF\n\n\n1. 抓取列表\n在之前的文章 爬虫必备工具，掌握它就解决了一半的问题 中介绍过如何分析一个网页上的请求。按照其中的方法，我们可以通过 开发者工具 的 Network 功能 找出专栏页面获取详细列表的请求：\n\n\nhttps://www.zhihu.com/api/v4/columns/crossin/articles\n\n\n\n观察返回结果中发现，通过 next 和 is_end 的值，我们能获取下一次列表请求的地址（相当于向下滚动页面的触发效果）以及判断是否已经拿到所有文章。\n而 data 中的 id、title、url 就是我们需要的数据。因为 url 可以通过 id拼出，所以我们的代码里未保存它。\n\n使用一个 while 循环，直到抓取完所有文章的 id 和 title，保存在文件中。\n\n\nwhile True:\n    resp = requests.get(url, headers=headers)\n    j = resp.json()\n    data = j['data']\n    for article in data:\n        # 保存id和title(略)\n    if j['paging']['is_end']:\n        break\n    url = j['paging']['next']\n    # 按 id 排序(略)\n    # 导入文件(略)\n\n\n\n2. 抓取文章\n有了所有文章的 id / url，后面的抓取就很简单了。文章主体内容就在 Post-RichText 的标签中。\n需要稍微花点功夫的是一些文本上的处理，比如原页面的图片效果，会加上 noscript标签和 `、highlight\">\n\n\nurl = 'https://zhuanlan.zhihu.com/p/' + id\nhtml = requests.get(url, headers=headers).text\nsoup = BeautifulSoup(html, 'lxml')\ncontent = soup.find(class_='Post-RichText').prettify()\n# 对content做处理(略)\nwith open(file_name, 'w') as f:\n    f.write(content)\n\n\n\n到这一步，就已经完成了所有内容的抓取，可以在本地阅读了。\n3. 导出 PDF\n为了更便于阅读，我们使用 wkhtmltopdf + pdfkit ，将这些 HTML 文件打包成 PDF。\nwkhtmltopdf 是一个 HTML 转 PDF 的工具，需要单独安装，具体可参考它的官网介绍。\n\nhttps:// wkhtmltopdf.org/downloads.html\nhttps:// github.com/JazzCore/python-pdfkit/wiki/Installing-wkhtmltopdf\n\npdfkit 是对此工具封装的 Python 库，可从 pip 安装：\n\n\npip install pdfkit\n\n\n使用起来很简单：\n\n\n# 获取htmls文件名列表(略)\npdfkit.from_file(sorted(htmls), 'zhihu.pdf')\n\n\n\n这样就完成了整个专栏导出。  \n不仅是知乎专栏，几乎大多数信息类网站，都是通过 1.抓取列表 2.抓取详细内容 这两个步骤来采集数据。因此这个代码稍加修改，即可用在很多别的网站上。只不过有些网站需登录后访问，那么就需要对 headers 里的 cookie 信息进行设置。此外，不同网站的请求接口、参数、限制都不尽相同，所以还是要具体问题具体分析。\n关于这些爬虫的开发技巧，都可以在我们的 爬虫实战 课程中学到。 有需要的请在公众号里回复 爬虫实战\n【源码下载】\n获取知乎专栏下载器源码，请在公众号（ Crossin的编程教室 ）里回复关键字 知乎\n除了代码外， 本专栏打包好的 PDF 也一并奉上，欢迎阅读与分享。\n════\n其他文章及回答：\n如何自学Python | 新手引导 | 精选Python问答 | Python单词表 | 人工智能 | 嘻哈 | 爬虫 | 我用Python | 高考 | requests | AI平台\n欢迎搜索及关注： Crossin的编程教室\n\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "1"}