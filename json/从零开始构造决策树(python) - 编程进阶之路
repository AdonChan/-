{"title": "从零开始构造决策树(python) - 编程进阶之路 ", "index": "python,机器学习", "content": "起步\n本章介绍如何不利用第三方库，仅用python自带的标准库来构造一个决策树。不过这可能需要你之前阅读过这方面的知识。\n前置阅读\n分类算法之决策树（理论篇）\n分类算法之决策树（应用篇）\n本文使用将使用《应用篇》中的训练集，向量特征仅有 0 和 1 两种情况。\n关于熵(entropy)的一些计算\n对于熵，根据前面提到的计算公式:\n$$\nH(X) = -\\sum_{i=1}^np_i\\log_2{(p_i)}\n$$\n对应的 python 代码:\nimport math\nimport collections\n\ndef entropy(rows: list) -> float:\n    \"\"\"\n    计算数组的熵\n    \"\"\"\n    result = collections.Counter()\n    result.update(rows)\n    rows_len = len(rows)\n    assert rows_len   # 数组长度不能为0\n    # 开始计算熵值\n    ent = 0.0\n    for r in result.values():\n        p = float(r) / rows_len\n        ent -= p * math.log2(p)\n    return ent\n条件熵的计算\n根据计算方法：\n$$\nH(Y|X) = \\sum_{i=1}^np_iH(Y|X=x_i)\n$$\n对应的 python 代码:\ndef condition_entropy(future_list: list, result_list: list) -> float:\n    \"\"\"\n    计算条件熵\n    \"\"\"\n    entropy_dict = collections.defaultdict(list)  # {0:[], 1:[]}\n    for future, value in zip(future_list, result_list):\n        entropy_dict[future].append(value)\n    # 计算条件熵\n    ent = 0.0\n    future_len = len(future_list)  # 数据个数\n    for value in entropy_dict.values():\n        p = len(value) / future_len * entropy(value)\n        ent += p\n\n    return ent\n其中参数 future_list 是某一特征向量组成的列表，result_list 是 label 列表。\n信息增益\n根据信息增益的计算方法：\n$$\ngain(A) = H(D) - H(D|A)\n$$\n对应的python代码:\ndef gain(future_list: list, result_list: list) -> float:\n    \"\"\"\n    获取某特征的信息增益\n    \"\"\"\n    info = entropy(result_list)\n    info_condition = condition_entropy(future_list, result_list)\n    return info - info_condition\n定义决策树的节点\n作为树的节点，要有左子树和右子树是必不可少的，除此之外还需要其他信息:\nclass DecisionNode(object):\n    \"\"\"\n    决策树的节点\n    \"\"\"\n    def __init__(self, col=-1, data_set=None, labels=None, results=None, tb=None, fb=None):\n        self.has_calc_index = []    # 已经计算过的特征索引\n        self.col = col              # col 是待检验的判断条件，对应列索引值\n        self.data_set = data_set    # 节点的 待检测数据\n        self.labels = labels        # 对应当前列必须匹配的值\n        self.results = results      # 保存的是针对当前分支的结果，有值则表示该点是叶子节点\n        self.tb = tb                # 当信息增益最高的特征为True时的子树\n        self.fb = fb                # 当信息增益最高的特征为False时的子树\n树的节点会有两种状态，叶子节点中 results 属性将保持当前的分类结果。非叶子节点中， col 保存着该节点计算的特征索引，根据这个索引来创建左右子树。\nhas_calc_index 属性表示在到达此节点时，已经计算过的特征索引。特征索引的数据集上表现是列的形式，如数据集（不包含结果集）:\n[\n    [1, 0, 1],\n    [0, 1, 1],\n    [0, 0, 1],\n]\n有三条数据，三个特征，那么第一个特征对应了第一列 [1, 0, 0] ，它的索引是 0 。\n递归的停止条件\n本章将构造出完整的决策树，所以递归的停止条件是所有待分析的训练集都属于同一类:\ndef if_split_end(result_list: list) -> bool:\n    \"\"\"\n    递归的结束条件，每个分支的结果集都是相同的分类\n    \"\"\"\n    result = collections.Counter(result_list)\n    return len(result) == 1\n从训练集中筛选最佳的特征\ndef choose_best_future(data_set: list, labels: list, ignore_index: list) -> int:\n    \"\"\"\n    从特征向量中筛选出最好的特征，返回它的特征索引\n    \"\"\"\n    result_dict = {}  # { 索引: 信息增益值 }\n    future_num = len(data_set[0])\n    for i in range(future_num):\n        if i in ignore_index: # 如果已经计算过了\n            continue\n        future_list = [x[i] for x in data_set]\n        result_dict[i] = gain(future_list, labels) # 获取信息增益\n    # 排序后选择第一个\n    ret = sorted(result_dict.items(), key=lambda x: x[1], reverse=True)\n    return ret[0][0]\n因此计算节点就是调用 best_index = choose_best_future(node.data_set, node.labels, node.has_calc_index) 来获取最佳的信息增益的特征索引。\n构造决策树\n决策树中需要一个属性来指向树的根节点，以及特征数量。不需要保存训练集和结果集，因为这部分信息是保存在树的节点中的。\nclass DecisionTreeClass():\n    def __init__(self):\n        self.future_num = 0      # 特征\n        self.tree_root = None    # 决策树根节点\n创建决策树\n这里需要递归来创建决策树：\ndef build_tree(self, node: DecisionNode):\n    # 递归条件结束\n    if if_split_end(node.labels):\n        node.results = node.labels[0] # 表明是叶子节点\n        return\n    #print(node.data_set)\n    # 不是叶子节点，开始创建分支\n    best_index = choose_best_future(node.data_set, node.labels, node.has_calc_index)\n    node.col = best_index\n\n    # 根据信息增益最大进行划分\n    # 左子树\n    tb_index = [i for i, value in enumerate(node.data_set) if value[best_index]]\n    tb_data_set     = [node.data_set[x] for x in tb_index]\n    tb_data_labels  = [node.labels[x] for x in tb_index]\n    tb_node = DecisionNode(data_set=tb_data_set, labels=tb_data_labels)\n    tb_node.has_calc_index = list(node.has_calc_index)\n    tb_node.has_calc_index.append(best_index)\n    node.tb = tb_node\n\n    # 右子树\n    fb_index = [i for i, value in enumerate(node.data_set) if not value[best_index]]\n    fb_data_set = [node.data_set[x] for x in fb_index]\n    fb_data_labels = [node.labels[x] for x in fb_index]\n    fb_node = DecisionNode(data_set=fb_data_set, labels=fb_data_labels)\n    fb_node.has_calc_index = list(node.has_calc_index)\n    fb_node.has_calc_index.append(best_index)\n    node.fb = fb_node\n\n    # 递归创建子树\n    if tb_index:\n        self.build_tree(node.tb)\n    if fb_index:\n        self.build_tree(node.fb)\n根据信息增益的特征索引将训练集再划分为左右两个子树。\n训练函数\n也就是要有一个 fit 函数:\ndef fit(self, x: list, y: list):\n    \"\"\"\n    x是训练集，二维数组。y是结果集，一维数组\n    \"\"\"\n    self.future_num = len(x[0])\n    self.tree_root = DecisionNode(data_set=x, labels=y)\n    self.build_tree(self.tree_root)\n    self.clear_tree_example_data(self.tree_root)\n清理训练集\n训练后，树节点中数据集和结果集等就没必要的，该模型只要 col 和 result 就可以了：\ndef clear_tree_example_data(self, node: DecisionNode):\n    \"\"\"\n    清理tree的训练数据\n    \"\"\"\n    del node.has_calc_index\n    del node.labels\n    del node.data_set\n    if node.tb:\n        self.clear_tree_example_data(node.tb)\n    if node.fb:\n        self.clear_tree_example_data(node.fb)\n预测函数\n提供一个预测函数:\ndef _predict(self, data_test: list, node: DecisionNode):\n    if node.results:\n        return node.results\n    col = node.col\n    if data_test[col]:\n        return self._predict(data_test, node.tb)\n    else:\n        return self._predict(data_test, node.fb)\n\ndef predict(self, data_test):\n    \"\"\"\n    预测\n    \"\"\"\n    return self._predict(data_test, self.tree_root)\n测试\n数据集使用前面《应用篇》中的向量化的训练集：\nif __name__ == \"__main__\":\n    dummy_x = [\n        [0, 0, 1, 0, 1, 1, 0, 0, 1, 0, ],\n        [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, ],\n        [1, 0, 0, 0, 1, 1, 0, 0, 1, 0, ],\n        [0, 1, 0, 0, 1, 0, 0, 1, 1, 0, ],\n        [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, ],\n        [0, 1, 0, 1, 0, 0, 1, 0, 0, 1, ],\n        [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, ],\n        [0, 0, 1, 0, 1, 0, 0, 1, 1, 0, ],\n        [0, 0, 1, 0, 1, 0, 1, 0, 0, 1, ],\n        [0, 1, 0, 0, 1, 0, 0, 1, 0, 1, ],\n        [0, 0, 1, 1, 0, 0, 0, 1, 0, 1, ],\n        [1, 0, 0, 1, 0, 0, 0, 1, 1, 0, ],\n        [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, ],\n        [0, 1, 0, 1, 0, 0, 0, 1, 1, 0, ],\n    ]\n    dummy_y = [0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]\n\n    tree = DecisionTreeClass()\n    tree.fit(dummy_x, dummy_y)\n\n    test_row = [1, 0, 0, 0, 1, 1, 0, 0, 1, 0, ]\n    print(tree.predict(test_row))  # output: 1\n附录\n本次使用的完整代码:\n# coding: utf-8\nimport math\nimport collections\n\ndef entropy(rows: list) -> float:\n    \"\"\"\n    计算数组的熵\n    :param rows:\n    :return:\n    \"\"\"\n    result = collections.Counter()\n    result.update(rows)\n    rows_len = len(rows)\n    assert rows_len   # 数组长度不能为0\n    # 开始计算熵值\n    ent = 0.0\n    for r in result.values():\n        p = float(r) / rows_len\n        ent -= p * math.log2(p)\n    return ent\n\ndef condition_entropy(future_list: list, result_list: list) -> float:\n    \"\"\"\n    计算条件熵\n    \"\"\"\n    entropy_dict = collections.defaultdict(list)  # {0:[], 1:[]}\n    for future, value in zip(future_list, result_list):\n        entropy_dict[future].append(value)\n    # 计算条件熵\n    ent = 0.0\n    future_len = len(future_list)\n    for value in entropy_dict.values():\n        p = len(value) / future_len * entropy(value)\n        ent += p\n\n    return ent\n\ndef gain(future_list: list, result_list: list) -> float:\n    \"\"\"\n    获取某特征的信息增益\n    \"\"\"\n    info = entropy(result_list)\n    info_condition = condition_entropy(future_list, result_list)\n    return info - info_condition\n\n\nclass DecisionNode(object):\n    \"\"\"\n    决策树的节点\n    \"\"\"\n    def __init__(self, col=-1, data_set=None, labels=None, results=None, tb=None, fb=None):\n        self.has_calc_index = []    # 已经计算过的特征索引\n        self.col = col              # col 是待检验的判断条件，对应列索引值\n        self.data_set = data_set    # 节点的 待检测数据\n        self.labels = labels        # 对应当前列必须匹配的值\n        self.results = results      # 保存的是针对当前分支的结果，有值则表示该点是叶子节点\n        self.tb = tb                # 当信息增益最高的特征为True时的子树\n        self.fb = fb                # 当信息增益最高的特征为False时的子树\n\ndef if_split_end(result_list: list) -> bool:\n    \"\"\"\n    递归的结束条件，每个分支的结果集都是相同的分类\n    :param result_list:\n    :return:\n    \"\"\"\n    result = collections.Counter()\n    result.update(result_list)\n    return len(result) == 1\n\ndef choose_best_future(data_set: list, labels: list, ignore_index: list) -> int:\n    \"\"\"\n    从特征向量中筛选出最好的特征，返回它的特征索引\n    \"\"\"\n    result_dict = {}  # { 索引: 信息增益值 }\n    future_num = len(data_set[0])\n    for i in range(future_num):\n        if i in ignore_index: # 如果已经计算过了\n            continue\n        future_list = [x[i] for x in data_set]\n        result_dict[i] = gain(future_list, labels) # 获取信息增益\n    # 排序后选择第一个\n    ret = sorted(result_dict.items(), key=lambda x: x[1], reverse=True)\n    return ret[0][0]\n\n\nclass DecisionTreeClass():\n    def __init__(self):\n        self.future_num = 0      # 特征\n        self.tree_root = None    # 决策树根节点\n\n    def build_tree(self, node: DecisionNode):\n        # 递归条件结束\n        if if_split_end(node.labels):\n            node.results = node.labels[0] # 表明是叶子节点\n            return\n        #print(node.data_set)\n        # 不是叶子节点，开始创建分支\n        best_index = choose_best_future(node.data_set, node.labels, node.has_calc_index)\n        node.col = best_index\n\n        # 根据信息增益最大进行划分\n        # 左子树\n        tb_index = [i for i, value in enumerate(node.data_set) if value[best_index]]\n        tb_data_set     = [node.data_set[x] for x in tb_index]\n        tb_data_labels  = [node.labels[x] for x in tb_index]\n        tb_node = DecisionNode(data_set=tb_data_set, labels=tb_data_labels)\n        tb_node.has_calc_index = list(node.has_calc_index)\n        tb_node.has_calc_index.append(best_index)\n        node.tb = tb_node\n\n        # 右子树\n        fb_index = [i for i, value in enumerate(node.data_set) if not value[best_index]]\n        fb_data_set = [node.data_set[x] for x in fb_index]\n        fb_data_labels = [node.labels[x] for x in fb_index]\n        fb_node = DecisionNode(data_set=fb_data_set, labels=fb_data_labels)\n        fb_node.has_calc_index = list(node.has_calc_index)\n        fb_node.has_calc_index.append(best_index)\n        node.fb = fb_node\n\n        # 递归创建子树\n        if tb_index:\n            self.build_tree(node.tb)\n        if fb_index:\n            self.build_tree(node.fb)\n\n    def clear_tree_example_data(self, node: DecisionNode):\n        \"\"\"\n        清理tree的训练数据\n        :return:\n        \"\"\"\n        del node.has_calc_index\n        del node.labels\n        del node.data_set\n        if node.tb:\n            self.clear_tree_example_data(node.tb)\n        if node.fb:\n            self.clear_tree_example_data(node.fb)\n\n    def fit(self, x: list, y: list):\n        \"\"\"\n        x是训练集，二维数组。y是结果集，一维数组\n        \"\"\"\n        self.future_num = len(x[0])\n        self.tree_root = DecisionNode(data_set=x, labels=y)\n        self.build_tree(self.tree_root)\n        self.clear_tree_example_data(self.tree_root)\n\n    def _predict(self, data_test: list, node: DecisionNode):\n        if node.results:\n            return node.results\n        col = node.col\n        if data_test[col]:\n            return self._predict(data_test, node.tb)\n        else:\n            return self._predict(data_test, node.fb)\n\n    def predict(self, data_test):\n        \"\"\"\n        预测\n        \"\"\"\n        return self._predict(data_test, self.tree_root)\n\n\nif __name__ == \"__main__\":\n    dummy_x = [\n        [0, 0, 1, 0, 1, 1, 0, 0, 1, 0, ],\n        [0, 0, 1, 1, 0, 1, 0, 0, 1, 0, ],\n        [1, 0, 0, 0, 1, 1, 0, 0, 1, 0, ],\n        [0, 1, 0, 0, 1, 0, 0, 1, 1, 0, ],\n        [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, ],\n        [0, 1, 0, 1, 0, 0, 1, 0, 0, 1, ],\n        [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, ],\n        [0, 0, 1, 0, 1, 0, 0, 1, 1, 0, ],\n        [0, 0, 1, 0, 1, 0, 1, 0, 0, 1, ],\n        [0, 1, 0, 0, 1, 0, 0, 1, 0, 1, ],\n        [0, 0, 1, 1, 0, 0, 0, 1, 0, 1, ],\n        [1, 0, 0, 1, 0, 0, 0, 1, 1, 0, ],\n        [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, ],\n        [0, 1, 0, 1, 0, 0, 0, 1, 1, 0, ],\n    ]\n    dummy_y = [0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0]\n\n    tree = DecisionTreeClass()\n    tree.fit(dummy_x, dummy_y)\n\n    test_row = [1, 0, 0, 0, 1, 1, 0, 0, 1, 0, ]\n    print(tree.predict(test_row))\n\n                ", "mainLikeNum": ["4 "], "mainBookmarkNum": "5"}