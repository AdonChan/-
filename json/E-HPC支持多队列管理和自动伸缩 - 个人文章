{"title": "E-HPC支持多队列管理和自动伸缩 - 个人文章 ", "index": "消息队列,集群,配置,python", "content": "阿里云E-HPC（弹性高性能计算）在最近的发布中支持多队列调度以及管理，另外发布针对多队列调度自动伸缩的策略支持。本文主要介绍以下内容\n\n多队列调度的应用背景和应用场景\nE-HPC支持多队列调度的功能实现\n介绍各种HPC调度器类型对队列和节点组是如何配置管理的\n介绍如何通过OpenApi的方式调用E-HPC多队列调度相关功能\n\n前言\n传统的HPC本地集群迁云过程中，部分会采用混合云的模式，例如如下模式，\n\n云上计算资源规格可能是和本地的计算节点规格不一样，这就导致单个集群里需要支持多种规格的计算资源，HPC集群一般会用不同队列(job queue)或者节点组(node group)来管理不同规格的节点，然后分发作业到不同的队列以达到区分云上作业和本地作业；\n有客户有需求在一个E-HPC集群里面运行不同类型作业，每种类型的作业对资源的需求是不同的，例如前处理作业需要普通8核32GiB内存的ECS虚拟机，后端计算性任务需要使用裸金属服务器。  E-HPC支持多队列\nE-HPC通过发布以下功能支持多队列部署：\n\n扩容的时候支持指定新的实例规格\n创建集群和扩容的时候支持加入指定队列，如果队列不存在会自动创建队列\n提交作业的时候支持提交到指定的队列\n\n\n自动伸缩服务支持多队列弹性策略的配置，针对每个队列可以配置如下信息：\n\n自动扩容的实例规格\n扩容付费类型，是按量付费，或者抢占式实例\n如果是抢占式实例，出价策略，是系统自动出价还是设定最高价格\n\n其余的伸缩配置共享集群全局配置，也可以设定部分队列启用自动伸缩，部分队列不自动伸缩\n\nHPC集群对多队列的支持\nE-HPC支持创建部署多种HPC调度集群，不同HPC调度器类型对队列的支持情形是不同的，这里做一些简要的介绍\nPBSPro\nPBSPro有两种队列类型,\n\nexecution: 可执行队列，作业必须在execution队列里才能被分发运行\nrouting: 用来分发作业到其他队列，目标队列可以是routing或者execution队列\n\nPBSPro默认会创建execution队列workq，该队列默认被创建和启用, 扩容节点时如果没有指定queue，队列workq里的作业可以分发到该节点计算。以下是pbspro队列相关的命令\nqmgr -c \"create queue high queue_type = execution\"\nqmgr -c \"set queue high started = true\"\nqmgr -c \"set queue high enabled = true\"\n# 设置节点的队列信息为high，将只能运行队列high里的作业\nqmgr -c \"set node node001 queue = high\"\n目前E-HPC对PBSPro集群队列的管理，都是针对execution队列\nSlurm\nSlurm里对应队列的概念是partitions，partitions可以认为是节点组，将节点分成多个set；partitions也可以被认为是作业队列，可以对该partition下运行的作业设置限制，例如作业运行时间限制，用户权限限制等等。Slurm默认的partition是comp，所有计算节点都属于该partition以下是Slurm关于partition的相关配置\n# 创建新的partition，并且指定该partition节点， 但是该配置不是持久化的，重启slurmctld服务就会覆盖该配置\nscontrol create PartitionName=heavy nodes=compute0\n\n# 通过修改配置文件的方式\n# 打开文件/opt/slurm/17.02.4/etc/slurm.conf， 可以看到文末的partition配置\nPartitionName=comp Nodes=ALL Default=YES MaxTime=INFINITE State=UP\n可以添加新的partition，例如\nPartitionName=light Nodes=compute0,compute1 Default=YES MaxTime=INFINITE State=UP\n# 重启slurmctld\nsystem restart slurmctld\nLSF/CUBE\nLSF或者CUBE的默认队列是normal， 所有的节点默认加入该队列，可以配置节点或者节点组加入某个队列，队列配置信息如下\n# 打开队列配置文件lsb.queues （CUBE的配置路径是/opt/cubeman/etc， LSF类似）\n# 增加如下队列配置\nBegin Queue\nQUEUE_NAME   = high\nPRIORITY     = 30\nNICE         = 20\n#QJOB_LIMIT   = 60         # job limit of the queue\n#UJOB_LIMIT   = 5               # job limit per user\n#PJOB_LIMIT   = 2               # job limit per processor\n#RUN_WINDOW   = 5:19:00-1:8:30 20:00-8:30\n#r1m         = 0.7/2.0        # loadSched/loadStop\n#r15m          = 1.0/2.5\n#pg          = 4.0/8\n#ut           = 0.2\n#io          = 50/240\n#CPULIMIT     = 180/apple      # 3 hours of host apple\n#FILELIMIT    = 20000\n#MEMLIMIT     = 5000           # jobs bigger than this (5M) will be niced\n#DATALIMIT    = 20000          # jobs data segment limit\n#STACKLIMIT   = 2048\n#CORELIMIT    = 20000\n#PROCLIMIT    = 5              # job processor limit\n#USERS        = all            # users who can submit jobs to this queue\nHOSTS        = high            # hostgroup high\n#PRE_EXEC     = /usr/local/lsf/misc/testq_pre >> /tmp/pre.out\n#POST_EXEC    = /usr/local/lsf/misc/testq_post |grep -v \"Hey\"\n#REQUEUE_EXIT_VALUES = 55 34 78\nDESCRIPTION  = For normal low priority jobs, running only if hosts are \\\nlightly loaded.\nEnd Queue\n\n# 打开hostgroup配置文件lsb.hosts,最后增加节点组配置（CUBE的配置路径是/opt/cubeman/etc， LSF类似）\nBegin HostGroup\nGROUP_NAME    GROUP_MEMBER    # Key words\nhigh        (compute0 compute1)    # Define a host group\nEnd HostGroup\n\n# 重启服务\nservice cubeman restart\nSGE(Sun Grid Engine)\nSGE默认队列是all.q，默认节点组是@allhosts，所有节点都默认在该节点组以下是SGE关于队列的相关配置\n# 添加节点组\nqconf -ahgrp @high\n\ngroup_name @high\nhostlist compute0 compute1\n\n# 添加队列\nqconf -aq high\n指定节点组\nhostlist              @high\nAPI调用示例\n由于部分客户和合作伙伴是通过OpenAPI和E-HPC对接，这里介绍一下API如何调用, 以python为示例代码，其他语言的示例代码可以通过OpenAPI Explorer来查看其他语言的示例代码\nCreateCluster 创建集群\n#!/usr/bin/env python\n#coding=utf-8\n\nfrom aliyunsdkcore.client import AcsClient\nfrom aliyunsdkcore.request import CommonRequest\nclient = AcsClient('<accessKeyId>', '<accessSecret>','cn-hangzhou')\n\nrequest = CommonRequest()\nrequest.set_accept_format('json')\nrequest.set_domain('ehpc.cn-hangzhou.aliyuncs.com')\nrequest.set_method('GET')\nrequest.set_version('2018-04-12')\nrequest.set_action_name('CreateCluster')\n\n# 设置队列，创建的计算节点会被指定为该队列，该队列会被自动创建\nrequest.add_query_param('JobQueue', 'high')\n\n# 设置CreateCluster其他参数\n......\n\nresponse = client.do_action_with_exception(request)\nAddNodes\n#!/usr/bin/env python\n#coding=utf-8\n\nfrom aliyunsdkcore.client import AcsClient\nfrom aliyunsdkcore.request import CommonRequest\nclient = AcsClient('<accessKeyId>', '<accessSecret>','cn-hangzhou')\n\nrequest = CommonRequest()\nrequest.set_accept_format('json')\nrequest.set_domain('ehpc.cn-hangzhou.aliyuncs.com')\nrequest.set_method('GET')\nrequest.set_version('2018-04-12')\nrequest.set_action_name('AddNodes')\n# 设置队列，新扩容的计算节点会被指定为该队列，该队列如果不存在会被自动创建\nrequest.add_query_param('JobQueue', 'high')\n\n# 设置AddNodes其他参数\n......\n\nresponse = client.do_action_with_exception(request)\nListQueues\n新增API用于查询集群队列列表\n#!/usr/bin/env python\n#coding=utf-8\n\nfrom aliyunsdkcore.client import AcsClient\nfrom aliyunsdkcore.request import CommonRequest\nclient = AcsClient('<accessKeyId>', '<accessSecret>','cn-hangzhou')\n\nrequest = CommonRequest()\nrequest.set_accept_format('json')\nrequest.set_domain('ehpc.cn-hangzhou.aliyuncs.com')\nrequest.set_method('GET')\nrequest.set_version('2018-04-12')\nrequest.set_action_name('ListQueues')\n\nrequest.add_query_param('RegionId', 'cn-hangzhou')\nrequest.add_query_param('ClusterId', '<clusterId>')\n\nresponse = client.do_action_with_exception(request)\nSubmitJob\n#!/usr/bin/env python\n#coding=utf-8\n\nfrom aliyunsdkcore.client import AcsClient\nfrom aliyunsdkcore.request import CommonRequest\nclient = AcsClient('<accessKeyId>', '<accessSecret>','cn-hangzhou')\n\nrequest = CommonRequest()\nrequest.set_accept_format('json')\nrequest.set_domain('ehpc.cn-hangzhou.aliyuncs.com')\nrequest.set_method('GET')\nrequest.set_version('2018-04-12')\nrequest.set_action_name('SubmitJob')\n\n# 指定作业提交到该队列中\nrequest.add_query_param('JobQueue', 'high')\n\n# 设置SubmitJob其他参数\n......\n\nresponse = client.do_action_with_exception(request)\nSetAutoScaleConfig\n#!/usr/bin/env python\n#coding=utf-8\n\nfrom aliyunsdkcore.client import AcsClient\nfrom aliyunsdkcore.request import CommonRequest\nclient = AcsClient('<accessKeyId>', '<accessSecret>','cn-hangzhou')\n\nrequest = CommonRequest()\nrequest.set_accept_format('json')\nrequest.set_domain('ehpc.cn-hangzhou.aliyuncs.com')\nrequest.set_method('GET')\nrequest.set_version('2018-04-12')\nrequest.set_action_name('SetAutoScaleConfig')\n\n# 对于队列high，设定扩容实例规格为GPU实例ecs.gn6v-c8g1.8xlargee，按量付费\nrequest.add_query_param('Queues.1.QueueName', 'high')\nrequest.add_query_param('Queues.1.InstanceType', 'ecs.gn6v-c8g1.8xlarge')\nrequest.add_query_param('Queues.1.SpotStrategy', 'NoSpot')\nrequest.add_query_param('Queues.1.SpotPriceLimit', '0')\n\n# 对于队列low，设定扩容实例规格为ecs.g5.large，扩容抢占式实例，最高出价为0.1\nrequest.add_query_param('Queues.2.QueueName', 'low')\nrequest.add_query_param('Queues.2.InstanceType', 'ecs.g5.large')\nrequest.add_query_param('Queues.2.SpotStrategy', 'SpotWithPriceLimit')\nrequest.add_query_param('Queues.2.SpotPriceLimit', '0.1')\n\n# 设置SetAutoScaleConfig其他参数\n......\n\nresponse = client.do_action_with_exception(request)\nLSF/CUBE集群的额外设置\nLSF/CUBE由于需要license，在创建好集群之后，需要用户手动配置license认证，然后手动配置队列和节点组信息，配置方法在上述章节已经提及，然后后续扩容节点或者自动伸缩就可以做到自动化多队列管理\n本文作者：缘督\n阅读原文\n本文为云栖社区原创内容，未经允许不得转载。\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}