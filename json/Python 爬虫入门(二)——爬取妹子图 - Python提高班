{"title": "Python 爬虫入门(二)——爬取妹子图 - Python提高班 ", "index": "网页爬虫,python", "content": "Python 爬虫入门\n听说你写代码没动力？本文就给你动力，爬取妹子图。如果这也没动力那就没救了。\nGitHub 地址: https://github.com/injetlee/Python/blob/master/%E7%88%AC%E8%99%AB%E9%9B%86%E5%90%88/meizitu.py\n公众号：【智能制造专栏】。欢迎关注，分享智能制造与编程那些事。\n爬虫成果\n\n当你运行代码后,文件夹就会越来越多，如果爬完的话会有2000多个文件夹，20000多张图片。不过会很耗时间，可以在最后的代码设置爬取页码范围。\n本文目标\n\n熟悉 Requests 库，Beautiful Soup 库\n熟悉多线程爬取\n送福利，妹子图\n\n网站结构\n我们从 http://meizitu.com/a/more_1.html 这个链接进去，界面如图一所示\n图一：\n可以看到是一组一组的套图，点击任何一组图片会进入到详情界面，如图二所示\n图二:\n\n可以看到图片是依次排开的，一般会有十张左右的图片。\n实现思路\n看了界面的结构，那么我们的思路就有了。\n\n构造 url 链接，去请求图一所示的套图列表界面，拿到每一个页面中的套图列表。\n分别进入每个套图中去，下载相应的图片。\n\n代码说明\n\n\n下载界面的函数,利用 Requests 很方便实现。\ndef download_page(url):\n    '''\n    用于下载页面\n    '''\n    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0\"}\n    r = requests.get(url, headers=headers)\n    r.encoding = 'gb2312'\n    return r.text\n\n\n获取图一所示的所有套图列表,函数中 link 表示套图的链接，text表示套图的名字\ndef get_pic_list(html):\n    '''\n    获取每个页面的套图列表,之后循环调用get_pic函数获取图片\n    '''\n    soup = BeautifulSoup(html, 'html.parser')\n    pic_list = soup.find_all('li', class_='wp-item')\n    for i in pic_list:\n        a_tag = i.find('h3', class_='tit').find('a')\n        link = a_tag.get('href')  # 套图链接\n        text = a_tag.get_text()   # 套图名字\n        get_pic(link, text)\n\n\n传入上一步中获取到的套图链接及套图名字,获取每组套图里面的图片,并保存,我在代码中注释了。\ndef get_pic(link, text):\n    '''\n    获取当前页面的图片,并保存\n    '''\n    html = download_page(link)  # 下载界面\n    soup = BeautifulSoup(html, 'html.parser')\n    pic_list = soup.find('div', id=\"picture\").find_all('img')  # 找到界面所有图片\n    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0\"}\n    create_dir('pic/{}'.format(text))\n    for i in pic_list:\n        pic_link = i.get('src')  # 拿到图片的具体 url\n        r = requests.get(pic_link, headers=headers)  # 下载图片，之后保存到文件\n        with open('pic/{}/{}'.format(text, pic_link.split('/')[-1]), 'wb') as f:\n            f.write(r.content)\n            time.sleep(1)\n\n\n完整代码\n完整代码如下，包括了创建文件夹，利用多线程爬取，我设置的是5个线程，可以根据自己机器自己来设置一下。\nimport requests\nimport os\nimport time\nimport threading\nfrom bs4 import BeautifulSoup\n\n\ndef download_page(url):\n    '''\n    用于下载页面\n    '''\n    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0\"}\n    r = requests.get(url, headers=headers)\n    r.encoding = 'gb2312'\n    return r.text\n\n\ndef get_pic_list(html):\n    '''\n    获取每个页面的套图列表,之后循环调用get_pic函数获取图片\n    '''\n    soup = BeautifulSoup(html, 'html.parser')\n    pic_list = soup.find_all('li', class_='wp-item')\n    for i in pic_list:\n        a_tag = i.find('h3', class_='tit').find('a')\n        link = a_tag.get('href')\n        text = a_tag.get_text()\n        get_pic(link, text)\n\n\ndef get_pic(link, text):\n    '''\n    获取当前页面的图片,并保存\n    '''\n    html = download_page(link)  # 下载界面\n    soup = BeautifulSoup(html, 'html.parser')\n    pic_list = soup.find('div', id=\"picture\").find_all('img')  # 找到界面所有图片\n    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0\"}\n    create_dir('pic/{}'.format(text))\n    for i in pic_list:\n        pic_link = i.get('src')  # 拿到图片的具体 url\n        r = requests.get(pic_link, headers=headers)  # 下载图片，之后保存到文件\n        with open('pic/{}/{}'.format(text, pic_link.split('/')[-1]), 'wb') as f:\n            f.write(r.content)\n            time.sleep(1)   # 休息一下，不要给网站太大压力，避免被封\n\n\ndef create_dir(name):\n    if not os.path.exists(name):\n        os.makedirs(name)\n\n\ndef execute(url):\n    page_html = download_page(url)\n    get_pic_list(page_html)\n\n\ndef main():\n    create_dir('pic')\n    queue = [i for i in range(1, 72)]   # 构造 url 链接 页码。\n    threads = []\n    while len(queue) > 0:\n        for thread in threads:\n            if not thread.is_alive():\n                threads.remove(thread)\n        while len(threads) < 5 and len(queue) > 0:   # 最大线程数设置为 5\n            cur_page = queue.pop(0)\n            url = 'http://meizitu.com/a/more_{}.html'.format(cur_page)\n            thread = threading.Thread(target=execute, args=(url,))\n            thread.setDaemon(True)\n            thread.start()\n            print('{}正在下载{}页'.format(threading.current_thread().name, cur_page))\n            threads.append(thread)\n\n\nif __name__ == '__main__':\n    main()\n好了，之后运行，我们的爬虫就会孜孜不倦的为我们下载漂亮妹子啦。\n\n                ", "mainLikeNum": ["13 "], "mainBookmarkNum": "9"}