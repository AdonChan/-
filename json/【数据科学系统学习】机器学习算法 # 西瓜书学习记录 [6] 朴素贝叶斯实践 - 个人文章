{"title": "【数据科学系统学习】机器学习算法 # 西瓜书学习记录 [6] 朴素贝叶斯实践 - 个人文章 ", "index": "机器学习,朴素贝叶斯,python", "content": "本篇内容为《机器学习实战》第 4 章 基于概率论的分类方法：朴素贝叶斯程序清单。所用代码为 python3。\n\n\n朴素贝叶斯优点：在数据较少的情况下仍然有效，可以处理多类别问题。 缺点：对于输入数据的准备方式较为敏感。 适用数据类型：标称型数据。\n\n\n使用 Python 进行文本分类\n简单描述这个过程为：从文本中获取特征，构建分类器，进行分类输出结果。这里的特征是来自文本的词条 (token)，需要将每一个文本片段表示为一个词条向量，其中值为 1 表示词条出现在文档中，0 表示词条未出现。\n接下来给出将文本转换为数字向量的过程，然后基于这些向量来计算条件概率，并在此基础上构建分类器。\n下面我们以在线社区的留言板为例，给出一个用来过滤的例子。为了不影响社区的发展，我们需要屏蔽侮辱性的言论，所以要构建一个快速过滤器，如果某条留言使用来负面或者侮辱性的语言，就将该留言标识为内容不当。对此问题建立两个类别：侮辱类和非侮辱类，分别使用 1 和 0 来表示。\n准备数据：从文本中构建词向量\n程序清单 4-1 词表到向量的转换函数\n'''\nCreated on Sep 10, 2018\n\n@author: yufei\n'''\n\n# coding=utf-8\nfrom numpy import *\n\n# 创建一些实例样本\ndef loadDataSet():\n    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n    classVec = [0,1,0,1,0,1]    # 1 代表侮辱性文字，0 代表正常言论\n\n    \"\"\"\n    变量 postingList 返回的是进行词条切分后的文档集合。\n    留言文本被切分成一些列词条集合，标点符号从文本中去掉\n    变量 classVec 返回一个类别标签的集合。\n    这些文本的类别由人工标注，标注信息用于训练程序以便自动检测侮辱性留言。\n    \"\"\"\n    return postingList, classVec\n\n\"\"\"\n创建一个包含在所有文档中出现的不重复词的列表\n是用python的 Set 数据类型\n将词条列表输给 Set 构造函数，set 就会返回一个不重复词表\n\"\"\"\ndef createVocabList(dataSet):\n    # 创建一个空集合\n    vocabSet = set([])\n    # 将每篇文档返回的新词集合添加进去，即创建两个集合的并集\n    for document in dataSet:\n        vocabSet = vocabSet | set(document)\n    # 获得词汇表\n    return list(vocabSet)\n\n# 参数：词汇表，某个文档\ndef setOfWords2Vec(vocabList, inputSet):\n    # 创建一个和词汇表等长的向量，将其元素都设置为 0\n    returnVec = [0] * len(vocabList)\n    # 遍历文档中所有单词\n    for word in inputSet:\n        # 如果出现词汇表中的单词，将输出的文档向量中的对应值设为 1\n        if word in vocabList:\n            returnVec[vocabList.index(word)] = 1\n        else:\n            print('the word: %s is not in my Vocabulary!' % word)\n    # 输出文档向量，向量元素为 1 或 0\n    return returnVec\n在 python 提示符下，执行代码并得到结果：\n>>> import bayes\n>>> list0Posts, listClasses = bayes.loadDataSet()\n>>> myVocabList = bayes.createVocabList(list0Posts)\n>>> myVocabList\n['problems', 'mr', 'ate', 'buying', 'not', 'garbage', 'how', 'maybe', 'stupid', 'cute', 'stop', 'help', 'dalmation', 'take', 'is', 'worthless', 'him', 'flea', 'park', 'my', 'I', 'to', 'licks', 'steak', 'dog', 'love', 'quit', 'so', 'please', 'posting', 'has', 'food']\n\n即可得到的一个不会出现重复单词的词表myVocabList，目前该词表还没有排序。\n继续执行代码：\n>>> bayes.setOfWords2Vec(myVocabList, list0Posts[3])\n[0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n>>> bayes.setOfWords2Vec(myVocabList, list0Posts[0])\n[0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0]\n\n函数setOfWords2Vec使用词汇表或者说想要检查的所有单词作为输入，然后为其中每一个单词构建一个特征。一旦给定一篇文章（本例中指一条留言），该文档就会被转换为词向量。\n训练算法：从词向量计算概率\n函数伪代码如下：\n··· 计算每个类别中的文档数目\n··· 对每篇训练文档：\n······ 对每个类别：\n········· 如果词条出现在文档中—>增加该词条的计数值\n········· 增加所有词条的计数值\n······ 对每个类别：\n········· 对每个词条：\n············ 将该词条对数目除以总词条数目得到条件概率\n······ 返回每个类别对条件概率\n\n程序清单 4-2 朴素贝叶斯分类器训练函数\n'''\nCreated on Sep 11, 2018\n\n@author: yufei\n'''\n# 参数：文档矩阵 trainMatrix，每篇文档的类别标签所构成的向量 trainCategory\ndef trainNB0(trainMatrix, trainCategory):\n    numTrainDocs = len(trainMatrix) #文档的个数\n    numWords = len(trainMatrix[0])  #获取第一篇文档的单词长度\n\n    \"\"\"\n    计算文档属于侮辱性文档的概率\n    用类别为1的个数除以总篇数\n    sum([0,1,0,1,0,1])=3，也即是 trainCategory 里面 1 的个数\n    \"\"\"\n    pAbusive = sum(trainCategory) / float(numTrainDocs)\n\n    \"\"\"\n    初始化概率\n    当利用贝叶斯分类器对文档分类时，计算多个概率的乘积以获得属于某个类别的概率\n    把所有词出现次数初始化为1，分母初始化为2，用log避免数太小被约掉\n    \"\"\"\n    p0Num = ones(numWords)\n    p1Num = ones(numWords)\n\n    p0Denom = 2.0\n    p1Denom = 2.0\n\n    # 遍历训练集 trainMatrix 中的所有文档\n    for i in range(numTrainDocs):\n        # 侮辱性词语在某个文档中出现\n        if trainCategory[i] == 1:\n            # 该词对应个数加一，即分子把所有的文档向量按位置累加\n            # trainMatrix[2] = [1,0,1,1,0,0,0];trainMatrix[3] = [1,1,0,0,0,1,1]\n            p1Num += trainMatrix[i]\n            # 文档总词数加一，即对于分母\n            # 把trainMatrix[2]中的值先加起来为3,再把所有这个类别的向量都这样累加起来，这个是计算单词总数目\n            p1Denom += sum(trainMatrix[i])\n        # 正常词语在某个文档中出现，同上\n        else:\n            p0Num += trainMatrix[i]\n            p0Denom +=sum(trainMatrix[i])\n\n    \"\"\"\n    对每个元素除以该类别的总词数，得条件概率\n    防止太多的很小的数相乘造成下溢。对乘积取对数\n    # p1Vect = log(p1Num / p1Denom)\n    # p0Vect = log(p0Num / p0Denom)\n    \"\"\"\n    \n    p1Vect = p1Num / p1Denom\n    p0Vect = p0Num / p0Denom\n\n    \"\"\"\n    函数返回两个向量和一个概率\n    返回每个类别的条件概率，是一个向量\n    在向量里面和词汇表向量长度相同\n    每个位置代表这个单词在这个类别中的概率\n    \"\"\"\n    return p0Vect, p1Vect, pAbusive\n\n在 python 提示符下，执行代码并得到结果：\n>>> from numpy import *\n>>> importlib.reload(bayes)\n<module 'bayes' from '/Users/Desktop/Coding/bayes.py'>\n>>> list0Posts, listClasses = bayes.loadDataSet()\n>>> myVocabList = bayes.createVocabList(list0Posts)\n\n以上，调入数据后构建了一个包含所有词的列表myVocabList\n>>> trainMat = []\n>>> for postinDoc in list0Posts:\n...     trainMat.append(bayes.setOfWords2Vec(myVocabList, postinDoc))\n\n这个for循环使用词向量来填充trainMat列表。\n继续给出属于侮辱性文档的概率以及两个类别的概率向量。\n>>> p0V, p1V, pAb = bayes.trainNB0(trainMat, listClasses)\n\n查看变量的内部值\n>>> pAb\n0.5\n>>> p0V\narray([0.03846154, 0.07692308, 0.03846154, 0.07692308, 0.07692308,\n       0.07692308, 0.07692308, 0.03846154, 0.03846154, 0.03846154,\n       0.07692308, 0.07692308, 0.15384615, 0.07692308, 0.07692308,\n       0.07692308, 0.03846154, 0.07692308, 0.07692308, 0.07692308,\n       0.07692308, 0.07692308, 0.03846154, 0.07692308, 0.11538462,\n       0.07692308, 0.07692308, 0.03846154, 0.03846154, 0.03846154,\n       0.07692308, 0.03846154])\n>>> p1V\narray([0.0952381 , 0.04761905, 0.0952381 , 0.0952381 , 0.14285714,\n       0.04761905, 0.04761905, 0.0952381 , 0.0952381 , 0.14285714,\n       0.04761905, 0.04761905, 0.04761905, 0.04761905, 0.04761905,\n       0.04761905, 0.0952381 , 0.04761905, 0.04761905, 0.04761905,\n       0.0952381 , 0.04761905, 0.0952381 , 0.04761905, 0.0952381 ,\n       0.04761905, 0.04761905, 0.19047619, 0.0952381 , 0.0952381 ,\n       0.04761905, 0.0952381 ])\n\n我们发现文档属于侮辱类的概率pAb为 0.5，查看pV1的最大值 0.19047619，它出现在第 27 个下标位置，查看myVocabList的第 27 个下标位置该词为 stupid，说明这是最能表征类别 1 的单词。\n测试算法：根据现实情况修改分类器\n程序清单 4-3 朴素贝叶斯分类函数\n'''\nCreated on Sep 11, 2018\n\n@author: yufei\n'''\n# vec2Classify: 要分类的向量\ndef classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n    p1 = sum(vec2Classify * p1Vec) + log(pClass1)\n    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n    if p1 > p0:\n        return 1\n    else:\n        return 0\n\ndef  testingNB():\n    list0Posts, listClasses = loadDataSet()\n    myVocabList = createVocabList(list0Posts)\n    trainMat = []\n    for posinDoc in list0Posts:\n        trainMat.append(setOfWords2Vec(myVocabList, posinDoc))\n    p0V, p1V, pAb = trainNB0(array(trainMat), array(listClasses))\n\n    testEntry = ['love', 'my','dalmation']\n    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))\n    testEntry = ['stupid', 'garbage']\n    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))\n在 python 提示符下，执行代码并得到结果：\n>>> importlib.reload(bayes)\n<module 'bayes' from '/Users/Desktop/Coding/bayes.py'>\n>>> bayes.testingNB()\n['love', 'my', 'dalmation'] classified as:  0\n['stupid', 'garbage'] classified as:  1\n\n分类器输出结果，分类正确。\n准备数据：文档词袋模型\n词集模型：将每个词的出现与否作为一个特征。即我们上面所用到的。词袋模型：将每个词出现次数作为一个特征。每遇到一个单词，其词向量对应值 +1，而不是全设置为 1。\n对函数setOfWords2Vec()进行修改，修改后的函数为bagOfWords2VecMN。\n程序清单 4-4 朴素贝叶斯词袋模型\ndef bagOfWords2VecMN(vocabList, inputSet):\n    returnVec = [0] * len(vocabList)\n    for word in inputSet:\n        if word in inputSet:\n            returnVec[vocabList.index(word)] += 1\n    return returnVec\n修改的地方为：每当遇到一个单词时，它会增加词向量中的对应值，而不只是将对应的数值设为 1。\n下面我们将利用该分类器来过滤垃圾邮件。\n\n示例：使用朴素贝叶斯过滤垃圾邮件\n测试算法：使用朴素贝叶斯进行交叉验证\n程序清单 4-5 文件解析及完整的垃圾邮件测试函数\n'''\nCreated on Sep 11, 2018\n\n@author: yufei\n'''\n\n\"\"\"\n接受一个大字符串并将其解析为字符串列表\n\"\"\"\ndef textParse(bigString):    #input is big string, #output is word list\n    import re\n    listOfTokens = re.split(r'\\W*', bigString)\n    # 去掉小于两个字符的字符串，并将所有字符串转换为小写\n    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n\n\"\"\"\n对贝叶斯垃圾邮件分类器进行自动化处理\n\"\"\"\ndef spamTest():\n    docList=[]; classList = []; fullText =[]\n    #导入并解析文本文件为词列表\n    for i in range(1,26):\n        wordList = textParse(open('email/spam/%d.txt' % i, encoding='ISO-8859-1').read())\n        docList.append(wordList)\n        fullText.extend(wordList)\n        classList.append(1)\n        \n        wordList = textParse(open('email/ham/%d.txt' % i, encoding='ISO-8859-1').read())\n        docList.append(wordList)\n        fullText.extend(wordList)\n        classList.append(0)\n    vocabList = createVocabList(docList)#create vocabulary\n    trainingSet = list(range(50)); testSet=[]           #create test set\n    \n    for i in range(10):\n        randIndex = int(random.uniform(0,len(trainingSet)))\n        testSet.append(trainingSet[randIndex])\n        del(trainingSet[randIndex])\n        \n    trainMat=[]; trainClasses = []\n    \n    # 遍历训练集的所有文档，对每封邮件基于词汇表并使用 bagOfWords2VecMN 来构建词向量\n    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n        trainClasses.append(classList[docIndex])\n    # 用上面得到的词在 trainNB0 函数中计算分类所需的概率\n    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n    errorCount = 0\n    \n    # 对测试集分类\n    for docIndex in testSet:        #classify the remaining items\n        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n        # 如果邮件分类错误，错误数加 1 \n        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n            errorCount += 1\n            print (\"classification error\",docList[docIndex])\n    # 给出总的错误百分比\n    print ('the error rate is: ',float(errorCount)/len(testSet))\n    #return vocabList,fullText\n在 python 提示符下，执行代码并得到结果：\n>>> importlib.reload(bayes)\n<module 'bayes' from '/Users/Desktop/Coding/bayes.py'>\n>>> bayes.spamTest()\nclassification error ['home', 'based', 'business', 'opportunity', 'knocking', 'your', 'door', 'don', 'rude', 'and', 'let', 'this', 'chance', 'you', 'can', 'earn', 'great', 'income', 'and', 'find', 'your', 'financial', 'life', 'transformed', 'learn', 'more', 'here', 'your', 'success', 'work', 'from', 'home', 'finder', 'experts']\nthe error rate is:  0.1\n\n函数spamTest()会输出在 10 封随机选择的电子邮件上的分类错误率。由于是随机选择的，所以每次的输出结果可能有些差别。如果想要更好地估计错误率，那么就应该将上述过程重复多次求平均值。\n这里的代码需要注意的两个地方是：1、直接使用语句 wordList = textParse(open('email/spam/%d.txt' % i).read()) 报错 UnicodeDecodeError: 'utf-8' codec can't decode byte 0x92 in position 884: invalid start byte。这是因为在文件里可能存在不是以 utf-8 格式保存的字符，需改为wordList = textParse(open('email/spam/%d.txt' % i, encoding='ISO-8859-1').read())。\n2、将随机选出的文档添加到测试集后，要同时将其从训练集中删除，使用语句 del(trainingSet[randIndex])，此时会报错 TypeError: 'range' object doesn't support item deletion，这是由于 python2 和 python3 的不同而导致的。在 python2 中可以直接执行，而在 python3 中需将 trainingSet 设为 trainingSet = list(range(50))，而不是 trainingSet = range(50)，即必须让它是一个 list 再进行删除操作。\n\n\n以上，我们就用朴素贝叶斯对文档进行了分类。\n\n参考链接：《机器学习实战》笔记之四——基于概率论的分类方法：朴素贝叶斯UnicodeDecodeError: 'utf-8' codec can't decode byte 0x92 in position 884: invalid start byte\n不足之处，欢迎指正。\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "1"}