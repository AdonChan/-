{"title": "如何优雅地使用TensorBoard - 自然语言处理与深度学习的火花 ", "index": "自然语言处理,nlp,deeplearning,python,tensorflow", "content": "为什么需要TensorBoard\n当我们训练一个deep learning模型时，怎么样判断当前是过拟合，还是欠拟合等状态呢？实践中，我们常常会将数据集分为三部分：train、validation、test。训练过程中，我们让模型尽力拟合train数据集，在validation数据集上测试拟合程度。当训练过程结束后，我们在test集上测试模型最终效果。有经验的炼丹师往往会通过模型在train和validation上的表现，来判断当前是否是过拟合，是否是欠拟合。这个时候，TensorBoard就派上了大用场！\nTensorBoard的效果\n\n有没有觉的一目了然呢？我强烈推荐大家使用TensorBoard，使用后炼丹功力显著提升！\n如何使用TensorBoard\n下面，我来讲一下如何使用TensorBoard。要使用，也要优雅！如果你喜欢自己梳理知识，自己尝试，那么不妨阅读官方文档：戳这里查看官方文档不然的话，就随着老夫玩转TensorBoard吧 ^0^\n熟悉一个新知识的时候，应该将不必要的东西最精简化，将注意力集中到我们最关注的地方，所以，我写了一个最简单的模型，在这个模型的基础上对TensorBoard进行探索。\n首先看一下这个极简的线性模型：\nimport tensorflow as tf\nimport random\n\nclass Model(object):\n\n    def __init__(self):\n        self.input_x = tf.placeholder(dtype=tf.float32, shape=[None, ], name='x')\n        self.input_y = tf.placeholder(dtype=tf.float32, shape=[None, ], name='y')\n\n        W = tf.Variable(tf.random_uniform([1], -1.0, 1.0), dtype=tf.float32)\n        b = tf.Variable(tf.random_uniform([1], -1.0, 1.0), dtype=tf.float32)\n\n        y_predict = self.input_x * W + b\n        self.loss = tf.reduce_sum(tf.abs(y_predict - self.input_y))\n相信这个模型大家很快就能看懂，所以就不多说了。接下来看构造数据的代码：\nx_all = []\ny_all = []\n\nrandom.seed(10)\nfor i in range(3000):\n    x = random.random()\n    y = 0.3 * x + 0.1 + random.random()\n    x_all.append(x)\n    y_all.append(y)\n\nx_all = np.array(x_all)\ny_all = np.array(y_all)\nshuffle_indices = np.random.permutation(np.arange(len(x_all)))\n\nx_shuffled = x_all[shuffle_indices]\ny_shuffled = y_all[shuffle_indices]\n\nbound = int(len(x_all) / 10 * 7)\n\nx_train = x_shuffled[:bound]\ny_train = y_shuffled[:bound]\n\nx_val = x_shuffled[bound:]\ny_val = y_shuffled[bound:]\n这段代码里做了三件事：\n\n构造3000个符合y = 0.3 * x + b关系，且增加了随机噪声的数据\n对数据进行shuffle\n按照7比3的比例，将3000个数据集划分为训练集和验证集两部分\n\n下面是对数据按batch取出：\ndef batch_iter(data, batch_size, num_epochs, shuffle=True):\n    \"\"\"\n    Generates a batch iterator for a dataset.\n    \"\"\"\n    data = np.array(data)\n    data_size = len(data)\n    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n    for epoch in range(num_epochs):\n        # Shuffle the data at each epoch\n        if shuffle:\n            shuffle_indices = np.random.permutation(np.arange(data_size))\n            shuffled_data = data[shuffle_indices]\n        else:\n            shuffled_data = data\n        for batch_num in range(num_batches_per_epoch):\n            start_index = batch_num * batch_size\n            end_index = min((batch_num + 1) * batch_size, data_size)\n            yield shuffled_data[start_index:end_index]\n然后就到了比较本篇博客的核心部分：首先我来描述一下关键的函数（大部分同学内心一定是拒绝的 2333，所以建议先看下面的代码，然后再反过头来看函数的介绍）：\n\n\ntf.summary.scalar(name, tensor, collections=None, family=None)，调用这个函数来观察Tensorflow的Graph中某个节点\n\ntensor：我们想要在TensorBoard中观察的节点\nname：为该节点设置名字，在TensorBoard中我们观察的曲线将会以name命名\n\n\n\ntf.summary.merge(inputs, collections=None, name=None)\ninputs：由scalar函数返回值组成的list\n\n\ntf.summary.FileWriter,在给定的目录中创建一个事件文件(event file)，将summraies保存到该文件夹中。\n\n\n__init__(logdir, graph=None, max_queue=10, flush_secs=120, graph_def=None, filename_suffix=None)\n\nlogdir：保存event file的路径\ngraph：　Tensorflow构建的graph，可通过session.graph获得\n\n\n\nadd_summary(summary, global_step=None)\n\nsummary：我们将tf.summary.merge的返回值记为summary_op，然后在调用sess.run(summary_op)后，将会返回一个summary\nglobal_step：当前训练步数\n\n\n\n\n\nwith tf.Graph().as_default():\n    sess = tf.Session()\n    with sess.as_default():\n        m = model.Model()\n        global_step = tf.Variable(0, name='global_step', trainable=False)\n        optimizer = tf.train.AdamOptimizer(1e-2)\n        grads_and_vars = optimizer.compute_gradients(m.loss)\n        train_op = optimizer.apply_gradients(grads_and_vars=grads_and_vars, global_step=global_step)\n\n        loss_summary = tf.summary.scalar('loss', m.loss)\n\n        train_summary_op = tf.summary.merge([loss_summary])\n        train_summary_writer = tf.summary.FileWriter('./summary/train', sess.graph)\n\n        dev_summary_op = tf.summary.merge([loss_summary])\n        dev_summary_writer = tf.summary.FileWriter('./summary/dev', sess.graph)\n\n\n        def train_step(x_batch, y_batch):\n            feed_dict = {m.input_x: x_batch,\n                         m.input_y: y_batch}\n            _, step, summaries, loss = sess.run(\n                [train_op, global_step, train_summary_op, m.loss], feed_dict)\n            train_summary_writer.add_summary(summaries, step)\n\n        def dev_step(x_batch, y_batch):\n            feed_dict = {m.input_x: x_batch,\n                         m.input_y: y_batch}\n\n            step, summaries, loss = sess.run(\n                [global_step, dev_summary_op, m.loss], feed_dict)\n            dev_summary_writer.add_summary(summaries, step)\n\n        sess.run(tf.global_variables_initializer())\n        batches = batch_iter(list(zip(x_train, y_train)), 100, 100)\n        for batch in batches:\n            x_batch, y_batch = zip(*batch)\n            train_step(x_batch, y_batch)\n            current_step = tf.train.global_step(sess, global_step)\n            if current_step % 3 == 0:\n                print('\\nEvaluation:')\n                dev_step(x_val, y_val)\n现在我们就可以使用TensorBoard查看训练过程了~~在terminal中输入如下命令:\n\ntensorboard --logdir=summary\n响应TensorBoard 0.4.0rc3 at http://liudaoxing-Lenovo-Rescuer-15ISK:6006 (Press CTRL+C to quit)\n\n在浏览器中打开　'http://liudaoxing-Lenovo-Resc...:6006'\n\n没错！这就是我们train和validation过程中loss的情况。\n点击GRAPHS,就可以看到网络的结构\n麻雀虽小，五脏俱全。希望大家有收获～\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}