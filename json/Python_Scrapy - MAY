{"title": "Python_Scrapy - MAY ", "index": "python爬虫,python", "content": "Scrapy框架\n\n\nScrapy是用纯Python实现一个为了爬取网站数据、提取结构性数据而编写的应用框架\n\nScrapy使用了(Twisted:['twɪstɪd])(其主要对手是Tornado)异步网络框架来处理网络通讯\n\n安装\n通过pip安装Scrapy框架: pip install Scrapy\nScrapy中文维护站点\n基本使用\n创建项目：scrapy startproject mySpider\n项目目录结构：\nscrapy.cfg：项目的配置文件\n\nmySpider/：项目的Python模块，将会从这里引用代码\n\nmySpider/items.py：项目的目标文件\n\nmySpider/pipelines.py：项目的管道文件\n\nmySpider/settings.py：项目的设置文件\n\nmySpider/spiders/ ：存储爬虫代码目录\n在items.py文件中配置字段\nimport scrapy\n\n\nclass SueItems(scrapy.Item):\n    name = scrapy.Field()\n    title = scrapy.Field()\n    info = scrapy.Field()\n爬取数据：\n#coding=utf-8\n\nimport scrapy\nfrom mySpider.items import SueItems\n\nclass TeSpider(scrapy.Spider):\n    # 爬虫名字\n    name = 'ing'\n    # 允许爬虫作用的范围 固定属性名allowed_domains\n    allowed_domains = ['http://www.itcast.cn/']\n    # 爬虫起始的url 固定属性名start_urls\n    start_urls = ['http://www.itcast.cn/channel/teacher.shtml']\n\n    def parse(self, response):\n        # with open('teacher.html', 'w') as f:\n        #     f.write(response.body.decode())\n\n        tearcherList = response.xpath('//div[@class=\"li_txt\"]')\n\n        itemTeachs = []\n        for item in tearcherList:\n            itemTeach = SueItems()\n            # name\n            name = item.xpath('./h3/text()').extract()\n            # title\n            title = item.xpath('./h4/text()').extract()\n            # info\n            info = item.xpath('./p/text()').extract()\n\n            itemTeach['name'] = name[0]\n            itemTeach['title'] = title[0]\n            itemTeach['info'] = info[0]\n            itemTeachs.append(itemTeach)\n\n        return itemTeachs\nscrapy项目基本步骤:\n\n创建项目scrapy startproject xxxx\n\n编写items.py文件，设置需要保存的数据字段\n进入xxxx/spiders,编写爬虫文件，文件中name就是爬虫名\n运行scrapy crawl 爬虫名，scrapy crawl 爬虫名 -o json/csv/xml\n\n\nSelectors选择器\nScrapy Selectors 内置 XPath 和 CSS Selector 表达式机制\n\nxpath(): 传入xpath表达式，返回该表达式所对应的所有节点的selector list列表\nextract(): 序列化该节点为Unicode字符串并返回list\ncss(): 传入CSS表达式，返回该表达式所对应的所有节点的selector list列表，语法同 BeautifulSoup4\nre(): 根据传入的正则表达式对数据进行提取，返回Unicode字符串list列表\n\n最常用到是xpath()\npipeline\nyield把一个函数当作生成器使用,这个函数会肯定有迭代或者循环，每次函数执行到yield到时候会返回一个值，程序会在这里暂停，函数返回生成器。\n在settings.py文件中打开pipline的配置\nITEM_PIPELINES = {\n   'mySpider.pipelines.MyspiderPipeline': 300,\n    # '项目.pipeline文件.class名': 300\n}\n在piplines.py中写对应的class\nimport json\n\n\nclass suePipeline():\n    def __init__(self):\n        self.file = open('teacher.json', 'wb')\n\n    # process_item 是类中必须要的方法，用来处理item数据\n    def process_item(self, item, spider): # item: yield返回的数据\n        content = json.dumps(dict(item), ensure_ascii=False) + \"\\n\"\n        self.file.write(content.encode('utf-8'))\n        return item\n\n    # 可选方法，执行结束后执行该方法\n    def close_spider(self, spider):\n        self.file.close()\n把数据交给管道文件和重新发起请求：\n# 将数据重新发给调度器入队列，出队列，交给下载器下载 新的数据\nyield scrapy.Request(self.url + str(self.offset), callback=self.parse)\n\n# 将数据交给管道文件处理\nyield item\nspider文件中的编写：\nclass TencentpositionSpider(scrapy.Spider):\n    name = 'tencentPosition'\n    allowed_domains = ['tencent.com']\n    # https://hr.tencent.com/position.php?&start=3720#a\n    offset = 0\n    url = 'https://hr.tencent.com/position.php?&start='\n    start_urls = [url + str(offset)]\n\n    def parse(self, response):\n        for each in response.xpath('//tr[@class=\"even\"] | //tr[@class=\"odd\"]'):\n            # 初始化模型对象\n            item = TencenthrItem()\n            item['positionName'] = each.xpath('./td[1]/a/text()').extract()[0]\n            item['positionLink'] = each.xpath('./td[1]/a/@href').extract()[0]\n            item['positionType'] = each.xpath('./td[2]/text()').extract()[0]\n            item['positionNum'] = each.xpath('./td[3]/text()').extract()[0]\n            item['workLocation'] = each.xpath('./td[4]/text()').extract()[0]\n            item['publishTime'] = each.xpath('./td[5]/text()').extract()[0]\n\n            # 将数据交给管道文件处理\n            yield item\n\n        if self.offset < 3720:\n            self.offset += 10\n        # 将数据重新发给调度器入队列，出队列，交给下载器下载 新的数据\n        yield scrapy.Request(self.url + str(self.offset), callback=self.parse)\nscrapy整体步骤\n创建项目\n通过命令行创建scrapy项目scrapy startproject xxxx\n设置需要的字段\n编写items.py文件，设置需要保存的数据字段\n编写爬虫文件\n\n进入xxxx/spiders,编写爬虫文件，文件中name就是创建的爬虫名\n可以通过命令行创建爬虫文件：scrapy genspider tencentPosition 'tencent.com'\n\n\n\n需要初始化模型数据,导入配置的字段名的文件：\nfrom tencenthr.items import TencenthrItem\n使用piplines文件\n在settings.py配置piplines.py中的类名\n在piplines.py编写逻辑：\n运行\n运行: scrapy crawl 爬虫名\n\n\nitem文件\n爬虫文件\n\npiplines文件\n\nsetting文件\n\n使用配置文件和重写scrapy中的类的方法\npipeline文件中\n# -*- coding: utf-8 -*-\nimport os\n\nimport scrapy\n# 引入scrapy中的处理pipelines中的images文件，继承ImagesPipeline，并重写其中`get_media_requests（重新发送请求）`,`item_completed（爬取结果处理逻辑）`方法\nfrom scrapy.pipelines.images import ImagesPipeline\n# 获取`setting.py`中定义的变量。\nfrom scrapy.utils.project import get_project_settings\n\n\nclass ImagePipeline(ImagesPipeline):\n    # 获取setting.py中的变量值\n    IMAGES_STORE = get_project_settings().get('IMAGES_STORE')\n\n    def get_media_requests(self, item, info):\n        image_url = item['imagelink']\n        yield scrapy.Request(image_url)\n\n    def item_completed(self, results, item, info):\n        image_path = [x['path'] for ok, x in results if ok]\n        os.rename(self.IMAGES_STORE + '/' + image_path[0], self.IMAGES_STORE + '/' + item['nickname'] + '.jpg')\n        item['imagePath'] = self.IMAGES_STORE + '/' + item['nickname']\n        return item\n\nparse() 方法工作机制\n1. 因为使用yield,而不是return,prase函数将会被当成生成器使用。scrapy会逐一获取parse方法中生成的结果，并判断该结果是一个什么样的类型；\n2. 如果是request则加入爬取队列，如果是item类型则使用pipline处理，其它类型则返回错误信息。\n3. scrapy取到第一部分的request不会立马就发送这个request，只是把这个request放到队列里，然后接着从生成器里获取；\n4. 取尽第一部分的request，然后再获取第二部分的item，取到item了，就会放到对应的pipline里处理；\n5. prase()方法作为回调函数赋值给了Request，指定parse()方法来处理这些请求scrapy.Requset(url, callback=self.parse)\n6. Request对象经过调度，执行生成scrapy.http.response()的响应对象，并送回给parse()方法，直到调度器中没有Request（递归思路）\n7. 取尽之后，parse()工作结束，引擎再根据队列和piplines中内容去执行响应的操作；\n8. 程序在取得各个页面的items前，会先处理完之前所有resquest队列里的请求，然后再提取items。\ncrawlSpiders\nsrawlSpiders作用：通过匹配规则，自动发送其它请求，不需要通过yeild scrapy.Response()手动发送\n使用命令来创建crawlSpiders文件：\nscrapy  genspider -t crawl tencent tencent.com\n使用scrapy终端：\nscrapy shell 'http://capi.douyucdn.cn/api/v1/getVerticalRoom?limit=20&offset=0'\nLinkExtract链接提取规则\nLinkExtractors作用: 提取连接\nfrom scrapy.linkextractors import LinkExtractor\npage_lx = LinkExtractor(allow=('&start=\\d+')) # 正则匹配\npage_lx.extract_links(response) # 提取链接，接收response对象，并返回一个scrapy.link.Link对象\nLinkExtractor参数：\nclass scrapy.linkextractors.LinkExtractor(\n    allow = (),\n    deny = (),\n    allow_domains = (),\n    deny_domains = (),\n    deny_extensions = None,\n    restrict_xpaths = (),\n    tags = ('a','area'),\n    attrs = ('href'),\n    canonicalize = True,\n    unique = True,\n    process_value = None\n)\nallow: 满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配.allow_domains: 会被提取的链接的domains.\nRule爬取规则\nclass scrapy.spiders.Rule(\n        link_extractor, \n        callback = None, \n        cb_kwargs = None, \n        follow = None, \n        process_links = None, \n        process_request = None\n)\nlink_extractor: 是一个Link Extractor对象，用于定义需要提取的链接.callback: 从link_extractor中每获取到链接时，参数所指定的值作为回调函数，该回调函数接受一个response作为其第一个参数。（不能重写parse方法）follow : 是一个布尔(boolean)值，指定了根据该规则从response提取的链接是否需要跟进。 如果callback为None，follow 默认设置为True ，否则默认为False。\nrules = (\n        Rule(LinkExtractor(allow=r'Items/'), callback='parse_item', follow=True),\n    )\n爬虫文件：\n# -*- coding: utf-8 -*-\nimport scrapy\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\nfrom tencentSpider.items import tencentSpiderItem\n\nclass TencentSpiders(CrawlSpider):\n    name = 'tencent'\n    allowed_domains = ['tencent.com']\n    start_urls = ['http://tencent.com/']\n\n    page_link = LinkExtractor(allow=(r'start=\\d+'))\n\n    # 批量处理请求\n    rules = [\n        Rule(page_link, callback='parse_Tencent', follow=True) # follow:True 跟进连接请求\n    ]\n\n    def parse_Tencent(self, response):\n        i = tencentSpiderItem()\n        for each in response.xpath('//tr[@class=\"even\"] | //tr[@class=\"odd\"]'):\n            i['positionName'] = each.xpath('./td[1]/a/text()').extract()[0]\n            i['positionLink'] = each.xpath('./td[1]/a/@href').extract()[0]\n            i['positionType'] = each.xpath('./td[2]/text()').extract()[0]\n            i['positionNum'] = each.xpath('./td[3]/text()').extract()[0]\n            i['workLocation'] = each.xpath('./td[4]/text()').extract()[0]\n            i['publishTime'] = each.xpath('./td[5]/text()').extract()[0]\n\n            yield i\n\nlogging\nScrapy提供了log功能，可以通过logging模块使用。\n可以修改配置文件settings.py，任意位置添加:\nLOG_FILE = \"TencentSpider.log\"\nLOG_LEVEL = \"INFO\"\nLog levels\nScrapy提供5层logging级别：\n\n\nCRITICAL - 严重错误(critical)\n\nERROR - 一般错误(regular errors)\n\nWARNING - 警告信息(warning messages)\n\nINFO - 一般信息(informational messages)\n\nDEBUG - 调试信息(debugging messages)\n\nlogging设置\n\n\nLOG_ENABLED 默认: True，启用logging\n\nLOG_ENCODING 默认: 'utf-8'，logging使用的编码\n\nLOG_FILE 默认: None，在当前目录里创建logging输出文件的文件名\n\nLOG_LEVEL 默认: 'DEBUG'，log的最低级别\n\nLOG_STDOUT 默认: False 如果为 True，进程所有的标准输出(及错误)将会被重定向到log中。例如，执行 print \"hello\" ，其将会在Scrapy log中显示。\n\n一般设置保存日志的文件名和日志等级：\n# 保存日志信息的文件名\nLOG_FILE = 'tencent.log'\n# 保存日志等级，高于或者等于该等级信息都被保存\nLOG_LEVEL = 'INFO'\nscrapy模拟登陆\nscrapy框架模拟登陆与中间件\n模拟登陆的几种策略：\n\n拿取cookie\n需要提供账户密码(只需要提供post数据)\nscrapy模拟登陆方法\n\n'''\n提供账户/用户名/邮件 和 密码 模拟登陆\n'''\nimport scrapy\n\n\nclass RenrenSpider(scrapy.Splider):\n    name = 'renren'\n    allowed_domains = ['renren.com']\n    start_urls = []\n\n    def start_requires(self):\n        url = 'http://www.renren.com/PLogin.do'\n        yield scrapy.FormRequest(url=url, formdata={ 'email': 'xxxxxx@163.com',  'password': 'alarmchime'}, callback=self.parse_page)\n    \n    def parse_page(self, response):\n        with open('mo.html', 'w') as f:\n            f.write(response.body)\n\n'''\n首先发送登陆页面的get请求，获取到页面里的登陆必须的参数，\n然后和账户密码一起post到服务器，登陆成功。\n'''\nimport scrapy\n\n\nclass LoginSpider(scrapy.Spider):\n    name = 'example.com'\n    start_urls = ['http://www.example.com/users/login.php']\n\n    def parse(self, response):\n        return scrapy.FormRequest.from_response(\n            response,\n            formdata={'username': 'john', 'password': 'secret'},\n            callback=self.after_login\n        )\n\n    def after_login(self, response):\n        # check login succeed before going on\n        if \"authentication failed\" in response.body:\n            self.log(\"Login failed\", level=log.ERROR)\n            return\n\n        # continue scraping with authenticated session...\n模拟登陆\n发送POST请求\n\n可以使用:yield scrapy.FormRequest(url, formdata, callback)\n\n使用：重写Spider类的start_requests(self)方法\n\n如果希望程序执行一开始就发送POST请求，可以重写Spider类的start_requests(self)方法，并且不在调用start_urls里的url。\nclass mySpider(scrapy.Spider):\n    # start_urls = [\"http://www.example.com/\"]\n\n    def start_requests(self):\n        url = 'http://www.renren.com/PLogin.do'\n\n        # FormRequest 是Scrapy发送POST请求的方法\n        yield scrapy.FormRequest(\n            url = url,\n            formdata = {\"email\" : \"xxx@163.com\", \"password\" : \"axxxxe\"},\n            callback = self.parse_page\n        )\n    def parse_page(self, response):\n        # do something\n反反爬虫相关机制\n有些些网站使用特定的不同程度的复杂性规则防止爬虫访问，绕过这些规则是困难和复杂的，有时可能需要特殊的基础设施：scrpay自定义爬虫机制\n通常防止爬虫被反主要有以下几个策略\n\n动态设置User-Agent（随机切换User-Agent，模拟不同用户的浏览器信息）\n禁用Cookies(不启用cookies middleware, 不想Server发送cookies, 有些网站通过cookie的使用发现爬虫行为)在settings文件中配置COOKIES_ENABLED设置CookiesMiddleware的开关\n设置延迟下载（防止访问过于频繁，设置为 2秒 或更高）在settings文件中配置DOWNLOAD_DELAY = 3\n\n\nGoogle Cache 和 Baidu Cache使用谷歌/百度等搜索引擎服务器页面缓存获取页面数据。\n使用IP地址池：VPN和代理IP，现在大部分网站都是根据IP来。\n\n使用 Crawlera（专用于爬虫的代理组件），正确配置和设置下载中间件后，项目所有的request都是通过crawlera发出。\n  DOWNLOADER_MIDDLEWARES = {\n      'scrapy_crawlera.CrawleraMiddleware': 600\n  }\n\n  CRAWLERA_ENABLED = True\n  CRAWLERA_USER = '注册/购买的UserKey'\n  CRAWLERA_PASS = '注册/购买的Password'\n\n\nscrapy中间件\n中间件作用：\n\n当引擎传递给下载器当过程中，下载中间件可以对请求进行处理（例如增加http，header信息，增加proxy）\n在下载器完成http请求，传递响应给引擎的过程中，下载中间件可以对响应进行处理（例如进行zip解压）\n\n在settings配置文件中配置中间件\nDOWNLOADER_MIDDLEWARES = {\n    'scrapyProject.fileNameMiddlewares.className': 400\n    # 项目名.中间件文件名.类名\n    # 值是优先级，数字越小，优先级越高\n}\n创建settings取的中间件类，类中必须写process_request方法\nimport base64\nimport random\n\nfrom scrapy import signals\n\n# 获取setting中参数\nfrom scrapy.conf import settings\n\n# 随机User-Agent\nclass RandUserAgent(object):\n    def process_response(self, request, spider):\n        useragent = random.choice(settings['USER_AGENTS'])\n        request.headers.setdefault('User-Agent', useragent)\n\nclass RandomProxy(object):\n    def process_response(self, request, spider):\n        proxy = random.choice(settings['PROXIES'])\n\n        if proxy['user_parss'] is None:\n            # 没有代理账户验证的代理使用方式\n            request.mate['proxy'] = 'http://' + proxy['ip_prot']\n        else:\n            # 对账户密码进行base64编码转换\n            b64_userpass = base64.b64encode(proxy['user_parss'])\n            '''\n                b64encode的原理\n                1. 字符串转ascii码\n                2. ascii码 转 二进制\n                3. 6个一组（4组）\n                4. 高位补0\n                5. 然后得到十进制\n                6. 得出的结果对照base64编码表\n            '''\n            # 对应到代理服务器的信令格式\n            request.headers['Proxy-Authorization'] = 'Basic ' + b64_userpass\n\n            request.meta['proxy'] = \"http://\" + proxy['ip_port']\n为什么HTTP代理要使用base64编码\nHTTP代理的原理： 就是通过HTTP协议于代理服务器建立连接，协议信令中包含要连接到到远程主机到IP和端口号，如果有需要身份验证的话还需要加上授权信息，服务器收到信令后首先进行身份证验证，通过后才与远程主机建立连接，连接成功之后会返回给客户端200，表示验证通过。\n客户端收到收面的信令后表示成功建立连接，接下来要发送给远程主机的数据就可以发送给代理服务器了，代理服务器建立连接后会在根据IP地址和端口号对应的连接放入缓存，收到信令后再根据IP地址和端口号从缓存中找到对应的连接，将数据通过该连接转发出去。\n信令格式：\nCONNECT 59.64.128.198:21 HTTP/1.1\nHost: 59.64.128.198:21\nProxy-Authorization: Basic bGV2I1TU5OTIz\nUser-Agent: OpenFetion\nProxy-Authorization: 身份验证信息，Basic后面是字符串，是用户名和密码组合后进行base64编码的结果，也就是username:password进行base64.b64encode()编码\nsettings\n提供了，定制Scrapy组件的方法。\nsettings文档\nBOT_NAME\n默认: scrapybot\n当使用startproject命令创建项目时其也被自动赋值。\nCONCURRENT_ITEMS\n默认: 100\nItem Processor(即 Item Pipeline) 同时处理(每个response的)item的最大值。\nCONCURRENT_REQUESTS\n默认: 16\nScrapy downloader 并发请求(concurrent requests)的最大值。\nDEFAULT_REQUEST_HEADERS\n{\n'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n'Accept-Language': 'en',\n}\nScrapy HTTP Request使用的默认header。其他地方没有配置，才使用settings里面的headers配置\nDEPTH_LIMIT\n默认: 0\n爬取网站最大允许的深度(depth)值。如果为0，则没有限制。\nDOWNLOAD_DELAY\n默认: 0下载器在下载同一个网站下一个页面前需要等待的时间。该选项可以用来限制爬取速度， 减轻服务器压力。同时也支持小数\nDOWNLOAD_TIMEOUT\n默认: 180\n下载器超时时间(单位: 秒)。\nITEM_PIPELINES\n保存项目中启用的pipeline及其顺序的字典\n值(value)任意，不过值(value)习惯设置在0-1000范围内，值越小优先级越高。\nLOG_ENABLED\n默认: True\n是否启用logging\nLOG_LEVEL\n默认: 'DEBUG'\nlog的最低级别。可选的级别有: CRITICAL、 ERROR、WARNING、INFO、DEBUG 。\nscrapy-redis分布式\nscrapy-redis源码：\ngit clone https://github.com/rmax/scrapy-redis.git\nscrapy-redis源码项目事例中的setting.py配置\nSPIDER_MODULES = ['example.spiders']\nNEWSPIDER_MODULE = 'example.spiders'\n\nUSER_AGENT = 'scrapy-redis (+https://github.com/rolando/scrapy-redis)'\n\n# 使用了scrapy-redis里的去重组件，不使用scrapy中的去重组件\nDUPEFILTER_CLASS = \"scrapy_redis.dupefilter.RFPDupeFilter\"\n# 使用了scrapy-redis里的调度组件，不使用scrapy中的调度器\nSCHEDULER = \"scrapy_redis.scheduler.Scheduler\"\n# 允许暂停，redis请求记录不丢失\nSCHEDULER_PERSIST = True\n\n# 默认按照scrapy请求（按优先级顺序）队列形式\n# 按stored 排序顺序出队列\nSCHEDULER_QUEUE_CLASS = \"scrapy_redis.queue.SpiderPriorityQueue\"\n# 队列形式，先进先出\n#SCHEDULER_QUEUE_CLASS = \"scrapy_redis.queue.SpiderQueue\"\n# 栈形式，先进后出\n#SCHEDULER_QUEUE_CLASS = \"scrapy_redis.queue.SpiderStack\"\n\n# scrapy_redis.pipleines.RedisPipeline 支持将数据存储到Redis数据库里，必须启动\nITEM_PIPELINES = {\n    'example.pipelines.ExamplePipeline': 300,\n    'scrapy_redis.pipelines.RedisPipeline': 400,\n}\n\n# redis指定数据库的主机IP\nREDIS_HOST = '192.168.21.63'\n# 指定数据库的端口号\nREDIS_PROT = 6379\n\n# LOG日志\nLOG_LEVEL = 'DEBUG'\n\n# Introduce an artifical delay to make use of parallelism. to speed up the\n# crawl.\n\n# 下载延迟\nDOWNLOAD_DELAY = 1\nspiders\nfrom scrapy_redis.spiders import RedisSpider\n\n\nclass MySpider(RedisSpider):\n    \"\"\"Spider that reads urls from redis queue (myspider:start_urls).\"\"\"\n    name = 'myspider_redis'\n    redis_key = 'myspider:start_urls'\n    # 在redis启动的时候，`lpush mysipder:start_urls http://dmoz.org(爬取的第一个站点入口)`确认当前运行的地址\n\n    # `__init__`动态的获取allowd_domains = ['']\n    def __init__(self, *args, **kwargs):\n        # Dynamically define the allowed domains list.\n        domain = kwargs.pop('domain', '')\n        self.allowed_domains = filter(None, domain.split(','))\n        super(MySpider, self).__init__(*args, **kwargs)\n\n    def parse(self, response):\n        return {\n            'name': response.css('title::text').extract_first(),\n            'url': response.url,\n        }\n执行命令对改变：\nscrapy runspider myspidre_reids.py\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}