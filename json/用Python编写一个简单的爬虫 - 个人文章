{"title": "用Python编写一个简单的爬虫 - 个人文章 ", "index": "python爬虫,python", "content": "作者信息：\nAuthor : 黄志成(小黄)博客地址: 博客\n\n呐，这是一篇福利教程.为什么这么说呢.我们要爬取的内容是美图网站(嘿嘿，老司机都懂的)\n废话不多说.开始今天的表演.\n这个图集网站不要问我怎么来的.绝对不是我刻意找的.（其实是看了别的发的一篇文章,就想自己动手实现一下）\n\n我们今天的任务就是 将这些图集保存下来。\n首先我们需要获取到所有的列表，我们往下拉动滚动条,拉到底,会继续自动加载内容,我们通过浏览器的NetWork可以发现请求的数据包\n\n我们来分析一下这个数据包\nURL：https://www.toutiao.com/search_content/?offset=0&format=json&keyword=%E6%B8%85%E7%BA%AF%E7%BE%8E%E5%A5%B3&autoload=true&count=20&cur_tab=3&from=gallery\n\n通过url我们可以知道几个重要的参数\noffset 偏移量\ncount 数量\ncur_tab 当前分类\n\n这里很多朋友可能对偏移量不太了解,这里我用sql语句表示一下,如果了解sql的朋友 肯定就知道了\nmysql> SELECT * FROM art LIMIT offset , count\n\nmysql> SELECT * FROM table LIMIT 5,10;  // 检索记录行 6-15\n\nmysql> SELECT * FROM table LIMIT 95,1; // 检索记录行 96\n\n\n这里我每次读取一条,对一条进行操作.\nURL：https://www.toutiao.com/search_content/?offset=1&format=json&keyword=%E6%B8%85%E7%BA%AF%E7%BE%8E%E5%A5%B3&autoload=true&count=1&cur_tab=3&from=gallery\n\n每次对offset 进行自增即可了\n我们点击进去 看看数据的结构.\n\n我们需要获取到该图集的链接。\n进入这篇图集,在NetWork中并没有发现图集有关的请求接口,可能也是混排的.\n我们可以查看页面的源码\n\n原来真的是混排的写法.看了一下这里用到vue.具体怎么实现的我们不去探讨了,我们只需要取出数据即可。\n那如何取出呢? 提供两种方法,一种就是正则,一种就是自己写一个取文本的函数.这里我用第二种作为演示,下面是取文本的函数.\ndef txt_wrap_by(start_str, end, html):\n    start = html.find(start_str)\n    if start >= 0:\n        start += len(start_str)\n        end = html.find(end, start)\n        if end >= 0:\n            return html[start:end].strip()\n\n我们取出 JSON.parse(\"\") 中的数据\n观察数据,可以发现 我们取出 url 就可以了,这里的数据是json但是被转义了,我们就通过正则取出吧\n\n正则的语法如图上,最后我也会放出所有代码滴,大家放心.\n取到了uri 我们只要在前面拼上 http://p3.pstatp.com/ 即可.\n然后保存为图片即可~\n上面说的都是思路,最后放出代码~\nimport requests,os,json,re,datetime\n\n# 主函数\ndef main():\n    foreach_art_list()\n\ndef foreach_art_list():\n    # 判断目录下是否存在jilv.txt文件 如果存在则读取里面的数值\n    if os.path.exists('./jilv.txt'):\n        f = open('./jilv.txt')\n        n = f.read()\n        n = int(n)\n        f.close()\n    else:\n        n = 1    \n    while True:\n        url = 'http://www.toutiao.com/search_content/?offset=' + str(n) + '&format=json&keyword=%E6%B8%85%E7%BA%AF%E7%BE%8E%E5%A5%B3&autoload=true&count=1&cur_tab=3&from=gallery'\n        re = requests.get(url)\n        data = re.json()['data']\n        if not data:\n            break\n        # 运行图片下载函数\n        download_pic(data[0]['article_url'],n)\n        n = n+1\n        # 将n写入文件 防止程序运行出错 可以继续运行\n        with open('./jilv.txt', 'w') as f:\n            f.write(str(n))\n\ndef download_pic(url,n):\n    download_pic_url = 'http://p3.pstatp.com/'\n    # 这里必须带上协议头,否则会请求失败\n    header = {\n        'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.162 Safari/537.36'\n    }\n    res = requests.get(url,headers = header)\n    content = res.text\n    img_list_json = txt_wrap_by('gallery: JSON.parse(\"','\"),',content)\n    # 正则获取所有的uri\n    img_list = re.findall(r'uri\\\\\":\\\\\"(.*?)\\\\\"',img_list_json)\n    #判断是否有此目录\n    if 'img' not in os.listdir('.'):\n        os.mkdir('./img')\n    if str(n) not in os.listdir('./img'):\n        os.mkdir('./img/'+str(n))\n    for v in img_list:\n        img_path = download_pic_url + v\n        img_path = img_path.replace(\"\\\\\", \"\")\n        # 读取图片\n        atlas = requests.get(img_path).content\n        # 保存图片\n        with open( './img/' + str(n) + '/' + str(datetime.datetime.now()) +'.jpg', 'wb') as f:  # 把图片写入文件内\n            f.write(atlas)\n\n\n# 取出两个文本之间的内容\ndef txt_wrap_by(start_str, end, html):\n    start = html.find(start_str)\n    if start >= 0:\n        start += len(start_str)\n        end = html.find(end, start)\n        if end >= 0:\n            return html[start:end].strip()\n\n# 运行程序\nmain()\n\n最后 展示一下 运行结果：\n\n\n这个程序还有许多不完善的地方,我会在之后教程加入 redis 和 多线程 的写法,让他成为最快的爬虫~\n敬请期待~ 今天就到这里了. 又是周末！祝大家周末愉快。嘿嘿~ 看我的美图去了。\n\n                ", "mainLikeNum": ["4 "], "mainBookmarkNum": "5"}