{"title": "Scrapy 实战之爬取妹子图 - Python 学习之路 ", "index": "python", "content": "\n阅读文本大概需要 10 分钟。\n很多人说爬虫这门技术非常有趣，但不知道如何入门。其实爬虫入门非常简单，难的部分在于各大网站的反爬机制。当然对于一些简单的网站，还是非常容易爬取。\n学习爬虫首先要明确你的驱动力，是想爬一些知乎的数据，还是一些电影的资源。驱动力非常重要，这决定你是否有足够的兴趣继续学下去。\n很多人学习爬虫的第一驱动力就是爬取各大网站的妹子图片，比如比较有名的 mzitu。在爬这些网站的时候，即可以欣赏漂亮的妹子图，又能学习到技术，非常的 nice。\n\n今天我就结合非常好用的 scrapy 框架，去抓取一些妹子图片，并把爬取的数据保存到 mongodb 数据库中。本次要爬取的网站是 360 的图片搜索网站，\n地址：http://images.so.com/\n360 图片的妹子质量还是非常可以的，我随意放几张大家感受下。\n\n清纯可爱的\n\n文艺又气质的\n\n仙气十足的\n非常的赏心悦目。\n程序思路\n本次程序运行的环境是 windows 10 + python 3.6，运行本次程序前首先确保你已经安装好了 scrapy、pymongo 以及 mongodb 数据库。\n简单的分析了下 360 图片网站，并没有很强的反爬措施，并且网站的数据是以 Ajax 请求呈现。\n\n我们进一步查看请求的详情，观察返回的数据结构。\n\n返回的是 JSON 数据格式，其中 list 字段把图片的一些信息都保存在这里面。比如我们需要的图片地址信息 cover_imgurl。另外观察 Ajax 请求的参数信息，还有一个 sn 一直在变化，这个参数很明显就是偏移量。当 sn  为 30 时，返回的是前 30 张图片，依次类推，我们只需要改变 sn 的值就可以一直获取图片的信息。\n接下来我们只需要通过 scrapy 高性能的框架，把网站上的图片保存到本地即可。\n\n新建项目\n首先在本地创建一个 scrapy 项目并命名为 images360。通过已下的命名即可创建。\n<pre style=\"margin: 0px; padding: 0px; border-radius: 8px; background: rgb(248, 248, 248); max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important; word-break: normal !important; font-size: inherit; color: inherit; line-height: inherit;\">\nscrapy startproject images360\n</pre>\n随后就会有如下的项目结构\n\n接下来就是在 spiders 目录下新建一个 Spider，命令如下：\nscrapy genspider images images.so.com\n这样我们的项目都已创建好，最后项目的结构如下。\n\n程序代码\nsettings.py\n在 settings.py 里面会先定义一个变量 MAX_PAGE，表示我们需要爬取的最大页面，比如在此次的程序中我们设置的是 50，也就是爬取 50 页，每页 30 张，一共 1500 张图片。\nMAX_PAGE = 50\nsettings.py 文件中我们还设置一些数据库相关的配置信息。\nMONGO_URI = 'localhost'\nMONGO_DB = 'test'\nIMAGES_STORE = './images'\n并且需要注意的是我们要修改 settings.py 中的 ROBOTSTXT_OBEY 变量，将其设置为 False，否则无法抓取。\nROBOTSTXT_OBEY = False\nstart_requests()\n这个函数用来构造最开始的请求，用来生成 50 次请求。\n    def start_requests(self):\n        data = {'ch': 'photogtaphy', 'listtype': 'new'}\n        base_url = 'https://image.so.com/zj?0'\n        for page in range(1, self.settings.get('MAX_PAGE') + 1):\n            data['sn'] = page * 30\n            params = urlencode(data)\n            url = base_url + params\n            yield Request(url, self.parse\n提取信息\n我们会在 items.py 文件中定义一个 Images360Item 类，用来定义我们的数据结构。\nclass Images360Item(Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    collection = table = 'images'\n    id = Field()\n    url = Field()\n    title = Field()\n    thumb = Field()\n其中包括图片的 ID、链接、标题、缩略图。另外还有两个属性 collection 和 table，都定义为 images 字符串，代表 MongoDB 存储的 Collection 名称。\n接下来我们提取 Spider 里有关信息，在 parse() 方法改写成如下所示：\n    def parse(self, response):\n        result = json.loads(response.text)\n        for image in result.get('list'):\n            item = Images360Item()\n            item['id'] = image.get('imageid')\n            item['url'] = image.get('qhimg_url')\n            item['title'] = image.get('group_title')\n            item['thumb'] = image.get('qhimg_thumb_url')\n            yield item\n这样我们就完成了信息的提取，接下来就需要把抓取的信息保存到 MongoDB 中。\nMongoDB\n首先确保你本地已经安装好了 MongoDB，并且已经正常启动。我们用一个 MongoPipeline 将信息保存到 MongoDB 中，在 pipelines.py 里添加如下类的实现：\nclass MongoPipeline(object):\n    def __init__(self, mongo_uri, mongo_db):\n        self.mongo_uri = mongo_uri\n        self.mongo_db = mongo_db\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(\n            mongo_uri=crawler.settings.get('MONGO_URI'),\n            mongo_db=crawler.settings.get('MONGO_DB')\n        )\n\n    def open_spider(self, spider):\n        self.client = pymongo.MongoClient(self.mongo_uri)\n        self.db = self.client[self.mongo_db]\n\n    def process_item(self, item, spider):\n        self.db[item.collection].insert(dict(item))\n        return item\n\n    def close_spider(self, spider):\n        self.client.close()\n\n</pre>\nScrapy 提供了专门处理下载的 Pipeline，包括文件下载和图片下载。下载文件和图片的原理与抓取页面的原理一样，因此下载过程支持异步和多线程，下载十分高效。\n我们首先在 settings.py 定义一个 IMAGES_STORE 变量，用来表示图片存储的路径。\nIMAGES_STORE = './images'\n内置的 ImagesPipeline 会默认读取 Item 的 image_urls 字段，并认为该字段是一个列表形式，它会遍历 Item 的 image_urls 字段，然后取出每个 URL 进行图片下载。\n但是现在生成的 Item 的图片链接字段并不是 image_urls 字符表示的，也不是列表形式，而是单个的 URL。所以为了实现下载，我们需要重新定义下载的部分逻辑，即要自定义 ImagePipeline，继承内置的 ImagesPipeline，从而实现我们自己的图片下载逻辑。\nclass ImagePipeline(ImagesPipeline):\n    def file_path(self, request, response=None, info=None):\n        url = request.url\n        file_name = url.split('/')[-1]\n        return file_name\n\n    def item_completed(self, results, item, info):\n        image_paths = [x['path'] for ok, x in results if ok]\n        if not image_paths:\n            raise DropItem('Image Downloaded Failed')\n        return item\n\n    def get_media_requests(self, item, info):\n        yield Request(item['url'])\n最后我们需要在 settings.py 中把我们定义好的 Item Pipeline 打开，修改 settings.py 中的 ITEM_PIPELINES 即可。\nITEM_PIPELINES = {\n   'images360.pipelines.ImagePipeline': 300,\n   'images360.pipelines.MongoPipeline': 301\n}\n最后我们只需要运行程序，即可执行爬取，程序运行命名如下：\nscrapy crawl images\n完整代码我已上传到微信公众号后台，在「痴海」公众号后台回复「360」即可获取。\n本文首发于公众号「痴海」，后台回复「1024」即可获取最新编程资源。\n比如这样的：史上最全 Python 学习资料，PDF 电子书大合集\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}