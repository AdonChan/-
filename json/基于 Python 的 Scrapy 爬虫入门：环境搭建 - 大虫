{"title": "基于 Python 的 Scrapy 爬虫入门：环境搭建 - 大虫 ", "index": "scrapy,python", "content": "目录\n\n基于 Python 的 Scrapy 爬虫入门：环境搭建\n基于 Python 的 Scrapy 爬虫入门：页面提取\n基于 Python 的 Scrapy 爬虫入门：图片处理\n\n\n作为一个全栈工程师（:-) HoHo），前端后台都懂点是必须的，但是Python 很菜，Scrapy 更菜，没错，这就是 Full Stack Developer 的特点，什么都懂点，什么都不精，我们的特点就是上手快，忘得也很快，不过配合强大的搜索引擎，写些小东西出来是不成问题的！\n言归正传，最近想用爬虫抓取一些内容来充实自己的博客，以前用过 phpspider，基本能满足要求，但是不够强大，所以选用 Scrapy，顺便回忆一下快忘光的 Python，谁让它这么火呢。\n一、基础环境\n由于不是职业的Web开发者，因此环境是基于Windows的。\n1. Python 安装\n到 python.org 下载最新版 Python 安装包，我使用的是3.6.3 32位版本，注意如果安装了64位版本，以后所使用的包也都需要64位(很大一部分包无32/64位区分，可以通用)。安装程序默认安装pip包管理工具，并设置了相关环境变量：添加 %Python% 及 %Python%\\Scripts 到 PATH 中（%Python%是你的安装目录），你运行的 Python 程序或脚本都在 Scripts 中，包都安装在 Lib\\site-packages 中。\n2. 配置 pip 国内镜像源\nPython之所以强大正是因为各种功能齐全的开发包，由于众所周知的原因 pip下载模块速度很慢，因此为了保证pip下载顺利，建议替换成国内的安装源镜像：\n\n\n创建文件 %HOMEPATH%\\pip\\pip.ini，内容如下：\n[global]\ntrusted-host=mirrors.aliyun.com\nindex-url=http://mirrors.aliyun.com/pypi/simple/\n\n\n上面这个是清华大学的镜像，另外附上其他几个好用的，据说每30分钟同步官网\n阿里云：http://mirrors.aliyun.com/pypi/simple/\n豆瓣网：http://pypi.doubanio.com/simple/\n科技大学：http://mirrors.ustc.edu.cn/pypi/web/simple/\n清华大学：https://pypi.tuna.tsinghua.edu.cn/simple/\n\n\n如果不嫌麻烦的话也可以每次安装时指定：\npip -i http://pypi.douban.com/simple install Flask\n\n\n3. 换一个趁手的命令行\n由于Python中经常要用到命令行工具，但Windows自带的cmd或PowerShell逼格太低，换个字体还得折腾大半天，因此有必要换一个好用的，推荐cmder mini版：https://github.com/cmderdev/c...，Python输出的调试信息可以根据颜色区分，不用像cmd那样找半天了。\n4. 安装基础包\nvirtualenv\n基本上Python每个项目都会用到大量的模块，比如本文中的Scrapy爬虫，pip install scrapy后除了Scrapy本身外，还会下载数十个依赖包，如果经常用Python做各种开发，site-packages会越来越庞大，可能有些包只在一个项目中用到，或者删除包后依赖包并没有被删除，总之不太好管理，作为强迫症患者是决不能忍受的。\n好在有一个工具 virtualenv 可以方便管理 Python 的环境，它可以创建一个隔离的Python虚拟开发环境，使用它你可以同时安装多个Python版本，方便同时多个项目的开发，每个项目之间的包安装与使用都是独立的，互不干扰，通过命令可以随时切换各个虚拟环境，如果不再使用，把整个虚拟环境删除即可同时删除其中所有的模块包，保持全局环境的干净。  \n为了便于使用，我选择安装virtualenvwrapper-win模块，它依赖于virtualenv，包含Windows下面易于使用的批处理脚本，其实只是调用了 virtualenv 功能而已：\npip install virtualenvwrapper-win\nvirtualenvwrapper 常用命令：\n\n\nworkon: 列出虚拟环境列表\n\nlsvirtualenv: 同上\n\nmkvirtualenv: 新建虚拟环境\n\nworkon [虚拟环境名称]: 切换虚拟环境\n\nrmvirtualenv: 删除虚拟环境\n\ndeactivate: 离开虚拟环境\n\nwheel\nwheel 是python中的解包和打包工具，因此有必要安装到全局环境中，有些模块使用pip安装总是失败，可以尝试先下载whl文件，再使用wheel本地安装的方式安装。\npip install wheel\npypiwin32\n既然在Windows下开发，win32api也是必不可少的包，因此也装到全局环境中，下次新建虚拟项目环境用到时就不必每次再下载一次了。\n\n当然，以上2，3，4其实都不是必须的，但是建好基本环境有利于以后的开发少兜圈子。\n二、Scrapy 安装\n打开cmder命令行工具\n\n创建Scrapy虚拟环境： mkvirtualenv Scrapy，默认情况下会创建%HOMEPATH%\\Envs目录，所有的虚拟环境都会产生一个子目录保存在此，里面包含Python基本程序文件以及pip,wheel,setuptools库文件。如果想修改Envs默认路径，在Windows中可添加一个 %WORKON_HOME%  环境变量指定新的目录。\n切换到Scrapy环境：workon scrapy ，执行后在命令行提示符前面会多出 (Scrapy) 字符，表示当前处于Scrapy虚拟环境中，同时添加了当前环境中的相关路径在系统 %PATH% 搜索路径中。\n\n安装Scrapy包：pip install scrapy，自己好几次都遇到Twisted模块安装失败的问题，貌似是编译失败，缺少Microsoft Visual C++ 14.0导致：\n\n我没有按要求安装Microsoft Visual C++ 14.0编译工具进行编译安装，而是下载已打包的whl文件进行本地安装，此时wheel便派上了用场，到 https://www.lfd.uci.edu/~gohl... 下载twisted的whl文件（注意对应Python版本）\n\n再使用 pip install Twisted‑17.9.0‑cp36‑cp36m‑win32.whl 来进行安装，本地安装twisted成功，由于之前被错误中断，建议再执行一次 pip install scrapy 防止有依赖包没有安装到。\n注意：Windows平台需要额外安装 pypiwin32 模块，否则在Scrapy执行爬虫时会报错：ModuleNotFoundError: No module named 'win32api'\n\n\n\n至此 Scrapy 环境搭建完成，所有的模块存放在 %HOMEPATH%\\Envs\\Scrapy 中，如果不再使用，只需要命令行执行 rmvirtualenv scrapy，整个目录都会被删除，所有依赖模块都会被清理干净。\n\n                ", "mainLikeNum": ["3 "], "mainBookmarkNum": "8"}