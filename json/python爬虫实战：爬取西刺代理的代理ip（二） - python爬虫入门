{"title": "python爬虫实战：爬取西刺代理的代理ip（二） - python爬虫入门 ", "index": "python", "content": "\n爬虫实战（二）：爬取西刺代理的代理ip\n\n对于刚入门的同学来说，本次实战稍微有点难度，但是简单的爬取图片、文本之类的又没营养，所以这次我选择了爬取西刺代理的ip地址，爬取的代理ip也能在以后的学习中用到\n\n本次实战用的主要知识很多，其中包括：\n\nrequests.Session()自动保存cookie\n利用抓包工具获取cookie；\nBeautifulSoup和xpath匹配html文档中的标签\nsubprocess测试ip并获取运行时间及匹配的丢包数\n\n\n代码如下：\n\n\"\"\"\n案例名称：学习使用ip代理池\n需求：从网上找一个代理ip的网站，然后获取网站上的\n100个ip，组成代理ip池，然后随机抽取其中一个ip，\n并对该ip进行连通性测试，如果该ip可以，我们可以将\n该ip作为代理ip来使用\n\n思路：\n    1，先获取西刺代理网站上的ip(100)\n    2, 随机抽取其中一个ip，并检测其连通性\n    3，如果该ip可用，则可以作为代理ip使用\n编码：\n测试：\n\"\"\"\n\nimport requests\nfrom bs4 import BeautifulSoup\nfrom lxml import etree\nimport subprocess as sp\nimport random\nimport re\n\n\n\"\"\"\n函数说明:获取代理ip网站的ip\n\"\"\"\ndef get_proxys(page):\n    #requests的Session()可以自动保存cookie，\n    #不需要自己维护cookie内容\n    S = requests.Session()\n    #目标网址的url\n    target_url = 'http://www.xicidaili.com/nn/%d' %page\n    target_headers = {\n        'Upgrade-Insecure-Requests': '1',\n        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',\n        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n        'Referer': 'http://www.xicidaili.com/nn/',\n        'Accept-Encoding': 'gzip, deflate, sdch',\n        'Accept-Language': 'zh-CN,zh;q=0.8'\n    }\n    target_response = S.get(url=target_url,\n                            headers=target_headers)\n    target_response.encoding = 'utf-8'\n    target_html = target_response.text\n    # print(target_html)\n\n    #解析数据（ip,port,protocol）\n    bf1_ip_list = BeautifulSoup(target_html,'lxml')\n    bf2_ip_list = BeautifulSoup(str(bf1_ip_list.find_all(id='ip_list')),'lxml')\n    ip_list_info = bf2_ip_list.table.contents\n\n    proxys_list = []\n    for index in range(len(ip_list_info)):\n        if index % 2 == 1 and index != 1:\n            dom = etree.HTML(str(ip_list_info[index]))\n            ip = dom.xpath('//td[2]')\n            port = dom.xpath('//td[3]')\n            protocol = dom.xpath('//td[6]')\n            proxys_list.append(protocol[0].text.lower()\n                               + \"#\" + ip[0].text\n                               + \"#\" + port[0].text)\n    return proxys_list\n\n\"\"\"\n函数说明:检测代理ip的连通性\n参数:\n    ip--代理的ip地址\n    lose_time--匹配的丢包数\n    waste_time--匹配平均时间\n返回值:\n    average_time--代理ip的平均耗时\n\"\"\"\ndef check_ip(ip, lose_time, waste_time):\n    cmd = \"ping -n 3 -w 3 %s\"\n    #执行命令\n    p = sp.Popen(cmd %ip, stdin=sp.PIPE,\n                 stdout=sp.PIPE,\n                 stderr=sp.PIPE,\n                 shell=True)\n    #获取返回结果并解码\n    out = p.stdout.read().decode('GBK')\n    lose_time = lose_time.findall(out)\n\n    if len(lose_time) == 0:\n        lose = 3\n    else:\n        lose = int(lose_time[0])\n    #如果丢包数大于2，那么我们返回平均耗时1000\n    if lose > 2:\n        #返回false（1000）\n        return 1000\n    else:\n        #平均时间\n        average = waste_time.findall(out)\n        if len(average) == 0:\n            return 1000\n        else:\n            average_time = int(average[0])\n            #返回平均耗时\n            return average_time\n\n\n\"\"\"\n函数说明:初始化正则表达式\n返回值:\n    lose_time--匹配丢包数\n    waste_time--匹配平均时间\n\"\"\"\ndef initpattern():\n    #匹配丢包数\n    lose_time = re.compile(u\"丢失 = (\\d+)\",re.IGNORECASE)\n    #匹配平均时间\n    waste_time = re.compile(u\"平均 = (\\d+)ms\",re.IGNORECASE)\n    return lose_time, waste_time\n\n\nif __name__ == '__main__':\n    #初始化正则表达式\n    lose_time, waste_time = initpattern()\n    #获取ip代理\n    proxys_list = get_proxys(1)\n\n\n    #如果平均时间超过200ms，则重新选取ip\n    while True:\n        #从100个ip中随机选取一个ip作为代理进行网络访问\n        proxy = random.choice(proxys_list)\n        split_proxy = proxy.split('#')\n        #获取ip\n        ip = split_proxy[1]\n        #检查ip\n        average_time = check_ip(ip, lose_time, waste_time)\n\n        if average_time > 200:\n            #去掉不能使用的ip\n            proxys_list.remove(proxy)\n            print(\"ip链接超时，重新获取中...\")\n        else:\n            break\n\n    proxys_list.remove(proxy)\n    proxys_dict = {split_proxy[0]:split_proxy[1]\n                    + \":\" + split_proxy[2]}\n    print(\"使用代理:\", proxys_dict)\n\n\n今天的代码有点难以理解，但是要按照代码步骤及规范理解起来并不难，小伙伴们加油，我自己也加油！\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "0"}