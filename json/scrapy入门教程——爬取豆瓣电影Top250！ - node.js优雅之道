{"title": "scrapy入门教程——爬取豆瓣电影Top250！ - node.js优雅之道 ", "index": "scrapy,mongodb,python", "content": "本课只针对python3环境下的Scrapy版本（即scrapy1.3+）\n选取什么网站来爬取呢？\n对于歪果人，上手练scrapy爬虫的网站一般是官方练手网站 http://quotes.toscrape.com\n我们中国人，当然是用豆瓣Top250啦！https://movie.douban.com/top250\n\n第一步，搭建准备\n为了创造一个足够干净的环境来运行scrapy，使用virtualenv是不错的选择。\n>>> mkdir douban250 && cd douban250\n>>> virtualenv -p python3.5 doubanenv\n首先要保证已经安装有virtualenv和python3.x版本，上面命令为创建python3.5环境的virtualenv\nvirtualenv教程：廖雪峰python教程-virtualenv\n开启virtualenv，并安装scrapy\n>>> source doubanenv/bin/activate\n>>> pip install scrapy\n使用scrapy初始化项目一个项目，比如我们命名为douban_crawler\n>>> scrapy startproject douban_crawler\n这时生成了一个目录结构\ndouban_crawler/\n    douban.cfg\n    douban_crawler/\n        __init__.py\n        items.py\n        middlewares.py\n        piplines.py\n        setting.py\n并且命令行会给你一个提示：\nYou can start your first spider with:\n    cd douban_crawler\n    scrapy genspider example example.com\n按提示执行这两步吧！\n>>> cd douban_crawler\n>>> scrapy genspider douban movie.douban.com/top250\ngenspider后目录结构中增加了spider目录\ndouban_crawler/\n    douban.cfg\n    douban_crawler/\n        spiders/\n            __init__.py\n            douban.py\n        __init__.py\n        items.py\n        middlewares.py\n        piplines.py\n        setting.py\n在pycharm中设置好项目\n\n首先，在pycharm中打开douban_crawler/\n然后设置pycharm的虚拟环境：\n\nPerference > Project:douban_crawler > Project Interpreter 点击设置图标> Add Local > existing environment；把预置的python解析器，切换到刚刚创立的virtualenv下的doubanenv > bin > python3.5\n开始爬取\n准备工作完成，可以开始爬取了！\n打开spiders/目录下的douban.py文件\n# douban_crawler/ > spiders/ > douban.py\n\nimport scrapy\n\nclass DoubanSpider(scrapy.Spider):\n    name = 'douban'\n    allowed_domains = ['movie.douban.com/top250']\n    start_urls = ['http://movie.douban.com/top250/']\n\n    def parse(self, response):\n        pass\nstart_urls就是我们需要爬取的网址啦！\n把start_urls中的http://movie.douban.com/top250/ \n改成https://movie.douban.com/top250/\n接下来我们将改写parse()函数，进行解析。\n解析豆瓣250条目\n使用chrome或firefox浏览器打开https://movie.douban.com/top250/\n使用右键菜单中的检查(inspect)分析元素,可以看出:\n'.item'包裹了一个个词条\n每一个词条下面\n'.pic a img'的src属性包含了封面图片地址'.info .hd a'的src属性包含了豆瓣链接'.info .hd a .title'中的文字包含了标题，因为每个电影会有多个别名，我们只用取第一个标题就行了。'.info .bd .star .rating_num'包含了分数'.info .bd .quote span.inq'包含了短评\n另外导演、年代、主演、简介等信息需要点击进入条目的才能爬取，我们先爬取以上五条信息吧！\n按照刚刚的解析，填写parse()函数\n# douban_crawler/ > spiders/ > douban.py\n\n# -*- coding: utf-8 -*-\nimport scrapy\n\nclass DoubanSpider(scrapy.Spider):\n    name = 'douban'\n    allowed_domains = ['movie.douban.com/top250']\n    start_urls = ['https://movie.douban.com/top250/']\n\n    def parse(self, response):\n        items = response.css('.item')\n        for item in items:\n            yield {\n                'cover_pic': item.css('.pic a img::attr(src)').extract_first(),\n                'link': item.css('.info .hd a::attr(href)').extract_first(),\n                'title': item.css('.info .hd a .title::text').extract_first(),\n                'rating': item.css('.info .bd .star .rating_num::text').extract_first(),\n                'quote': item.css('.info .bd .quote span.inq::text').extract_first()\n            }\n\n.css()运行类似于jquery或pyquery的解析器，但它可以用::text或::attr(href)来直接获取属性或文字，应当说比jquery解析器还要更方便。\n当然，.css只能返回一个对象，而需要具体的文字或属性，则需要.extract()或.extract_first()\n其中.extract()返回所有符合条件的文字或属性数组，而.extract_first()只返回查询到的第一条文字或属性。\n在shell里验证解析语句是否正确\n这里我们需要给大家一个窍门，就是先用shell验证刚刚写的css对不对\n在pycharm窗口左下方打开terminal命令行输入：\n>>> scrapy shell https://movie.douban.com/top250\n会出来一堆的返回信息，最后会出来一堆提示\n[s] Available Scrapy objects:\n[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n[s]   crawler    <scrapy.crawler.Crawler object at 0x10d276f28>\n[s]   item       {}\n[s]   request    <GET https://movie.douban.com/top250>\n[s]   response   <403 https://movie.douban.com/top250>\n[s]   settings   <scrapy.settings.Settings object at 0x10e543128>\n[s]   spider     <DefaultSpider 'default' at 0x10fa99080>\n[s] Useful shortcuts:\n[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \n[s]   shelp()           Shell help (print this help)\n[s]   view(response)    View response in a browser\n\n我们看到response一项\n——什么？返回的403错误？\n原来我们爬取豆瓣没有设置User-Agent请求头，而豆瓣不接受无请求头的Get请求，最终返回了403错误。\n这时赶紧在settings.py里面加入一行\n#  spider_crawler/ > settings.py\n\nUSER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36' \n保存文件后，重新运行\n>>> scrapy shell https://movie.douban.com/top250\n提示：\n[s]   response   <200 https://movie.douban.com/top250>\n这时就可以开始检验刚刚的解析语句能不能获得想要的结果了：\nresponse.css('.item')\n返回了一个<Selector>对象数组，我们取其中的第一个selector\n>>> items = response.css('.item')\n>>> item = item[0]\n\n#把cover_pic对应的解析语句拷贝过来\n>>> item.css('.pic a img::attr(src)').extract_first()\n返回\n'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p480747492.jpg'\n即证明解析语句正确，其它的四项可以一一验证\n>>> item.css('.pic a img::attr(src)').extract_first()\n'https://img3.doubanio.com/view/photo/s_ratio_poster/public/p480747492.jpg'\n>>> item.css('.info .hd a::attr(href)').extract_first()\n'https://movie.douban.com/subject/1292052/'\n>>> item.css('.info .hd a .title::text').extract_first()\n'肖申克的救赎'\n>>> item.css('.info .bd .star .rating_num::text').extract_first()\n'9.6'\n>>> item.css('.info .bd .quote span.inq::text').extract_first()\n'希望让人自由。'\n这时候用exit()退出shell再运行爬虫\n>>> scrapy crawl douban\n就可以看到解析后的数据输出了！\n翻页爬取全部250条数据\n刚刚我们初步爬取了一下，但这一页只显示25条，如何通过翻页爬到全部250条呢？\n通过chrome浏览器的“检查”功能，我们找到豆瓣页面上的“下页”所对应的链接：\nresponse.css('.paginator .next a::attr(href)')\n现在我们改写一个douban.py\n# douban_crawler/ > spiders/ > douban.py\nclass DoubanSpider(scrapy.Spider):\n    name = 'douban'\n    allowed_domains = ['movie.douban.com/top250']\n    start_urls = ['https://movie.douban.com/top250']\n\n    def parse(self, response):\n        items = response.css('.item')\n        for item in items:\n            yield {\n                'cover_pic': item.css('.pic a img::attr(src)').extract_first(),\n                'link': item.css('.info .hd a::attr(href)').extract_first(),\n                'title': item.css('.info .hd a .title::text').extract_first(),\n                'rating': item.css('.info .bd .star .rating_num::text').extract_first(),\n                'quote': item.css('.info .bd .quote span.inq::text').extract_first()\n            }\n        next_page = response.css('.paginator .next a::attr(href)').extract_first()\n        if next_page:\n            next_page_real = response.urljoin(next_page)\n            yield scrapy.Request(next_page_real, callback=self.parse,dont_filter=True)\n以上通过response.urljoin()返回了拼接后的真实url然后通过一个递归返回Request对象，遍历了所有的页面。\n注意！爬豆爬一定要加入dont_filter=True选项，因为scrapy只要解析到网站的Url有'filter='，就会自动进行过滤处理，把处理结果分配到相应的类别，但偏偏豆瓣url里面的filter为空不需要分配，所以一定要关掉这个选项。\n这时再运行一次爬虫\n>>> scrapy crawl douban\n250条数据已经爬取出来啦！\n存储数据到文件\n很简单，运行爬虫时加个-o就输出啦！\n>>> scrapy crawl douban -o douban.csv\n于是你看到当前目录下多了一个.csv文件，存储的正是我们想要的结果！\n你也可以用 .json .xml .pickle .jl 等数据类型存储，scrapy也是支持的！\n利用Items把爬到的数据结构化\nScrapy的Item功能很类似于Django或其它mvc框架中的model作用，即把数据转化成固定结构，这样才能便出保存和展示。\n我们打开 items.py文件,如下定义一个名为DoubanItem的数据类型。\n# douban_clawler > items.py\nimport scrapy\n\nclass DoubanItem(scrapy.Item):\n    title = scrapy.Field()\n    link = scrapy.Field()\n    rating = scrapy.Field()\n    cover_pic = scrapy.Field()\n    quote = scrapy.Field()\n有了DoubanItem后，就可以改造douban.py里面的parse函数，使爬取的信息全部转化为Item形式啦\n# -*- coding: utf-8 -*-\nimport scrapy\nfrom douban_crawler.items import DoubanItem\n\nclass DoubanSpider(scrapy.Spider):\n    name = 'douban'\n    allowed_domains = ['movie.douban.com/top250']\n    start_urls = ['https://movie.douban.com/top250']\n\n    def parse(self, response):\n        items = response.css('.item')\n        for item in items:\n            yield DoubanItem({\n                'cover_pic': item.css('.pic a img::attr(src)').extract_first(),\n                'link': item.css('.info .hd a::attr(href)').extract_first(),\n                'title': item.css('.info .hd a .title::text').extract_first(),\n                'rating': item.css('.info .bd .star .rating_num::text').extract_first(),\n                'quote': item.css('.info .bd .quote span.inq::text').extract_first()\n            })\n\n        next_page = response.css('.paginator .next a::attr(href)').extract_first()\n        if next_page:\n            next_page_real = response.urljoin(next_page)\n            yield scrapy.Request(next_page_real, callback=self.parse,dont_filter=True)\n非常简单，只修改了两行：\n\n引入DoubanItem\n原来yield的一个dict格式，现在直接在DoubanItem中传入dict就可以把dict转化成DoubanItem对象了！\n\n现在你可以scrapy crawl douban再试一次爬取，看是不是已经转换成了DoubanItem形式了？\n存储数据到MongoDB\n有了DoubanItem数据结构，我们就可以保存进MongoDB啦！\n保存到MongoDB，我们需要用到pipline组件，没错，就是上面那一堆文件中的piplines.py\n之前我们确保两件事：\n\nmongodb服务已经开启。如果没有开启请sudo mongod开启本地。\npymongo包已安装。如果没有安装请pip install pymongo\n\n使用pipline，请记住四个字！\n启！\n（开启爬虫）对应于open_spider，在spider开启时调用\n在这里面启动mongodb\n承！\n（承接爬取任务）对应于from_clawler，它有几个特点：\n\n它是类对象，所以必须加上@classmethod。\n只要有这个函数，它就一定会被调用。\n它必须返回我们Pipline本身对象的实例。\n它有进入Scrapy所有核心组件的能力，也就是说可以通过它访问到settings对象。\n\n转！\n(转换对象)对应于process_item，它有几个特点：\n\n必须返回Item对象或raise DropItem()错误\n在这个函数内将传入爬取到的item并进行其它操作。\n\n合！\n(闭合爬虫)于应于close_spider，在spider关闭是调用\n我们可以在这个函数内关闭MongoDB\n有了以上知道，我们上代码！\n# douban_crawler > piplines.py\nimport pymongo\n\n\nclass DoubanCrawlerPipeline(object):\n    def __init__(self, mongo_uri, mongo_db):\n        self.mongo_uri = mongo_uri\n        self.mongo_db = mongo_db\n\n    def open_spider(self, spider):\n        self.client = pymongo.MongoClient(self.mongo_uri)\n        self.db = self.client[self.mongo_db]\n\n    @classmethod\n    def from_crawler(cls, crawler):\n        return cls(\n            mongo_uri = crawler.settings.get('MONGO_URI'),\n            mongo_db = crawler.settings.get('MONGO_DB')\n        )\n\n    def process_item(self, item, spider):\n        self.db['douban250'].insert_one(dict(item))\n        return item\n\n    def close_spider(self, spider):\n        self.client.close()\n同时在setting中添加MONGODB的配置\nMONGO_URI = \"localhost\"\nMONGO_DB = \"douban\"\n还有非常重要的一步！在setting中打开pipline的注释！！\nITEM_PIPELINES = {\n   'douban_crawler.pipelines.DoubanCrawlerPipeline': 300,\n}\n现在打开crawler\nscrapy crawl douban\n爬到的信息已经保存到数据库了！\n\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "7"}