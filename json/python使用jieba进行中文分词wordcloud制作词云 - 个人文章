{"title": "python使用jieba进行中文分词wordcloud制作词云 - 个人文章 ", "index": "python,jieba分词", "content": "准备工作\n抓取数据存到txt文档中，了解jieba\n问题\n\njieba分词分的不太准确，比如机器学习会被切成机器和学习两个词，使用自定义词典，原本的想法是只切出自定义词典里的词，但实际上不行，所以首先根据jieba分词结果提取出高频词并自行添加部分词作为词典，切词完毕只统计自定义词典里出现过的词\nwordcloud自身不支持中文词云，需要指定中文字体，并且现在大部分的博客提供的generate_from_frequencies方法的参数与现在的wordcloud的参数不同，现在这个方法接收的是dict类型\n\n代码\n# -*- coding: utf-8 -*-\nimport jieba\nimport os\nimport codecs\nfrom scipy.misc import imread\nimport matplotlib as mpl \nimport matplotlib.pyplot as plt \nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nclass GetWords(object):\n    def __init__(self, dict_name, file_list , dic_list):\n        self.dict_name = dict_name\n        self.file_list = file_list\n        self.dic_list = dic_list\n    #获取自定义词典\n    def get_dic(self):  \n        dic = open(self.dict_name, 'r')\n        while 1:\n            line = dic.readline().decode('utf-8').strip()\n            self.dic_list.append(line)\n            if not line:\n                break\n            pass\n            \n    def get_word_to_cloud(self):\n        for file in self.file_list:\n            with codecs.open('../spider/' + file, \"r\",encoding='utf-8', errors='ignore') as string:\n                string = string.read().upper()\n                res = jieba.cut(string, HMM=False)\n                reslist = list(res)\n                wordDict = {}\n                for i in reslist:\n                    if i not in self.dic_list:\n                        continue\n                    if i in wordDict:\n                        wordDict[i]=wordDict[i]+1\n                    else:\n                        wordDict[i] = 1\n\n            coloring = imread('test.jpeg')\n\n            wc = WordCloud(font_path='msyh.ttf',mask=coloring,\n                    background_color=\"white\", max_words=50,\n                    max_font_size=40, random_state=42)\n\n            wc.generate_from_frequencies(wordDict)\n\n            wc.to_file(\"%s.png\"%(file))\n\ndef set_dic():\n    _curpath=os.path.normpath( os.path.join( os.getcwd(), os.path.dirname(__file__) ))\n    settings_path = os.environ.get('dict.txt')\n    if settings_path and os.path.exists(settings_path):\n        jieba.set_dictionary(settings_path)\n    elif os.path.exists(os.path.join(_curpath, 'data/dict.txt.big')):\n        jieba.set_dictionary('data/dict.txt.big')\n    else:\n        print \"Using traditional dictionary!\"\n \nif __name__ == '__main__':\n    set_dic()\n    file_list = ['data_visualize.txt', 'data_dev.txt', 'data_mining.txt', 'data_arc.txt', 'data_analysis.txt']\n    dic_name = 'dict.txt'\n    dic_list = []\n    getwords = GetWords(dic_name, file_list, dic_list)\n    getwords.get_dic()\n    getwords.get_word_to_cloud()\n\n词云示例\n此图为爬取拉勾网数据挖掘工程师岗位需要制作的词云\n源码\ngithub\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "1"}