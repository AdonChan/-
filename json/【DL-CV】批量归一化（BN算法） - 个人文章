{"title": "【DL-CV】批量归一化（BN算法） - 个人文章 ", "index": "python,人工智能,深度学习,计算机视觉", "content": "【DL-CV】数据预处理&权重初始化<前篇---后篇>【DL-CV】正则化，Dropout\n先来交代一下背景：在网络训练的过程中，参数的更新会导致网络的各层输入数据的分布不断变化，那么各层在训练的过程中就需要不断的改变以适应这种新的数据分布，从而造成网络训练困难，收敛变慢（而且网络越深越难），在论文中这个问题被称为“Internal Covariate Shift”。为了解决这个问题出现了批量归一化的算法，他对每一层的输入进行归一化，保证每层的输入数据分布是稳定的，从而加速训练\n批量归一化（Batch Normalization/BN）\n\nNormalization——“归一化”\nBatch——“批”，一批样本输入，batch_size=m，m个样本\n\n与激活函数层、卷积层（cnn）、全连接层（FC）、池化层一样，批量归一化也属于网络的一层，简称BN。BN通常用在FC/cnn之后，激活函数层之前，他对FC/cnn的输出的每一维进行归一化（归一化至：均值0、方差为1），然后变换重构后再把数据喂给下一层网络。\n归一化\n对输入数据的每一维xk（每一列）进行归一化得到$\\hat{x}^{k}$$$\\hat{x}^{(k)} = \\frac{x^{k} - E[x^{k}]}{\\sqrt{Var[x^{k}]}}$$\n\n通过该公式我们强行把数据归一化，特征分布成功被压缩至标准差为1。但这种变化是有破坏性的，原来的特征分布都被搞乱了那还学个鬼？——为了解决这一问题，我们对归一化后的数据再进行变换重构，以恢复原来的特征\n变换重构\n对于归一化后输入$\\hat{x}^{k}$，我们再引入两个可供学习的参数$\\gamma$和$\\beta$使$$y^{k} = \\gamma^{k}\\hat{x}^{k}+\\beta^{k}$$注意如果$\\gamma^{k}=\\sqrt{Var[x^k]}$，$\\beta^{k}=E[x^k]$时，是可以恢复原有的特征的。这就是引入这两个参数的意义，虽然网络不太可能学习到$\\gamma^{k}=\\sqrt{Var[x^k]}$，$\\beta^{k}=E[x^k]$，但是好歹通过学习到这两个参数尽可能地回复原有的特征（实践中的表现说明这是有用的）\n\n最后总结BN操作：（m是mini batch size，根号中的ε是一个很小的数防止分母为0）\n测试时！\n上面讲了这么多其实都是训练时的BN算法，训练时是一个一个batch进去的，训练完成后得到$\\gamma$和$\\beta$。但是在测试时，我们只是丢一个样本进去让网络预测，试问一个样本何来均值和方差？均值和方差最好的来源是整个训练集，但是整个训练集往往太大，我们不太想算。能不能从现成的 mini batch 的均值和方差（因为在训练时算了出来）估计出整个训练集的均值和方差呢——当然可以，因为mini batch是随机取样的：$$E[x] = E_B[\\mu_B]$$$$Var[x] = \\frac{m}{m-1}E_B[\\sigma_B^2]$$即平均值为所有mini-batch的平均值的平均值，而方差为每个batch的方差的无偏估计。当网络训练完后，我们通过训练时保存下来的每个batch的均值与方差估算出全局均值与方差供测试时使用，也就是说当网络训练完后每个BN层中的均值也和标准差也会随之确定。\nBN好处有啥\n\n能选择较大的初始学习率，提高训练收敛速度\n你再也不用去理会过拟合中drop out、L2正则项参数的选择问题，采用BN算法后，你可以移除这两项了参数，或者可以选择更小的L2正则约束参数了，因为BN自带正则化的特性，能提高网络泛化能力\n减少梯度爆炸/消失的出现，特别对于tanh，sigmoid这类会两端饱和的激活函数\n减少对参数初始化的依赖，就算参数初始化得不好，在BN的加持下网络还是表现较好\n\n虽然BN是2015才提出来的（看上去像个新米），但是他表现也超凡，BN算法已经被大量使用于深度学习中，真的是nb\n补充\n\nBatch Normalization 论文\n以上的BN使用是基于全连接层的，输入数据都是二维矩阵。BN也可以用在cnn中，用法不同，可以参考此文章\n\n推荐 BN导读\n\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}