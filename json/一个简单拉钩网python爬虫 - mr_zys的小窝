{"title": "一个简单拉钩网python爬虫 - mr_zys的小窝 ", "index": "python,网页爬虫", "content": "前期准备\nDon't be evil！\n主要就是分析需要抓取的页面，或许会有意外惊喜，直接找到获取数据的接口。\n首选打开拉钩网首页，发现一个职位至少有一个一级分类、二级分类和一个标签。比如需要点击查找Java相关的职位，需要找到技术、后端开发，然后点击Java，才能获取Java相关的职位列表。\n\n职位列表就可以得到很多的关于职位的信息了，如职位名称、公司名称、公司logo、所属行政区、商区等信息，但是需要提取这些信息的时候，xpath会让人写的很头疼。\n\n如果想这样解析页面获取数据有以下几个问题：\n\n职位列表最多只有30页\n职位关键信息获取不全\n\n比如一个职位的地点应该属于XX市XX区XX商区XX楼，这种信息只靠解析页面是获取不全的。不过当点击城市的时候，查看发送的网络请求，可以发现一个可喜的接口，这个接口将会返回一个职位列表的json数据。\n\n但是因为为了防止爬虫调用这个接口，使用接口的时候必须要带上cookie。访问首页的时候，会获取部分cookie值，但是唯独没有SEARCH_ID这个值。\n\n但是这个cookie不是平白无辜的产生的，想想调用这个接口的页面入口，观察response的header，会发现有Set-Cookie:SEARCH_ID=7587e152a3b14eec8bb0f29e774e4094; Version=1; Max-Age=86400; Expires=Sun, 26-Feb-2017 17:16:05 GMT; Path=/。\n\n这样前期的准备工作就完成了，可以开始编码了。\n开始编码\n通过前期分析，可以发现需要做这么几项工作：\n\n访问首页和职位过滤页面获取cookie\n带上cookie调用接口\n\n具体的流程：\n\n访问首页，获取所有职位的标签\n按城市调用接口，同时获取所有cookie\n保存数据\n\n所以需要一些工具。\n动态加载页面和获取cookie：\n\nSelenium with Python\nPhantomJS\n\n带上cookie调用接口：\nRequests: HTTP for Humans\n源码\n结果\n抓取的总量（已经将数据保存在mongodb中了）：\n\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "10"}