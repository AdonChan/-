{"title": "【虫术】资深爬虫师带你爬取代理IP - 个人文章 ", "index": "windows,mongodb,python", "content": "有时候在网站看小说，会莫名跳出来一个“疑似机器恶意爬取，暂时无法访问”这样类似的网站提示，需要刷新一下或者输入一个验证码才能重新进入，这样的情况偶有发生，相信大家都有遇到过。出现这个现象的原因就是我们浏览的网页采取了反爬虫的措施，特别做爬虫爬取网页，在某个ip单位时间请求网页次数过多时，服务器会拒绝服务，这种情况就是由于访问频率引起的封ip，这种情况靠解封不能很好的解决，所以我们就想到了伪装本机ip去请求网页，也就是我们今天要讲的使用代理ip。\n\n目前网上有许多代理ip，有免费的也有付费的，例如西刺代理，豌豆代理，快代理等等，免费的虽然不用花钱但有效的代理很少且不稳定，付费的可能会好一点，不过今天我只爬取免费的西刺代理并将检测是否可用，将可用ip存入MongoDB，方便下次取出。\n运行平台：Windows\nPython版本：Python3.6\nIDE: Sublime Text\n其他：Chrome浏览器\n简述流程为：\n步骤1：了解requests代理如何使用\n步骤2：从西刺代理网页爬取到ip和端口\n步骤3：检测爬取到的ip是否可用\n步骤4：将爬取的可用代理存入MongoDB\n步骤5：从存入可用ip的数据库里随机抽取一个ip,测试成功后返回\n对于requests来说，代理的设置比较简单，只需要传入proxies参数即可。\n不过需要注意的是，这里我是在本机安装了抓包工具Fiddler，并用它在本地端口8888创建了一个HTTP代理服务（用Chrome插件SwitchyOmega），即代理服务为：127.0.0.1:8888，我们只要设置好这个代理，就可以成功将本机ip切换成代理软件连接的服务器ip了。\n\nimport requests\n\nproxy = '127.0.0.1:8888'\nproxies = {\n    'http':'http://' + proxy,\n    'https':'http://' + proxy\n}\n\ntry:\n    response = requests.get('http://httpbin.org/get',proxies=proxies)\n    print(response.text)\nexcept requests.exceptions.ConnectionError as e:\n    print('Error',e.args)\n\n\n这里我是用来http://httpbin.erg/get作为测...，我们访问该网页可以得到请求的有关信息，其中origin字段就是客户端ip，我们可以根据返回的结果判断代理是否成功。返回结果如下：\n{\n\"args\":{}，\n\"headers\":{\n    \"Accept\":\"*/*\",\n    \"Accept-Encoding\":\"gzip, deflate\",\n    \"Connection\":\"close\",\n    \"Host\":\"httpbin.org\",\n    \"User-Agent\":\"python-requests/2.18.4\"\n},\n\"origin\":\"xx.xxx.xxx.xxx\",\n\"url\":\"http://httpbin.org/get\"\n}\n接下来我们便开始爬取西刺代理，首先我们打开Chrome浏览器查看网页，并找到ip和端口元素的信息。\n\n可以看到，西刺代理以表格存储ip地址及其相关信息，所以我们用BeautifulSoup提取时很方便便能提取出相关信息，但是我们需要注意的是，爬取的ip很有可能出现重复的现象，尤其是我们同时爬取多个代理网页又存储到同一数组中时，所以我们可以使用集合来去除重复的ip。\n27def scrawl_xici_ip(num):\n 28    '''\n 29    爬取代理ip地址，代理的url是西刺代理\n 30    '''  \n 31    ip_list = []\n 32    for num_page in range(1,num):\n 33        url = url_ip + str(num_page)\n 34        response = requests.get(url,headers=headers)\n 35        if response.status_code == 200:\n 36            content = response.text\n 37            soup = BeautifulSoup(content,'lxml')\n 38            trs = soup.find_all('tr')\n 39            for i in range(1,len(trs)):\n 40                tr = trs[i]\n 41                tds = tr.find_all('td')      \n 42                ip_item = tds[1].text + ':' + tds[2].text\n 43                # print(ip_item)\n 44                ip_list.append(ip_item)\n 45                ip_set = set(ip_list) # 去掉可能重复的ip\n 46                ip_list = list(ip_set)\n 47            time.sleep(count_time) # 等待5秒\n 48    return ip_list\n\n将要爬取页数的ip爬取好后存入数组，然后再对其中的ip逐一测试。\n 51def ip_test(url_for_test,ip_info):\n 52    '''\n 53    测试爬取到的ip，测试成功则存入MongoDB\n 54    '''\n 55    for ip_for_test in ip_info:\n 56        # 设置代理\n 57        proxies = {\n 58            'http': 'http://' + ip_for_test,\n 59            'https': 'http://' + ip_for_test,\n 60            }\n 61        print(proxies)\n 62        try:\n 63            response = requests.get(url_for_test,headers=headers,proxies=proxies,timeout=10)\n 64            if response.status_code == 200:\n 65                ip = {'ip':ip_for_test}\n 66                print(response.text)\n 67                print('测试通过')\n 68                write_to_MongoDB(ip)    \n 69        except Exception as e:\n 70            print(e)\n 71            continue\n\n这里就用到了上面提到的requests设置代理的方法，我们使用http://httpbin.org/ip作为测试...，它可以直接返回我们的ip地址，测试通过后再存入MomgoDB数据库。\n\n存入MongoDB的方法在上一篇糗事百科爬取已经提过了。连接数据库然后指定数据库和集合，再将数据插入就OK了。\n 74def write_to_MongoDB(proxies):\n 75    '''\n 76    将测试通过的ip存入MongoDB\n 77    '''\n 78    client = pymongo.MongoClient(host='localhost',port=27017)\n 79    db = client.PROXY\n 80    collection = db.proxies\n 81    result = collection.insert(proxies)\n 82    print(result)\n 83    print('存储MongoDB成功')\n\n最后运行查看一下结果吧\n\n如果对Python编程、网络爬虫、机器学习、数据挖掘、web开发、人工智能、面试经验交流。感兴趣可以519970686，群内会有不定期的发放免费的资料链接，这些资料都是从各个技术网站搜集、整理出来的，如果你有好的学习资料可以私聊发我，我会注明出处之后分享给大家。\n稍等，运行了一段时间后，难得看到一连三个测试通过，赶紧截图保存一下，事实上是，毕竟是免费代理，有效的还是很少的，并且存活时间确实很短，不过，爬取的量大，还是能找到可用的，我们只是用作练习的话，还是勉强够用的。现在看看数据库里存储的吧。\n\n因为爬取的页数不多，加上有效ip也少，再加上我没怎么爬，所以现在数据库里的ip并不多，不过也算是将这些ip给存了下来。现在就来看看怎么随机取出来吧。\n 85\n 86def get_random_ip():\n 87    '''\n 88    随机取出一个ip\n 89    '''\n 90    client = pymongo.MongoClient(host='localhost',port=27017)\n 91    db = client.PROXY\n 92    collection = db.proxies\n 93    items = collection.find()\n 94    length = items.count()\n 95    ind = random.randint(0,length-1)\n 96    useful_proxy = items[ind]['ip'].replace('\\n','')\n 97    proxy = {\n 98        'http': 'http://' + useful_proxy,\n 99        'https': 'http://' + useful_proxy,\n100        }   \n101    response = requests.get(url_for_test,headers=headers,proxies=proxy,timeout=10)\n102    if response.status_code == 200:\n103        return useful_proxy\n104    else:\n105        print('此{ip}已失效'.format(useful_proxy))\n106        collection.remove(useful_proxy)\n107        print('已经从MongoDB移除')\n108        get_random_ip()\n109\n\n由于担心放入数据库一段时间后ip会失效，所以取出前我重新进行了一次测试，如果成功再返回ip，不成功的话就直接将其移出数据库。\n\n这样我们需要使用代理的时候，就能通过数据库随时取出来了。\n总的代码如下：\nimport random\nimport requests\nimport time\nimport pymongo\nfrom bs4 import BeautifulSoup\n\n# 爬取代理的URL地址，选择的是西刺代理\nurl_ip = \"http://www.xicidaili.com/nt/\"\n\n# 设定等待时间\nset_timeout = 5\n\n# 爬取代理的页数，2表示爬取2页的ip地址\nnum = 2\n\n# 代理的使用次数\ncount_time = 5\n\n# 构造headers\nheaders = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.146 Safari/537.36'}\n\n# 测试ip的URL\nurl_for_test = 'http://httpbin.org/ip'\n\ndef scrawl_xici_ip(num):\n    '''\n    爬取代理ip地址，代理的url是西刺代理\n    '''  \n    ip_list = []\n    for num_page in range(1,num):\n        url = url_ip + str(num_page)\n        response = requests.get(url,headers=headers)\n        if response.status_code == 200:\n            content = response.text\n            soup = BeautifulSoup(content,'lxml')\n            trs = soup.find_all('tr')\n            for i in range(1,len(trs)):\n                tr = trs[i]\n                tds = tr.find_all('td')      \n                ip_item = tds[1].text + ':' + tds[2].text\n                # print(ip_item)\n                ip_list.append(ip_item)\n                ip_set = set(ip_list) # 去掉可能重复的ip\n                ip_list = list(ip_set)\n            time.sleep(count_time) # 等待5秒\n    return ip_list\n\ndef ip_test(url_for_test,ip_info):\n    '''\n    测试爬取到的ip，测试成功则存入MongoDB\n    '''\n    for ip_for_test in ip_info:\n        # 设置代理\n        proxies = {\n            'http': 'http://' + ip_for_test,\n            'https': 'http://' + ip_for_test,\n            }\n        print(proxies)\n        try:\n            response = requests.get(url_for_test,headers=headers,proxies=proxies,timeout=10)\n            if response.status_code == 200:\n                ip = {'ip':ip_for_test}\n                print(response.text)\n                print('测试通过')\n                write_to_MongoDB(ip)    \n        except Exception as e:\n            print(e)\n            continue\n\ndef write_to_MongoDB(proxies):\n    '''\n    将测试通过的ip存入MongoDB\n    '''\n    client = pymongo.MongoClient(host='localhost',port=27017)\n    db = client.PROXY\n    collection = db.proxies\n    result = collection.insert(proxies)\n    print(result)\n    print('存储MongoDB成功')\n\ndef get_random_ip():\n    '''\n    随机取出一个ip\n    '''\n    client = pymongo.MongoClient(host='localhost',port=27017)\n    db = client.PROXY\n    collection = db.proxies\n    items = collection.find()\n    length = items.count()\n    ind = random.randint(0,length-1)\n    useful_proxy = items[ind]['ip'].replace('\\n','')\n    proxy = {\n        'http': 'http://' + useful_proxy,\n        'https': 'http://' + useful_proxy,\n        }   \n    response = requests.get(url_for_test,headers=headers,proxies=proxy,timeout=10)\n    if response.status_code == 200:\n        return useful_proxy\n    else:\n        print('此{ip}已失效'.format(useful_proxy))\n        collection.remove(useful_proxy)\n        print('已经从MongoDB移除')\n        get_random_ip()\n\ndef main():\n    ip_info = []\n    ip_info = scrawl_xici_ip(2)\n    sucess_proxy = ip_test(url_for_test,ip_info)\n    finally_ip = get_random_ip()\n    print('取出的ip为：' + finally_ip)\n\nif __name__ == '__main__':\n    main()\n\n\n\n【给技术人一点关爱！！！】\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "1"}