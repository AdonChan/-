{"title": "Python工具分析风险数据 - 互联网业务风险控制 ", "index": "python", "content": "随着网络安全信息数据大规模的增长,应用数据分析技术进行网络安全分析成为业界研究热点，小安在这次小讲堂中带大家用Python工具对风险数据作简单分析，主要是分析蜜罐日志数据，来看看一般大家都使用代理ip干了一些啥事。\n大家可能会问啥是蜜罐，网上一些黑客或技术人员经常做一些＂事情＂的时候，需要隐藏自己身份，这样他们会使用代理IP来办事。而蜜罐(Honeypot)是一种新型的主动防御的安全技术，它是一个专门为了被攻击或入侵而设置的欺骗系统——既可以用于保护产品系统，又可用于搜集黑客信息，是一种配置灵活、形式多样的网络安全技术。\n说得通俗一点就是提供大量代理IP，引诱一些不法分子来使用代理这些代理ip，从而搜集他们的信息。\n数据分析工具介绍\n工欲善其事，必先利其器，在此小安向大家介绍一些Python数据分析的“神兵利器“。\nPython中著名的数据分析库Panda\nPandas库是基于NumPy 的一种工具，该工具是为了解决数据分析任务而创建，也是围绕着 Series 和 DataFrame 两个核心数据结构展开的，其中Series 和 DataFrame 分别对应于一维的序列和二维的表结构。\nPandas提供了大量能使我们快速便捷地处理数据的函数和方法。这个库优点很多，简单易用，接口抽象得非常好，而且文档支持实在感人。你很快就会发现，它是使Python成为强大而高效的数据分析环境的重要因素之一。\n数据可视化采用Python上最常用的Matplotlib库\nMatplotlib是一个Python的图形框架，也是Python最著名的绘图库，它提供了一整套和Matlab相似的命令API，十分适合交互式地进行制图。\n我们有了这些“神兵利器“在手，下面将带大家用Python这些工具对蜜罐代理数据作一个走马观花式的分析介绍。\n1.引入工具–加载数据分析包\n启动IPython notebook，加载运行环境：\n%matplotlib inline\nimport pandas as pd\nfrom datetime import timedelta, datetime\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n2.数据准备\n俗话说: 巧妇难为无米之炊。小安分析的数据主要是用户使用代理IP访问日志记录信息，要分析的原始数据以CSV的形式存储。这里首先要介绍到pandas.read_csv这个常用的方法，它将数据读入DataFrame。\nanalysis_data = pd.read_csv('./honeypot_data.csv') \n\n对的, 一行代码就可以将全部数据读到一个二维的表结构DataFrame变量，感觉很简单有木有啊！！！当然了用Pandas提供的IO工具你也可以将大文件分块读取，再此小安测试了一下性能，完整加载约21530000万条数据也大概只需要90秒左右，性能还是相当不错。\n3.数据管窥\n一般来讲，分析数据之前我们首先要对数据有一个大体上的了解，比如数据总量有多少，数据有哪些变量，数据变量的分布情况，数据重复情况，数据缺失情况，数据中异常值初步观测等等。下面小安带小伙伴们一起来管窥管窥这些数据。\n使用shape方法查看数据行数及列数\nanalysis_data.shape\nOut: (21524530, 22) #这是有22个维度，共计21524530条数据记的DataFrame\n\n使用head()方法默认查看前5行数据，另外还有tail()方法是默认查看后5行,当然可以输入参数来查看自定义行数\nanalysis_data.head(10) \n\n\n这里可以了解到我们数据记录有用户使用代理IP日期，代理header信息，代理访问域名，代理方法，源ip以及蜜罐节点信息等等。在此小安一定一定要告诉你，小安每次做数据分析时必定使用的方法–describe方法。pandas的describe()函数能对数据进行快速统计汇总：\n\n对于数值类型数据，它会计算出每个变量： 总个数，平均值，最大值，最小值，标准差，50%分位数等等；\n非数值类型数据，该方法会给出变量的： 非空值数量、unique数量(等同于数据库中distinct方法)、最大频数变量和最大频数。\n\n由head()方法我们可以发现数据中包含了数值变量、非数值变量，我们首先可以利用dtypes方法查看DataFrame中各列的数据类型，用select_dtypes方法将数据按数据类型进行分类。然后，利用describe方法返回的统计值对数据有个初步的了解：\ndf.select_dtypes(include=['O']).describe() \n\n\ndf.select_dtypes(include=['float64']).describe()\n\n\n简单的观察上面变量每一维度统计结果，我们可以了解到大家获取代理数据的长度平均1670个字节左右。同时，也能发现字段scan_os_sub_fp，scan_scan_mode等存在空值等等信息。这样我们能对数据整体上有了一个大概了解。\n4.数据清洗\n由于源数据通常包含一些空值甚至空列，会影响数据分析的时间和效率，在预览了数据摘要后，需要对这些无效数据进行处理。\n一般来说，移除一些空值数据可以使用dropna方法， 当你使用该方法后，检查时发现 dropna() 之后几乎移除了所有行的数据，一查Pandas用户手册，原来不加参数的情况下， dropna() 会移除所有包含空值的行。 \n如果你只想移除全部为空值的列，需要加上 axis 和 how 两个参数：\nanalysis_data.dropna(axis=1, how='all')\n\n另外，也可以通过dropna的参数subset移除指定列为空的数据，和设置thresh值取移除每非None数据个数小于thresh的行。\nanalysis_data.dropna(subset=['proxy_host', 'srcip'])\n#移除proxy_host字段或srcip字段没有值的行\nanalysis_data.dropna(thresh=10)\n#移除所有行字段中有值属性小于10的行\n\n5.统计分析\n再对数据中的一些信息有了初步了解过后，原始数据有22个变量。从分析目的出发，我将从原始数据中挑选出局部变量进行分析。这里就要给大家介绍pandas的数据切片方法loc。\nloc([start_row_index:end_row_index,[‘timestampe’, ‘proxy_host’, ‘srcip’]])是pandas重要的切片方法，逗号前面是对行进行切片；逗号后的为列切片，也就是挑选要分析的变量。\n如下，我这里选出日期，host和源IP字段——\nanalysis_data = analysis_data.loc([:, [‘timestampe’,'proxy_host','srcip']])\n\n首先让我们来看看蜜罐代理每日使用数据量，我们将数据按日统计，了解每日数据量PV，并将结果画出趋势图。\ndaily_proxy_data = analysis_data[analysis_data.module=='proxy']\ndaily_proxy_visited_count = daily_proxy_data.timestamp.value_counts().sort_index()\ndaily_proxy_visited_count.plot()\n\n\n对数据列的丢弃，除无效值和需求规定之外，一些表自身的冗余列也需要在这个环节清理，比如说DataFrame中的index号、类型描述等，通过对这些数据的丢弃，从而生成新的数据，能使数据容量得到有效的缩减，进而提高计算效率。\n由上图分析可知蜜罐代理使用量在6月5号，19-22号和25号这几天呈爆炸式增长。那么这几天数据有情况，不正常，具体是神马情况，不急，后面小安带大家一起来慢慢揪出来到底是那些人(源ip) 干了什么“坏事”。\n进一步分析, 数据有异常后，再让我们来看看每天去重IP数据后量及其增长量。可以按天groupby后通过nunique()方法直接算出来每日去重IP数据量。\ndaily_proxy_data = analysis_data[analysis_data.module=='proxy']\ndaily_proxy_visited_count = daily_proxy_data.groupby(['proxy_host']).srcip.nunique()\ndaily_proxy_visited_count.plot()\n\n究竟大部分人（源ip）在干神马？干神马？干神马？让我们来看看被访问次数最多host的哪些，即同一个host关联的IP个数，为了方便我们只查看前10名热门host。\n先选出host和ip字段，能过groupby方法来group 每个域名(host)，再对每个域名的ip访问里unique统计。\nhost_associate_ip = proxy_data.loc[:, ['proxy_host', 'srcip']]\ngrouped_host_ip = host_associate_ip.groupby(['proxy_host']).srcip.nunique()\nprint(grouped_host_ip.sort_values(ascending=False).head(10))\n\n\n\n代理访问host\n源ip\n\n\n\ncol 3 is\nright-aligned\n\n\ncol 2 is\ncentered\n\n\nzebra stripes\nare neat\n\n\nwww.gan**.com\n1113\n\n\nwap.gan**.com\n913\n\n\nwebim.gan**.com\n710\n\n\ncgi.**.qq.com\n621\n\n\nwww.baidu.com\n615\n\n\nloc.*.baidu.com\n543\n\n\nwww.gan**.com\n1113\n\n\nbaidu.com\n515\n\n\nwww.google.com\n455\n\n\nwww.bing.com\n428\n\n\n12**.ip138.com\n405\n\n\n\n再细细去看大家到底做了啥——查看日志数据发现原来在收集像二手车价格，工人招聘等等信息。从热门host来看，总得来说大家使用代理主要还是获取百度，qq，Google，Bing这类妇孺皆知网站的信息。\n下面再让我们来看看是谁用代理IP“干事”最多，也就是看看谁的IP访问不同host的个数最多。\nhost_associate_ip = proxy_data.loc[:, ['proxy_host', 'srcip']]\ngrouped_host_ip = host_associate_ip.groupby(['srcip'_host']).proxy_host.nunique()\nprint(grouped_host_ip.sort_values(ascending=False).head(10))\n\n\n哦，发现目标IP为123..*.155的小伙子有大量访问记录, 进而查看日志，原来他在大量收集酒店信息。 好了，这样我们就大概能知道谁在干什么了，再让我们来看看他们使用proxy持续时长，谁在长时间里使用proxy。 代码如下——\n这里不给大家细说代码了，只给出如下伪代码。\ndate_ip = analysis_data.loc[:,['timestamp','srcip']]\ngrouped_date_ip = date_ip.groupby(['timestamp', 'srcip'])\n#计算每个源ip(srcip)的访问日期\nall_srcip_duration_times = ...\n#算出最长连续日期天数\nduration_date_cnt =  count_date(all_srcip_duration_times)\n\n\n好了，到此我也就初略的知道那些人做什么，谁用代理时长最长等等问题额。取出ip = 80...38的用户使用代理ip访问数据日志，发现原来这个小伙子在长时间获取搜狐images。\n蜜罐在全国各地部署多个节点，再让我们来看看每个源ip扫描蜜罐节点总个数，了解IP扫描节点覆盖率。结果见如下：\n# 每个IP扫描的IP扫描节点总个数\nnode = df[df.module=='scan']\nnode = node.loc[:,['srcip','origin_details']]\ngrouped_node_count = node.groupby(['srcip']).count()\nprint grouped_node_count.sort_values(['origin_details'], ascending=False).head(10)\n\n\n由上述两表初步可知，一些结论：如源ip为182...205的用户长时间对蜜罐节点进行扫描，mark危险用户等等。\n结语\n给大家简单介绍的用python工具，主要是pandas库来分析数据，当然这个库的功能非常强大，更多的还是要大家自己去使用和探索。\n反爬虫\n文章来源：http://bigsec.com/\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "16"}