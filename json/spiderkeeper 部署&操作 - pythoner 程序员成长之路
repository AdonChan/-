{"title": "spiderkeeper 部署&操作 - pythoner 程序员成长之路 ", "index": "python", "content": "前言\n最近发现了一个spdierkeeper的库,这个库的主要用途是在于.配合这scrpyd管理你的爬虫,支持一键式部署,定时采集任务,启动,暂停等一系列的操作.简单来说将scrapyd的api进行封装,最大限度减少你跟命令行交互次数.不得说这个是很棒的事情.\n\nhttps://github.com/DormyMo/Sp...  SpiderKeeper的github连接\n环境配置\n由于 scrapyd是基于python3+以上的版本兼容性较好,所以我们需要的环境为\n\npython3.4+\nscrapyd\nscrapy & scrapy相关的库\nscrapy_reids 如果缺少这个在部署任务会报错 (Reids.KeyErroe:'xxx'), xxx我们部署的任务.\nSpiderKeeper  `pip install spiderkeeper' 就可以安装.\n\n启动&配置\n安装完成之后,便可以启动服务器了.博主本人使用的是ubuntu,所以就以ubuntu为例,win&macos进本一样.\n单台服务器\nsudo spiderkeeper   \n# 启动单台服务器,默认启动本地的 http://localhost:6800 scrapyd的服务 | spiderkeeper的默认端口为5000.\n连接多台scrapyd服务器.在分布式中我们的服务器中肯定不止一台,使用spiderkeeper可以很好的解决这个问题\nsudo spiderkeeper --server=http://localhost:6800 --server=http://111.111.111.111:6800 \n #启动一个spiderkeeper可以同时部署两台服务器的spider\n更改用户名&密码&更改端口号\nconfig.py 更改用户名&密码\n# Statement for enabling the development environment\nimport os\n\nDEBUG = True\n\n# Define the application directory\n\n\nBASE_DIR = os.path.abspath(os.path.dirname(__file__))\n\nSQLALCHEMY_DATABASE_URI = 'sqlite:///' + os.path.join(os.path.abspath('.'), 'SpiderKeeper.db')\nSQLALCHEMY_TRACK_MODIFICATIONS = False\nDATABASE_CONNECT_OPTIONS = {}\n\n# Application threads. A common general assumption is\n# using 2 per available processor cores - to handle\n# incoming requests using one and performing background\n# operations using the other.\nTHREADS_PER_PAGE = 2\n\n# Enable protection agains *Cross-site Request Forgery (CSRF)*\nCSRF_ENABLED = True\n\n# Use a secure, unique and absolutely secret key for\n# signing the data.\nCSRF_SESSION_KEY = \"secret\"\n\n# Secret key for signing cookies\nSECRET_KEY = \"secret\"\n\n# log\nLOG_LEVEL = 'INFO'\n\n# spider services\nSERVER_TYPE = 'scrapyd'\nSERVERS = ['http://localhost:6800']\n\n# basic auth 这里更改用户名&密码\nNO_AUTH = False\nBASIC_AUTH_USERNAME = 'admin'\nBASIC_AUTH_PASSWORD = 'admin'\nBASIC_AUTH_FORCE = True\n\nrun.py更改端口号\ndef parse_opts(config):\n    parser = OptionParser(usage=\"%prog [options]\",\n                          description=\"Admin ui for spider service\")\n    parser.add_option(\"--host\",\n                      help=\"host, default:0.0.0.0\", \n                      dest='host',\n                      default='0.0.0.0')#bind ip 绑定ip 默认全部人可以访问\n    parser.add_option(\"--port\",\n                      help=\"port, default:5000\", \n                      dest='port',\n                      type=\"int\",\n                      default=5000)#默认端口号5000 可以根据你的需求设计\n    parser.add_option(\"--username\",\n                      help=\"basic auth username ,default: %s\" % config.get('BASIC_AUTH_USERNAME'),\n                      dest='username',\n                      default=config.get('BASIC_AUTH_USERNAME'))\n    parser.add_option(\"--password\",\n                      help=\"basic auth password ,default: %s\" % config.get('BASIC_AUTH_PASSWORD'),\n                      dest='password',\n                      default=config.get('BASIC_AUTH_PASSWORD'))\n    parser.add_option(\"--type\",\n                      help=\"access spider server type, default: %s\" % config.get('SERVER_TYPE'),\n                      dest='server_type',\n                      default=config.get('SERVER_TYPE'))\n    parser.add_option(\"--server\",\n                      help=\"servers, default: %s\" % config.get('SERVERS'),\n                      dest='servers',\n                      action='append',\n                      default=[])\n    parser.add_option(\"--database-url\",\n                      help='SpiderKeeper metadata database default: %s' % config.get('SQLALCHEMY_DATABASE_URI'),\n                      dest='database_url',\n                      default=config.get('SQLALCHEMY_DATABASE_URI'))\n\n    parser.add_option(\"--no-auth\",\n                      help=\"disable basic auth\",\n                      dest='no_auth',\n                      action='store_true')\n    parser.add_option(\"-v\", \"--verbose\",\n                      help=\"log level\",\n                      dest='verbose',\n                      action='store_true')\n    return parser.parse_args()\n\n部署&运行\n启动scrapyd使用scrapy-deploy将你的文件部署到你本地的服务器上面,你本地的scrapyd获得相应的 .egg文件.\npython C:\\Users\\dengyi\\AppData\\Local\\Programs\\Python\\Python36\\Scripts\\scrapyd-deploy cqvip -p Cqvip\n启动 spiderkeeper博主这里是启动了多个,进入界面 http://localhost:5000\n\nDeploy 部署,建立任务的第一步计入Deploy创建一个新的工程我们起名为test.\n\n\n\n将我们的本地的egg文件上传到到Deploy.\n如果你是多台服务器的话那么这一步便将你所有scrpayd上,部署Wangfang文件.\n\nDashboard 是仪表盘在这里你可以启动你的spider跟监控spider的运行状态.\n\n\n\n到这里一个完美的spiderkeeper就搭建成功啦.\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "1"}