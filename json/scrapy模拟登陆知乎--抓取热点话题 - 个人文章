{"title": "scrapy模拟登陆知乎--抓取热点话题 - 个人文章 ", "index": "crawler,scrapy,python", "content": "折腾了将近两天，中间数次想要放弃，还好硬着头皮搞下去了，在此分享出来，希望有同等需求的各位能少走一些弯路。 源码放在了github上， 欢迎前往查看。 若是帮你解决了问题，或者给了你启发，不要吝啬给加一星。\n工具准备\n在开始之前，请确保 scrpay 正确安装，手头有一款简洁而强大的浏览器， 若是你有使用 postman 那就更好了。\nscrapy genspider zhihu\n使用以上命令生成知乎爬虫,代码如下:\n# -*- coding: utf-8 -*-\nimport scrapy\n\n\nclass ZhihuSpider(scrapy.Spider):\n    name = 'zhihu'\n    allowed_domains = ['www.zhihu.com']\n    start_urls = ['http://www.zhihu.com/']\n\n    def parse(self, response):\n        pass\n有一点切记，不要忘了启用 Cookies, 切记切记 ：\n# Disable cookies (enabled by default)\nCOOKIES_ENABLED = True\n模拟登陆\n过程如下：\n进入登录页，获取 Header 和 Cookie 信息，完善的 Header 信息能尽量伪装爬虫， 有效 Cookie 信息能迷惑知乎服务端，使其认为当前登录非首次登录，若无有效 Cookie 会遭遇验证码。 在抓取数据之前，请在浏览器中登录过知乎，这样才使得 Cookie 是有效的。\n\nHeader 和 Cookie 整理如下:\nheaders = {\n    'Host':\n    'www.zhihu.com',\n    'Connection':\n    'keep-alive',\n    'Origin':\n    'https://www.zhihu.com',\n    'User-Agent':\n    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n    'Content-Type':\n    'application/x-www-form-urlencoded; charset=UTF-8',\n    'Accept':\n    '*/*',\n    'X-Requested-With':\n    'XMLHttpRequest',\n    'DNT':\n    1,\n    'Referer':\n    'https://www.zhihu.com/',\n    'Accept-Encoding':\n    'gzip, deflate, br',\n    'Accept-Language':\n    'zh-CN,zh;q=0.8,en;q=0.6',\n}\n\ncookies = {\n    'd_c0':\n    '\"AHCAtu1iqAmPTped76X1ZdN0X_qAwhjdLUU=|1458699045\"',\n    '__utma':\n    '51854390.1407411155.1458699046.1458699046.1458699046.1',\n    '__utmv':\n    '51854390.000--|3=entry_date=20160322=1',\n    '_zap':\n    '850897bb-cba4-4d0b-8653-fd65e7578ac2',\n    'q_c1':\n    'b7918ff9a5514d2981c30050c8c732e1|1502937247000|1491446589000',\n    'aliyungf_tc':\n    'AQAAAHVgviiNyQsAOhSntJ5J/coWYtad',\n    '_xsrf':\n    'b12fdca8-cb35-407a-bc4c-6b05feff37cb',\n    'l_cap_id':\n    '\"MDk0MzRjYjM4NjAwNDU0MzhlYWNlODQ3MGQzZWM0YWU=|1503382513|9af99534aa22d5db92c7f58b45f3f3c772675fed\"',\n    'r_cap_id':\n    '\"M2RlNDZjN2RkNTBmNGFmNDk2ZjY4NjIzY2FmNTE4NDg=|1503382513|13370a99ee367273b71d877de17f05b2986ce0ef\"',\n    'cap_id':\n    '\"NmZjODUxZjQ0NzgxNGEzNmJiOTJhOTlkMTVjNWIxMDQ=|1503382513|dba2e9c6af7f950547474f827ef440d7a2950163\"',\n}\n在浏览器中，模拟登陆，抓取登陆请求信息。 \n\n从图中可以看到 _xsrf 参数, 这个参数与登陆验证信息无关，但很明显是由登陆页面携带的信息。 Google了下 xsrf 的含义， 用于防范 跨站请求伪造 。 \n\n整理以上，代码如下：\nloginUrl = 'https://www.zhihu.com/#signin'\nsiginUrl = 'https://www.zhihu.com/login/email'\n\n\ndef start_requests(self):\n    return [\n        scrapy.http.FormRequest(\n            self.loginUrl,\n            headers=self.headers,\n            cookies=self.cookies,\n            meta={'cookiejar': 1},\n            callback=self.post_login)\n    ]\n\n\ndef post_login(self, response):\n    xsrf = response.css(\n        'div.view-signin > form > input[name=_xsrf]::attr(value)'\n    ).extract_first()\n    self.headers['X-Xsrftoken'] = xsrf\n\n    return [\n        scrapy.http.FormRequest(\n            self.siginUrl,\n            method='POST',\n            headers=self.headers,\n            meta={'cookiejar': response.meta['cookiejar']},\n            formdata={\n                '_xsrf': xsrf,\n                'captcha_type': 'cn',\n                'email': 'xxxxxx@163.com',\n                'password': 'xxxxxx',\n            },\n            callback=self.after_login)\n    ]\n\n设置Bearer Token\n经过上述步骤登陆成功了，有点小激动，有没有！ 但苦难到此还远没有结束，这个时候尝试抓取最近热门话题，直接返回 code:401 ,未授权的访问。 授权信息未设置，导致了此类错误，莫非遗漏了什么，看来只能在浏览器中追踪请求参数来侦测问题。 在浏览器的请求中，包含了Bearer Token, 而我在scrapy中模拟的请求中未包含此信息， 所以我被服务器认定为未授权的。 通过观察发现 Bearer Token 的关键部分，就是 Cookies 中的 z_c0 包含的信息。z_c0 包含的信息，是在登陆完成时种下的，所以从登陆完成返回的登陆信息里，获取要设置的 Cookie 信息， 然后拼接出 Bearer Token,最后设置到 Header 中。\n代码整理如下:\ndef after_login(self, response):\n    jdict = json.loads(response.body)\n    print('after_login', jdict)\n    if jdict['r'] == 0:\n        z_c0 = response.headers.getlist('Set-Cookie')[2].split(';')[0].split(\n            '=')[1]\n        self.headers['authorization'] = 'Bearer ' + z_c0\n        return scrapy.http.FormRequest(\n            url=self.feedUrl,\n            method='GET',\n            meta={'cookiejar': response.meta['cookiejar']},\n            headers=self.headers,\n            formdata={\n                'action_feed': 'True',\n                'limit': '10',\n                'action': 'down',\n                'after_id': str(self.curFeedId),\n                'desktop': 'true'\n            },\n            callback=self.parse)\n    else:\n        print(jdict['error'])\n获取数据\n上述步骤后，数据获取就水到渠成了，为了检测成功与否， 把返回信息写到文件中,而且只获取前五十个,代码如下：\nfeedUrl = 'https://www.zhihu.com/api/v3/feed/topstory'\nnextFeedUrl = ''\ncurFeedId = 0\n\n\ndef parse(self, response):\n    with open('zhihu.json', 'a') as fd:\n        fd.write(response.body)\n    jdict = json.loads(response.body)\n    jdatas = jdict['data']\n    for entry in jdatas:\n        entry['pid'] = entry['id']\n        yield entry\n\n    jpaging = jdict['paging']\n    self.curFeedId += len(jdatas)\n    if jpaging['is_end'] == False and self.curFeedId < 50:\n        self.nextFeedUrl = jpaging['next']\n        yield self.next_request(response)\n\n\ndef next_request(self, response):\n    return scrapy.http.FormRequest(\n        url=self.nextFeedUrl,\n        method='GET',\n        meta={'cookiejar': response.meta['cookiejar']},\n        headers=self.headers,\n        callback=self.parse)\n最终获取的数据如下图所示：\n写在最后\n知乎的数据，只有登录完成之后，才可有效的获取，所以模拟登陆是无法忽略不管的。 所谓的模拟登陆，只是在scrapy中尽量的模拟在浏览器中的交互过程，使服务端无感抓包过程。 请求中附加有效的 Cookies 和 Headers 头信息，可有效的迷惑服务端， 同时在交互的过程中，获取后续请求必要信息和认证信息，使得整个流程能不断先前。\n若是你遇到什么问题，尽量提出来，欢迎一起来讨论解决。 源码放在了github上， 欢迎前往查看。 若是帮你解决了问题，或者给了你启发，不要吝啬给加一星。\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "12"}