{"title": "使用 Pandas 分析 Apache 日志 - yexiaoxiaobai ", "index": "matplotlib,python,日志分析,apache,pandas", "content": "\n  本文的作者是 Nikolay Koldunov，本文原文是Apache log analysis with Pandas\n\n\n\n  注本文的图有问题，没法引用，还是去原文看下，这里作为一个引子。\n\n\n%pylab inline\n\n\n欢迎来到 pylab，一个基于 matplotlib 的 Python 环境【backend: module://IPython.kernel.zmq.pylab.backend_inline】。想要了解更多信息，请键入 'help(pylab)'。\n\n在这个笔记中，我们将展示一个使用 pandas 分析 Apache 访问日志的简单示例。这是我第一次使用 pandas，并且我确定会有更好以及更有效率的方式来做这里展示的事情。所以评论，建议和修正我的蹩脚英语是非常欢迎的。你可以给我发送邮件或者是为这个笔记的 github 创建一个 PR。\n\n加载和解析数据\n\n我们将需要 apachelog 模块，用来解析日志。我们也需要知道设置在 Apache 配置中的日志格式。在我的案例中，我没有访问 Apache 配置，但是主机托管服务提供商在他的帮助页提供了日志格式的描述。下面是它自己的格式以及每个元素的简单描述：\n\nformat = r'%V %h  %l %u %t \\\"%r\\\" %>s %b \\\"%i\\\" \\\"%{User-Agent}i\\\" %T'\n\n\n这里（大部分拷贝自这个 SO 文章）：\n\n%V          - 根据 UseCanonicalName 设置的服务器名字\n%h          - 远程主机（客户端 IP）\n%l          - identity of the user determined by identd (not usually used since not reliable)\n%u          - 由 HTTP authentication 决定的 user name\n%t          - 服务器完成处理这个请求的时间\n%r          - 来自客户端的请求行。 （\"GET / HTTP/1.0\"）\n%>s         - 服务器端返回给客户端的状态码（200， 404 等等。）\n%b          - 响应给客户端的响应报文大小 （in bytes）\n\\\"%i\\\"      - Referer is the page that linked to this URL.\nUser-agent  - the browser identification string\n%T          - Apache 请求时间\n\n\nIn [3]:import apachelog, sys\n\n\n设置格式：\n\nIn [4]:fformat = r'%V %h %l %u %t \\\"%r\\\" %>s %b \\\"%i\\\" \\\"%{User-Agent}i\\\" %T'\n\n\n创建一个解析器：\n\nIn [5]:p = apachelog.parser(fformat)\n\n\n简单字符串：\n\nkoldunov.net 85.26.235.202 - - [16/Mar/2013:00:19:43 +0400] \"GET /?p=364 HTTP/1.0\" 200 65237 \"http://koldunov.net/?p=364\" \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11\" 0\n\n\nIn [6]:sample_string = 'koldunov.net 85.26.235.202 - - [16/Mar/2013:00:19:43 +0400] \"GET /?p=364 HTTP/1.0\" 200 65237 \"http://koldunov.net/?p=364\" \"Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11\" 0'\n\n\nIn [7]:data = p.parse(sample_string)\n\n\nIn [8]:data\n\n\nOut[8]:\n{'%>s': '200',\n '%T': '0',\n '%V': 'koldunov.net',\n '%b': '65237',\n '%h': '85.26.235.202',\n '%i': 'http://koldunov.net/?p=364',\n '%l': '-',\n '%r': 'GET /?p=364 HTTP/1.0',\n '%t': '[16/Mar/2013:00:19:43 +0400]',\n '%u': '-',\n '%{User-Agent}i': 'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11'}\n\n\n这就是解释器的工作。现在让我们加载真实世界的数据（示例文件位于这里和这里）：\n\nIn [9]:log = open('access_log_for_pandas').readlines()\n\n\n解析每一行，并且创建一个字典列表：\n\nIn [10]:\nlog_list = []\nfor line in log:\n       try:\n          data = p.parse(line)\n       except:\n          sys.stderr.write(\"Unable to parse %s\" % line)\n       data['%t'] = data['%t'][1:12]+' '+data['%t'][13:21]+' '+data['%t'][22:27]\n\n       log_list.append(data)\n\n\n我们不得不调整时间格式位，否则的话 pandas 将不能解析它。\n\n创建和调整数据帧\n\n这将创建一个字典列表，可以转化到一个数据帧：\n\nimport pandas as pd\nimport numpy as np\nfrom pandas import Series, DataFrame, Panel\n\n\ndf = DataFrame(log_list)\n\n\n展示数据帧的前两行：\n\ndf[0:2]\n\n\n\n\n-\n  %>s\n  %T\n  %V\n  %b\n  %h\n  %i\n  %l\n  %r\n  %t\n  %u\n  %{User-Agent}i\n\n\n\n0\n  200\n  0\n  www.oceanographers.ru\n  26126\n  109.165.31.156\n  -\n  -\n  GET /index.php?option=com_content&task=section...\n  16/Mar/2013 08:00:25 +0400\n  -\n  Mozilla/5.0 (Windows NT 6.1; rv:19.0) Gecko/20...\n\n\n1\n  200\n  0\n  www.oceanographers.ru\n  10532\n  109.165.31.156\n  \nhttp://www.oceanographers.ru/index.php?option=...\n  -\n  GET /templates/ja_procyon/css/template_css.css...\n  16/Mar/2013 08:00:25 +0400\n  -\n  Mozilla/5.0 (Windows NT 6.1; rv:19.0) Gecko/20...\n\n\n\n我们不准备使用所有的数据，因此让我们删除一些列：\n\ndel df['%T']; del df['%V']; del df['%i']; del df['%l']; del df['%u']; del df['%{User-Agent}i']\n\n\n并且把这些列重命名成人类可理解的格式：\n\ndf = df.rename(columns={'%>s': 'Status', '%b':'b', \n                        '%h':'IP', '%r':'Request', '%t': 'Time'})\n\n\n结果数据帧的前 5 行：\n\ndf.head()\n\n\n\n\n-\n  Status\n  b\n  IP\n  Request\n  Time\n\n\n\n0\n  200\n  26126\n  109.165.31.156\n  GET /index.php?option=com_content&task=section...\n  16/Mar/2013 08:00:25 +0400\n\n\n1\n  200\n  10532\n  109.165.31.156\n  GET /templates/ja_procyon/css/template_css.css...\n  16/Mar/2013 08:00:25 +0400\n\n\n2\n  200\n  1853\n  109.165.31.156\n  GET /templates/ja_procyon/switcher.js HTTP/1.0\n  16/Mar/2013 08:00:25 +0400\n\n\n3\n  200\n  37153\n  109.165.31.156\n  GET /includes/js/overlib_mini.js HTTP/1.0\n  16/Mar/2013 08:00:25 +0400\n\n\n4\n  200\n  3978\n  109.165.31.156\n  GET /modules/ja_transmenu/transmenuh.css HTTP/1.0\n  16/Mar/2013 08:00:25 +0400\n\n\n\n转换时间列成 datetime 格式并做一个索引出来（pop 将丢弃原始的 Time 列）：\n\ndf.index = pd.to_datetime(df.pop('Time'))\n\n\nStatus 变量是一个 string 类型，因此我们需要把它转换成 int：\n\ndf['Status'] = df['Status'].astype('int')\n\n\n一些 b 列的行包含 '-' 字符，我们需要使用 astype 转换它们：\n\ndf['b'][93]\n\n\nOut[19]:\n'-'\n\n\n我们可以为该列使用一个通用的函数，它们将把所有的破折号转换成 NaN，并且剩余的转换成 floats，另外把 bytes 转换成 megabytes：\n\ndef dash2nan(x):\n    if x == '-':\n        x = np.nan\n    else:\n        x = float(x)/1048576.\n\n    return x\n\n\ndf['b'] = df['b'].apply(dash2nan)\n\n\n我相信有一个更优雅的方式来做到这一点。\n\n流量分析\n\n首先，最简单的散点：从该网站的出口流量：\n\ndf['b'].plot()\n\n\n<matplotlib.axes.AxesSubplot at 0xbf7574c>\n\n\n看起来在早上 9 点左右有人从网站下载了一些大的东西。\n\n但是实际上你想知道的第一件事是你的网站有多少的访问量，以及它们的时间分布。我们从 b 变量的 5 分钟间隔重新取样，并计算每个时间跨度的请求数。实际上，在这个示例中不管我们使用哪个变量，这些数字将表明有多少次请求该网站的信息请求。\n\ndf_s = df['b'].resample('5t', how='count')\ndf_s.plot()\n\n\nOut[23]:\n<matplotlib.axes.AxesSubplot at 0xc14588c>\n\n\n![此处输入图片的描述][8]\n\n我们不仅仅计算每个时间的请求数，也计算每个时间段的总流量：\n\ndf_b = df['b'].resample('10t', how=['count','sum'])\ndf_b['count'].plot( color='r')\nlegend()\ndf_b['sum'].plot(secondary_y=True)\n\n\nOut[24]:\n<matplotlib.axes.AxesSubplot at 0xc2d53ac>\n\n\n![此处输入图片的描述][9]\n\n正如你所看到的，服务器请求数和流量是不一致的，相关性其实并不是非常高：\n\ndf_b.corr()\n\n\n|-| count|  sum\n|count| 1.000000|   0.512629\n|sum|   0.512629|   1.000000\n\n我们可以仔细看下早高峰：\n\ndf_b['2013-03-16 6:00':'2013-03-16 10:00']['sum'].plot()\n\n\nOut[26]:\n<matplotlib.axes.AxesSubplot at 0xc3f5dac>\n\n\n![此处输入图片的描述][10]\n\n看起来流量峰值是由一个请求引起的。让我们找出这个请求。选择所有响应大于 20 Mb 的请求：\n\ndf[df['b']>20]\n\n\n\n\n-\n  Status\n  b\n  IP\n  Request\n\n\n\nTime\n  \n  \n  \n  \n\n\n2013-03-16 09:02:59\n  200\n  21.365701\n  77.50.248.20\n  GET /books/Bondarenko.pdf HTTP/1.0\n\n\n\n这是一本书的 pdf 文件，这就解释了在 2013-03-16 09:02:59 的流量出口峰值。\n\n接近 20 Mb 是一个大的请求（至少对于我们网站），但是服务器响应的典型大小是？响应大小（小于 20Mb）的立方图看起来像这样：\n\ncc = df[df['b']<20]\ncc.b.hist(bins=10)\n\n\nOut[28]:\n<matplotlib.axes.AxesSubplot at 0xc52374c>\n\n\n![此处输入图片的描述][11]\n\n因此，大部分的文件是小于 0.5 Mb。实际上它们甚至更小：\n\ncc = df[df['b']<0.3]\ncc.b.hist(bins=100)\n\n\nOut[29]:\n<matplotlib.axes.AxesSubplot at 0xc5760ec>\n\n\n![此处输入图片的描述][12]\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "6"}