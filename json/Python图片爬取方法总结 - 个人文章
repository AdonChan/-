{"title": "Python图片爬取方法总结 - 个人文章 ", "index": "网页爬虫,scrapy,python", "content": "1. 最常见爬取图片方法\n对于图片爬取，最容易想到的是通过urllib库或者requests库实现。具体两种方法的实现如下：\n1.1 urllib\n使用urllib.request.urlretrieve方法，通过图片url和存储的名称完成下载。\n'''\nSignature: request.urlretrieve(url, filename=None, reporthook=None, data=None)\nDocstring:\nRetrieve a URL into a temporary location on disk.\n\nRequires a URL argument. If a filename is passed, it is used as\nthe temporary file location. The reporthook argument should be\na callable that accepts a block number, a read size, and the\ntotal file size of the URL target. The data argument should be\nvalid URL encoded data.\n\nIf a filename is passed and the URL points to a local resource,\nthe result is a copy from local file to new file.\n\nReturns a tuple containing the path to the newly created\ndata file as well as the resulting HTTPMessage object.\nFile:      ~/anaconda/lib/python3.6/urllib/request.py\nType:      function\n'''\n\n参数 finename 指定了保存本地路径（如果参数未指定，urllib会生成一个临时文件保存数据。）\n参数 reporthook 是一个回调函数，当连接上服务器、以及相应的数据块传输完毕时会触发该回调，我们可以利用这个回调函数来显示当前的下载进度。\n参数 data 指 post 到服务器的数据，该方法返回一个包含两个元素的(filename, headers)元组，filename 表示保存到本地的路径，header 表示服务器的响应头。\n\n使用示例：\nrequest.urlretrieve('https://img3.doubanio.com/view/photo/photo/public/p454345512.jpg', 'kids.jpg')\n但很有可能返回403错误（Forbidden），如：http://www.qnong.com.cn/uploa...。Stack Overflow指出原因：This website is blocking the user-agent used by urllib, so you need to change it in your request.\n给urlretrieve加上User-Agent还挺麻烦，方法如下：\nimport urllib\n\nopener = request.build_opener()\nheaders = ('User-Agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:53.0) Gecko/20100101 Firefox/53.0')\nopener.addheaders = [headers]\nrequest.install_opener(opener)\nrequest.urlretrieve('http://www.qnong.com.cn/uploadfile/2016/0416/20160416101815887.jpg', './dog.jpg')\n1.2 requests\n使用requests.get()获取图片，但要将参数stream设为True。\nimport requests\n\nreq = requests.get('http://www.qnong.com.cn/uploadfile/2016/0416/20160416101815887.jpg', stream=True)\n\nwith open('dog.jpg', 'wb') as wr:\n    for chunk in req.iter_content(chunk_size=1024):\n        if chunk:\n            wr.write(chunk)\n            wr.flush()\nrequests添加User-Agent也很方便，使用headers参数即可。\n\n2. Scrapy 支持的方法\n2.1 ImagesPipeline\nScrapy 自带 ImagesPipeline 和 FilePipeline 用于图片和文件下载，最简单使用 ImagesPipeline 只需要在 settings 中配置。\n# settings.py\nITEM_PIPELINES = {\n    'scrapy.pipelines.images.ImagesPipeline': 500\n}\n\nIMAGES_STORE = 'pictures'  # 图片存储目录\nIMAGES_MIN_HEIGHT = 400  # 小于600*400的图片过滤\nIMAGES_MIN_WIDTH = 600\n# items.py\nimport scrapy\n\nclass PictureItem(scrapy.Item):\n    image_urls = scrapy.Field()\n# myspider.py\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\n\n\nfrom ..items import BeePicture\n\nclass PicSpider(CrawlSpider):\n    name = 'pic'\n    allowed_domains = ['qnong.com.cn']\n    start_urls = ['http://www.qnong.com.cn/']\n\n    rules = (\n        Rule(LinkExtractor(allow=r'.*?', restrict_xpaths=('//a[@href]')), callback='parse_item', follow=True),\n    )\n\n    def parse_item(self, response):\n        for img_url in response.xpath('//img/@src').extract():\n            item = PictureItem()\n            item['image_urls'] = [response.urljoin(img_url)]\n            yield item\n2.2 自定义 Pipeline\n默认情况下，使用ImagePipeline组件下载图片的时候，图片名称是以图片URL的SHA1值进行保存的。\n如：图片URL: http://www.example.com/image.jpgSHA1结果：3afec3b4765f8f0a07b78f98c07b83f013567a0a则图片名称：3afec3b4765f8f0a07b78f98c07b83f013567a0a.jpg\n想要以自定义图片文件名需要重写 ImagesPipeline 的file_path方法。参考：https://doc.scrapy.org/en/lat...。\n# settings.py\nITEM_PIPELINES = {\n    'qnong.pipelines.MyImagesPipeline': 500,\n}\n# items.py\nimport scrapy\n\nclass PictureItem(scrapy.Item):\n    image_urls = scrapy.Field()\n    images = scrapy.Field()\n    image_paths = scrapy.Field()\n# myspider.py\nfrom scrapy.linkextractors import LinkExtractor\nfrom scrapy.spiders import CrawlSpider, Rule\n\n\nfrom ..items import BeePicture\n\nclass PicSpider(CrawlSpider):\n    name = 'pic'\n    allowed_domains = ['qnong.com.cn']\n    start_urls = ['http://www.qnong.com.cn/']\n\n    rules = (\n        Rule(LinkExtractor(allow=r'.*?', restrict_xpaths=('//a[@href]')), callback='parse_item', follow=True),\n    )\n\n    def parse_item(self, response):\n        for img_url in response.xpath('//img/@src').extract():\n            item = PictureItem()\n            item['image_urls'] = [response.urljoin(img_url)]\n            yield item\n# pipelines.py\nfrom scrapy.exceptions import DropItem\nfrom scrapy.pipelines.images import ImagesPipeline\nimport scrapy\n\nclass MyImagesPipeline(ImagesPipeline):\n    def get_media_requests(self, item, info):\n        for img_url in item['image_urls']:\n            yield scrapy.Request(img_url)\n\n    def item_completed(self, results, item, info):\n        image_paths = [x['path'] for ok, x in results if ok]\n        if not image_paths:\n            raise DropItem('Item contains no images')\n        item['image_paths'] = image_paths\n        return item\n\n    def file_path(self, request, response=None, info=None):\n        image_guid = request.url.split('/')[-1]\n        return 'full/%s' % (image_guid)\n2.3 FilesPipeline 和 ImagesPipeline 工作流程\nFilesPipeline\n\n在一个爬虫里，你抓取一个项目，把其中图片的URL放入 file_urls 组内。\n项目从爬虫内返回，进入项目管道。\n当项目进入 FilesPipeline，file_urls 组内的 URLs 将被 Scrapy 的调度器和下载器（这意味着调度器和下载器的中间件可以复用）安排下载，当优先级更高，会在其他页面被抓取前处理。项目会在这个特定的管道阶段保持“locker”的状态，直到完成文件的下载（或者由于某些原因未完成下载）。\n当文件下载完后，另一个字段(files)将被更新到结构中。这个组将包含一个字典列表，其中包括下载文件的信息，比如下载路径、源抓取地址（从 file_urls 组获得）和图片的校验码(checksum)。 files 列表中的文件顺序将和源 file_urls 组保持一致。如果某个图片下载失败，将会记录下错误信息，图片也不会出现在 files 组中。\n\nImagesPipeline\n\n在一个爬虫里，你抓取一个项目，把其中图片的 URL 放入 images_urls 组内。\n项目从爬虫内返回，进入项目管道。\n当项目进入 Imagespipeline，images_urls 组内的URLs将被Scrapy的调度器和下载器（这意味着调度器和下载器的中间件可以复用）安排下载，当优先级更高，会在其他页面被抓取前处理。项目会在这个特定的管道阶段保持“locker”的状态，直到完成文件的下载（或者由于某些原因未完成下载）。\n当文件下载完后，另一个字段(images)将被更新到结构中。这个组将包含一个字典列表，其中包括下载文件的信息，比如下载路径、源抓取地址（从 images_urls 组获得）和图片的校验码(checksum)。 images 列表中的文件顺序将和源 images_urls 组保持一致。如果某个图片下载失败，将会记录下错误信息，图片也不会出现在 images 组中。\n\nScrapy 不仅可以下载图片，还可以生成指定大小的缩略图。Pillow 是用来生成缩略图，并将图片归一化为 JPEG/RGB 格式，因此为了使用图片管道，你需要安装这个库。\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "11"}