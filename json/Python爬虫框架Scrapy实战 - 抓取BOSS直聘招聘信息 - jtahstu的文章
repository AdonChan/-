{"title": "Python爬虫框架Scrapy实战 - 抓取BOSS直聘招聘信息 - jtahstu的文章 ", "index": "php,python,scrapy", "content": "原文地址： http://www.jtahstu.com/blog/s...\nPython爬虫框架Scrapy实战 - 抓取BOSS直聘招聘信息\n零、开发环境\n\nMacBook Pro (13-inch, 2016, Two Thunderbolt 3 ports)\nCPU : 2 GHz Intel Core i5\nRAM : 8 GB 1867 MHz LPDDR3\nPython 版本: v3.6.3 [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\nMongoDB 版本: v3.4.7\n\nMongoDB 可视化工具 ：MongoBooster v4.1.3\n\n一、准备工作\n安装 Scrapy\npip3 install scrapy\n如果顺利的话,会像本人这样,装了一大堆软件包\n\n参考翻译文档的安装教程：http://scrapy-chs.readthedocs...\n官方 GitHub 地址：https://github.com/scrapy/scrapy\n二、新建项目\nscrapy startproject www_zhipin_com\n如果顺利的话,会像本人这样\n\n三、定义要抓取的 Item\n在items.py 文件中定义一个类\nclass WwwZhipinComItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    pid = scrapy.Field()\n    positionName = scrapy.Field()\n    positionLables = scrapy.Field()\n    workYear = scrapy.Field()\n    salary = scrapy.Field()\n    city = scrapy.Field()\n    education = scrapy.Field()\n    companyShortName = scrapy.Field()\n    industryField = scrapy.Field()\n    financeStage = scrapy.Field()\n    companySize = scrapy.Field()\n    time = scrapy.Field()\n    updated_at = scrapy.Field()\n四、分析页面\n一般一条招聘像下面这样\n\nhtml 结构如下\n\n爬虫中就是使用 css 选择器获取标签里的文字或链接等\n五、爬虫代码\n在 spiders 目录下新建 zhipin_spider.py\n# -*- coding: utf-8 -*-\nimport scrapy\nimport time\nfrom www_zhipin_com.items import WwwZhipinComItem\n\n\nclass ZhipinSpider(scrapy.Spider):\n    # spider的名字定义了Scrapy如何定位(并初始化)spider，所以其必须是唯一的。 不过您可以生成多个相同的spider实例(instance)，这没有任何限制。 name是spider最重要的属性，而且是必须的\n    name = 'zhipin'\n    \n    # 可选。包含了spider允许爬取的域名(domain)列表(list)。 当 OffsiteMiddleware 启用时， 域名不在列表中的URL不会被跟进。\n    allowed_domains = ['www.zhipin.com']\n    \n    # URL列表。当没有制定特定的URL时，spider将从该列表中开始进行爬取。\n    # 这里我们进行了指定，所以不是从这个 URL 列表里爬取\n    start_urls = ['http://www.zhipin.com/'] \n    \n    # 要爬取的页面，可以改为自己需要搜的条件，这里搜的是 上海-PHP，其他条件都是不限\n    positionUrl = 'http://www.zhipin.com/c101020100/h_101020100/?query=php'\n    curPage = 1\n\n    # 发送 header，伪装为浏览器\n    headers = {\n        'x-devtools-emulate-network-conditions-client-id': \"5f2fc4da-c727-43c0-aad4-37fce8e3ff39\",\n        'upgrade-insecure-requests': \"1\",\n        'user-agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36\",\n        'accept': \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n        'dnt': \"1\",\n        'accept-encoding': \"gzip, deflate\",\n        'accept-language': \"zh-CN,zh;q=0.8,en;q=0.6\",\n        'cookie': \"__c=1501326829; lastCity=101020100; __g=-; __l=r=https%3A%2F%2Fwww.google.com.hk%2F&l=%2F; __a=38940428.1501326829..1501326829.20.1.20.20; Hm_lvt_194df3105ad7148dcf2b98a91b5e727a=1501326839; Hm_lpvt_194df3105ad7148dcf2b98a91b5e727a=1502948718; __c=1501326829; lastCity=101020100; __g=-; Hm_lvt_194df3105ad7148dcf2b98a91b5e727a=1501326839; Hm_lpvt_194df3105ad7148dcf2b98a91b5e727a=1502954829; __l=r=https%3A%2F%2Fwww.google.com.hk%2F&l=%2F; __a=38940428.1501326829..1501326829.21.1.21.21\",\n        'cache-control': \"no-cache\",\n        'postman-token': \"76554687-c4df-0c17-7cc0-5bf3845c9831\"\n    }\n\n    //该方法必须返回一个可迭代对象(iterable)。该对象包含了spider用于爬取的第一个Request。\n    //该方法仅仅会被Scrapy调用一次，因此您可以将其实现为生成器。\n    def start_requests(self):\n        return [self.next_request()]\n\n    //负责处理response并返回处理的数据以及(/或)跟进的URL。\n    def parse(self, response):\n        print(\"request -> \" + response.url)\n        job_list = response.css('div.job-list > ul > li')\n        for job in job_list:\n            item = WwwZhipinComItem()\n            job_primary = job.css('div.job-primary')\n            item['pid'] = job.css(\n                'div.info-primary > h3 > a::attr(data-jobid)').extract_first().strip()\n            item[\"positionName\"] = job_primary.css(\n                'div.info-primary > h3 > a::text').extract_first().strip()\n            item[\"salary\"] = job_primary.css(\n                'div.info-primary > h3 > a > span::text').extract_first().strip()\n            info_primary = job_primary.css(\n                'div.info-primary > p::text').extract()\n            item['city'] = info_primary[0].strip()\n            item['workYear'] = info_primary[1].strip()\n            item['education'] = info_primary[2].strip()\n            item['companyShortName'] = job_primary.css(\n                'div.info-company > div.company-text > h3 > a::text'\n            ).extract_first().strip()\n            company_infos = job_primary.css(\n                'div.info-company > div.company-text > p::text').extract()\n            if len(company_infos) == 3: # 有一条招聘这里只有两项，所以加个判断\n                item['industryField'] = company_infos[0].strip()\n                item['financeStage'] = company_infos[1].strip()\n                item['companySize'] = company_infos[2].strip()\n            item['positionLables'] = job.css(\n                'li > div.job-tags > span::text').extract()\n            item['time'] = job.css('span.time::text').extract_first().strip()\n            item['updated_at'] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n            yield item\n\n        self.curPage += 1\n        time.sleep(5) # 停停停！听听听！都给我停下来听着！睡一会(～﹃～)~zZ\n        yield self.next_request()\n\n    # 发送请求\n    def next_request(self):\n        return scrapy.http.FormRequest(\n            self.positionUrl + (\"&page=%d&ka=page-%d\" %\n                                (self.curPage, self.curPage)),\n            headers=self.headers,\n            callback=self.parse)\n运行脚本\nscrapy crawl zhipin -o item.json\n这里会在项目目录下生成 item.json 的一个 json 文件\n运行情况如下\nhttp://cdn.jtup.cc/blog/video...\nPoint 1 设置 UTF-8 编码\n但是不巧，往往这是一个 Unicode 编码的文件，所以需要加个设置\n在 settings.py中添加(PS:也可以在运行的时候带上这个参数)\nFEED_EXPORT_ENCODING = 'utf-8'\n亲测以下方法是不能解决问题的\n\nPoint 2 慢一点\n注意不要爬的太快，因为 BOSS 直聘只会显示20页的招聘信息，所以理论上这个脚本只要执行20次即可，那么间隔时间尽量设置长一点，本人爬的时候设置的是5秒，但是后面稍微快了一点就六字真言了，还好我已经把数据爬到了\n慢一点，才能快一点！\n\nPoint 3 修改为自定义的条件\n可以修改 zhipin_spider.py 第18行 positionUrl 的链接，把 PHP 修改为 Java 或 Python，把城市编码（'c101020100' == 上海）换成你需要查询的城市，即可爬取自定的岗位，这就很灵性了！\n六、保存到数据库\n一条json数据如下\n{\n    \"pid\": \"16115932\",\n    \"positionName\": \"PHP后台开发工程师\",\n    \"salary\": \"13K-20K\",\n    \"city\": \"上海\",\n    \"workYear\": \"1-3年\",\n    \"education\": \"本科\",\n    \"companyShortName\": \"蜻蜓FM\",\n    \"industryField\": \"互联网\",\n    \"financeStage\": \"D轮及以上\",\n    \"companySize\": \"100-499人\",\n    \"positionLables\": [\n      \"PHP\"\n    ],\n    \"time\": \"发布于昨天\",\n    \"updated_at\": \"2017-12-10 17:36:21\"\n  },\n使用软件将json文件导入到 MongoDB 中,以备后面的使用\n七、不足\n\n这里招聘的详细要求还没有爬取\n刚抓到的数据还没初步处理\n\n\n本项目开源地址：http://git.jtahstu.com/jtahst...\n\n八、后记\n有人可能会问，爬这些数据有什么用呢，现在又不跳槽。\n本人的回答是，那当然肯定必须有用啊，所谓防患于未然、知己知彼，百战不殆，只有及时了解市面上的需求，才能有针对性的提升自己、学习技术，从另一方面来看，那有关钱途的事都是大事啊。\nok，本文到此为止，下一篇就是让我们来好好分析，招聘 PHP  程序员，企业到底需要招聘的是什么样的 PHP程序员。PHP 程序员需要具备哪些常规技能和哪些冷门技能，应该点亮怎样的技能树，敬请期待。\n非常感谢你能抽出三五分钟看完这篇 Python 基础入门的文章 ， ^_^\n九、致谢\n在文章的创作过程中，参考了以下文档和文章等\n\nScrapy 1.0 中文文档\nScrapy安装、爬虫入门教程、爬虫实例（豆瓣电影爬虫） - 博客园\nScrapy笔记02- 完整示例\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "11"}