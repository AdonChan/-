{"title": "scrapy学习笔记(三)：使用item与pipeline保存数据 - 个人文章 ", "index": "网页爬虫,python", "content": "最近真是忙的吐血。。。\n上篇写的是直接在爬虫中使用mongodb，这样不是很好，scrapy下使用item才是正经方法。在item中定义需要保存的内容，然后在pipeline处理item，爬虫流程就成了这样：\n抓取 --> 按item规则收集需要数据 -->使用pipeline处理（存储等）\n定义item,在items.py中定义抓取内容\n# -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# http://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\n\nclass GetquotesItem(scrapy.Item):\n    # define the fields for your item here like:\n    # 定义我们需要抓取的内容：\n    # 1.名言内容\n    # 2.作者\n    # 3.标签\n    content = scrapy.Field()\n    author = scrapy.Field()\n    tags = scrapy.Field()\n\n我们将数据库的配置信息保存在setting.py文件中，方便调用\nMONGODB_HOST = 'localhost'\nMONGODB_PORT = 27017\nMONGODB_DBNAME = 'store_quotes2'\nMONGODB_TABLE = 'quotes2'\n另外，在setting.py文件中一点要将pipeline注释去掉，要不然pipeline不会起作用：\n#ITEM_PIPELINES = {\n#    'getquotes.pipelines.SomePipeline': 300,\n#}\n改成\nITEM_PIPELINES = {\n    'getquotes.pipelines.GetquotesPipeline': 300,\n}\n现在在pipeline.py中定义处理item方法：\n# -*- coding: utf-8 -*-\n\n# Define your item pipelines here\n#\n# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html\n\n# 将setting导入，以使用定义内容\nfrom scrapy.conf import settings\nimport pymongo\n\nclass GetquotesPipeline(object):\n\n    # 连接数据库\n    def __init__(self):\n        \n        # 获取数据库连接信息\n        host = settings['MONGODB_HOST']\n        port = settings['MONGODB_PORT']\n        dbname = settings['MONGODB_DBNAME']\n        client = pymongo.MongoClient(host=host, port=port)\n        \n        # 定义数据库\n        db = client[dbname]\n        self.table = db[settings['MONGODB_TABLE']]\n    \n    # 处理item\n    def process_item(self, item, spider):\n            # 使用dict转换item，然后插入数据库\n            quote_info = dict(item)\n            self.table.insert(quote_info)\n            return item\n相应的，myspider.py中的代码变化一下\nimport scrapy\nimport pymongo\n\n# 别忘了导入定义的item\nfrom getquotes.items import GetquotesItem\n\nclass myspider(scrapy.Spider):\n\n    # 设置爬虫名称\n    name = \"get_quotes\"\n\n    # 设置起始网址\n    start_urls = ['http://quotes.toscrape.com']\n\n    '''\n        # 配置client，默认地址localhost，端口27017\n        client = pymongo.MongoClient('localhost',27017)\n        # 创建一个数据库，名称store_quote\n        db_name = client['store_quotes']\n        # 创建一个表\n        quotes_list = db_name['quotes']\n    '''\n    def parse(self, response):\n\n        #使用 css 选择要素进行抓取，如果喜欢用BeautifulSoup之类的也可以\n        #先定位一整块的quote，在这个网页块下进行作者、名言,标签的抓取\n        for quote in response.css('.quote'):\n            '''\n            # 将页面抓取的数据存入mongodb,使用insert\n            yield self.quotes_list.insert({\n                'author' : quote.css('small.author::text').extract_first(),\n                'tags' : quote.css('div.tags a.tag::text').extract(),\n                'content' : quote.css('span.text::text').extract_first()\n            })\n            '''\n            item = GetquotesItem()\n            item['author'] = quote.css('small.author::text').extract_first()\n            item['content'] = quote.css('span.text::text').extract_first()\n            item['tags'] = quote.css('div.tags a.tag::text').extract()\n            yield item\n\n\n        # 使用xpath获取next按钮的href属性值\n        next_href = response.xpath('//li[@class=\"next\"]/a/@href').extract_first()\n        # 判断next_page的值是否存在\n        if next_href is not None:\n\n            # 如果下一页属性值存在，则通过urljoin函数组合下一页的url:\n            # www.quotes.toscrape.com/page/2\n            next_page = response.urljoin(next_href)\n\n            #回调parse处理下一页的url\n            yield scrapy.Request(next_page,callback=self.parse)\n\n可以再scrapy输出信息中看到pipeline启用\n\n再来看看数据库保存情况\n\n完美保存\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "3"}