{"title": "Python之爬虫相关库的使用 - 个人文章 ", "index": "python", "content": "requests库\n发送请求\n修改头\n我们在发起请求时，可以添加一个头，防止被识别而被阻止请求\n>>> headers = {'User-agent':'Mozilla/5.0 (Windows NT 6.1;Win64;x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55/0/2883.87 Safari/537.36'}\n# 使用时\n>>> r = requests.get('http://xxxx/',headers=headers)\nGET请求\n普通的发送请求\n>>> r = requests.get(\"https://www.baidu.com\")\n这里就有了一个名叫r的Response对象\nPOST请求\n>>> payload = {'user'='Ansible','passwd'='123456'}\n>>> r = requests.post(\"https://xxxx/post\",data=payload)\n其他HTTP请求类型\n>>> r = requests.put(\"http://xxx/put\")    # PUT请求\n>>> r = requests.delete(\"http://xxx/delete\")   # Delete请求\n>>> r = requests.head(\"http://xxx/get\")    # head请求\n>>> r = requests.options(\"http://xxx/options\")   # options请求\n用URL传递参数\n假如存在一个http://xxx/xxx.php?key=val，可以使用如下方法修改key和value的值\n>>> payload = {'key1':'value1','key2':'value2'}\n>>> r = requests.get(\"http://xxxx/get\",params=payload)\n>>> print(r.url)\nhttp://xxx/xxx.php?key1=value1&key2=value2\n响应内容\n答应消息头信息\n>>> r.header\n打印网页源代码\n>>> r.text \n打印出状态码\n>>> r.status_code\n或者也可以通过raise_for_status()方法判断\n>>> r.raise_for_status()  # 判断返回的状态码是不是200，如果不是，就会引发HTTPError异常 \n打印二进制内容\n一般用于抓取图片之类的二进制文件\n>>> r.content\n打印cookie\n>>> r.cookies\n>>> for key , value in res.cookies.items():   #获取字典的键值对来查看cookie\n    print(key+ '=' + value)\n打印请求历史\n可能会遇到http请求重定向至https的情况\n>>> r.history\n打印编码方式\n>>> r.encoding\n# 如果要改变编码方式，直接赋值\n>>> r.encoding = 'ISO-8859-1'\n# 获取原始代码\n>>> r.encoding = r.apparent_encoding\n打印json内容\n>>> r.json  #前提要有json格式的数据\n原始响应内容\n>>> r = requests.get('https://github.com/timeline.json', stream=True)\n>>> r.raw\n进阶\n文件上传\n>>> url = 'http://httpbin.org/post'\n>>> files = {'file': open('report.xls', 'rb')}\n\n>>> r = requests.post(url, files=files)\n>>> r.text\n{\n  ...\n  \"files\": {\n    \"file\": \"<censored...binary...data>\"\n  },\n  ...\n}\n保持会话\n>>> s = requests.Session()\n>>> s.get('http://httpbin.org/cookies/set/number/123456789')\n>>> r = s.get('http://httpbin.org/cookies')\n>>> print(r.text)\n验证SSL证书\n检查某个主机的 SSL证书\n>>> res = requests.get('https://www.baidu.com',verify=True)  # verify参数默认值为True\n自己指定证书\n>>> res = requests.get('https://www.baidu.com',cert=('/path/server.crt', '/path/key'))\n代理设置\n>>> proxies = {\n    \"http\" : \"http://10.10.1.10:3128\",\n    \"https\": \"http://10.10.1.10:1080\",\n    # \"http\":\"http://user:password@10.10.1.10:3128/\"  使用有用户名密码的代理\n    # \"http\": 'socks5://user:pass@host:port',\n    # \"https\": 'socks5://user:pass@host:port'        使用socks代理\n}\n>>> res = requests.get('http://www.baidu.com',proxies=proxies)\n超时设置\n>>> res = requests.get('http://www.baidu.com',timeout = 0.5)  # timeout可加参数 none\n>>> print(res.status_code)\n异常处理\n# python3\nfrom requests.exceptions import ReadTimeout, ConnectionError, RequestException\ntry:\n    resp = requests.get('http://httpbin.org/get', timeout=0.5)\n    print(resp.status_code)\nexcept ReadTimeout： # 访问超时的错误\n    print('Timeout')\nexcept ConnectionError: # 网络中断连接错误\n    print('Connect error')\nexcept RequestException: # 父类错误\n    print('Error')\nBeautifulSoup库\nBeautifulSoup库解析器\nbs4的HTML解析器\n要安装bs4库\nsoup = BeautifulSoup(mk,'html.parser')\nlxml的HTML的解析器\nsoup = BeautifulSoup(mk,'lxml')\n安装lxml库\nlxml的XML解析器\nsoup = BeautifulSoup(mk,'xml')\n安装lxml库\nhtml5lib的解析器\nsoup = BeautifulSoup(mk,'html5lib')\n安装html5lib\n标签内容\n>>> soup.div.parrent\n父标签\n>>> soup.div.parrent.parrent\n父标签的标签\n>>> tag = soup.div\n>>> tag.attrs\n以字典方式显示tag中的内容\n>>> tag.string\n打印标签中的内容\n>>> tag.parrent\n内容遍历\n标签树的下行遍历\n.contents\n子节点列表，将<tag>所有儿子节点存入列表例如\n>>> soup.head.contents\n其他\n\n\n下行遍历\n说明\n\n\n\n.children\n子节点的迭代类型，与.contents类似，用于循环遍历儿子节点\n\n\n.descendants\n子孙节点的迭代类型，包含所有子孙节点，用于循环遍历\n\n\n\n标签树的上行遍历\n\n\n上行遍历\n说明\n\n\n\n.parent\n节点的父亲标签\n\n\n.parents\n节点先辈标签的迭代类型，用于循环遍历先辈节点\n\n\n\n标签树的平行遍历\n\n\n平行遍历\n说明\n\n\n\n.next_sibling\n返回按照HTML文本顺序的下一个平行节点标签\n\n\n.previous_sibling\n返回按照HTML文本顺序的上一个平行节点标签\n\n\n.next_siblings\n迭代类型，返回按照HTML文本顺序的后续所有平行节点标签\n\n\n.previous_siblings\n迭代类型，返回按照HTML文本顺序的前续所有平行节点标签\n\n\n\n查找方法\n<>.find_all(name,attrs,recursive,string,**kwargs)\n返回一个列表类型，储存查找的结果。\nname：对标签名称的检索字符串\n例如：\nsoup.find_all('a')\n返回了所有<a>标签\nattrs：对标签属性值的检索字符串，可标注属性检索\n例如\nsoup.find_all('p','course')\n返回了class属性为course的<p>标签或者\nsoup.find_all(id='link1')\n以列表方式返回所有id为link1的标签\n例子\n比如说我现在想要全部a标签里的链接href\nfor link in soup.find_all('a'):\n    print(link.get('href'))\nrecursive：是否对子孙全部检索，默认为True\nsoup.find_all('a',recursive=False)\nstring：<>..</>中字符串区域的检索字符串\nsoup.find_all(string = 'Basic Python')\n一般结合正则表达式使用\nre库\n正则表达式库。参考文章：https://zhuanlan.zhihu.com/p/...\n常用元字符\n\n\n代码\n说明\n\n\n\n.\n匹配任意字符(除了n)\n\n\ns\n匹配任意的空白符，包括空格,制表符(Tab),换行符,中文全角空格等\n\n\nS\n匹配非空白字符\n\n\nd\n匹配数字\n\n\nD\n匹配非数字\n\n\nw\n匹配单词字符[a-zA-Z0-9]\n\n\nW\n匹配非单词字符\n\n\nd\n匹配单词的开始或结束\n\n\nD\n匹配不是单词开头或结束的位置\n\n\n^\n匹配字符串的开始\n\n\n$\n匹配字符串的结束\n\n\n\n常用限定符\n\n\n代码/语法\n说明\n\n\n\n*\n匹配字符0次或者无限次\n\n\n+\n匹配字符1次或者无限次\n\n\n?\n匹配字符0次或者1次\n\n\n{n}\n重复n次\n\n\n{n,}\n重复n次或者更多次\n\n\n{n,m}\n重复n到m次\n\n\n\nPython正则表达式语法\nimport re\nregex = re.compile(r'正则表达式')    # 创建一个 Regex 对象，使用 r'' 原始字符串不需要转义\nregex.match()   \nregex.search()  # 返回一个 Match 对象，包含被查找字符串中的第一次被匹配的文本 \nregex.findall() # 返回一组字符串列表，包含被查找字符串中的所有匹配\nregex.sub()  # 替换字符串，接收两个参数，新字符串和正则表达式\nMatch对象的属性\n\n\n属性\n说明\n\n\n\n.string\n待匹配的文本\n\n\n.re\n匹配时使用的pattern对象\n\n\n.pos\n正则表达式搜索文本开始位置\n\n\n.endpos\n正则表达式搜索文本的结束位置\n\n\n\n简单例子\n基础\n>>> pa = re.compile(r'imooc')\n>>> regex = pa.match(str1)\n>>> regex.group()\nimooc\n或者\n>>> regex = re.match(r'\\w','a')\n>>> regex.group()\na\n匹配6个字符的单词\n>>> import re\n>>> regex = re.compile(r'\\b\\w{6}\\b')    # 匹配6个字符的单词\n>>> regex.search('My phone number is 421-2343-121')\n>>> text = regex.search('My phone number is 421-2343-121')\n>>> text.group()    \n'number'\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}