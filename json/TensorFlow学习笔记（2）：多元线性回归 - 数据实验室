{"title": "TensorFlow学习笔记（2）：多元线性回归 - 数据实验室 ", "index": "scikit-learn,tensorflow,python", "content": "前言\n本文使用tensorflow训练多元线性回归模型，并将其与scikit-learn做比较。数据集来自Andrew Ng的网上公开课程Deep Learning\n代码\n#!/usr/bin/env python\n# -*- coding=utf-8 -*-\n# @author: 陈水平\n# @date: 2016-12-30\n# @description: compare multi linear regression of tensor flow to scikit-learn based on data from deep learning cource of Andrew Ng\n# @ref: http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=DeepLearning&doc=exercises/ex3/ex3.html\n#\n\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\n\n# Read x and y\nx_data = np.loadtxt(\"ex3x.dat\").astype(np.float32)\ny_data = np.loadtxt(\"ex3y.dat\").astype(np.float32)\n\n\n# We evaluate the x and y by sklearn to get a sense of the coefficients.\nreg = linear_model.LinearRegression()\nreg.fit(x_data, y_data)\nprint \"Coefficients of sklearn: K=%s, b=%f\" % (reg.coef_, reg.intercept_)\n\n\n# Now we use tensorflow to get similar results.\n\n# Before we put the x_data into tensorflow, we need to standardize it\n# in order to achieve better performance in gradient descent;\n# If not standardized, the convergency speed could not be tolearated.\n# Reason:  If a feature has a variance that is orders of magnitude larger than others, \n# it might dominate the objective function \n# and make the estimator unable to learn from other features correctly as expected.\nscaler = preprocessing.StandardScaler().fit(x_data)\nprint scaler.mean_, scaler.scale_\nx_data_standard = scaler.transform(x_data)\n\n\nW = tf.Variable(tf.zeros([2, 1]))\nb = tf.Variable(tf.zeros([1, 1]))\ny = tf.matmul(x_data_standard, W) + b\n\nloss = tf.reduce_mean(tf.square(y - y_data.reshape(-1, 1)))/2\noptimizer = tf.train.GradientDescentOptimizer(0.3)\ntrain = optimizer.minimize(loss)\n\ninit = tf.initialize_all_variables()\n\n\nsess = tf.Session()\nsess.run(init)\nfor step in range(100):\n    sess.run(train)\n    if step % 10 == 0:\n        print step, sess.run(W).flatten(), sess.run(b).flatten()\n\nprint \"Coefficients of tensorflow (input should be standardized): K=%s, b=%s\" % (sess.run(W).flatten(), sess.run(b).flatten())\nprint \"Coefficients of tensorflow (raw input): K=%s, b=%s\" % (sess.run(W).flatten() / scaler.scale_, sess.run(b).flatten() - np.dot(scaler.mean_ / scaler.scale_, sess.run(W)))\n\n输出如下：\nCoefficients of sklearn: K=[  139.21066284 -8738.02148438], b=89597.927966\n[ 2000.6809082      3.17021275] [  7.86202576e+02   7.52842903e-01]\n0 [ 31729.23632812  16412.6484375 ] [ 102123.7890625]\n10 [ 97174.78125      5595.25585938] [ 333681.59375]\n20 [ 106480.5703125    -3611.31201172] [ 340222.53125]\n30 [ 108727.5390625    -5858.10302734] [ 340407.28125]\n40 [ 109272.953125     -6403.52148438] [ 340412.5]\n50 [ 109405.3515625    -6535.91503906] [ 340412.625]\n60 [ 109437.4921875    -6568.05371094] [ 340412.625]\n70 [ 109445.296875     -6575.85644531] [ 340412.625]\n80 [ 109447.1875       -6577.75097656] [ 340412.625]\n90 [ 109447.640625     -6578.20654297] [ 340412.625]\nCoefficients of tensorflow (input should be standardized): K=[ 109447.7421875    -6578.31152344], b=[ 340412.625]\nCoefficients of tensorflow (raw input): K=[  139.21061707 -8737.9609375 ], b=[ 89597.78125]\n思考\n对于梯度下降算法，变量是否标准化很重要。在这个例子中，变量一个是面积，一个是房间数，量级相差很大，如果不归一化，面积在目标函数和梯度中就会占据主导地位，导致收敛极慢。\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "6"}