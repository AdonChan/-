{"title": "快速制作规则及获取规则提取器API - 一起学习python网络爬虫 ", "index": "编程语言,网页抓取,数据采集,网络爬虫,python", "content": "\n1. 引言\n前面文章的测试案例都用到了集搜客Gooseeker提供的规则提取器，在网页抓取工作中，调试正则表达式或者XPath都是特别繁琐的，耗时耗力，工作枯燥，如果有一个工具可以快速生成规则，而且可以可视化的即时验证，就能把程序员解放出来，投入到创造性工作中。\n之前文章所用的例子中的规则都是固定的，如何自定义规则再结合提取器提取我们想要的网页内容呢？对于程序员来说，理想的目标是掌握一个通用的爬虫框架，每增加一个新目标网站就要跟着改代码，这显然不是好工作模式。这就是本篇文章的主要内容了，本文使用一个案例说明怎样将新定义的采集规则融入到爬虫框架中。也就是用可视化的集搜客GooSeeker爬虫软件针对亚马逊图书商品页做一个采集规则，并结合规则提取器抓取网页内容。\n2. 安装集搜客GooSeeker爬虫软件\n2.1. 前期准备\n进入集搜客官网产品页面，下载对应版本。我的电脑上已经安装了Firefox 38，所以这里只需下载爬虫。\n2.2 安装爬虫\n打开Firefox –> 点击菜单工具 –> 附加组件 –> 点击右上角附加组件的工具 –> 选择从文件安装附加组件 -> 选中下载好的爬虫xpi文件 –> 立即安装下一步下一步\n3. 开始制作抓取规则\n3.1 运行规则定义软件\n点击浏览器菜单：工具-> MS谋数台 弹出MS谋数台窗口。\n3.2 做规则\n在网址栏输入我们要采集的网站链接，然后回车。当页面加载完成后，在工作台页面依次操作：命名主题名 -> 创建规则 -> 新建整理箱 -> 在浏览器菜单选择抓取内容，命名后保存。\n4. 申请规则提取器API KEY\n打开集搜客Gooseeke官网，注册登陆后进入会员中心 -> API -> 申请API\n5. 结合提取器API敲一个爬虫程序\n5.1 引入Gooseeker规则提取器模块gooseeker.py\n(下载地址: gooseeker/core at master · FullerHua/gooseeker · GitHub), 选择一个存放目录，这里为E:demogooseeker.py\n5.2 与gooseeker.py同级创建一个.py后缀文件\n如这里为E:Demothird.py，再以记事本打开，敲入代码:注释：代码中的31d24931e043e2d5364d03b8ff9cc77e 就是API KEY，用你申请的代替；amazon_book_pc 是规则的主题名，也用你的主题名代替\n# -*- coding: utf-8 -*-\n# 使用GsExtractor类的示例程序\n# 以webdriver驱动Firefox采集亚马逊商品列表\n# xslt保存在xslt_bbs.xml中\n# 采集结果保存在third文件夹中\nimport os\nimport time\nfrom lxml import etree\nfrom selenium import webdriver\nfrom gooseeker import GsExtractor\n\n# 引用提取器\nbbsExtra = GsExtractor()   \nbbsExtra.setXsltFromAPI(\"31d24931e043e2d5364d03b8ff9cc77e\", \"amazon_book_pc\") # 设置xslt抓取规则\n\n# 创建存储结果的目录\ncurrent_path = os.getcwd()\nres_path = current_path + \"/third-result\"\nif os.path.exists(res_path):\n    pass\nelse:\n    os.mkdir(res_path)\n\n# 驱动火狐\ndriver = webdriver.Firefox()\nurl = \"https://www.amazon.cn/s/ref=sr_pg_1?rh=n%3A658390051%2Cn%3A!658391051%2Cn%3A658414051%2Cn%3A658810051&page=1&ie=UTF8&qid=1476258544\"\ndriver.get(url)\ntime.sleep(2)\n\n# 获取总页码\ntotal_page = driver.find_element_by_xpath(\"//*[@class='pagnDisabled']\").text\ntotal_page = int(total_page) + 1\n\n# 用简单循环加载下一页链接（也可以定位到下一页按钮，循环点击）\nfor page in range(1,total_page):\n    # 获取网页内容\n    content = driver.page_source.encode('utf-8')\n\n    # 获取docment\n    doc = etree.HTML(content)\n    # 调用extract方法提取所需内容\n    result = bbsExtra.extract(doc)\n\n    # 保存结果\n    file_path = res_path + \"/page-\" + str(page) + \".xml\"\n    open(file_path,\"wb\").write(result)\n    print('第' + str(page) + '页采集完毕，文件:' + file_path)\n\n    # 加载下一页\n    if page < total_page - 1:\n        url = \"https://www.amazon.cn/s/ref=sr_pg_\" + str(page + 1) + \"?rh=n%3A658390051%2Cn%3A!658391051%2Cn%3A658414051%2Cn%3A658810051&page=\" + str(page + 1) + \"&ie=UTF8&qid=1476258544\"\n        driver.get(url)\n        time.sleep(2)\nprint(\"~~~采集完成~~~\")\ndriver.quit()\n\n5.3 执行third.py\n打开命令提示窗口，进入third.py文件所在目录，输入命令 :python third.py 回车\n5.4 查看结果文件\n进入third.py文件所在目录，找到名称为result-2的文件夹然后打开\n6. 总结\n制作规则时，由于定位选择的是偏好id，而采集网址的第二页对应页面元素的id属性有变化，所以第二页内容提取出现了问题，然后对照了一下网页元素发现class是一样的，果断将定位改为了偏好class，这下提取就正常了。下一篇《在Python3.5下安装和测试Scrapy爬网站》简单介绍Scrapy的使用方法。\n7. 集搜客GooSeeker开源代码下载源\nGooSeeker开源Python网络爬虫GitHub源\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}