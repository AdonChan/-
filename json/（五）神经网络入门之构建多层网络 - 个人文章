{"title": "（五）神经网络入门之构建多层网络 - 个人文章 ", "index": "python", "content": "作者：chen_h微信号 & QQ：862251340微信公众号：coderpai简书地址：https://www.jianshu.com/p/cb6...\n\n这篇教程是翻译Peter Roelants写的神经网络教程，作者已经授权翻译，这是原文。\n该教程将介绍如何入门神经网络，一共包含五部分。你可以在以下链接找到完整内容。\n\n（一）神经网络入门之线性回归\nLogistic分类函数\n（二）神经网络入门之Logistic回归（分类问题）\n（三）神经网络入门之隐藏层设计\nSoftmax分类函数\n（四）神经网络入门之矢量化\n（五）神经网络入门之构建多层网络\n\n多层网络的推广\n\n这部分教程将介绍两部分：\n\n多层网络的泛化\n随机梯度下降的最小批处理分析\n\n在这个教程中，我们把前馈神经网络推到任意数量的隐藏层。其中的概念我们都通过矩阵乘法和非线性变换来进行系统的说明。我们通过构建一个由两层隐藏层组成的小型网络去识别手写数字识别，来说明神经网络向多层神经网络的泛化能力。这个神经网络将是通过随机梯度下降算法进行训练。\n我们先导入教程需要使用的软件包。\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets, cross_validation, metrics\nfrom matplotlib.colors import colorConverter, ListedColormap\nimport itertools\nimport collections\n手写数字集\n在这个教程中，我们使用scikit-learn提供的手写数字集。这个手写数字集包含1797张8*8的图片。在处理中，我们可以把像素铺平，形成一个64维的向量。下图展示了每个数字的图片。注意，这个数据集和MNIST手写数字集是不一样，MNIST是一个大型的数据集，而这个只是一个小型的数据集。\n我们会先对这个数据集进行一个预处理，将这个数据集切分成以下几部分：\n\n一个训练集，用于模型的训练。（输入数据：X_train，目标数据：T_train）\n一个验证的数据集，用于去评估模型的性能，如果模型在训练数据集上面出现过拟合了，那么可以终止训练了。（输入数据：X_validation，目标数据：T_avlidation）\n一个测试数据集，用于最终对模型的测试。（输入数据：X_test，目标数据：T_test）\n\n# load the data from scikit-learn.\ndigits = datasets.load_digits()\n\n# Load the targets.\n# Note that the targets are stored as digits, these need to be \n#  converted to one-hot-encoding for the output sofmax layer.\nT = np.zeros((digits.target.shape[0],10))\nT[np.arange(len(T)), digits.target] += 1\n\n# Divide the data into a train and test set.\nX_train, X_test, T_train, T_test = cross_validation.train_test_split(\n    digits.data, T, test_size=0.4)\n# Divide the test set into a validation set and final test set.\nX_validation, X_test, T_validation, T_test = cross_validation.train_test_split(\n    X_test, T_test, test_size=0.5)\n# Plot an example of each image.\nfig = plt.figure(figsize=(10, 1), dpi=100)\nfor i in range(10):\n    ax = fig.add_subplot(1,10,i+1)\n    ax.matshow(digits.images[i], cmap='binary') \n    ax.axis('off')\nplt.show()\n\n网络层的泛化\n在第四部分中，我们设计的神经网络通过矩阵相乘实现一个线性转换和一个非线性函数的转换。\n在进行非线性函数处理时，我们是对每个神经元进行处理的，这样的好处是可以帮助我们更加容易的进行理解和计算。\n我们利用Python classes构造了三个层：\n\n一个线性转换层LinearLayer\n\n一个Logistic函数LogisticLayer\n\n一个softmax函数层SoftmaxOutputLayer\n\n\n在正向传递时，每个层可以通过get_output函数计算该层的输出结果，这个结果将被下一层作为输入数据进行使用。在反向传递时，每一层的输入的梯度可以通过get_input_grad函数计算得到。如果是最后一层，那么梯度计算方程将利用目标结果进行计算。如果是中间的某一层，那么梯度就是梯度计算函数的输出结果。如果每个层有迭代参数的话，那么可以在get_params_iter函数中实现，并且在get_params_grad函数中按照原来的顺序实现参数的梯度。\n注意，在softmax层中，梯度和损失函数的计算将根据输入样本的数量进行计算。也就是说，这将使得梯度与损失函数和样本数量之间是相互独立的，以至于当我们改变批处理的数量时，对别的参数不会产生影响。\n# Define the non-linear functions used\ndef logistic(z): \n    return 1 / (1 + np.exp(-z))\n\ndef logistic_deriv(y):  # Derivative of logistic function\n    return np.multiply(y, (1 - y))\n    \ndef softmax(z): \n    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n# Define the layers used in this model\nclass Layer(object):\n    \"\"\"Base class for the different layers.\n    Defines base methods and documentation of methods.\"\"\"\n    \n    def get_params_iter(self):\n        \"\"\"Return an iterator over the parameters (if any).\n        The iterator has the same order as get_params_grad.\n        The elements returned by the iterator are editable in-place.\"\"\"\n        return []\n    \n    def get_params_grad(self, X, output_grad):\n        \"\"\"Return a list of gradients over the parameters.\n        The list has the same order as the get_params_iter iterator.\n        X is the input.\n        output_grad is the gradient at the output of this layer.\n        \"\"\"\n        return []\n    \n    def get_output(self, X):\n        \"\"\"Perform the forward step linear transformation.\n        X is the input.\"\"\"\n        pass\n    \n    def get_input_grad(self, Y, output_grad=None, T=None):\n        \"\"\"Return the gradient at the inputs of this layer.\n        Y is the pre-computed output of this layer (not needed in this case).\n        output_grad is the gradient at the output of this layer \n         (gradient at input of next layer).\n        Output layer uses targets T to compute the gradient based on the \n         output error instead of output_grad\"\"\"\n        pass\nclass LinearLayer(Layer):\n    \"\"\"The linear layer performs a linear transformation to its input.\"\"\"\n    \n    def __init__(self, n_in, n_out):\n        \"\"\"Initialize hidden layer parameters.\n        n_in is the number of input variables.\n        n_out is the number of output variables.\"\"\"\n        self.W = np.random.randn(n_in, n_out) * 0.1\n        self.b = np.zeros(n_out)\n        \n    def get_params_iter(self):\n        \"\"\"Return an iterator over the parameters.\"\"\"\n        return itertools.chain(np.nditer(self.W, op_flags=['readwrite']),\n                               np.nditer(self.b, op_flags=['readwrite']))\n    \n    def get_output(self, X):\n        \"\"\"Perform the forward step linear transformation.\"\"\"\n        return X.dot(self.W) + self.b\n        \n    def get_params_grad(self, X, output_grad):\n        \"\"\"Return a list of gradients over the parameters.\"\"\"\n        JW = X.T.dot(output_grad)\n        Jb = np.sum(output_grad, axis=0)\n        return [g for g in itertools.chain(np.nditer(JW), np.nditer(Jb))]\n    \n    def get_input_grad(self, Y, output_grad):\n        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n        return output_grad.dot(self.W.T)\nclass LogisticLayer(Layer):\n    \"\"\"The logistic layer applies the logistic function to its inputs.\"\"\"\n    \n    def get_output(self, X):\n        \"\"\"Perform the forward step transformation.\"\"\"\n        return logistic(X)\n    \n    def get_input_grad(self, Y, output_grad):\n        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n        return np.multiply(logistic_deriv(Y), output_grad)\nclass SoftmaxOutputLayer(Layer):\n    \"\"\"The softmax output layer computes the classification propabilities at the output.\"\"\"\n    \n    def get_output(self, X):\n        \"\"\"Perform the forward step transformation.\"\"\"\n        return softmax(X)\n    \n    def get_input_grad(self, Y, T):\n        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n        return (Y - T) / Y.shape[0]\n    \n    def get_cost(self, Y, T):\n        \"\"\"Return the cost at the output of this output layer.\"\"\"\n        return - np.multiply(T, np.log(Y)).sum() / Y.shape[0]\n样本模型\n接下来的部分，我们会实现设计的各个网络层，以及层与层之间的线性转换，神经元的非线性激活。\n在这个教程中，我们使用的样本模型是由两个隐藏层，Logistic函数作为激活函数，最后使用softmax函数作为分类的一个神经网络模型。第一层的隐藏层将输入的数据从64维度降维到20维度。第二层的隐藏层将前一层输入的20维度经过映射之后，还是以20维度输出。最后一层的输出层是一个10维度的分类结果。下图具体描述了这种架构的实现：\n\n这个神经网络被表示成一种序列模型，即当前层的输入数据是前一层的输出数据，当前层的输出数据将成为下一层的输入数据。第一层作为序列的第0位，最后一层作为序列的索引最后位置。\n# Define a sample model to be trained on the data\nhidden_neurons_1 = 20  # Number of neurons in the first hidden-layer\nhidden_neurons_2 = 20  # Number of neurons in the second hidden-layer\n# Create the model\nlayers = [] # Define a list of layers\n# Add first hidden layer\nlayers.append(LinearLayer(X_train.shape[1], hidden_neurons_1))\nlayers.append(LogisticLayer())\n# Add second hidden layer\nlayers.append(LinearLayer(hidden_neurons_1, hidden_neurons_2))\nlayers.append(LogisticLayer())\n# Add output layer\nlayers.append(LinearLayer(hidden_neurons_2, T_train.shape[1]))\nlayers.append(SoftmaxOutputLayer())\nBP算法\nBP算法在正向传播过程和反向传播过程中的具体细节已经在第四部分中进行了详细的解释，如果对此还有疑问，建议再去学习一下。这一部分，我们只单纯实现在多层神经网络中的BP算法。\n正向传播过程\n在下列代码中，forward_step函数实现了正向传播过程。get_output函数实现了每层的输出结果。这些激活的输出结果被保存在activations列表中。\n# Define the forward propagation step as a method.\ndef forward_step(input_samples, layers):\n    \"\"\"\n    Compute and return the forward activation of each layer in layers.\n    Input:\n        input_samples: A matrix of input samples (each row is an input vector)\n        layers: A list of Layers\n    Output:\n        A list of activations where the activation at each index i+1 corresponds to\n        the activation of layer i in layers. activations[0] contains the input samples.  \n    \"\"\"\n    activations = [input_samples] # List of layer activations\n    # Compute the forward activations for each layer starting from the first\n    X = input_samples\n    for layer in layers:\n        Y = layer.get_output(X)  # Get the output of the current layer\n        activations.append(Y)  # Store the output for future processing\n        X = activations[-1]  # Set the current input as the activations of the previous layer\n    return activations  # Return the activations of each layer\n反向传播过程\n在反向传播过程中，backward_step函数实现了反向传播过程。反向传播过程的计算是从最后一层开始的。先利用get_input_grad函数得到最初的梯度。然后，利用get_params_grad函数计算每一层的误差函数的梯度，并且把这些梯度保存在一个列表中。\n# Define the backward propagation step as a method\ndef backward_step(activations, targets, layers):\n    \"\"\"\n    Perform the backpropagation step over all the layers and return the parameter gradients.\n    Input:\n        activations: A list of forward step activations where the activation at \n            each index i+1 corresponds to the activation of layer i in layers. \n            activations[0] contains the input samples. \n        targets: The output targets of the output layer.\n        layers: A list of Layers corresponding that generated the outputs in activations.\n    Output:\n        A list of parameter gradients where the gradients at each index corresponds to\n        the parameters gradients of the layer at the same index in layers. \n    \"\"\"\n    param_grads = collections.deque()  # List of parameter gradients for each layer\n    output_grad = None  # The error gradient at the output of the current layer\n    # Propagate the error backwards through all the layers.\n    #  Use reversed to iterate backwards over the list of layers.\n    for layer in reversed(layers):   \n        Y = activations.pop()  # Get the activations of the last layer on the stack\n        # Compute the error at the output layer.\n        # The output layer error is calculated different then hidden layer error.\n        if output_grad is None:\n            input_grad = layer.get_input_grad(Y, targets)\n        else:  # output_grad is not None (layer is not output layer)\n            input_grad = layer.get_input_grad(Y, output_grad)\n        # Get the input of this layer (activations of the previous layer)\n        X = activations[-1]\n        # Compute the layer parameter gradients used to update the parameters\n        grads = layer.get_params_grad(X, output_grad)\n        param_grads.appendleft(grads)\n        # Compute gradient at output of previous layer (input of current layer):\n        output_grad = input_grad\n    return list(param_grads)  # Return the parameter gradients\n梯度检查\n正如在第四部分中的分析，我们通过比较数值梯度和反向传播计算的梯度，来分析梯度是否正确。\n在代码中，get_params_iter函数实现了得到每一层的参数，并且返回一个所有参数的迭代。get_params_grad函数根据反向传播，得到每一个参数对应的梯度。\n# Perform gradient checking\nnb_samples_gradientcheck = 10 # Test the gradients on a subset of the data\nX_temp = X_train[0:nb_samples_gradientcheck,:]\nT_temp = T_train[0:nb_samples_gradientcheck,:]\n# Get the parameter gradients with backpropagation\nactivations = forward_step(X_temp, layers)\nparam_grads = backward_step(activations, T_temp, layers)\n\n# Set the small change to compute the numerical gradient\neps = 0.0001\n# Compute the numerical gradients of the parameters in all layers.\nfor idx in range(len(layers)):\n    layer = layers[idx]\n    layer_backprop_grads = param_grads[idx]\n    # Compute the numerical gradient for each parameter in the layer\n    for p_idx, param in enumerate(layer.get_params_iter()):\n        grad_backprop = layer_backprop_grads[p_idx]\n        # + eps\n        param += eps\n        plus_cost = layers[-1].get_cost(forward_step(X_temp, layers)[-1], T_temp)\n        # - eps\n        param -= 2 * eps\n        min_cost = layers[-1].get_cost(forward_step(X_temp, layers)[-1], T_temp)\n        # reset param value\n        param += eps\n        # calculate numerical gradient\n        grad_num = (plus_cost - min_cost)/(2*eps)\n        # Raise error if the numerical grade is not close to the backprop gradient\n        if not np.isclose(grad_num, grad_backprop):\n            raise ValueError('Numerical gradient of {:.6f} is not close to the backpropagation gradient of {:.6f}!'.format(float(grad_num), float(grad_backprop)))\nprint('No gradient errors found')\nNo gradient errors found\nBP算法中的随机梯度下降\n这个教程我们使用一个梯度下降的改进版，称为随机梯度下降，来优化我们的损失函数。在一整个训练集上面，随机梯度下降算法只选择一个子集按照负梯度的方向进行更新。这样处理有以下几个好处：第一，在一个大型的训练数据集上面，我们可以节省时间和内存，因为这个算法减少了很多的矩阵操作。第二，增加了训练样本的多样性。\n损失函数需要和输入样本的数量之间相互独立，因为在随机梯度算法处理的每一个过程中，样本子集的数量这一信息都被使用了。这也是为什么我们使用损失函授的均方误差，而不是平方误差。\n批处理的最小数量\n训练样本的子集经常被称之为最小批处理单位。在下面的代码中，我们将最小批处理单位设置成25，并且将输入数据和目标数据打包成一个元祖输入到网络中。\n# Create the minibatches\nbatch_size = 25  # Approximately 25 samples per batch\nnb_of_batches = X_train.shape[0] / batch_size  # Number of batches\n# Create batches (X,Y) from the training set\nXT_batches = zip(\n    np.array_split(X_train, nb_of_batches, axis=0),  # X samples\n    np.array_split(T_train, nb_of_batches, axis=0))  # Y targets\n随机梯度下降算法的更新\n在代码中，update_params函数中实现了对每个参数的更新操作。在每一次的迭代中，我们都使用最简单的梯度下降算法来处理参数的更新，即：\n\n其中，μ是学习率。\nnb_of_iterations函数实现了，更新操作将会在一整个训练集上面进行多次迭代，每一次迭代都是取最小批处理单位的数据量。在每次全部迭代完之后，模型将会在验证集上面进行测试。如果在验证集上面，经过三次的完全迭代，损失函数的值没有下降，那么我们就认为模型已经过拟合了，需要终止模型的训练。或者经过设置的最大值300次，模型也会被终止训练。所以的损失误差值将会被保存下来，以便后续的分析。\n# Define a method to update the parameters\ndef update_params(layers, param_grads, learning_rate):\n    \"\"\"\n    Function to update the parameters of the given layers with the given gradients\n    by gradient descent with the given learning rate.\n    \"\"\"\n    for layer, layer_backprop_grads in zip(layers, param_grads):\n        for param, grad in itertools.izip(layer.get_params_iter(), layer_backprop_grads):\n            # The parameter returned by the iterator point to the memory space of\n            #  the original layer and can thus be modified inplace.\n            param -= learning_rate * grad  # Update each parameter\n# Perform backpropagation\n# initalize some lists to store the cost for future analysis        \nminibatch_costs = []\ntraining_costs = []\nvalidation_costs = []\n\nmax_nb_of_iterations = 300  # Train for a maximum of 300 iterations\nlearning_rate = 0.1  # Gradient descent learning rate\n\n# Train for the maximum number of iterations\nfor iteration in range(max_nb_of_iterations):\n    for X, T in XT_batches:  # For each minibatch sub-iteration\n        activations = forward_step(X, layers)  # Get the activations\n        minibatch_cost = layers[-1].get_cost(activations[-1], T)  # Get cost\n        minibatch_costs.append(minibatch_cost)\n        param_grads = backward_step(activations, T, layers)  # Get the gradients\n        update_params(layers, param_grads, learning_rate)  # Update the parameters\n    # Get full training cost for future analysis (plots)\n    activations = forward_step(X_train, layers)\n    train_cost = layers[-1].get_cost(activations[-1], T_train)\n    training_costs.append(train_cost)\n    # Get full validation cost\n    activations = forward_step(X_validation, layers)\n    validation_cost = layers[-1].get_cost(activations[-1], T_validation)\n    validation_costs.append(validation_cost)\n    if len(validation_costs) > 3:\n        # Stop training if the cost on the validation set doesn't decrease\n        #  for 3 iterations\n        if validation_costs[-1] >= validation_costs[-2] >= validation_costs[-3]:\n            break\n    \nnb_of_iterations = iteration + 1  # The number of iterations that have been executed\nminibatch_x_inds = np.linspace(0, nb_of_iterations, num=nb_of_iterations*nb_of_batches)\niteration_x_inds = np.linspace(1, nb_of_iterations, num=nb_of_iterations)\n# Plot the cost over the iterations\nplt.plot(minibatch_x_inds, minibatch_costs, 'k-', linewidth=0.5, label='cost minibatches')\nplt.plot(iteration_x_inds, training_costs, 'r-', linewidth=2, label='cost full training set')\nplt.plot(iteration_x_inds, validation_costs, 'b-', linewidth=3, label='cost validation set')\n# Add labels to the plot\nplt.xlabel('iteration')\nplt.ylabel('$\\\\xi$', fontsize=15)\nplt.title('Decrease of cost over backprop iteration')\nplt.legend()\nx1,x2,y1,y2 = plt.axis()\nplt.axis((0,nb_of_iterations,0,2.5))\nplt.grid()\nplt.show()\n\n模型在测试集上面的性能\n最后，我们在测试集上面进行模型的最终测试。在这个模型中，我们最后的训练正确率是96%。\n最后的结果可以利用混淆图进行更加深入的分析。这个表展示了每一个手写数字被分类为什么数字的数量。下图是利用scikit-learn的confusion_matrix方法实现的。\n比如，数字8被误分类了五次，其中，两次被分类成了2，两次被分类成了5，一次被分类成了9。\n# Get results of test data\ny_true = np.argmax(T_test, axis=1)  # Get the target outputs\nactivations = forward_step(X_test, layers)  # Get activation of test samples\ny_pred = np.argmax(activations[-1], axis=1)  # Get the predictions made by the network\ntest_accuracy = metrics.accuracy_score(y_true, y_pred)  # Test set accuracy\nprint('The accuracy on the test set is {:.2f}'.format(test_accuracy))\nThe accuracy on the test set is 0.96\n# Show confusion table\nconf_matrix = metrics.confusion_matrix(y_true, y_pred, labels=None)  # Get confustion matrix\n# Plot the confusion table\nclass_names = ['${:d}$'.format(x) for x in range(0, 10)]  # Digit class names\nfig = plt.figure()\nax = fig.add_subplot(111)\n# Show class labels on each axis\nax.xaxis.tick_top()\nmajor_ticks = range(0,10)\nminor_ticks = [x + 0.5 for x in range(0, 10)]\nax.xaxis.set_ticks(major_ticks, minor=False)\nax.yaxis.set_ticks(major_ticks, minor=False)\nax.xaxis.set_ticks(minor_ticks, minor=True)\nax.yaxis.set_ticks(minor_ticks, minor=True)\nax.xaxis.set_ticklabels(class_names, minor=False, fontsize=15)\nax.yaxis.set_ticklabels(class_names, minor=False, fontsize=15)\n# Set plot labels\nax.yaxis.set_label_position(\"right\")\nax.set_xlabel('Predicted label')\nax.set_ylabel('True label')\nfig.suptitle('Confusion table', y=1.03, fontsize=15)\n# Show a grid to seperate digits\nax.grid(b=True, which=u'minor')\n# Color each grid cell according to the number classes predicted\nax.imshow(conf_matrix, interpolation='nearest', cmap='binary')\n# Show the number of samples in each cell\nfor x in xrange(conf_matrix.shape[0]):\n    for y in xrange(conf_matrix.shape[1]):\n        color = 'w' if x == y else 'k'\n        ax.text(x, y, conf_matrix[y,x], ha=\"center\", va=\"center\", color=color)       \nplt.show()\n\n完整代码，点击这里\n\n作者：chen_h微信号 & QQ：862251340简书地址：https://www.jianshu.com/p/cb6...\nCoderPai 是一个专注于算法实战的平台，从基础的算法到人工智能算法都有设计。如果你对算法实战感兴趣，请快快关注我们吧。加入AI实战微信群，AI实战QQ群，ACM算法微信群，ACM算法QQ群。长按或者扫描如下二维码，关注 “CoderPai” 微信号（coderpai）\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "1"}