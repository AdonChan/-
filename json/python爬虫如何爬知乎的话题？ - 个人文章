{"title": "python爬虫如何爬知乎的话题？ - 个人文章 ", "index": "python,网页爬虫,知乎,代码规范", "content": "因为要做观点，观点的屋子类似于知乎的话题，所以得想办法把他给爬下来，搞了半天最终还是妥妥的搞定了，代码是python写的，不懂得麻烦自学哈！懂得直接看代码，绝对可用\n#coding:utf-8\n\"\"\"\n@author:haoning\n@create time:2015.8.5\n\"\"\"\nfrom __future__ import division  # 精确除法\nfrom Queue import Queue\nfrom __builtin__ import False\nimport json\nimport os\nimport re\nimport platform\nimport uuid\nimport urllib\nimport urllib2\nimport sys\nimport time\nimport MySQLdb as mdb\nfrom bs4 import BeautifulSoup\n\nreload(sys)\nsys.setdefaultencoding( \"utf-8\" )\n\nheaders = {\n   'User-Agent' : 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:35.0) Gecko/20100101 Firefox/35.0',\n   'Content-Type':'application/x-www-form-urlencoded; charset=UTF-8',\n   'X-Requested-With':'XMLHttpRequest',\n   'Referer':'https://www.zhihu.com/topics',\n   'Cookie':'__utma=51854390.517069884.1416212035.1416212035.1416212035.1; q_c1=c02bf44d00d240798bfabcfc95baeb56|1455778173000|1416205243000; _za=b1c8ae35-f986-46a2-b24a-cb9359dc6b2a; aliyungf_tc=AQAAAJ1m71jL1woArKqF22VFnL/wRy6C; _xsrf=9d494558f9271340ab24598d85b2a3c8; cap_id=\"MDNiMjcwM2U0MTRhNDVmYjgxZWVhOWI0NTA2OGU5OTg=|1455864276|2a4ce8247ebd3c0df5393bb5661713ad9eec01dd\"; n_c=1; _alicdn_sec=56c6ba4d556557d27a0f8c876f563d12a285f33a'\n}\n\nDB_HOST = '127.0.0.1'\nDB_USER = 'root'\nDB_PASS = 'root'\n\nqueue= Queue() #接收队列\nnodeSet=set()\nkeywordSet=set()\nstop=0\noffset=-20\nlevel=0\nmaxLevel=7\ncounter=0\nbase=\"\"\n\nconn = mdb.connect(DB_HOST, DB_USER, DB_PASS, 'zhihu', charset='utf8')\nconn.autocommit(False)\ncurr = conn.cursor()\n\ndef get_html(url):\n    try:\n        req = urllib2.Request(url)\n        response = urllib2.urlopen(req,None,3) #在这里应该加入代理\n        html = response.read()\n        return html\n    except:\n        pass\n    return None\n\ndef getTopics():\n    url = 'https://www.zhihu.com/topics'\n    print url\n    try:\n        req = urllib2.Request(url)\n        response = urllib2.urlopen(req) #鍦ㄨ繖閲屽簲璇ュ姞鍏ヤ唬鐞�\n        html = response.read().decode('utf-8')\n        print html\n        soup = BeautifulSoup(html)\n        lis = soup.find_all('li', {'class' : 'zm-topic-cat-item'})\n        \n        for li in lis:\n            data_id=li.get('data-id')\n            name=li.text\n            curr.execute('select id from classify_new where name=%s',(name))\n            y= curr.fetchone()\n            if not y:\n                curr.execute('INSERT INTO classify_new(data_id,name)VALUES(%s,%s)',(data_id,name))\n        conn.commit()\n    except Exception as e:\n        print \"get topic error\",e\n        \n\ndef get_extension(name):  \n    where=name.rfind('.')\n    if where!=-1:\n        return name[where:len(name)]\n    return None\n\n\ndef which_platform():\n    sys_str = platform.system()\n    return sys_str\n\ndef GetDateString():\n    when=time.strftime('%Y-%m-%d',time.localtime(time.time()))\n    foldername = str(when)\n    return foldername \n\ndef makeDateFolder(par,classify):\n    try:\n        if os.path.isdir(par):\n            newFolderName=par + '//' + GetDateString() + '//'  +str(classify)\n            if which_platform()==\"Linux\":\n                newFolderName=par + '/' + GetDateString() + \"/\" +str(classify)\n            if not os.path.isdir( newFolderName ):\n                os.makedirs( newFolderName )\n            return newFolderName\n        else:\n            return None \n    except Exception,e:\n        print \"kk\",e\n    return None \n\ndef download_img(url,classify):\n    try:\n        extention=get_extension(url)\n        if(extention is None):\n            return None\n        req = urllib2.Request(url)\n        resp = urllib2.urlopen(req,None,3)\n        dataimg=resp.read()\n        name=str(uuid.uuid1()).replace(\"-\",\"\")+\"_www.guandn.com\"+extention\n        top=\"E://topic_pic\"\n        folder=makeDateFolder(top, classify)\n        filename=None\n        if folder is not None:\n            filename  =folder+\"//\"+name\n        try:\n            if \"e82bab09c_m\" in str(url):\n                return True\n            if not os.path.exists(filename):\n                file_object = open(filename,'w+b')\n                file_object.write(dataimg)\n                file_object.close()\n                return '/room/default/'+GetDateString()+'/'+str(classify)+\"/\"+name\n            else:\n                print \"file exist\"\n                return None\n        except IOError,e1:\n            print \"e1=\",e1\n            pass\n    except Exception as e:\n        print \"eee\",e\n        pass\n    return None #如果没有下载下来就利用原来网站的链接\n\ndef getChildren(node,name):\n    global queue,nodeSet\n    try:\n        url=\"https://www.zhihu.com/topic/\"+str(node)+\"/hot\"\n        html=get_html(url)\n        if html is None:\n            return\n        soup = BeautifulSoup(html)\n        p_ch='父话题'\n        node_name=soup.find('div', {'id' : 'zh-topic-title'}).find('h1').text\n        topic_cla=soup.find('div', {'class' : 'child-topic'})\n        if topic_cla is not None:\n            try:\n                p_ch=str(topic_cla.text)\n                aList = soup.find_all('a', {'class' : 'zm-item-tag'}) #获取所有子节点\n                if u'子话题' in p_ch:\n                    for a in aList:\n                        token=a.get('data-token')\n                        a=str(a).replace('\\n','').replace('\\t','').replace('\\r','')\n                        start=str(a).find('>')\n                        end=str(a).rfind('</a>')\n                        new_node=str(str(a)[start+1:end])\n                        curr.execute('select id from rooms where name=%s',(new_node)) #先保证名字绝不相同\n                        y= curr.fetchone()\n                        if not y:\n                            print \"y=\",y,\"new_node=\",new_node,\"token=\",token\n                            queue.put((token,new_node,node_name))\n            except Exception as e:\n                print \"add queue error\",e\n    except Exception as e:\n        print \"get html error\",e\n        \n    \n\ndef getContent(n,name,p,top_id):\n    try:\n        global counter\n        curr.execute('select id from rooms where name=%s',(name)) #先保证名字绝不相同\n        y= curr.fetchone()\n        print \"exist?? \",y,\"n=\",n\n        if not y:\n            url=\"https://www.zhihu.com/topic/\"+str(n)+\"/hot\"\n            html=get_html(url)\n            if html is None:\n                return\n            soup = BeautifulSoup(html)\n            title=soup.find('div', {'id' : 'zh-topic-title'}).find('h1').text\n            pic_path=soup.find('a',{'id':'zh-avartar-edit-form'}).find('img').get('src')\n            description=soup.find('div',{'class':'zm-editable-content'})\n            if description is not None:\n                description=description.text\n                \n            if (u\"未归类\" in title or u\"根话题\" in title): #允许入库，避免死循环\n                description=None\n                \n            tag_path=download_img(pic_path,top_id)\n            print \"tag_path=\",tag_path\n            if (tag_path is not None) or tag_path==True:\n                if tag_path==True:\n                    tag_path=None\n                father_id=2 #默认为杂谈\n                curr.execute('select id from rooms where name=%s',(p))\n                results = curr.fetchall()\n                for r in results:\n                    father_id=r[0]\n                name=title\n                curr.execute('select id from rooms where name=%s',(name)) #先保证名字绝不相同\n                y= curr.fetchone()\n                print \"store see..\",y\n                if not y:\n                    friends_num=0\n                    temp = time.time()\n                    x = time.localtime(float(temp))\n                    create_time = time.strftime(\"%Y-%m-%d %H:%M:%S\",x) # get time now\n                    create_time\n                    creater_id=None\n                    room_avatar=tag_path\n                    is_pass=1\n                    has_index=0\n                    reason_id=None  \n                    #print father_id,name,friends_num,create_time,creater_id,room_avatar,is_pass,has_index,reason_id\n                    ######################有资格入库的内容\n                    counter=counter+1\n                    curr.execute(\"INSERT INTO rooms(father_id,name,friends_num,description,create_time,creater_id,room_avatar,is_pass,has_index,reason_id)VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\",(father_id,name,friends_num,description,create_time,creater_id,room_avatar,is_pass,has_index,reason_id))\n                    conn.commit() #必须时时进入数据库，不然找不到父节点\n                    if counter % 200==0:\n                        print \"current node\",name,\"num\",counter\n    except Exception as e:\n        print \"get content error\",e       \n\ndef work():\n    global queue\n    curr.execute('select id,node,parent,name from classify where status=1')\n    results = curr.fetchall()\n    for r in results:\n        top_id=r[0]\n        node=r[1]\n        parent=r[2]\n        name=r[3]\n        try:\n            queue.put((node,name,parent)) #首先放入队列\n            while queue.qsize() >0:\n                n,p=queue.get() #顶节点出队\n                getContent(n,p,top_id)\n                getChildren(n,name) #出队内容的子节点\n            conn.commit()\n        except Exception as e:\n            print \"what's wrong\",e  \n            \ndef new_work():\n    global queue\n    curr.execute('select id,data_id,name from classify_new_copy where status=1')\n    results = curr.fetchall()\n    for r in results:\n        top_id=r[0]\n        data_id=r[1]\n        name=r[2]\n        try:\n            get_topis(data_id,name,top_id)\n        except:\n            pass\n\n\ndef get_topis(data_id,name,top_id):\n    global queue\n    url = 'https://www.zhihu.com/node/TopicsPlazzaListV2'\n    isGet = True;\n    offset = -20;\n    data_id=str(data_id)\n    while isGet:\n        offset = offset + 20\n        values = {'method': 'next', 'params': '{\"topic_id\":'+data_id+',\"offset\":'+str(offset)+',\"hash_id\":\"\"}'}\n        try:\n            msg=None\n            try:\n                data = urllib.urlencode(values)\n                request = urllib2.Request(url,data,headers)\n                response = urllib2.urlopen(request,None,5)\n                html=response.read().decode('utf-8')\n                json_str = json.loads(html)\n                ms=json_str['msg']\n                if len(ms) <5:\n                    break\n                msg=ms[0]\n            except Exception as e:\n                print \"eeeee\",e\n            #print msg\n            if msg is not None:\n                soup = BeautifulSoup(str(msg))\n                blks = soup.find_all('div', {'class' : 'blk'})\n                for blk in blks:\n                    page=blk.find('a').get('href')\n                    if page is not None:\n                        node=page.replace(\"/topic/\",\"\") #将更多的种子入库\n                        parent=name\n                        ne=blk.find('strong').text\n                        try:\n                            queue.put((node,ne,parent)) #首先放入队列\n                            while queue.qsize() >0:\n                                n,name,p=queue.get() #顶节点出队\n                                size=queue.qsize()\n                                if size > 0:\n                                    print size\n                                getContent(n,name,p,top_id)\n                                getChildren(n,name) #出队内容的子节点\n                            conn.commit()\n                        except Exception as e:\n                            print \"what's wrong\",e  \n        except urllib2.URLError, e:\n            print \"error is\",e\n            pass \n            \n        \nif __name__ == '__main__':\n    i=0\n    while i<400:\n        new_work()\n        i=i+1\n\n说下数据库的问题，我这里就不传附件了，看字段自己建立，因为这确实太简单了，我是用的mysql，你看自己的需求自己建。\n有什么不懂得麻烦去去转盘网找我，因为这个也是我开发的，上面会及时更新qq群号，这里不留qq号啥的，以免被系统给K了。\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}