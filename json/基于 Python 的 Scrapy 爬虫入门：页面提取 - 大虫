{"title": "基于 Python 的 Scrapy 爬虫入门：页面提取 - 大虫 ", "index": "scrapy,python", "content": "目录\n\n基于 Python 的 Scrapy 爬虫入门：环境搭建\n基于 Python 的 Scrapy 爬虫入门：页面提取\n基于 Python 的 Scrapy 爬虫入门：图片处理\n\n\n下面创建一个爬虫项目，以图虫网为例抓取图片。\n一、内容分析\n打开 图虫网，顶部菜单“发现” “标签”里面是对各种图片的分类，点击一个标签，比如“美女”，网页的链接为：https://tuchong.com/tags/美女/，我们以此作为爬虫入口，分析一下该页面：\n打开页面后出现一个个的图集，点击图集可全屏浏览图片，向下滚动页面会出现更多的图集，没有页码翻页的设置。Chrome右键“检查元素”打开开发者工具，检查页面源码，内容部分如下：\n<div class=\"content\">\n    <div class=\"widget-gallery\">\n        <ul class=\"pagelist-wrapper\">\n            <li class=\"gallery-item...\n可以判断每一个li.gallery-item是一个图集的入口，存放在ul.pagelist-wrapper下，div.widget-gallery是一个容器，如果使用 xpath 选取应该是：//div[@class=\"widget-gallery\"]/ul/li，按照一般页面的逻辑，在li.gallery-item下面找到对应的链接地址，再往下深入一层页面抓取图片。\n但是如果用类似 Postman 的HTTP调试工具请求该页面，得到的内容是：\n<div class=\"content\">\n    <div class=\"widget-gallery\"></div>\n</div>\n也就是并没有实际的图集内容，因此可以断定页面使用了Ajax请求，只有在浏览器载入页面时才会请求图集内容并加入div.widget-gallery中，通过开发者工具查看XHR请求地址为：\nhttps://tuchong.com/rest/tags/美女/posts?page=1&count=20&order=weekly&before_timestamp=\n参数很简单，page是页码，count是每页图集数量，order是排序，before_timestamp为空，图虫因为是推送内容式的网站，因此before_timestamp应该是一个时间值，不同的时间会显示不同的内容，这里我们把它丢弃，不考虑时间直接从最新的页面向前抓取。\n请求结果为JSON格式内容，降低了抓取难度，结果如下：\n{\n  \"postList\": [\n    {\n      \"post_id\": \"15624611\",\n      \"type\": \"multi-photo\",\n      \"url\": \"https://weishexi.tuchong.com/15624611/\",\n      \"site_id\": \"443122\",\n      \"author_id\": \"443122\",\n      \"published_at\": \"2017-10-28 18:01:03\",\n      \"excerpt\": \"10月18日\",\n      \"favorites\": 4052,\n      \"comments\": 353,\n      \"rewardable\": true,\n      \"parent_comments\": \"165\",\n      \"rewards\": \"2\",\n      \"views\": 52709,\n      \"title\": \"微风不燥  秋意正好\",\n      \"image_count\": 15,\n      \"images\": [\n        {\n          \"img_id\": 11585752,\n          \"user_id\": 443122,\n          \"title\": \"\",\n          \"excerpt\": \"\",\n          \"width\": 5016,\n          \"height\": 3840\n        },\n        {\n          \"img_id\": 11585737,\n          \"user_id\": 443122,\n          \"title\": \"\",\n          \"excerpt\": \"\",\n          \"width\": 3840,\n          \"height\": 5760\n        },\n        ...\n      ],\n      \"title_image\": null,\n      \"tags\": [\n        {\n          \"tag_id\": 131,\n          \"type\": \"subject\",\n          \"tag_name\": \"人像\",\n          \"event_type\": \"\",\n          \"vote\": \"\"\n        },\n        {\n          \"tag_id\": 564,\n          \"type\": \"subject\",\n          \"tag_name\": \"美女\",\n          \"event_type\": \"\",\n          \"vote\": \"\"\n        }\n      ],\n      \"favorite_list_prefix\": [],\n      \"reward_list_prefix\": [],\n      \"comment_list_prefix\": [],\n      \"cover_image_src\": \"https://photo.tuchong.com/443122/g/11585752.webp\",\n      \"is_favorite\": false\n    }\n  ],\n  \"siteList\": {...},\n  \"following\": false,\n  \"coverUrl\": \"https://photo.tuchong.com/443122/ft640/11585752.webp\",\n  \"tag_name\": \"美女\",\n  \"tag_id\": \"564\",\n  \"url\": \"https://tuchong.com/tags/%E7%BE%8E%E5%A5%B3/\",\n  \"more\": true,\n  \"result\": \"SUCCESS\"\n}\n根据属性名称很容易知道对应的内容含义，这里我们只需关心 postlist 这个属性，它对应的一个数组元素便是一个图集，图集元素中有几项属性我们需要用到：\n\n\nurl：单个图集浏览的页面地址\n\npost_id：图集编号，在网站中应该是唯一的，可以用来判断是否已经抓取过该内容\n\nsite_id：作者站点编号 ，构建图片来源链接要用到\n\ntitle：标题\n\nexcerpt：摘要文字\n\ntype：图集类型，目前发现两种，一种multi-photo是纯照片，一种text是文字与图片混合的文章式页面，两种内容结构不同，需要不同的抓取方式，本例中只抓取纯照片类型，text类型直接丢弃\n\ntags：图集标签，有多个\n\nimage_count：图片数量\n\nimages：图片列表，它是一个对象数组，每个对象中包含一个img_id属性需要用到\n\n根据图片浏览页面分析，基本上图片的地址都是这种格式： https://photo.tuchong.com/{site_id}/f/{img_id}.jpg ，很容易通过上面的信息合成。\n二、创建项目\n\n进入cmder命令行工具，输入workon scrapy 进入之前建立的虚拟环境，此时命令行提示符前会出现(Scrapy) 标识，标识处于该虚拟环境中，相关的路径都会添加到PATH环境变量中便于开发及使用。\n输入 scrapy startproject tuchong 创建项目 tuchong\n进入项目主目录，输入 scrapy genspider photo tuchong.com 创建一个爬虫名称叫 photo (不能与项目同名)，爬取 tuchong.com 域名（这个需要修改，此处先输个大概地址），的一个项目内可以包含多个爬虫\n\n经过以上步骤，项目自动建立了一些文件及设置，目录结构如下：\n(PROJECT)\n│  scrapy.cfg\n│\n└─tuchong\n    │  items.py\n    │  middlewares.py\n    │  pipelines.py\n    │  settings.py\n    │  __init__.py\n    │\n    ├─spiders\n    │  │  photo.py\n    │  │  __init__.py\n    │  │\n    │  └─__pycache__\n    │          __init__.cpython-36.pyc\n    │\n    └─__pycache__\n            settings.cpython-36.pyc\n            __init__.cpython-36.pyc\n\n\nscrapy.cfg：基础设置\n\nitems.py：抓取条目的结构定义\n\nmiddlewares.py：中间件定义，此例中无需改动\n\npipelines.py：管道定义，用于抓取数据后的处理\n\nsettings.py：全局设置\n\nspiders\\photo.py：爬虫主体，定义如何抓取需要的数据\n\n三、主要代码\nitems.py 中创建一个TuchongItem类并定义需要的属性，属性继承自 scrapy.Field 值可以是字符、数字或者列表或字典等等：\nimport scrapy\n\nclass TuchongItem(scrapy.Item):\n    post_id = scrapy.Field()\n    site_id = scrapy.Field()\n    title = scrapy.Field()\n    type = scrapy.Field()\n    url = scrapy.Field()\n    image_count = scrapy.Field()\n    images = scrapy.Field()\n    tags = scrapy.Field()\n    excerpt = scrapy.Field()\n    ...\n这些属性的值将在爬虫主体中赋予。\nspiders\\photo.py 这个文件是通过命令 scrapy genspider photo tuchong.com 自动创建的，里面的初始内容如下：\nimport scrapy\n\nclass PhotoSpider(scrapy.Spider):\n    name = 'photo'\n    allowed_domains = ['tuchong.com']\n    start_urls = ['http://tuchong.com/']\n\n    def parse(self, response):\n        pass\n爬虫名 name，允许的域名 allowed_domains（如果链接不属于此域名将丢弃，允许多个） ，起始地址 start_urls 将从这里定义的地址抓取（允许多个）函数 parse 是处理请求内容的默认回调函数，参数 response 为请求内容，页面内容文本保存在 response.body 中，我们需要对默认代码稍加修改，让其满足多页面循环发送请求，这需要重载 start_requests 函数，通过循环语句构建多页的链接请求，修改后代码如下：\nimport scrapy, json\nfrom ..items import TuchongItem\n\nclass PhotoSpider(scrapy.Spider):\n    name = 'photo'\n    # allowed_domains = ['tuchong.com']\n    # start_urls = ['http://tuchong.com/']\n\n    def start_requests(self):\n        url = 'https://tuchong.com/rest/tags/%s/posts?page=%d&count=20&order=weekly';\n        # 抓取10个页面，每页20个图集\n        # 指定 parse 作为回调函数并返回 Requests 请求对象\n        for page in range(1, 11):\n            yield scrapy.Request(url=url % ('美女', page), callback=self.parse)\n\n    # 回调函数，处理抓取内容填充 TuchongItem 属性\n    def parse(self, response):\n        body = json.loads(response.body_as_unicode())\n        items = []\n        for post in body['postList']:\n            item = TuchongItem()\n            item['type'] = post['type']\n            item['post_id'] = post['post_id']\n            item['site_id'] = post['site_id']\n            item['title'] = post['title']\n            item['url'] = post['url']\n            item['excerpt'] = post['excerpt']\n            item['image_count'] = int(post['image_count'])\n            item['images'] = {}\n            # 将 images 处理成 {img_id: img_url} 对象数组\n            for img in post.get('images', ''):\n                img_id = img['img_id']\n                url = 'https://photo.tuchong.com/%s/f/%s.jpg' % (item['site_id'], img_id)\n                item['images'][img_id] = url\n\n            item['tags'] = []\n            # 将 tags 处理成 tag_name 数组\n            for tag in post.get('tags', ''):\n                item['tags'].append(tag['tag_name'])\n            items.append(item)\n        return items\n经过这些步骤，抓取的数据将被保存在 TuchongItem 类中，作为结构化的数据便于处理及保存。\n前面说过，并不是所有抓取的条目都需要，例如本例中我们只需要 type=\"multi_photo 类型的图集，并且图片太少的也不需要，这些抓取条目的筛选操作以及如何保存需要在pipelines.py中处理，该文件中默认已创建类 TuchongPipeline 并重载了 process_item 函数，通过修改该函数只返回那些符合条件的 item，代码如下：\n...\n    def process_item(self, item, spider):\n        # 不符合条件触发 scrapy.exceptions.DropItem 异常，符合条件的输出地址\n        if int(item['image_count']) < 3:\n            raise DropItem(\"美女太少: \" + item['url'])\n        elif item['type'] != 'multi-photo':\n            raise DropItem(\"格式不对: \" + + item['url'])\n        else:\n            print(item['url'])\n        return item\n...\n当然如果不用管道直接在 parse 中处理也是一样的，只不过这样结构更清晰一些，而且还有功能更多的FilePipelines和ImagePipelines可供使用，process_item将在每一个条目抓取后触发，同时还有 open_spider 及 close_spider 函数可以重载，用于处理爬虫打开及关闭时的动作。\n\n注意：管道需要在项目中注册才能使用，在 settings.py 中添加：\nITEM_PIPELINES = {\n    'tuchong.pipelines.TuchongPipeline': 300, # 管道名称: 运行优先级(数字小优先)\n}\n另外，大多数网站都有反爬虫的 Robots.txt 排除协议，设置 ROBOTSTXT_OBEY = True 可以忽略这些协议，是的，这好像只是个君子协定。如果网站设置了浏览器User Agent或者IP地址检测来反爬虫，那就需要更高级的Scrapy功能，本文不做讲解。\n\n四、运行\n返回 cmder 命令行进入项目目录，输入命令：\nscrapy crawl photo\n终端会输出所有的爬行结果及调试信息，并在最后列出爬虫运行的统计信息，例如：\n[scrapy.statscollectors] INFO: Dumping Scrapy stats:\n{'downloader/request_bytes': 491,\n 'downloader/request_count': 2,\n 'downloader/request_method_count/GET': 2,\n 'downloader/response_bytes': 10224,\n 'downloader/response_count': 2,\n 'downloader/response_status_count/200': 2,\n 'finish_reason': 'finished',\n 'finish_time': datetime.datetime(2017, 11, 27, 7, 20, 24, 414201),\n 'item_dropped_count': 5,\n 'item_dropped_reasons_count/DropItem': 5,\n 'item_scraped_count': 15,\n 'log_count/DEBUG': 18,\n 'log_count/INFO': 8,\n 'log_count/WARNING': 5,\n 'response_received_count': 2,\n 'scheduler/dequeued': 1,\n 'scheduler/dequeued/memory': 1,\n 'scheduler/enqueued': 1,\n 'scheduler/enqueued/memory': 1,\n 'start_time': datetime.datetime(2017, 11, 27, 7, 20, 23, 867300)}\n主要关注ERROR及WARNING两项，这里的 Warning 其实是不符合条件而触发的 DropItem 异常。\n五、保存结果\n大多数情况下都需要对抓取的结果进行保存，默认情况下 item.py 中定义的属性可以保存到文件中，只需要命令行加参数 -o {filename} 即可：\nscrapy crawl photo -o output.json # 输出为JSON文件\nscrapy crawl photo -o output.csv  # 输出为CSV文件\n注意：输出至文件中的项目是未经过 TuchongPipeline 筛选的项目，只要在 parse 函数中返回的 Item 都会输出，因此也可以在 parse 中过滤只返回需要的项目\n如果需要保存至数据库，则需要添加额外代码处理，比如可以在 pipelines.py 中 process_item 后添加:\n...\n    def process_item(self, item, spider):\n        ...\n        else:\n            print(item['url'])\n            self.myblog.add_post(item) # myblog 是一个数据库类，用于处理数据库操作\n        return item\n...\n为了在插入数据库操作中排除重复的内容，可以使用 item['post_id'] 进行判断，如果存在则跳过。\n本项目中的抓取内容只涉及了文本及图片链接，并未下载图片文件，如需下载图片，可以通过两种方式：\n\n安装 Requests 模块，在 process_item 函数中下载图片内容，同时在保存数据库时替换为本地图片路径。\n使用 ImagePipelines 管道下载图片，具体使用方法下回讲解。\n\n\n                ", "mainLikeNum": ["7 "], "mainBookmarkNum": "67"}