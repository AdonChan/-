{"title": "mnist 机器学习入门笔记(一)  学习softmax模型 - 个人文章 ", "index": "python", "content": "学习softmax回归模型\n一. 下载mnist数据集\n新建一个download.py 代码如下：\n\"\"\"Functions for downloading and reading MNIST data.\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport tempfile\n\nimport numpy\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nfrom tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n运行则会自动下载mnist数据集\n二. softmax模型参数\nmnist.train.images(像素点)：下载的mnist中的mnist.train包含60000万张简单的验证码图片， 我们将每张图片看作 28 * 28 = 784个像素点。如此我们可以用两个维度量表示整个数据集，维度一：图片序号， 维度二：像素点序号。 那么整个数据集最大像素点为[60000, 784]mnist.train.labels(标签)：接下来我们的任务是识别每张图片中的数字， 所以我们给每张图片设立一个标签， 标签值介于0～9之间（共十个值）, 所以那个数据集的标签就可以做成两个维度， 维度一： 图片序号（60000）， 维度二：标签值序号（10）， 那么最大的标签可以表示为[60000, 10]\n三. softmax数学推导\n对于这里的数学推导，我就不过多说了。只能赞叹人类的智慧是伟大的， 然后简单分析下，不会数学推导的，我们可以这样来理解，分析一张图片的标签到底是数字几， 我们需要看图片中的每个像素点像数字几， 我们将每个像素点像某个标签的概率进行加权计算 w表示784个像素点中每个像素点更像数字几的加权， 然后再加上最终计算出的数字的干扰偏置量b即可。 大致理解和最终推导式\n四. softmax实现\n导入tensorflowimport tensorflow as tf定义像素：x = tf.placeholder(tf.float32, [None, 784])x不是一个特定的值，而是一个占位符placeholder，我们在TensorFlow运行计算时输入这个值。如此我们希望能输入任意数量的图片，所以在像素点参数中，第一个维度是无法确定的，所以我们用[None，784 ]来表示定义w:W = tf.Variable(tf.zeros([784,10]))定义b:b = tf.Variable(tf.zeros([10]))实现softmax等式：y = tf.nn.softmax(tf.matmul(x,W) + b)\n五.训练模型\n评估模型我们使用交叉熵作为成本函数。\n首先需要添加一个新的占位符用于输入正确值：y_ = tf.placeholder(\"float\", [None,10])计算交叉熵的表达式可以实现为：cross_entropy = -tf.reduce_sum(y_*tf.log(y))现在我们知道我们需要我们的模型做什么啦，用TensorFlow来训练它是非常容易的。因为TensorFlow拥有一张描述你各个计算单元的图，它可以自动地使用反向传播算法(backpropagation algorithm)来有效地确定你的变量是如何影响你想要最小化的那个成本值的。然后，TensorFlow会用你选择的优化算法来不断地修改变量以降低成本。这里我们使用梯度下降算法来计算梯度下降算法:qtrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)我们已经设置好了我们的模型。在运行计算之前，我们需要添加一个操作来初始化我们创建的变量：init = tf.initialize_all_variables()启动我们的模型，并且初始化变量：sess = tf.Session()sess.run(init)然后开始训练模型，这里我们让模型循环训练1000次！\nfor i in range(1000):\n  batch_xs, batch_ys = mnist.train.next_batch(100)\n  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n评估模型\ntf.argmax 是一个非常有用的函数，它能给出某个tensor对象在某一维上的其数据最大值所在的索引值。由于标签向量是由0,1组成，因此最大值1所在的索引位置就是类别标签，比如tf.argmax(y,1)返回的是模型对于任一输入x预测到的标签值，而 tf.argmax(y_,1) 代表正确的标签，我们可以用 tf.equal 来检测我们的预测是否真实标签匹配。correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))该函数返回单个实例等正确性，返回结果为bool值。所以我们需要把结果转化为浮点数然后再求取平均值。accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))    最后我们获得正确率为：print sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}