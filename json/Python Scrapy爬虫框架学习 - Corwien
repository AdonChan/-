{"title": "Python Scrapy爬虫框架学习 - Corwien ", "index": "scrapy,python爬虫,python", "content": "Scrapy 是用Python实现一个为爬取网站数据、提取结构性数据而编写的应用框架。\n一、Scrapy框架简介\nScrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。\n其最初是为了 页面抓取 (更确切来说, 网络抓取 )所设计的， 也可以应用在获取API所返回的数据(例如 Amazon Associates Web Services ) 或者通用的网络爬虫。\n二、架构流程图\n接下来的图表展现了Scrapy的架构，包括组件及在系统中发生的数据流的概览(绿色箭头所示)。 下面对每个组件都做了简单介绍，并给出了详细内容的链接。数据流如下所描述。\n\n1、组件\nScrapy Engine\n引擎负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件。 详细内容查看下面的数据流(Data Flow)部分。\n调度器(Scheduler)\n调度器从引擎接受request并将他们入队，以便之后引擎请求他们时提供给引擎。\n下载器(Downloader)\n下载器负责获取页面数据并提供给引擎，而后提供给spider。\nSpiders\nSpider是Scrapy用户编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个spider负责处理一个特定(或一些)网站。 更多内容请看 Spiders 。\nItem Pipeline\nItem Pipeline负责处理被spider提取出来的item。典型的处理有清理、 验证及持久化(例如存取到数据库中)。 更多内容查看 Item Pipeline 。\n下载器中间件(Downloader middlewares)\n下载器中间件是在引擎及下载器之间的特定钩子(specific hook)，处理Downloader传递给引擎的response。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。更多内容请看 下载器中间件(Downloader Middleware) 。\nSpider中间件(Spider middlewares)\nSpider中间件是在引擎及Spider之间的特定钩子(specific hook)，处理spider的输入(response)和输出(items及requests)。 其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。更多内容请看 Spider中间件(Middleware) 。\n2、数据流(Data flow)\nScrapy中的数据流由执行引擎控制，其过程如下:\n\n引擎打开一个网站(open a domain)，找到处理该网站的Spider并向该spider请求第一个要爬取的URL(s)。\n引擎从Spider中获取到第一个要爬取的URL并在调度器(Scheduler)以Request调度。\n引擎向调度器请求下一个要爬取的URL。\n调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件(请求(request)方向)转发给下载器(Downloader)。\n一旦页面下载完毕，下载器生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给引擎。\n引擎从下载器中接收到Response并通过Spider中间件(输入方向)发送给Spider处理。\nSpider处理Response并返回爬取到的Item及(跟进的)新的Request给引擎。\n引擎将(Spider返回的)爬取到的Item给Item Pipeline，将(Spider返回的)Request给调度器。\n(从第二步)重复直到调度器中没有更多地request，引擎关闭该网站。\n\n3、事件驱动网络(Event-driven networking)\nScrapy基于事件驱动网络框架 Twisted 编写。因此，Scrapy基于并发性考虑由非阻塞(即异步)的实现。\n关于异步编程及Twisted更多的内容请查看下列链接:\n三、4步制作爬虫\n\n新建项目（scrapy startproject xxx）:新建一个新的爬虫项目\n明确目标（编写items.py）:明确你想要抓取的目标\n制作爬虫（spiders/xxsp der.py）:制作爬虫开始爬取网页\n存储内容（pipelines.py）:设计管道存储爬取内容\n\n四、安装框架\n这里我们使用 conda 来进行安装：\nconda install scrapy\n或者使用 pip 进行安装：\npip install scrapy\n查看安装：\n➜  spider scrapy -h\nScrapy 1.4.0 - no active project\n\nUsage:\n  scrapy <command> [options] [args]\n\nAvailable commands:\n  bench         Run quick benchmark test\n  fetch         Fetch a URL using the Scrapy downloader\n  genspider     Generate new spider using pre-defined templates\n  runspider     Run a self-contained spider (without creating a project)\n  settings      Get settings values\n  shell         Interactive scraping console\n  startproject  Create new project\n  version       Print Scrapy version\n  view          Open URL in browser, as seen by Scrapy\n\n  [ more ]      More commands available when run from project directory\n\nUse \"scrapy <command> -h\" to see more info about a command\n1.创建项目\n➜  spider scrapy startproject SF\nNew Scrapy project 'SF', using template directory '/Users/kaiyiwang/anaconda2/lib/python2.7/site-packages/scrapy/templates/project', created in:\n    /Users/kaiyiwang/Code/python/spider/SF\n\nYou can start your first spider with:\n    cd SF\n    scrapy genspider example example.com\n➜  spider\n使用 tree 命令可以查看项目结构：\n➜  SF tree\n.\n├── SF\n│   ├── __init__.py\n│   ├── items.py\n│   ├── middlewares.py\n│   ├── pipelines.py\n│   ├── settings.py\n│   └── spiders\n│       └── __init__.py\n└── scrapy.cfg\n\n2.在spiders 目录下创建模板\n➜  spiders scrapy genspider sf \"https://segmentfault.com\"\nCreated spider 'sf' using template 'basic' in module:\n  SF.spiders.sf\n➜  spiders\n这样，就生成了一个项目文件 sf.py\n# -*- coding: utf-8 -*-\nimport scrapy\nfrom SF.items import SfItem\n\n\nclass SfSpider(scrapy.Spider):\n    name = 'sf'\n    allowed_domains = ['https://segmentfault.com']\n    start_urls = ['https://segmentfault.com/']\n\n    def parse(self, response):\n        # print response.body\n        # pass\n        node_list = response.xpath(\"//h2[@class='title']\")\n\n        # 用来存储所有的item字段的\n        # items = []\n        for node in node_list:\n            # 创建item字段对象，用来存储信息\n            item = SfItem()\n            # .extract() 将xpath对象转换为 Unicode字符串\n            title = node.xpath(\"./a/text()\").extract()\n\n            item['title'] = title[0]\n\n            # 返回抓取到的item数据，给管道文件处理，同时还回来继续执行后边的代码\n            yield.item\n            #return item\n            #return scrapy.Request(url)\n            #items.append(item)\n\n\n\n\n\n\n命令：\n# 测试爬虫是否正常, sf为爬虫的名称\n➜  scrapy check sf\n\n# 运行爬虫\n➜  scrapy crawl sf\n3.item pipeline\n当 item 在Spider中被收集之后，它将会被传递到 item Pipeline, 这些 item Pipeline 组件按定义的顺序处理 item.\n每个 Item Pipeline 都是实现了简单方法的Python 类，比如决定此Item是丢弃或存储，以下是 item pipeline 的一些典型应用：\n\n验证爬取得数据（检查item包含某些字段，比如说name字段）\n查重（并丢弃）\n将爬取结果保存到文件或者数据库总（数据持久化）\n\n 编写 item pipeline 编写 item pipeline 很简单，item pipeline 组件是一个独立的Python类，其中 process_item()方法必须实现。\nfrom scrapy.exceptions import DropItem\n\nclass PricePipeline(object):\n\n    vat_factor = 1.15\n\n    def process_item(self, item, spider):\n        if item['price']:\n            if item['price_excludes_vat']:\n                item['price'] = item['price'] * self.vat_factor\n            return item\n        else:\n            raise DropItem(\"Missing price in %s\" % item)\n4.选择器(Selectors)\n当抓取网页时，你做的最常见的任务是从HTML源码中提取数据。Selector 有四个基本的方法，最常用的还是Xpath\n\n\nxpath():传入xpath表达式，返回该表达式所对应的所有节点的selector list 列表。\n\nextract(): 序列化该节点为Unicode字符串并返回list\n\ncss():传入CSS表达式，返回该表达式所对应的所有节点的selector list 列表，语法同 BeautifulSoup4\n\nre():根据传入的正则表达式对数据进行提取，返回Unicode 字符串list 列表\n\nScrapy提取数据有自己的一套机制。它们被称作选择器(seletors)，因为他们通过特定的 XPath 或者 CSS 表达式来“选择” HTML文件中的某个部分。\nXPath 是一门用来在XML文件中选择节点的语言，也可以用在HTML上。 CSS 是一门将HTML文档样式化的语言。选择器由它定义，并与特定的HTML元素的样式相关连。\nScrapy选择器构建于 lxml 库之上，这意味着它们在速度和解析准确性上非常相似。\nXPath表达式的例子：\n/html/head/title: 选择<HTML>文档中<head>标签内的<title>元素\n/html/head/title/text(): 选择上面提到的<title>元素的问题\n//td: 选择所有的<td> 元素\n//div[@class=\"mine\"]:选择所有具有 class=\"mine\" 属性的 div 元素\n更多XPath 语法总结请看这里。\n五、爬取招聘信息\n1.爬取腾讯招聘信息\n爬取的地址：http://hr.tencent.com/positio...\n1.1 创建项目\n> scrapy startproject Tencent\n\nYou can start your first spider with:\n    cd Tencent\n    scrapy genspider example example.com\n\n需要抓取网页的元素：\n\n我们需要爬取以下信息：职位名：positionName职位链接：positionLink职位类型：positionType职位人数：positionNumber工作地点：workLocation发布时点：publishTime\n在 items.py 文件中定义爬取的字段：\n# -*- coding: utf-8 -*-\n\n# Define here the models for your scraped items\n#\n# See documentation in:\n# http://doc.scrapy.org/en/latest/topics/items.html\n\nimport scrapy\n\n# 定义字段\nclass TencentItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n\n    # 职位名\n    positionName = scrapy.Field()\n\n    # 职位链接\n    positionLink = scrapy.Field()\n\n    # 职位类型\n    positionType = scrapy.Field()\n\n    # 职位人数\n    positionNumber = scrapy.Field()\n\n    # 工作地点\n    workLocation = scrapy.Field()\n\n    # 发布时点\n    publishTime = scrapy.Field()\n\n    pass\n\n1.2 写spider爬虫\n使用命令创建\n➜  Tencent scrapy genspider tencent \"tencent.com\"\nCreated spider 'tencent' using template 'basic' in module:\n  Tencent.spiders.tencent\n生成的 spider 在当前目录下的 spiders/tencent.py\n➜  Tencent tree\n.\n├── __init__.py\n├── __init__.pyc\n├── items.py\n├── middlewares.py\n├── pipelines.py\n├── settings.py\n├── settings.pyc\n└── spiders\n    ├── __init__.py\n    ├── __init__.pyc\n    └── tencent.py\n我们可以看下生成的这个初始化文件 tencent.py\n# -*- coding: utf-8 -*-\nimport scrapy\n\n\nclass TencentSpider(scrapy.Spider):\n    name = 'tencent'\n    allowed_domains = ['tencent.com']\n    start_urls = ['http://tencent.com/']\n\n    def parse(self, response):\n        pass\n\n对初识文件tencent.py进行修改：\n# -*- coding: utf-8 -*-\nimport scrapy\nfrom Tencent.items import TencentItem\n\nclass TencentSpider(scrapy.Spider):\n    name = 'tencent'\n    allowed_domains = ['tencent.com']\n    baseURL = \"http://hr.tencent.com/position.php?&start=\"\n    offset = 0  # 偏移量\n    start_urls = [baseURL + str(offset)]\n\n    def parse(self, response):\n\n        # 请求响应\n        # node_list = response.xpath(\"//tr[@class='even'] or //tr[@class='odd']\")\n         node_list = response.xpath(\"//tr[@class='even'] | //tr[@class='odd']\")\n\n        for node in node_list:\n            item = TencentItem()   # 引入字段类\n\n            # 文本内容, 取列表的第一个元素[0], 并且将提取出来的Unicode编码 转为 utf-8\n            item['positionName'] = node.xpath(\"./td[1]/a/text()\").extract()[0].encode(\"utf-8\")\n            item['positionLink'] = node.xpath(\"./td[1]/a/@href\").extract()[0].encode(\"utf-8\")         # 链接属性\n            item['positionType'] = node.xpath(\"./td[2]/text()\").extract()[0].encode(\"utf-8\")\n            item['positionNumber'] = node.xpath(\"./td[3]/text()\").extract()[0].encode(\"utf-8\")\n            item['workLocation'] = node.xpath(\"./td[4]/text()\").extract()[0].encode(\"utf-8\")\n            item['publishTime'] = node.xpath(\"./td[5]/text()\").extract()[0].encode(\"utf-8\")\n\n            # 返回给管道处理\n            yield item\n\n        # 先爬 2000 页数据\n        if self.offset < 2000:\n            self.offset += 10\n            url = self.baseURL + self.offset\n            yield scrapy.Request(url, callback = self.parse)\n\n\n\n\n\n\n        #pass\n\n写管道文件 pipelines.py：\n# -*- coding: utf-8 -*-\n\n# Define your item pipelines here\n#\n# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html\n\nimport json\n\nclass TencentPipeline(object):\n    def __init__(self):\n        self.f = open(\"tencent.json\", \"w\")\n\n    # 所有的item使用共同的管道\n    def process_item(self, item, spider):\n        content = json.dumps(dict(item), ensure_ascii = False) + \",\\n\"\n        self.f.write(content)\n        return item\n\n    def close_spider(self, spider):\n        self.f.close()\n\n\n管道写好之后，在 settings.py 中启用管道\n# Configure item pipelines\n# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html\nITEM_PIPELINES = {\n    'Tencent.pipelines.TencentPipeline': 300,\n}\n运行：\n> scrapy crawl tencent\n\nFile \"/Users/kaiyiwang/Code/python/spider/Tencent/Tencent/spiders/tencent.py\", line 21, in parse\n    item['positionName'] = node.xpath(\"./td[1]/a/text()\").extract()[0].encode(\"utf-8\")\nIndexError: list index out of range\n请求响应这里写的有问题，Xpath或应该为这种写法：\n  # 请求响应\n        # node_list = response.xpath(\"//tr[@class='even'] or //tr[@class='odd']\")\n         node_list = response.xpath(\"//tr[@class='even'] | //tr[@class='odd']\")\n\n然后再执行命令：\n> scrapy crawl tencent\n执行结果文件 tencent.json ：\n{\"positionName\": \"23673-财经运营中心热点运营组编辑\", \"publishTime\": \"2017-12-02\", \"positionLink\": \"position_detail.php?id=32718&keywords=&tid=0&lid=0\", \"positionType\": \"内容编辑类\", \"workLocation\": \"北京\", \"positionNumber\": \"1\"},\n{\"positionName\": \"MIG03-腾讯地图高级算法评测工程师（北京）\", \"publishTime\": \"2017-12-02\", \"positionLink\": \"position_detail.php?id=30276&keywords=&tid=0&lid=0\", \"positionType\": \"技术类\", \"workLocation\": \"北京\", \"positionNumber\": \"1\"},\n{\"positionName\": \"MIG10-微回收渠道产品运营经理（深圳）\", \"publishTime\": \"2017-12-02\", \"positionLink\": \"position_detail.php?id=32720&keywords=&tid=0&lid=0\", \"positionType\": \"产品/项目类\", \"workLocation\": \"深圳\", \"positionNumber\": \"1\"},\n{\"positionName\": \"MIG03-iOS测试开发工程师（北京）\", \"publishTime\": \"2017-12-02\", \"positionLink\": \"position_detail.php?id=32715&keywords=&tid=0&lid=0\", \"positionType\": \"技术类\", \"workLocation\": \"北京\", \"positionNumber\": \"1\"},\n{\"positionName\": \"19332-高级PHP开发工程师（上海）\", \"publishTime\": \"2017-12-02\", \"positionLink\": \"position_detail.php?id=31967&keywords=&tid=0&lid=0\", \"positionType\": \"技术类\", \"workLocation\": \"上海\", \"positionNumber\": \"2\"}\n1.3 通过下一页爬取\n我们上边是通过总的页数来抓取每页数据的，但是没有考虑到每天的数据是变化的，所以，需要爬取的总页数不能写死，那该怎么判断是否爬完了数据呢？其实很简单，我们可以根据下一页来爬取，只要下一页没有数据了，就说明数据已经爬完了。\n\n我们通过 下一页 看下最后一页的特征：\n\n下一页的按钮为灰色，并且链接为 class='noactive'属性了，我们可以根据此特性来判断是否到最后一页了。\n # 写死总页数，先爬 100 页数据\n        \"\"\"\n  \n        if self.offset < 100:\n            self.offset += 10\n            url = self.baseURL + str(self.offset)\n            yield scrapy.Request(url, callback = self.parse)\n        \"\"\"\n\n\n        # 使用下一页爬取数据\n        if len(response.xpath(\"//a[@class='noactive' and @id='next']\")) == 0:\n            url = response.xpath(\"//a[@id='next']/@href\").extract()[0]\n            yield scrapy.Request(\"http://hr.tencent.com/\" + url, callback = self.parse)\n修改后的tencent.py文件：\n# -*- coding: utf-8 -*-\nimport scrapy\nfrom Tencent.items import TencentItem\n\nclass TencentSpider(scrapy.Spider):\n    # 爬虫名\n    name = 'tencent'\n    # 爬虫爬取数据的域范围\n    allowed_domains = ['tencent.com']\n    # 1.需要拼接的URL\n    baseURL = \"http://hr.tencent.com/position.php?&start=\"\n    # 需要拼接的URL地址的偏移量\n    offset = 0  # 偏移量\n\n    # 爬虫启动时，读取的URL地址列表\n    start_urls = [baseURL + str(offset)]\n\n    # 用来处理response\n    def parse(self, response):\n\n        # 提取每个response的数据\n        node_list = response.xpath(\"//tr[@class='even'] | //tr[@class='odd']\")\n\n        for node in node_list:\n\n            # 构建item对象，用来保存数据\n            item = TencentItem()\n\n            # 文本内容, 取列表的第一个元素[0], 并且将提取出来的Unicode编码 转为 utf-8\n            print node.xpath(\"./td[1]/a/text()\").extract()\n\n            item['positionName'] = node.xpath(\"./td[1]/a/text()\").extract()[0].encode(\"utf-8\")\n            item['positionLink'] = node.xpath(\"./td[1]/a/@href\").extract()[0].encode(\"utf-8\")         # 链接属性\n\n            # 进行是否为空判断\n            if len(node.xpath(\"./td[2]/text()\")):\n                item['positionType'] = node.xpath(\"./td[2]/text()\").extract()[0].encode(\"utf-8\")\n            else:\n                item['positionType'] = \"\"\n\n            item['positionNumber'] = node.xpath(\"./td[3]/text()\").extract()[0].encode(\"utf-8\")\n            item['workLocation'] = node.xpath(\"./td[4]/text()\").extract()[0].encode(\"utf-8\")\n            item['publishTime'] = node.xpath(\"./td[5]/text()\").extract()[0].encode(\"utf-8\")\n\n            # yield的重要性，是返回数据后还能回来接着执行代码，返回给管道处理，如果为return 整个函数都退出了\n            yield item\n\n        # 第一种写法：拼接URL，适用场景：页面没有可以点击的请求链接，必须通过拼接URL才能获取响应\n        \"\"\"\n  \n        if self.offset < 100:\n            self.offset += 10\n            url = self.baseURL + str(self.offset)\n            yield scrapy.Request(url, callback = self.parse)\n        \"\"\"\n\n\n        # 第二种写法：直接从response获取需要爬取的连接，并发送请求处理，直到连接全部提取完（使用下一页爬取数据）\n        if len(response.xpath(\"//a[@class='noactive' and @id='next']\")) == 0:\n            url = response.xpath(\"//a[@id='next']/@href\").extract()[0]\n            yield scrapy.Request(\"http://hr.tencent.com/\" + url, callback = self.parse)\n\n\n        #pass\n\nOK，通过 根据下一页我们成功爬完招聘信息的所有数据。\n1.4 小结\n爬虫步骤：\n\n1.创建项目 scrapy project XXX\n2.scarpy genspider xxx \"http://www.xxx.com\"\n3.编写 items.py, 明确需要提取的数据\n4.编写 spiders/xxx.py, 编写爬虫文件，处理请求和响应，以及提取数据（yield item）\n\n5.编写 pipelines.py, 编写管道文件，处理spider返回item数据,比如本地数据持久化，写文件或存到表中。\n6.编写 settings.py，启动管道组件ITEM_PIPELINES，以及其他相关设置\n7.执行爬虫 scrapy crawl xxx\n\n\n有时候被爬取的网站可能做了很多限制，所以，我们请求时可以添加请求报头，scrapy 给我们提供了一个很方便的报头配置的地方，settings.py 中，我们可以开启:\n\n# Crawl responsibly by identifying yourself (and your website) on the user-agent\nUSER_AGENT = 'Tencent (+http://www.yourdomain.com)'\nUser-AGENT = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6)\n              AppleWebKit/537.36 (KHTML, like Gecko)\n              Chrome/62.0.3202.94 Safari/537.36\"\n\n\n# Override the default request headers:\nDEFAULT_REQUEST_HEADERS = {\n   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n   'Accept-Language': 'en',\n}\nscrapy 最大的适用场景是爬取静态页面，性能非常强悍，但如果要爬取动态的json数据，那就没必要了。\n\n相关文章：\nScrapy入门教程\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "10"}