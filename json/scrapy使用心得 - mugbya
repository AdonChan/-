{"title": "scrapy使用心得 - mugbya ", "index": "python,python爬虫,scrapy", "content": "前言\n在公司一部分业务是爬虫相关了，有涉及到登录，验证码，也有国外的4大社交网站。所以记录下\nscrapy 是什么\nscrapy 是一个异步爬虫框架，使用它，可以屏蔽很多复杂的底层设计，只需要解析下载下来的页面，更多了我们需要关注的是目标网站/页面爬取的难易程度，该怎么来实现它。虽然是，但是在爬取大量网站可能需要 用分布式的爬虫，当然scrapy 也有\n操作流程图\n\n指定一个起始url后，scrapy就可以根据以上原理图进行工作了。一个最简单的页面，指定页面的url进行第一次请求，经过引擎，交给调度器，然后调度器再返回给引擎，去下载这个页面，拿到这个页面就可以进行解析了。 这里明显看的出来绕了一个圈子，如果最简单的的页面，这样子会发现多了调度这一步。但是一般在实际业务中，特别是分布式爬虫，会有很多url 需要爬取，而且一些url是动态添加到待爬队列的，我们将所有的待爬都在调度器进行分配，当然这里也有其他操作，比如，一个url已经调度过，那么会进行标识，做到不再重复爬取。\n队列\nscrapy 默认的队列\nSCHEDULER_DISK_QUEUE = 'scrapy.squeues.PickleLifoDiskQueue'\nSCHEDULER_MEMORY_QUEUE = 'scrapy.squeues.LifoMemoryQueue'\nSCHEDULER_PRIORITY_QUEUE = 'queuelib.PriorityQueue'\n\n一般我们不关心这个队列结构，但是在做分布式时这个队列就需要替换\nscrapy_redis\nscrapy 本身是异步，但是不支持分布式爬取。 要做到分布式爬取，那么需要一个公共的待爬队列\nscrapy_redis 需要制定队列结构，可在 SpiderQueue，SpiderStack，  SpiderPriorityQueue 中选者一个，形如\nSCHEDULER_QUEUE_CLASS = \"scrapy_redis.queue.SpiderPriorityQueue\"\n\n\n更多知识\n《Learning Scrapy》（中文版）0 序言\n....以后再增加\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "0"}