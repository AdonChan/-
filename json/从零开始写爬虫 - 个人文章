{"title": "从零开始写爬虫 - 个人文章 ", "index": "scrapy,python,网页爬虫", "content": "几个朋友对爬虫很感兴趣，他们也都是开发人员，一个PHP开发两个JAVA开发，都没有过python项目开发经验，正好其中一个最近要爬一个网店的产品信息，所以希望我能拿这网站当demo写一个爬虫来给他们参考学习。要爬取的页是http://list.secoo.com/watches...，只要爬取产品相关了属性就可以。\n这就是要爬取的相关信息\n爬虫框架使用的是python的scrapy，这也是我现在项目中使用的爬虫框架。朋友们由于没有使用过scrapy，因此推荐他们去看一下框架文档，http://scrapy-chs.readthedocs...，这是scrapy的入门教程，基本上看完前面几章就能快速开发一个简单的爬虫了。\n一、生成代码scrapy提供了命令行工具来快速生成一个简单的爬虫，我们也使用这个工具来快速生成代码。在终端下面运行下面的命令就能快速生成一个scrapy爬虫的基础框架，免去一些最原始的代码开发操作。\nscrapy startproject secoo\n\n生成的项目文件就是上面的样子，spiders目录是存放爬虫代码，items.py是定义爬虫要爬取的字段信息，pipelines.py是用来处理爬虫爬到的数据的，比如用来做保存操作，像是存redis或是mysql等等，settings.py是项目的基础配置文件，定义一些爬虫的全局参数。生成完项目基础代码后还可以使用下面的命令行工具来生成爬虫代码最简单的代码，和创建项目一样，可以省去创建文件的麻烦。\nscrapy genspider watch secoo.com\n命令执行完成后我们就会在spiders目录下发现我们生成好的爬虫文件watch.py了。我们就在这个文件里开发爬虫的相关逻辑。\n二、 定义爬虫数据字段爬虫要爬页面，肯定是要从爬取到的页面中提取到我们想要的字段信息，这样的话我们就要先来定义一下哪些是我们要的字段，后面好在本地保存，这个就是scrapy里的items来负责的。我们打开根目录下面的items.py文件，这个文件里定义一个SecooItem的类，我们定义了4个字段，分别用来保存产品名、路径、价格和库存信息，这一步操作我们就完成了，后面的爬虫按这个定义好的格式来保存数据就可以了。\nimport scrapy\n\nclass SecooItem(scrapy.Item):\n    # define the fields for your item here like:\n    # name = scrapy.Field()\n    product_name = scrapy.Field()       #产品名\n    breadcrumb = scrapy.Field()         #面包屑路径\n    product_price = scrapy.Field()      #产品价格\n    product_stock = scrapy.Field()      #库存状态\n三、定义保存数据操作这一步是要来定义爬虫的数据提取出来以后，按照items.py定义的格式格式化以后是如何保存的。由于我们是写一个demo给大家来参考，因此这一部分代码是没有写实际的数据保存操作的，我们只定义了这个操作类，如果是想要进行保存数据操作的话可以直接修改SecooPipeline类的process_item方法，在这里可以和正常脚本操作数据库一样进行连接、写入操作的。\nclass SecooPipeline(object):\n    def process_item(self, item, spider):\n        return item\n\n四、页面爬取最后一步就是最重要的一步了，我们要进行爬虫逻辑的开发。在这里先贴上代码，然后再来解释每一步的操作。\n# -*- coding: utf-8 -*-\nimport scrapy\nfrom secoo.items import SecooItem\n\nclass WatchSpider(scrapy.Spider):\n    name = 'watch'\n    allowed_domains = ['secoo.com']\n    start_urls = ['http://list.secoo.com/watches/93-0-0-0-0-1-0-0-1-10-0-908_0.shtml']\n\n\n    def parse(self, response):\n        '''\n        解析商品列表页面的内容,提取商品页面链接并生成请求\n        :param response:\n        :return:\n        '''\n        #解析商品链接\n        goods_links = response.css('body .product_box .commodity-list dl .show_tips dt>a::attr(href)').extract()\n\n        for url in goods_links:\n            #分别去爬取商品信息\n            yield scrapy.Request(url, callback=self.parse_goods, dont_filter=True)\n\n        next_page = self.get_next_page_url(response)\n        if next_page:\n            yield scrapy.Request(next_page, callback=self.parse)\n\n\n    def get_next_page_url(self, response):\n        '''\n        获取下一个商品列表的链接\n        :param response:\n        :return:\n        '''\n        return response.css('a.next::attr(href)').extract_first()\n\n    def parse_goods(self, response):\n        '''\n        解析商品内容,提取要爬取的字段\n        :param response:\n        :return:\n        '''\n        name = response.css('.sopdetailsCon .contents .info_r .proName h2::text').extract_first()\n        price = response.css('#secooPriceJs::text').extract_first()\n        stock = response.css('#youhuo::text').extract_first()\n        breadcrumb = '->'.join(response.css('.smallNav p a::text').extract())\n\n        yield SecooItem({\n            'product_name': name,\n            'breadcrumb': breadcrumb,\n            'product_price': price,\n            'product_stock': stock,\n        })\n\n在使用命令行工具生成爬虫的时候就会帮我们定义好三个变量，一个是name，这个是定义爬虫的名称的；第二个是allowed_domains，这个是一个list，定义爬虫允许爬取的域名；最后一个start_urls变量也是一个list类型，是用来定义爬虫入口URL的，可以定义多个入口地址。接下来是parse方法，这个方法是框架默认的第一个处理方法，用来解析网页内容的。大概的逻辑在注释中写的比较清楚，逻辑就是我们解析那个产品列表页面，从其中提取产品列表链接，然后生成请求去爬取产品信息，然后再去提取产品列表页面的下一页链接，再生成请求去请求这个页面，处理回调方法就是parse方法，这样就能实现所有翻页爬取。在parse方法里提取出来的产品链接生成了请求之后，我把爬虫回调处理方法设置成parse_goods，因此我们要去定义一个parse_goods方法来处理产品页面html内容。通过xpath或是css的selector方法来提取我们想要爬取的内容，然后丢给pipelines.py来处理了。这样我们就把爬虫的所有逻辑实现完成了，最后一步不是验证爬虫的运行情况了。\n五、运行最后一步就是就是运行操作，我们在命令行下，进入项目目录，然后执行下面命令就能正常运行爬虫了，由于我们没有写保存操作，因此我们就把数据导出到一个json文件里。\nscrapy crawl watch --output=product.json\n\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "8"}