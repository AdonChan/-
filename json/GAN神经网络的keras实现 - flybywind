{"title": "GAN神经网络的keras实现 - flybywind ", "index": "神经网络,机器学习,python", "content": "参考文献：\n主要参考这篇文章 Generative Adversarial Networks, link\n为了方便说明和研究，我这里只是设计了一个非常简单的模型，对高斯分布样本进行生成。不过从下面的实验中，我还是发现了一些非常有用的特点，可以加深我们对GAN网络的了解。\nGAN原理\n具体原理可以参考上面的文献，不过在这里还是大概讲一下。其实GAN的原理非常简单，它有2个子网络组成，一个是Generator，即生成网络，它以噪音样本为输入，通过学习到的权重，把噪音转变（即生成）为有意义的信号；一个是Discriminator，即判别网络，他以信号为输入（可以来自generator生成的信号，也可以是真正的信号），通过学习来判别信号的真假，并输出一个0-1之间的概率。可以把Generator比喻为一个假的印钞机，而Discriminator则是验钞机，他们两个互相竞争，使得印钞机越来越真，同时验钞机也越来越准。但是最终我们是希望Generator越来越真，而Discriminator的输出都是0.5，即难以分辨～～\n而在训练的时候，则分两个阶段进行，第一个阶段是Discriminator的学习，此时固定Generator的权重不变，只更新Discriminator的权重。loss函数是：\n$$ \\frac{1}{m}\\sum_{i=1}^{m}[logD(x^i) + log(1 - D(G(z^i)))] $$\n其中m是batch_size, $x$表示真正的信号，$z$表示噪音样本。训练时分别从噪音分布和真实分布中选出m个噪音输入样本和m个真实信号样本，通过对以上的loss function最大化更新Discriminator的权重\n第二个阶段是对Generator进行训练，此时的loss function是：\n$$ \\frac{1}{m}\\sum_{i=1}^{m}[log(1 - D(G(z^i)))] $$\n不过，此时是对loss最小化来更新Generator的权重。\n另外，这2个阶段并不是交替进行的，而是执行K次Discriminator的更新，再执行1次Generator的更新。后面的实验结果也显示，K的选择非常关键。\n具体实现\n主要工具是 python + keras，用keras实现一些常用的网络特别容易，比如MLP、word2vec、LeNet、lstm等等，github上都有详细demo。但是稍微复杂些的就要费些时间自己写了。不过整体看，依然比用原生tf写要方便。而且，我们还可以把keras当初是学习tf的参考代码，里面很多写法都非常值得借鉴。\n废话不多说了，直接上代码吧：\nGANmodel\n只列出最主要的代码\n\n# 这是针对GAN特殊设计的loss function\ndef log_loss_discriminator(y_true, y_pred):\n    return - K.log(K.maximum(K.epsilon(), y_pred))\n    \ndef log_loss_generator(y_true, y_pred):\n    return K.log(K.maximum(K.epsilon(), 1. - y_pred))\n    \nclass GANModel:\n    def __init__(self, \n                 input_dim,\n                 log_dir = None):\n        '''\n            __tensor[0]: 定义了discriminateor的表达式,  对y进行判别，true samples\n            __tensor[1]: 定义了generator的表达式, 对x进行生成，noise samples\n        '''\n        if isinstance(input_dim, list):\n            input_dim_y, input_dim_x = input_dim[0], input_dim[1]\n        elif isinstance(input_dim, int):\n            input_dim_x = input_dim_y = input_dim\n        else:\n            raise ValueError(\"input_dim should be list or interger, got %r\" % input_dim) \n        # 必须使用名字，方便后面分别输入2个信号\n        self.__inputs = [layers.Input(shape=(input_dim_y,), name = \"y\"), \n                            layers.Input(shape=(input_dim_x,), name = \"x\")]\n        self.__tensors = [None, None] \n        self.log_dir = log_dir\n        self._discriminate_layers = []\n        self._generate_layers = []\n        self.train_status = defaultdict(list)\n        \n    def add_gen_layer(self, layer):\n        self._add_layer(layer, True)\n    def add_discr_layer(self, layer):\n        self._add_layer(layer)\n    def _add_layer(self, layer, for_gen=False):\n        idx = 0\n        if for_gen:\n            self._generate_layers.append(layer)\n            idx = 1\n        else:\n            self._discriminate_layers.append(layer)\n        \n        if self.__tensors[idx] is None:\n            self.__tensors[idx] = layer(self.__inputs[idx])\n        else:\n            self.__tensors[idx] = layer(self.__tensors[idx])\n            \n    def compile_discriminateor_model(self, optimizer = optimizers.Adam()):\n        if len(self._discriminate_layers) <= 0:\n            raise ValueError(\"you need to build discriminateor model before compile it\")\n        if len(self._generate_layers) <= 0:\n            raise ValueError(\"you need to build generator model before compile discriminateo model\")\n        # 通过指定trainable = False，可以freeze权重的更新。必须放在compile之前\n        for l in self._discriminate_layers:\n            l.trainable = True\n        for l in self._generate_layers:\n            l.trainable = False\n        discriminateor_out1 = self.__tensors[0]\n        discriminateor_out2 = layers.Lambda(lambda y: 1. - y)(self._discriminate_generated())\n        # 如果输出2个信号，keras会分别在各个信号上引用loss function，然后累加，对累加的结果进行\n        # minimize 更新。双下划线的model是参与训练的模型。\n        self.__discriminateor_model = Model(self.__inputs, [discriminateor_out1, discriminateor_out2])\n        self.__discriminateor_model.compile(optimizer, \n                                     loss = log_loss_discriminator)\n       \n        # 这个才是真正的discriminator model \n        self.discriminateor_model = Model(self.__inputs[0], self.__tensors[0])\n        self.discriminateor_model.compile(optimizer, \n                                     loss = log_loss_discriminator)\n        if self.log_dir is not None:\n            # 需要安装pydot和graphviz。没有的可以先注释掉\n            plot_model(self.__discriminateor_model, self.log_dir + \"/gan_discriminateor_model.png\", show_shapes = True) \n        \n    def compile_generator_model(self, optimizer = optimizers.Adam()):\n        if len(self._discriminate_layers) <= 0:\n            raise ValueError(\"you need to build discriminateor model before compile generator model\")\n        if len(self._generate_layers) <= 0:\n            raise ValueError(\"you need to build generator model before compile it\")\n        \n        for l in self._discriminate_layers:\n            l.trainable = False\n        for l in self._generate_layers:\n            l.trainable = True\n              \n        out = self._discriminate_generated()\n        self.__generator_model = Model(self.__inputs[1], out)\n        self.__generator_model.compile(optimizer, \n                                     loss = log_loss_generator)\n        # 这个才是真正的Generator模型\n        self.generator_model = Model(self.__inputs[1], self.__tensors[1])\n        if self.log_dir is not None:\n            plot_model(self.__generator_model, self.log_dir + \"/gan_generator_model.png\", show_shapes = True) \n\n    def train(self, sample_list, epoch = 3, batch_size = 32, step_per = 10, plot=False):\n        '''\n        step_per: 每隔几步训练一次generator，即K\n        '''\n        sample_noise, sample_true = sample_list[\"x\"], sample_list[\"y\"]\n        sample_count = sample_noise.shape[0]\n        batch_count = sample_count // batch_size \n        # 这里比较trick了，因为keras的model必须要一个y。但是gan其实是没有y的。只好伪造一个\n        # 满足keras的“无理”要求\n        psudo_y = np.ones((batch_size, ), dtype = 'float32')\n        if plot:\n            # plot the real data\n            fig = plt.figure()\n            ax = fig.add_subplot(1,1,1)\n            plt.ion()\n            plt.show() \n        for ei in range(epoch):\n            for i in range(step_per):\n                idx = random.randint(0, batch_count-1)\n                batch_noise = sample_noise[idx * batch_size : (idx+1) * batch_size]\n                idx = random.randint(0, batch_count-1)\n                batch_sample = sample_true[idx * batch_size : (idx+1) * batch_size]\n                self.__discriminateor_model.train_on_batch({\n                    \"y\":  batch_sample,\n                    \"x\": batch_noise}, \n                    [psudo_y, psudo_y])\n\n            idx = random.randint(0, batch_count-1)\n            batch_noise = sample_noise[idx * batch_size : (idx+1) * batch_size]\n            self.__generator_model.train_on_batch(batch_noise, psudo_y)\n            \n            if plot:\n                gen_result = self.generator_model.predict_on_batch(batch_noise)\n                self.train_status[\"gen_result\"].append(gen_result)\n                dis_result = self.discriminateor_model.predict_on_batch(gen_result)\n                self.train_status[\"dis_result\"].append(dis_result)\n                freq_g, bin_g = np.histogram(gen_result, density=True)\n                # norm to sum1\n                freq_g = freq_g * (bin_g[1] - bin_g[0])\n                bin_g = bin_g[:-1]\n                freq_d, bin_d = np.histogram(batch_sample, density=True)\n                freq_d = freq_d * (bin_d[1] - bin_d[0])\n                bin_d = bin_d[:-1]\n                ax.plot(bin_g, freq_g, 'go-', markersize = 4)\n                ax.plot(bin_d, freq_d, 'ko-', markersize = 8)\n                gen1d = gen_result.flatten()\n                dis1d = dis_result.flatten()\n                si = np.argsort(gen1d)\n                ax.plot(gen1d[si], dis1d[si], 'r--')\n                if (ei+1) % 20 == 0:\n                    ax.cla()\n                plt.title(\"epoch = %d\" % (ei+1))\n                plt.pause(0.05)\n        if plot:\n            plt.ioff()\n            plt.close()\nmain部分\n只列出主要部分：从中可以看到主要模型结构和参数取值\n    step_per = 20\n    sample_size = args.batch_size * 100\n\n    # 整个测试样本集合\n    noise_dim = 4\n    signal_dim = 1\n    x = np.random.uniform(-3, 3, size = (sample_size, noise_dim))\n    y = np.random.normal(size = (sample_size, signal_dim))\n    samples = {\"x\": x, \n               \"y\": y}\n    \n    gan = GANModel([signal_dim, noise_dim], args.log_dir)\n    gan.add_discr_layer(layers.Dense(200, activation=\"relu\"))\n    gan.add_discr_layer(layers.Dense(50, activation=\"softmax\"))\n    gan.add_discr_layer(layers.Lambda(lambda y: K.max(y, axis=-1, keepdims=True),\n                                 output_shape = (1,)))\n\n    gan.add_gen_layer(layers.Dense(200, activation=\"relu\"))\n    gan.add_gen_layer(layers.Dense(100, activation=\"relu\"))\n    gan.add_gen_layer(layers.Dense(50, activation=\"relu\"))\n    gan.add_gen_layer(layers.Dense(signal_dim))\n    \n    gan.compile_generator_model()\n    loger.info(\"compile generator finished\")\n    gan.compile_discriminateor_model()\n    loger.info(\"compile discriminator finished\")\n    \n    gan.train(samples, args.epoch, args.batch_size, step_per, plot=True)\n实验结果\nK的影响\n在论文中，作者就提到K对训练结果影响很大，使用上面的step_per = 20，我得到的结果比较理想：\n可以看到，最后Generator生成的数据（绿线）和真实的高斯分布（黑线）非常接近了，导致Discriminator也变得无法辨认了（p = 0.5）。\n但是把step_per设为3后，结果就发散的厉害，比较难收敛：在文章中，作者也提到，Discriminator和Generator必须匹配好，一般要多训练几次Discriminator再训练一次Generator，这是因为Discriminator是Generator的前提，如果D都没有训练好，那G的更新方向就会不准。\n输入噪音维度的影响\n另外，我还发现，noise_dim对结果影响也非常大。上面的noise_dim = 4, 后面我设置为1后，最后好像很难收敛到真正的高斯分布，总是比真的高斯差那么一点。\n所以，我的猜测是：Generator的输入其实可以看成是真实信号在其他维度上的映射，通过模型的学习过程，它找到了二者的映射关系，所以反过来可以认为Generator把真实信号分解到了高维空间里，此时，当然是维度越高信号被分解的越好，越容易接近真实信号。而且，从信号拟合角度看，因为我实验中的高斯信号是非线性的，而使用的激活函数都是线性函数，如果噪音也是1维的，相当于用一堆线性函数去拟合非线性函数，这种情况必须要在一个更高的维度上才能实现。\n训练一个稳定的GAN网络是一个非常复杂的过程，所幸已经有大神在这方面做了很多探索。详细请参考这里\n完整代码\n# demo_gan.py\n# -*- encoding: utf8 -*-\n'''\nGAN网络Demo\n'''\nimport os\nfrom os import path\nimport argparse\nimport logging\nimport traceback\nimport random\nimport pickle\nimport numpy as np\nimport tensorflow as tf\nfrom keras import optimizers \nfrom keras import layers\nfrom keras import callbacks, regularizers, activations\nfrom keras.engine import Model\nfrom keras.utils.vis_utils import plot_model\nimport keras.backend as K\nfrom collections import defaultdict\nfrom matplotlib import pyplot as plt\nimport app_logger\n\nloger = logging.getLogger(__name__)\n\n# 注意pred不能为负数，因为pred是一个概率。所以最后一个激活函数的选择要注意\ndef log_loss_discriminator(y_true, y_pred):\n    return - K.log(K.maximum(K.epsilon(), y_pred))\n    \ndef log_loss_generator(y_true, y_pred):\n    return K.log(K.maximum(K.epsilon(), 1. - y_pred))\n\nclass GANModel:\n    def __init__(self, \n                 input_dim,\n                 log_dir = None):\n        '''\n            __tensor[0]: 定义了discriminateor的表达式\n            __tensor[1]: 定义了generator的表达式\n        '''\n        # discriminateor 对y进行判别，true samples\n        # generator 对x进行生成，noise samples\n        if isinstance(input_dim, list):\n            input_dim_y, input_dim_x = input_dim[0], input_dim[1]\n        elif isinstance(input_dim, int):\n            input_dim_x = input_dim_y = input_dim\n        else:\n            raise ValueError(\"input_dim should be list or interger, got %r\" % input_dim) \n    \n        self.__inputs = [layers.Input(shape=(input_dim_y,), name = \"y\"), \n                            layers.Input(shape=(input_dim_x,), name = \"x\")]\n        self.__tensors = [None, None] \n        self.log_dir = log_dir\n        self._discriminate_layers = []\n        self._generate_layers = []\n        self.train_status = defaultdict(list)\n        \n    def add_gen_layer(self, layer):\n        self._add_layer(layer, True)\n    def add_discr_layer(self, layer):\n        self._add_layer(layer)\n    def _add_layer(self, layer, for_gen=False):\n        idx = 0\n        if for_gen:\n            self._generate_layers.append(layer)\n            idx = 1\n        else:\n            self._discriminate_layers.append(layer)\n        \n        if self.__tensors[idx] is None:\n            self.__tensors[idx] = layer(self.__inputs[idx])\n        else:\n            self.__tensors[idx] = layer(self.__tensors[idx])\n            \n    def compile_discriminateor_model(self, optimizer = optimizers.Adam()):\n        if len(self._discriminate_layers) <= 0:\n            raise ValueError(\"you need to build discriminateor model before compile it\")\n        if len(self._generate_layers) <= 0:\n            raise ValueError(\"you need to build generator model before compile discriminateo model\")\n        \n        for l in self._discriminate_layers:\n            l.trainable = True\n        for l in self._generate_layers:\n            l.trainable = False\n        discriminateor_out1 = self.__tensors[0]\n        discriminateor_out2 = layers.Lambda(lambda y: 1. - y)(self._discriminate_generated())\n        self.__discriminateor_model = Model(self.__inputs, [discriminateor_out1, discriminateor_out2])\n        self.__discriminateor_model.compile(optimizer, \n                                     loss = log_loss_discriminator)\n       \n        # 这个才是需要的discriminateor model \n        self.discriminateor_model = Model(self.__inputs[0], self.__tensors[0])\n        self.discriminateor_model.compile(optimizer, \n                                     loss = log_loss_discriminator)\n        #if self.log_dir is not None:\n        #    plot_model(self.__discriminateor_model, self.log_dir + \"/gan_discriminateor_model.png\", show_shapes = True) \n        \n    def compile_generator_model(self, optimizer = optimizers.Adam()):\n        if len(self._discriminate_layers) <= 0:\n            raise ValueError(\"you need to build discriminateor model before compile generator model\")\n        if len(self._generate_layers) <= 0:\n            raise ValueError(\"you need to build generator model before compile it\")\n        \n        for l in self._discriminate_layers:\n            l.trainable = False\n        for l in self._generate_layers:\n            l.trainable = True\n              \n        out = self._discriminate_generated()\n        self.__generator_model = Model(self.__inputs[1], out)\n        self.__generator_model.compile(optimizer, \n                                     loss = log_loss_generator)\n        # 这个才是真正需要的模型\n        self.generator_model = Model(self.__inputs[1], self.__tensors[1])\n        #if self.log_dir is not None:\n        #    plot_model(self.__generator_model, self.log_dir + \"/gan_generator_model.png\", show_shapes = True) \n\n    def train(self, sample_list, epoch = 3, batch_size = 32, step_per = 10, plot=False):\n        '''\n        step_per: 每隔几步训练一次generator\n        '''\n        sample_noise, sample_true = sample_list[\"x\"], sample_list[\"y\"]\n        sample_count = sample_noise.shape[0]\n        batch_count = sample_count // batch_size \n        psudo_y = np.ones((batch_size, ), dtype = 'float32')\n        if plot:\n            # plot the real data\n            fig = plt.figure()\n            ax = fig.add_subplot(1,1,1)\n            plt.ion()\n            plt.show() \n        for ei in range(epoch):\n            for i in range(step_per):\n                idx = random.randint(0, batch_count-1)\n                batch_noise = sample_noise[idx * batch_size : (idx+1) * batch_size]\n                idx = random.randint(0, batch_count-1)\n                batch_sample = sample_true[idx * batch_size : (idx+1) * batch_size]\n                self.__discriminateor_model.train_on_batch({\n                    \"y\":  batch_sample,\n                    \"x\": batch_noise}, \n                    [psudo_y, psudo_y])\n\n            idx = random.randint(0, batch_count-1)\n            batch_noise = sample_noise[idx * batch_size : (idx+1) * batch_size]\n            self.__generator_model.train_on_batch(batch_noise, psudo_y)\n            \n            if plot:\n                gen_result = self.generator_model.predict_on_batch(batch_noise)\n                self.train_status[\"gen_result\"].append(gen_result)\n                dis_result = self.discriminateor_model.predict_on_batch(gen_result)\n                self.train_status[\"dis_result\"].append(dis_result)\n                freq_g, bin_g = np.histogram(gen_result, density=True)\n                # norm to sum1\n                freq_g = freq_g * (bin_g[1] - bin_g[0])\n                bin_g = bin_g[:-1]\n                freq_d, bin_d = np.histogram(batch_sample, density=True)\n                freq_d = freq_d * (bin_d[1] - bin_d[0])\n                bin_d = bin_d[:-1]\n                ax.plot(bin_g, freq_g, 'go-', markersize = 4)\n                ax.plot(bin_d, freq_d, 'ko-', markersize = 8)\n                gen1d = gen_result.flatten()\n                dis1d = dis_result.flatten()\n                si = np.argsort(gen1d)\n                ax.plot(gen1d[si], dis1d[si], 'r--')\n                if (ei+1) % 20 == 0:\n                    ax.cla()\n                plt.title(\"epoch = %d\" % (ei+1))\n                plt.pause(0.05)\n        if plot:\n            plt.ioff()\n            plt.close()\n            \n            \n    def save_model(self, path_dir):\n        self.generator_model.save(path_dir + \"/gan_generator.h5\")\n        self.discriminateor_model.save(path_dir + \"/gan_discriminateor.h5\")\n    \n    def load_model(self, path_dir):\n        from keras.models import load_model\n        custom_obj = {\n            \"log_loss_discriminateor\": log_loss_discriminateor,\n            \"log_loss_generator\": log_loss_generator}\n        self.generator_model = load_model(path_dir + \"/gan_generator.h5\", custom_obj)\n        self.discriminateor_model = load_model(path_dir + \"/gan_discriminateor.h5\", custom_obj)\n    \n    def _discriminate_generated(self):\n        # 必须每次重新生成一下 \n        disc_t = self.__tensors[1]\n        for l in self._discriminate_layers:\n            disc_t = l(disc_t)            \n        return disc_t\n    \nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\"\"\"gan model demo (gaussian sample)\"\"\")\n    parser.add_argument(\"-m\", \"--model_dir\")\n    parser.add_argument(\"-log\", \"--log_dir\")\n    parser.add_argument(\"-b\", \"--batch_size\", type = int, default = 32)\n    parser.add_argument(\"-log_lvl\", \"--log_lvl\", default = \"info\",\n                        metavar = \"可以指定INFO，DEBUG，WARN, ERROR\")\n    parser.add_argument(\"-e\", \"--epoch\", type = int, default = 10)\n    \n    args = parser.parse_args()\n    \n    log_lvl = {\"info\": logging.INFO,\n               \"debug\": logging.DEBUG,\n               \"warn\": logging.WARN,\n               \"warning\": logging.WARN,\n               \"error\": logging.ERROR,\n               \"err\": logging.ERROR}[args.log_lvl.lower()]\n    app_logger.init(log_lvl)\n        \n    loger.info(\"args: %r\" % args)\n    step_per = 20\n    sample_size = args.batch_size * 100\n\n    # 整个测试样本集合\n    noise_dim = 4\n    signal_dim = 1\n    x = np.random.uniform(-3, 3, size = (sample_size, noise_dim))\n    y = np.random.normal(size = (sample_size, signal_dim))\n    samples = {\"x\": x, \n               \"y\": y}\n    \n    gan = GANModel([signal_dim, noise_dim], args.log_dir)\n    gan.add_discr_layer(layers.Dense(200, activation=\"relu\"))\n    gan.add_discr_layer(layers.Dense(50, activation=\"softmax\"))\n    gan.add_discr_layer(layers.Lambda(lambda y: K.max(y, axis=-1, keepdims=True),\n                                 output_shape = (1,)))\n\n    gan.add_gen_layer(layers.Dense(200, activation=\"relu\"))\n    gan.add_gen_layer(layers.Dense(100, activation=\"relu\"))\n    gan.add_gen_layer(layers.Dense(50, activation=\"relu\"))\n    gan.add_gen_layer(layers.Dense(signal_dim))\n    \n    gan.compile_generator_model()\n    loger.info(\"compile generator finished\")\n    gan.compile_discriminateor_model()\n    loger.info(\"compile discriminator finished\")\n    \n    gan.train(samples, args.epoch, args.batch_size, step_per, plot=True)\n    gen_results = gan.train_status[\"gen_result\"]\n    dis_results = gan.train_status[\"dis_result\"]\n\n    gen_result = gen_results[-1]\n    dis_result = dis_results[-1]\n    freq_g, bin_g = np.histogram(gen_result, density=True)\n    # norm to sum1\n    freq_g = freq_g * (bin_g[1] - bin_g[0])\n    bin_g = bin_g[:-1]\n    freq_d, bin_d = np.histogram(y, bins = 100, density=True)\n    freq_d = freq_d * (bin_d[1] - bin_d[0])\n    bin_d = bin_d[:-1]\n    plt.plot(bin_g, freq_g, 'go-', markersize = 4)\n    plt.plot(bin_d, freq_d, 'ko-', markersize = 8)\n    gen1d = gen_result.flatten()\n    dis1d = dis_result.flatten()\n    si = np.argsort(gen1d)\n    plt.plot(gen1d[si], dis1d[si], 'r--')\n    plt.savefig(\"img/gan_results.png\")\n    if not path.exists(args.model_dir):\n        os.mkdir(args.model_dir)\n    gan.save_model(args.model_dir)\n\n\n# app_logger.py\nimport logging\n\ndef init(lvl=logging.DEBUG):\n    log_handler = logging.StreamHandler()\n    # create formatter\n    formatter = logging.Formatter('[%(asctime)s] %(levelname)s %(filename)s:%(funcName)s:%(lineno)d > %(message)s')\n    log_handler.setFormatter(formatter)\n    logging.basicConfig(level = lvl, handlers = [log_handler])\n\n\n                ", "mainLikeNum": ["5 "], "mainBookmarkNum": "8"}