{"title": " python3.6+scrapy+mysql 爬虫实战 - 个人文章 ", "index": "python爬虫,网页爬虫,mysql,scrapy,python", "content": "最近闲着，把之前写的小爬虫分享一下，才疏学浅，仅当参考。\n[介绍文档]\n    python版本：python3.6\n    scrapy: 1.5.0\n    需要安装pymysql包支持访问mysql数据库\n    可以使用pip安装： pip install pymysql\n重要提示\n*或者按照下述方法执行一键安装依赖：pip install -r requirements.txt\n\n*重要事情说三遍：请确保你安装了mysql数据库！ 请确保你安装了mysql数据库！ 请确保你安装了mysql数据库！\n\n*所有平台的Mysql下载地址为： https://dev.mysql.com/downloads/挑选你需要的 MySQL Community Server 版本及对应的平台。\n\n\n爬虫工作配置\n第一步：下载github项目文件\ngit clone git@github.com:caffreycc/jb51.com_crawler.git\n\n或者直接到https://github.com/caffreycc/jb51.com_crawler.git 下载zip文件\n* 第二步：安装依赖:\n\npip install -r requirements.txt\n第三步：修改配置Config.py:\n    Config.py 为项目配置文件\n\n    host = '127.0.0.1' #改成你的数据库地址，如果需要保存在线服务器请填写数据库IP\n    dbname = 'your database naem'  # 数据库名字，请修改\n    user = 'your databse user'  # 数据库账号，请修改\n    psw = 'your password'  # 数据库密码，请修改\n    port = 3306  # 数据库端口，在dbhelper中使用,一般无需修改\n第四步：运行小爬虫\n    命令行cd到你的项目文件夹，运行以下命令：\n    或者直接在你的爬虫文件夹内shift + 右键 打开命令提示符或者powershell，运行以下命令\n    scrapy crawl Common_crawler\n\n    爬取的内容会自动保存到 config.py 中配置的mysql数据库中\n问题反馈\n　　有任何关于项目的问题欢迎提issues\n贡献代码\n本项目基于PythonCrawler-Scrapy-Mysql-File-Template开发，感谢作者@lawlite19（https://github.com/lawlite19）的开源分享精神。\n\n\n项目地址： python3.6+scrapy+mysql 爬虫实战\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "3"}