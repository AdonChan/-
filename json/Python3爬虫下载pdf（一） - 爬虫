{"title": "Python3爬虫下载pdf（一） - 爬虫 ", "index": "网页爬虫,python", "content": "Python3爬虫下载pdf（一）\n最近在学习python的爬虫，并且玩的不亦说乎，因此写个博客，记录并分享一下。\n需下载以下模块\n\nbs4 模块\nrequests 模块\n\n一、源码\n\"\"\"\n功能：下载指定url内的所有的pdf\n语法：将含有pdf的url放到脚本后面执行就可以了\n\"\"\"\n\nfrom bs4 import BeautifulSoup as Soup\nimport requests\nfrom sys import argv\n\ntry:\n    ##用于获取命令行参数，argv[0]是脚本的名称\n    root_url = argv[1]\nexcept:\n    print(\"please input url behind the script!!\")\n    exit()\n\n##获得含有所有a标签的一个列表\ndef getTagA(root_url):\n    res = requests.get(root_url)\n    soup = Soup(res.text,'html.parser')\n    temp = soup.find_all(\"a\")\n    return temp\n\n##从所有a标签中找到含有pdf的，然后下载\ndef downPdf(root_url,list_a):\n    number = 0\n    ##如果网站url是以类似xx/index.php格式结尾，那么只取最后一个/之前的部分\n    if not root_url.endswith(\"/\"):     \n        index = root_url.rfind(\"/\")\n        root_url = root_url[:index+1]\n    for name in list_a:\n        name02 = name.get(\"href\")\n        ##筛选出以.pdf结尾的a标签\n        if name02.lower().endswith(\".pdf\"):\n            pdf_name = name.string \n            number += 1\n            print(\"Download the %d pdf immdiately!!!\"%number,end='  ')\n            print(pdf_name+'downing.....') \n             ##因为要下载的是二进制流文件，将strem参数置为True     \n            response = requests.get(root_url+pdf_name,stream=\"TRUE\")\n            with open(pdf_name,'wb') as file:\n                for data in response.iter_content():\n                    file.write(data)\n\nif __name__ == \"__main__\":\n    downPdf(root_url,getTagA(root_url))\n二、亮点\n\n利用str.rfind(\"S\") 函数来获得 S 在str 从右边数第一次出现的index\n使用str.lower().endswith(\"S\") 函数来判断str 是否以S/s 结尾\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "1"}