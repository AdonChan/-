{"title": "大数据与云计算学习：数据分析（一） - 镜心的小树屋 ", "index": "python,数据分析", "content": "\n\npython基础\n先看看 基础\n注意点\n切割操作\n\n这里发现我们在取出list中的元素时候是左开右闭的，即[3,6) 索引6对应的元素7并没有被输出\n改变list中的元素\n\n添加删除元素\n\n两种拷贝list的方式\nlist2拷贝给y，y改变，list2也变\nlist2拷贝给y，y改变，list2不变\n删除实例的属性和删除字典属性的区别\na = {'a':1,'b':2}\ndel a['a']\na = classname()\ndel classname.attrname\nwith as\nhttps://www.cnblogs.com/DswCn...\nif name == '__main__':\nif __name__ == '__main__':\n一个python的文件有两种使用的方法，   第一是直接作为脚本执行，   第二是import到其他的python脚本中被调用（模块重用）执行。   因此if name == 'main':   的作用就是控制这两种情况执行代码的过程，   在if name == 'main': 下的代码只有在第一种情况下（即文件作为脚本直接执行）才会被执行，   而import到其他脚本中是不会被执行的。...\n\n函数 /方法\n正则表达式\n基础看这里\nimport re\nline = 'jwxddxsw33'\nif line == \"jxdxsw33\":\n    print(\"yep\")\nelse:\n    print(\"no\")\n\n# ^ 限定以什么开头\nregex_str = \"^j.*\"\nif re.match(regex_str, line):\n    print(\"yes\")\n#$限定以什么结尾\nregex_str1 = \"^j.*3$\"\nif re.match(regex_str, line):\n    print(\"yes\")\n\nregex_str1 = \"^j.3$\"\nif re.match(regex_str, line):\n    print(\"yes\")\n# 贪婪匹配\nregex_str2 = \".*(d.*w).*\"\nmatch_obj = re.match(regex_str2, line)\nif match_obj:\n    print(match_obj.group(1))\n# 非贪婪匹配\n# ？处表示遇到第一个d 就匹配\nregex_str3 = \".*?(d.*w).*\"\nmatch_obj = re.match(regex_str3, line)\nif match_obj:\n    print(match_obj.group(1))\n# * 表示>=0次　　＋　表示　>=0次\n# ? 表示非贪婪模式\n# + 的作用至少>出现一次  所以.+任意字符这个字符至少出现一次\nline1 = 'jxxxxxxdxsssssswwwwjjjww123'\nregex_str3 = \".*(w.+w).*\"\nmatch_obj = re.match(regex_str3, line1)\nif match_obj:\n    print(match_obj.group(1))\n# {2}限定前面的字符出现次数 {2,}2次以上 {2,5}最小两次最多5次\nline2 = 'jxxxxxxdxsssssswwaawwjjjww123'\nregex_str3 = \".*(w.{3}w).*\"\nmatch_obj = re.match(regex_str3, line2)\nif match_obj:\n    print(match_obj.group(1))\n\nline2 = 'jxxxxxxdxsssssswwaawwjjjww123'\nregex_str3 = \".*(w.{2}w).*\"\nmatch_obj = re.match(regex_str3, line2)\nif match_obj:\n    print(match_obj.group(1))\n\nline2 = 'jxxxxxxdxsssssswbwaawwjjjww123'\nregex_str3 = \".*(w.{5,}w).*\"\nmatch_obj = re.match(regex_str3, line2)\nif match_obj:\n    print(match_obj.group(1))\n\n# | 或\n\nline3 = 'jx123'\nregex_str4 = \"((jx|jxjx)123)\"\nmatch_obj = re.match(regex_str4, line3)\nif match_obj:\n    print(match_obj.group(1))\n    print(match_obj.group(2))\n# [] 表示中括号内任意一个\nline4 = 'ixdxsw123'\nregex_str4 = \"([hijk]xdxsw123)\"\nmatch_obj = re.match(regex_str4, line4)\nif match_obj:\n    print(match_obj.group(1))\n# [0,9]{9} 0到9任意一个 出现9次（9位数）\nline5 = '15955224326'\nregex_str5 = \"(1[234567][0-9]{9})\"\nmatch_obj = re.match(regex_str5, line5)\nif match_obj:\n    print(match_obj.group(1))\n# [^1]{9}\nline6 = '15955224326'\nregex_str6 = \"(1[234567][^1]{9})\"\nmatch_obj = re.match(regex_str6, line6)\nif match_obj:\n    print(match_obj.group(1))\n\n# [.*]{9} 中括号中的.和*就代表.*本身\nline7 = '1.*59224326'\nregex_str7 = \"(1[.*][^1]{9})\"\nmatch_obj = re.match(regex_str7, line7)\nif match_obj:\n    print(match_obj.group(1))\n\n#\\s 空格\nline8 = '你 好'\nregex_str8 = \"(你\\s好)\"\nmatch_obj = re.match(regex_str8, line8)\nif match_obj:\n    print(match_obj.group(1))\n\n# \\S 只要不是空格都可以（非空格）\nline9 = '你真好'\nregex_str9 = \"(你\\S好)\"\nmatch_obj = re.match(regex_str9, line9)\nif match_obj:\n    print(match_obj.group(1))\n\n# \\w  任意字符 和.不同的是 它表示[A-Za-z0-9_]\nline9 = '你adsfs好'\nregex_str9 = \"(你\\w\\w\\w\\w\\w好)\"\nmatch_obj = re.match(regex_str9, line9)\nif match_obj:\n    print(match_obj.group(1))\n\nline10 = '你adsf_好'\nregex_str10 = \"(你\\w\\w\\w\\w\\w好)\"\nmatch_obj = re.match(regex_str10, line10)\nif match_obj:\n    print(match_obj.group(1))\n#\\W大写的是非[A-Za-z0-9_]\nline11 = '你 好'\nregex_str11 = \"(你\\W好)\"\nmatch_obj = re.match(regex_str11, line11)\nif match_obj:\n    print(match_obj.group(1))\n\n# unicode编码 [\\u4E00-\\u\\9FA5] 表示汉字\nline12= \"镜心的小树屋\"\nregex_str12= \"([\\u4E00-\\u9FA5]+)\"\nmatch_obj = re.match(regex_str12,line12)\nif match_obj:\n    print(match_obj.group(1))\n\nprint(\"-----贪婪匹配情况----\")\nline13 = 'reading in 镜心的小树屋'\nregex_str13 = \".*([\\u4E00-\\u9FA5]+树屋)\"\nmatch_obj = re.match(regex_str13, line13)\nif match_obj:\n    print(match_obj.group(1))\n\nprint(\"----取消贪婪匹配情况----\")\nline13 = 'reading in 镜心的小树屋'\nregex_str13 = \".*?([\\u4E00-\\u9FA5]+树屋)\"\nmatch_obj = re.match(regex_str13, line13)\nif match_obj:\n    print(match_obj.group(1))\n\n#\\d数字\nline14 = 'XXX出生于2011年'\nregex_str14 = \".*(\\d{4})年\"\nmatch_obj = re.match(regex_str14, line14)\nif match_obj:\n    print(match_obj.group(1))\n\nregex_str15 = \".*?(\\d+)年\"\nmatch_obj = re.match(regex_str15, line14)\nif match_obj:\n    print(match_obj.group(1))\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\n###\n# 试写一个验证Email地址的正则表达式。版本一应该可以验证出类似的Email：\n#someone@gmail.com\n#bill.gates@microsoft.com\n###\n\nimport re\naddr = 'someone@gmail.com'\naddr2 = 'bill.gates@microsoft.com'\ndef is_valid_email(addr):\n    if re.match(r'[a-zA-Z_\\.]*@[a-aA-Z.]*',addr):\n        return True\n    else:\n        return False\n\nprint(is_valid_email(addr))\nprint(is_valid_email(addr2))\n\n# 版本二可以提取出带名字的Email地址：\n# <Tom Paris> tom@voyager.org => Tom Paris\n# bob@example.com => bob\n\naddr3 = '<Tom Paris> tom@voyager.org'\naddr4 = 'bob@example.com'\n\ndef name_of_email(addr):\n    r=re.compile(r'^(<?)([\\w\\s]*)(>?)([\\w\\s]*)@([\\w.]*)$')\n    if not r.match(addr):\n        return None\n    else:\n        m = r.match(addr)\n        return m.group(2)\n\nprint(name_of_email(addr3))\nprint(name_of_email(addr4))\n案例\n找出一个文本中词频最高的单词\ntext = 'the clown ran after the car and the car ran into the tent and the tent fell down on the clown and the car'\nwords = text.split()\nprint(words)\n\nfor word in words:# 初始化空列表\n    print(word)\n\n\n#步骤一：获得单词列表  相当于去重\nunique_words = list()\nfor word in words:\n   if(word not in unique_words):# 使用in判断某个元素是否在列表里\n       unique_words.append(word)\nprint(unique_words)\n\n\n#步骤二：初始化词频列表\n\n# [e]*n 快速初始化\ncounts = [0] * len(unique_words)\nprint(counts)\n\n# 步骤三：统计词频\nfor word in words:\n    index = unique_words.index(word)\n\n    counts[index] = counts[index] + 1\n    print(counts[index])\nprint(counts)\n# 步骤四：找出最高词频和其对应的单词\nbigcount = None #None 为空，初始化bigcount\nbigword = None\n\nfor i in range(len(counts)):\n    if bigcount is None or counts[i] > bigcount:\n        bigword = unique_words[i]\n        bigcount = counts[i]\nprint(bigword,bigcount)\n用字典的方式：\n# 案例回顾：找出一个文本中最高词频的单词\n\ntext = '''the clown ran after the car and the car ran into the tent \n        and the tent fell down on the clown and the car'''\nwords = text.split() # 获取单词的列表\n\n# 使用字典可以极大简化步骤\n# 获取单词-词频字典\ncounts = dict() # 初始化一个空字典\nfor word in words:\n    counts[word] = counts.get(word, 0) + 1  # 构造字典。注意get方法需要设定默认返回值0（当单词第一次出现时，词频为1）\nprint(counts)\n\n# 在字典中查找最高词频的单词\nbigcount = None\nbigword = None\nfor word,count in counts.items():\n    if bigcount is None or count > bigcount:\n        bigword = word\n        bigcount = count\n\nprint(bigword, bigcount)\n自定义一个每周工资计算器函数\n# 使用input()函数，从键盘读取输入的文本\n# a = input('请输入文本:')\n# print('您输入的内容是：',a)\n\ndef salary_calculator(): #没有参数的函数\n    user = str #初始化user为字符串变量\n    print(\"----工资计算器----\")\n\n    while True:\n        user = input(\"\\n请输入你的名字，或者输入0来结束报告: \")\n\n        if user == \"0\":\n            print(\"结束报告\")\n            break\n        else:\n            hours = float(input(\"请输入你的工作小时数：\"))\n            payrate =float(input(\"请输入你的单位时间工资： ￥\"))\n\n            if hours <= 40:\n                print(\"员工姓名:\",user)\n                print(\"加班小时数：0\")\n                print(\"加班费：￥0.00\")\n                regularpay = round(hours * payrate,2) # round函数保留小数点后两位\n                print(\"税前工资:￥\" + str(regularpay))\n\n\n            elif hours > 40:\n\n                overtimehours = round(hours - 40, 2)\n\n                print(\"员工姓名: \" + user)\n\n                print(\"加班小时数: \" + str(overtimehours))\n\n                regularpay = round(40 * payrate, 2)\n\n                overtimerate = round(payrate * 1.5, 2)\n\n                overtimepay = round(overtimehours * overtimerate)\n\n                grosspay = round(regularpay + overtimepay, 2)\n\n                print(\"常规工资: ￥\" + str(regularpay))\n\n                print(\"加班费: ￥\" + str(overtimepay))\n\n                print(\"税前工资: ￥\" + str(grosspay))\n\n#调用 salary_calculator\n\nsalary_calculator()\n这个实例中注意 python中关于round函数的小坑\n数据结构、函数、条件和循环\n包管理\n戳这里看有哪些流行python包——>awesom-python\n\n\nNumpy  处理数组/数据计算扩展\n\nndarray 一种多维数组对象\n利用数组进行数据处理\n用于数组的文件输入输出\n多维操作\n线性代数\n随机数生成\n随机漫步\n\nNumpy高级应用\n\nndarray 对象的内部机制\n高级数组操作\n广播\nufunc高级应用\n结构化和记录式数组\n更多有关排序\nNumPy的matrix类\n高级数组输入输出\n\n\n\n\nMatplotlib 数据可视化\n\n\nPandas   数据分析\n\npandas的数据结构\n基本功能\n汇总和计算描述统计\n处理缺失数据\n层次化索引\n聚合与分组\n逻辑回归基本原理\n\n\njupyter\n\npip3 install jupyter\njupyter notebook\n\n\nscipy\n描述性统计\n\nScikit-learn 数据挖掘、机器学习\n\nkeras  人工神经网络\n\ntensorflow 神经网络\n\n\n安装Python包管理工具pip，主要是用于安装 PyPI 上的软件包\n安装教程\nsudo apt-get install python3-pip\npip3 install numpy\npip3 install scipy\npip3 install matplotlib\n或者下这个安装脚本 get-pip.py\n\n包的引入方式\n因为python是面向对象的编程，推荐引入方式还是\nimport numpy\nnumpy.array([1,2,3])\n\n数据存储\n数据操作\n生成数据\n生成一组二维数组，有5000个元素，每个元素内表示 身高和体重\nimport numpy as np\n\n生成1000个经纬度位置，靠近（117，32），并输出位csv\nimport pandas as pd\nimport numpy as np\n\n# 任意的多组列表\nlng = np.random.normal(117,0.20,1000)\n\nlat = np.random.normal(32.00,0.20,1000)\n\n# 字典中的key值即为csv中列名\ndataframe = pd.DataFrame({'lng':lng,'lat':lat})\n\n\n#将DataFrame存储为csv,index表示是否显示行名，default=True\ndataframe.to_csv('data/lng-lat.csv',index = False, sep=',' )\n\n\nnumpy的常用操作\n#encoding=utf-8 \nimport numpy as np \ndef main():\n    lst = [[1,3,5],[2,4,6]]\n    print(type(lst))\n    np_lst = np.array(lst)\n    print(type(np_lst))\n    # 同一种numpy.array中只能有一种数据类型\n    # 定义np的数据类型\n    # 数据类型有：bool int int8 int16 int32 int64 int128 uint8 uint16 uint32 uint64 uint128 float16/32/64 complex64/128\n    np_lst = np.array(lst,dtype=np.float)\n\n    print(np_lst.shape)\n    print(np_lst.ndim)#数据的维度\n    print(np_lst.dtype)#数据类型\n    print(np_lst.itemsize) #每个元素的大小\n    print(np_lst.size)#数据大小 几个元素\n\n    # numpy array\n    print(np.zeros([2,4]))# 生成2行4列都是0的数组\n    print(np.ones([3,5]))\n\n    print(\"---------随机数Rand-------\") \n    print(np.random.rand(2,4))# rand用于产生0～1之间的随机数 2*4的数组\n    print(np.random.rand())\n    print(\"---------随机数RandInt-------\")\n    print(np.random.randint(1,10)) # 1~10之间的随机整数\n    print(np.random.randint(1,10,3))# 3个1～10之间的随机整数\n    print(\"---------随机数Randn 标准正太分布-------\")\n    print(np.random.randn(2,4)) # 2行4列的标准正太分布的随机整数\n    print(\"---------随机数Choice-------\")\n    print(np.random.choice([10,20,30]))# 指定在10 20 30 里面选一个随机数生成\n    print(\"---------分布Distribute-------\")\n    print(np.random.beta(1,10,100))# 生成beta分布\nif __name__ == \"__main__\":\n    main()\n\n\n常用函数举例\n计算红酒数据每一个属性的平均值（即每一列数据的平均值）\n数据分析工具\n数据可视化\n\n\n探索数据\n数据展示\n数据 ---> 故事\n\n\n\nmatplotlib 绘图基础\n\n函数曲线的绘制\n图形细节的设置\n\n\n\n案例分析：销售记录可视化\n\n条形图\n绘制多图\n饼图\n散点图\n直方图\n\nseaborn 数据可视化包\n\n分类数据的散点图\n分类数据的箱线图\n多变量图\n\n\n\n\n\n更多内容戳这里 数据可视化\n安装 matplotlib\n\n注意这里会报这样的错误\nImportError: No module named '_tkinter', please install the python3-tk package\n需要安装 python3-tk\n更多示例\n线图\n\n散点图 & 柱状图\n\n数据分析\npadans\n\n上层数据操作\ndataframe数据结构\n\n\n import pandas as pd\nbrics = pd.read_csv('/home/wyc/study/python_lession/python_lessions/数据分析/brics.csv',index_col = 0)\n\n\npandas基本操作\n\n\nimport numpy as np\nimport pandas as pd\n\ndef main():\n\n    #Data Structure\n    s = pd.Series([i*2 for i in range(1,11)])\n    print(type(s))\n\n    dates = pd.date_range(\"20170301\",periods=8)\n    df = pd.DataFrame(np.random.randn(8,5),index=dates,columns=list(\"ABCDE\"))\n    print(df)\n    # basic\n\n    print(df.head(3))\n    print(df.tail(3))\n    print(df.index)\n    print(df.values)\n    print(df.T)\n    # print(df.sort(columns=\"C\"))\n    print(df.sort_index(axis=1,ascending=False))\n    print(df.describe())\n\n    #select\n    print(type(df[\"A\"]))\n    print(df[:3])\n    print(df[\"20170301\":\"20170304\"])\n    print(df.loc[dates[0]])\n    print(df.loc[\"20170301\":\"20170304\",[\"B\",\"D\"]])\n    print(df.at[dates[0],\"C\"])\n\n\n    print(df.iloc[1:3,2:4])\n    print(df.iloc[1,4])\n    print(df.iat[1,4])\n\n    print(df[df.B>0][df.A<0])\n    print(df[df>0])\n    print(df[df[\"E\"].isin([1,2])])\n\n    # Set\n    s1 = pd.Series(list(range(10,18)),index = pd.date_range(\"20170301\",periods=8))\n    df[\"F\"]= s1\n    print(df)\n    df.at[dates[0],\"A\"] = 0\n    print(df)\n    df.iat[1,1] = 1\n    df.loc[:,\"D\"] = np.array([4]*len(df))\n    print(df)\n\n    df2 = df.copy()\n    df2[df2>0] = -df2\n    print(df2)\n\n    # Missing Value\n    df1 = df.reindex(index=dates[:4],columns = list(\"ABCD\") + [\"G\"])\n    df1.loc[dates[0]:dates[1],\"G\"]=1\n    print(df1)\n    print(df1.dropna())\n    print(df1.fillna(value=1))\n\n    # Statistic\n    print(df.mean())\n    print(df.var())\n\n    s = pd.Series([1,2,4,np.nan,5,7,9,10],index=dates)\n    print(s)\n    print(s.shift(2))\n    print(s.diff())\n    print(s.value_counts())\n    print(df.apply(np.cumsum))\n    print(df.apply(lambda x:x.max()-x.min()))\n\n    #Concat\n    pieces = [df[:3],df[-3:]]\n    print(pd.concat(pieces))\n\n    left = pd.DataFrame({\"key\":[\"x\",\"y\"],\"value\":[1,2]})\n    right = pd.DataFrame({\"key\":[\"x\",\"z\"],\"value\":[3,4]})\n    print('LEFT',left)\n    print('RIGHT', right)\n    print(pd.merge(left,right,on=\"key\",how=\"outer\"))\n    df3 = pd.DataFrame({\"A\": [\"a\",\"b\",\"c\",\"b\"],\"B\":list(range(4))})\n    print(df3.groupby(\"A\").sum())\n\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n# 首先产生一个叫gdp的字典\ngdp = {\"country\":[\"United States\", \"China\", \"Japan\", \"Germany\", \"United Kingdom\"],\n       \"capital\":[\"Washington, D.C.\", \"Beijing\", \"Tokyo\", \"Berlin\", \"London\"],\n       \"population\":[323, 1389, 127, 83, 66],\n       \"gdp\":[19.42, 11.8, 4.84, 3.42, 2.5],\n       \"continent\":[\"North America\", \"Asia\", \"Asia\", \"Europe\", \"Europe\"]}\n\nimport pandas as pd\ngdp_df = pd.DataFrame(gdp)\nprint(gdp_df)\n\n# 我们可以通过index选项添加自定义的行标签(label)\n# 使用column选项可以选择列的顺序\ngdp_df = pd.DataFrame(gdp, columns = [\"country\", \"capital\", \"population\", \"gdp\", \"continent\"],index = [\"us\", \"cn\", \"jp\", \"de\", \"uk\"])\nprint(gdp_df)\n\n#修改行和列的标签\n# 也可以使用index和columns直接修改\ngdp_df.index=[\"US\", \"CN\", \"JP\", \"DE\", \"UK\"]\ngdp_df.columns = [\"Country\", \"Capital\", \"Population\", \"GDP\", \"Continent\"]\nprint(gdp_df)\n# 增加rank列，表示他们的GDP处在前5位\ngdp_df[\"rank\"] = \"Top5 GDP\"\n# 增加国土面积变量,以百万公里计（数据来源：http://data.worldbank.org/）\ngdp_df[\"Area\"] = [9.15, 9.38, 0.37, 0.35, 0.24]\nprint(gdp_df)\n\n\n# 一个最简单的series\nseries = pd.Series([2,4,5,7,3],index = ['a','b','c','d','e'])\nprint(series)\n# 当我们使用点操作符来查看一个变量时，返回的是一个pandas series\n# 在后续的布尔筛选中使用点方法可以简化代码\n# US,...,UK是索引\nprint(gdp_df.GDP)\n\n\n# 可以直接查看索引index\nprint(gdp_df.GDP.index)\n# 类型是pandas.core.series.Series\nprint(type(gdp_df.GDP))\n\n#返回一个布尔型的series，在后面讲到的DataFrame的布尔索引中会大量使用\nprint(gdp_df.GDP > 4)\n\n# 我们也可以将series视为一个长度固定且有顺序的字典，一些用于字典的函数也可以用于series\ngdp_dict = {\"US\": 19.42, \"CN\": 11.80, \"JP\": 4.84, \"DE\": 3.42, \"UK\": 2.5}\ngdp_series = pd.Series(gdp_dict)\nprint(gdp_series)\n\n# 判断 ’US' 标签是否在gdp_series中\n\nprint(\"US\" in gdp_series)\n# 使用变量名加[[]]选取列\nprint(gdp_df[[\"Country\"]])\n# 可以同时选取多列\nprint(gdp_df[[\"Country\", \"GDP\"]])\n\n\n# 如果只是用[]则产生series\nprint(type(gdp_df[\"Country\"]))\n# 行选取和2d数组类似\n# 如果使用[]选取行，切片方法唯一的选项\nprint(gdp_df[2:5]) #终索引是不被包括的！\n\n#loc方法\n# 在上面例子中，我们使用行索引选取行，能不能使用行标签实现选取呢？\n# loc方法正是基于标签选取数据的方法\nprint(gdp_df.loc[[\"JP\",\"DE\"]])\n# 以上例子选取了所有的列\n# 我们可以加入需要的列标签\nprint(gdp_df.loc[[\"JP\",\"DE\"],[\"Country\",\"GDP\",\"Continent\"]])\n\n# 选取所有的行，我们可以使用:来表示选取所有的行\nprint(gdp_df.loc[:,[\"Country\",\"GDP\",\"Continent\"]])\n\n# 等价于gdp_df.loc[[\"JP\",\"DE\"]]\nprint(gdp_df.iloc[[2,3]])\n\nprint(gdp_df.loc[[\"JP\",\"DE\"],[\"Country\", \"GDP\", \"Continent\"]])\nprint(gdp_df.iloc[[2,3],[0,3,4]])\n\n# 选出亚洲国家，下面两行命令产生一样的结果\nprint(gdp_df[gdp_df.Continent == \"Asia\"])\n\nprint(gdp_df.loc[gdp_df.Continent == \"Asia\"])\n# 选出gdp大于3兆亿美元的欧洲国家\nprint(gdp_df[(gdp_df.Continent == \"Europe\") & (gdp_df.GDP > 3)])\n缺失值处理\n数据挖掘\n案例:Iris鸢尾花数据集让我们来看一下经典的iris数据:\n\n鸢尾花卉数据集， 来源 UCI 机器学习数据集\n\n它最初是埃德加·安德森采集的\n四个特征被用作样本的定量分析，它们分别是花萼(sepal)和花瓣(petal)的长度(length)和宽度(width)\n\n\n#####\n#数据的导入和观察\n#####\nimport pandas as pd\n# 用列表存储列标签\ncol_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n# 读取数据，并指定每一列的标签\niris = pd.read_csv('data/iris.txt', names = col_names)\n\n# 使用head/tail查看数据的头和尾\n\nprint(iris.head(10))\n\n# 使用info 方法查看数据的总体信息\niris.info()\n\n# 使用shape可以查看DataFrame的行数与列数\n# iris有150个观察值，5个变量\nprint(iris.shape)\n# 这里的品种(species)是分类变量(categorical variable)\n# 可以使用unique方法来对查看series中品种的名字\nprint(iris.species.unique())\n\n\n# 统计不同品种的数量\n# 使用DataFrame的value_counts方法来实现\nprint(iris.species.value_counts())\n\n#选取花瓣数据，即 petal_length 和 petal_width 这两列\n# 方法一：使用[[ ]]\npetal = iris[['petal_length','petal_width']]\nprint(petal.head())\n# 方法二：使用 .loc[ ]\npetal = iris.loc[:,['petal_length','petal_width']]\nprint(petal.head())\n# 方法三：使用 .iloc[ ]\npetal = iris.iloc[:,2:4]\nprint(petal.head())\n\n# 选取行索引为5-10的数据行\n# 方法一：使用[]\nprint(iris[5:11])\n# 方法二：使用 .iloc[]\nprint(iris.iloc[5:11,:])\n\n# 选取品种为 Iris-versicolor 的数据\nversicolor = iris[iris.species == 'Iris-versicolor']\nprint(versicolor.head())\n\n\n####\n#数据的可视化\n####\n#散点图\nimport matplotlib.pyplot as plt\n# 我们首先画散点图（sactter plot），x轴上画出花瓣的长度，y轴上画出花瓣的宽度\n# 我们观察到什么呢？\niris.plot(kind = 'scatter', x=\"petal_length\", y=\"petal_width\")\n# plt.show()\n\n# 使用布尔索引的方法分别获取三个品种的数据\nsetosa = iris[iris.species == 'Iris-setosa']\nversicolor = iris[iris.species == 'Iris-versicolor']\nvirginica = iris[iris.species == 'Iris-virginica']\n\nax = setosa.plot(kind='scatter', x=\"petal_length\", y=\"petal_width\", color='Red', label='setosa', figsize=(10,6))\nversicolor.plot(kind='scatter', x=\"petal_length\", y=\"petal_width\", color='Green', ax=ax, label='versicolor')\nvirginica.plot(kind='scatter', x=\"petal_length\", y=\"petal_width\", color='Orange', ax=ax, label='virginica')\nplt.show()\n\n#箱图\n#使用mean()方法获取花瓣宽度均值\nprint(iris.petal_width.mean())\n#使用median()方法获取花瓣宽度的中位数\nprint(iris.petal_width.median())\n# 可以使用describe方法来总结数值变量\nprint(iris.describe())\n\n\n# 绘制花瓣宽度的箱图\n# 箱图展示了数据中的中位数，四分位数，最大值，最小值\niris.petal_width.plot(kind='box')\n# plt.show()\n\n# 按品种分类，分别绘制不同品种花瓣宽度的箱图\niris[['petal_width','species']].boxplot(grid=False,by='species',figsize=(10,6))\n# plt.show()\n\nsetosa.describe()\n\n# 计算每个品种鸢尾花各个属性（花萼、花瓣的长度和宽度）的最小值、平均值又是分别是多少？ （提示：使用min、mean 方法。）\nprint(iris.groupby(['species']).agg(['min','mean']))\n\n#计算鸢尾花每个品种的花萼长度（sepal_length) 大于6cm的数据个数。\n# 方法1\nprint(iris[iris['sepal_length']> 6].groupby('species').size())\n# 方法2\ndef more_len(group,length=6):\n    return len(group[group['sepal_length'] > length])\nprint(iris.groupby(['species']).apply(more_len,6))\n\n缺失值处理 、数据透视表\n\n\n缺失值处理：pandas中的fillna()方法\n\npandas用nan(not a number)表示缺失数据，处理缺失数据有以下几种方法：\n\ndropna去除nan数据\nfillna使用默认值填入\nisnull 返回一个含有布尔值的对象，表示哪些是nan，哪些不是\nnotnull isnull的否定式\n\n\n\n数据透视表：pandas中的pivot_table函数\n\n我们用案例分析 - 泰坦尼克数据 来说明这个两个问题缺失值处理：\n\n真实数据往往某些变量会有缺失值。\n这里，cabin有超过70%以上的缺失值，我们可以考虑直接丢掉这个变量。 -- 删除某一列数据\n像Age这样的重要变量，有20%左右的缺失值，我们可以考虑用中位值来填补。-- 填补缺失值\n我们一般不提倡去掉带有缺失值的行，因为其他非缺失的变量可能提供有用的信息。-- 删除带缺失值的行\n\n# 读取常用的包\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#读取数据\ntitanic_df = pd.read_csv('data/titanic.csv')\n\n#查看前五行数据\nprint(titanic_df.head())\n\n# 数据的统计描述\n# describe函数查看部分变量的分布\n# 因为Survived是0-1变量，所以均值就是幸存人数的百分比，这个用法非常有用\nprint(titanic_df[[\"Survived\",\"Age\",\"SibSp\",\"Parch\"]].describe())\n\n# 使用include=[np.object]来查看分类变量\n# count: 非缺失值的个数\n# unique: 非重复值得个数\n# top: 最高频值\n# freq: 最高频值出现次数\n\nprint(titanic_df.describe(include=[np.object]))\n\n#不同舱位的分布情况是怎样的呢？\n# 方法1: value_counts\n# 查看不同舱位的分布\n# 头等舱：24%； 二等舱：21%； 三等舱：55%\n# value_counts 频数统计， len() 获取数据长度\nprint(titanic_df.Pclass.value_counts() / len(titanic_df))\n# 总共有891个乘客\n# Age有714个非缺失值，Cabin只有204个非缺失值。我们将会讲解如何处理缺失值\nprint(titanic_df.info())\n\n#方法2：group_by\n# sort_values 将结果排序\n(titanic_df.groupby(\"Pclass\").agg(\"size\")/len(titanic_df)).sort_values(ascending=False)\n\n# 填补年龄数据中的缺失值\n# 直接使用所有人年龄的中位数来填补\n# 在处理之前，查看Age列的统计值\nprint(titanic_df.Age.describe())\n\n# 重新载入原始数据\ntitanic_df=pd.read_csv(\"data/titanic.csv\")\n\n# 计算所有人年龄的均值\nage_median1 = titanic_df.Age.median()\n\n# 使用fillna填充缺失值,inplace=True表示在原数据titanic_df上直接进行修改\ntitanic_df.Age.fillna(age_median1,inplace=True)\n#查看Age列的统计值\nprint(titanic_df.Age.describe())\n#print(titanic_df.info())\n\n# 考虑性别因素，分别用男女乘客各自年龄的中位数来填补\n# 重新载入原始数据\ntitanic_df=pd.read_csv(\"data/titanic.csv\")\n# 分组计算男女年龄的中位数， 得到一个Series数据，索引为Sex\nage_median2 = titanic_df.groupby('Sex').Age.median()\n# 设置Sex为索引\ntitanic_df.set_index('Sex',inplace=True)\n# 使用fillna填充缺失值，根据索引值填充\ntitanic_df.Age.fillna(age_median2, inplace=True)\n# 重置索引，即取消Sex索引\ntitanic_df.reset_index(inplace=True)\n# 查看Age列的统计值\nprint(titanic_df.Age.describe())\n\n#同时考虑性别和舱位因素\n\n# 重新载入原始数据\ntitanic_df=pd.read_csv(\"data/titanic.csv\")\n# 分组计算不同舱位男女年龄的中位数， 得到一个Series数据，索引为Pclass,Sex\nage_median3 = titanic_df.groupby(['Pclass', 'Sex']).Age.median()\n# 设置Pclass, Sex为索引， inplace=True表示在原数据titanic_df上直接进行修改\ntitanic_df.set_index(['Pclass','Sex'], inplace=True)\nprint(titanic_df)\n\n# 使用fillna填充缺失值，根据索引值填充\ntitanic_df.Age.fillna(age_median3, inplace=True)\n# 重置索引，即取消Pclass,Sex索引\ntitanic_df.reset_index(inplace=True)\n\n# 查看Age列的统计值\ntitanic_df.Age.describe()\n\n将连续型变量离散化\n\n连续型变量离散化是建模中一种常用的方法\n离散化指的是将某个变量的所在区间分割为几个小区间，落在同一个区间的观测值用同一个符号表示\n以年龄为例，最小值是0.42（婴儿），最大值是80，如果我们想产生一个五个级（levels），我们可使用cut或者qcut函数\ncut函数将年龄的区间均匀分割为5分，而qcut则选取区间以至于每个区间里的观察值个数都是一样的（五等分）， 这里演示中使用cut函数。\n\n# 读取常用的包\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#读取数据\ntitanic_df = pd.read_csv('data/titanic.csv')\n\n#查看前五行数据\nprint(titanic_df.head())\n\n# 数据的统计描述\n# describe函数查看部分变量的分布\n# 因为Survived是0-1变量，所以均值就是幸存人数的百分比，这个用法非常有用\nprint(titanic_df[[\"Survived\",\"Age\",\"SibSp\",\"Parch\"]].describe())\n\n# 使用include=[np.object]来查看分类变量\n# count: 非缺失值的个数\n# unique: 非重复值得个数\n# top: 最高频值\n# freq: 最高频值出现次数\n\nprint(titanic_df.describe(include=[np.object]))\n\n#不同舱位的分布情况是怎样的呢？\n# 方法1: value_counts\n# 查看不同舱位的分布\n# 头等舱：24%； 二等舱：21%； 三等舱：55%\n# value_counts 频数统计， len() 获取数据长度\nprint(titanic_df.Pclass.value_counts() / len(titanic_df))\n# 总共有891个乘客\n# Age有714个非缺失值，Cabin只有204个非缺失值。我们将会讲解如何处理缺失值\nprint(titanic_df.info())\n\n#方法2：group_by\n# sort_values 将结果排序\n(titanic_df.groupby(\"Pclass\").agg(\"size\")/len(titanic_df)).sort_values(ascending=False)\n\n# 填补年龄数据中的缺失值\n# 直接使用所有人年龄的中位数来填补\n# 在处理之前，查看Age列的统计值\nprint(titanic_df.Age.describe())\n\n# 重新载入原始数据\ntitanic_df=pd.read_csv(\"data/titanic.csv\")\n\n# 计算所有人年龄的均值\nage_median1 = titanic_df.Age.median()\n\n# 使用fillna填充缺失值,inplace=True表示在原数据titanic_df上直接进行修改\ntitanic_df.Age.fillna(age_median1,inplace=True)\n#查看Age列的统计值\nprint(titanic_df.Age.describe())\n#print(titanic_df.info())\n\n# 考虑性别因素，分别用男女乘客各自年龄的中位数来填补\n# 重新载入原始数据\ntitanic_df=pd.read_csv(\"data/titanic.csv\")\n# 分组计算男女年龄的中位数， 得到一个Series数据，索引为Sex\nage_median2 = titanic_df.groupby('Sex').Age.median()\n# 设置Sex为索引\ntitanic_df.set_index('Sex',inplace=True)\n# 使用fillna填充缺失值，根据索引值填充\ntitanic_df.Age.fillna(age_median2, inplace=True)\n# 重置索引，即取消Sex索引\ntitanic_df.reset_index(inplace=True)\n# 查看Age列的统计值\nprint(titanic_df.Age.describe())\n\n#同时考虑性别和舱位因素\n\n# 重新载入原始数据\ntitanic_df=pd.read_csv(\"data/titanic.csv\")\n# 分组计算不同舱位男女年龄的中位数， 得到一个Series数据，索引为Pclass,Sex\nage_median3 = titanic_df.groupby(['Pclass', 'Sex']).Age.median()\n# 设置Pclass, Sex为索引， inplace=True表示在原数据titanic_df上直接进行修改\ntitanic_df.set_index(['Pclass','Sex'], inplace=True)\nprint(titanic_df)\n\n# 使用fillna填充缺失值，根据索引值填充\ntitanic_df.Age.fillna(age_median3, inplace=True)\n# 重置索引，即取消Pclass,Sex索引\ntitanic_df.reset_index(inplace=True)\n\n# 查看Age列的统计值\ntitanic_df.Age.describe()\n\n\n###\n#分析哪些因素会决定生还概率\n###\n\n# 舱位与生还概率\n#计算每个舱位的生还概率\n# 方法1：使用经典的分组-聚合-计算\n# 注意：因为Survived是0-1函数，所以均值即表示生还百分比\nprint(titanic_df[['Pclass', 'Survived']].groupby('Pclass').mean() \\\n    .sort_values(by='Survived', ascending=False))\n\n# 方法2：我们还可以使用pivot_table函数来实现同样的功能（本次课新内容）\n# pivot table中文为数据透视表\n# values: 聚合后被施加计算的值，这里我们施加mean函数\n# index: 分组用的变量\n# aggfunc: 定义施加的函数\nprint(titanic_df.pivot_table(values='Survived', index='Pclass', aggfunc=np.mean))\n\n# 绘制舱位和生还概率的条形图\n# 使用sns.barplot做条形图，图中y轴给出 Survived 均值的点估计\n#sns.barplot(data=titanic_df,x='Pclass',y='Survived',ci=None)\n# plt.show()\n\n#####\n#性别与生还概率\n#####\n# 方法1：groupby\nprint(titanic_df[[\"Sex\", \"Survived\"]].groupby('Sex').mean() \\\n    .sort_values(by='Survived', ascending=False))\n# 方法2：pivot_table\nprint(titanic_df.pivot_table(values=\"Survived\",index='Sex',aggfunc=np.mean))\n\n# 绘制条形图\n#sns.barplot(data=titanic_df,x='Sex',y='Survived',ci=None)\n#plt.show()\n\n\n#####\n#综合考虑舱位和性别的因素，与生还概率的关系\n#####\n# 方法1：groupby\nprint(titanic_df[['Pclass','Sex', 'Survived']].groupby(['Pclass', 'Sex']).mean())\n\n# 方法2：pivot_table\ntitanic_df.pivot_table(values='Survived', index=['Pclass', 'Sex'], aggfunc=np.mean)\n\n# 方法3：pivot_talbe\n# columns指定另一个分类变量，只不过我们将它列在列里而不是行里，这也是为什么这个变量称为columns\nprint(titanic_df.pivot_table(values=\"Survived\",index=\"Pclass\",columns=\"Sex\",aggfunc=np.mean))\n\n#绘制条形图：使用sns.barplot\n#sns.barplot(data=titanic_df,x='Pclass',y='Survived',hue='Sex',ci=None)\n# plt.show()\n\n# 绘制折线图：使用sns.pointplot\nsns.pointplot(data=titanic_df,x='Pclass',y=\"Survived\",hue=\"Sex\",ci=None)\n#plt.show()\n\n####\n#年龄与生还情况\n####\n#与上面的舱位、性别这些分类变量不同，年龄是一个连续的变量\n\n#生还组和罹难组的年龄分布直方图\n#使用seaborn包中的 FacetGrid().map() 来快速生成高质量图片\n# col='Survived'指定将图片在一行中做出生还和罹难与年龄的关系图\nsns.FacetGrid(titanic_df,col='Survived').\\\n    map(plt.hist,'Age',bins=20,normed=True)\n# plt.show()\n\n\n###\n#将连续型变量离散化\n###\n#我们使用cut函数\n#我们可以看到每个区间的大小是固定的,大约是16岁\n\ntitanic_df['AgeBand'] = pd.cut(titanic_df['Age'],5)\nprint(titanic_df.head())\n\n#查看落在不同年龄区间里的人数\n#方法1：value_counts(), sort=False表示不需要将结果排序\nprint(titanic_df.AgeBand.value_counts(sort=False))\n\n#方法2：pivot_table\nprint(titanic_df.pivot_table(values='Survived',index='AgeBand',aggfunc='count'))\n\n#查看各个年龄区间的生还率\nprint(titanic_df.pivot_table(values=\"Survived\",index='AgeBand',aggfunc=np.mean))\nsns.barplot(data=titanic_df,x='AgeBand',y='Survived',ci=None)\nplt.xticks(rotation=60)\nplt.show()\n\n\n####\n# 年龄、性别 与生还概率\n####\n# 查看落在不同区间里男女的生还概率\nprint(titanic_df.pivot_table(values='Survived',index='AgeBand', columns='Sex', aggfunc=np.mean))\n\nsns.pointplot(data=titanic_df, x='AgeBand', y='Survived', hue='Sex', ci=None)\nplt.xticks(rotation=60)\n\nplt.show()\n\n####\n#年龄、舱位、性别 与生还概率\n####\ntitanic_df.pivot_table(values='Survived',index='AgeBand', columns=['Sex', 'Pclass'], aggfunc=np.mean)\n\n\n\n# 回顾sns.pointplot 绘制舱位、性别与生还概率的关系图\nsns.pointplot(data=titanic_df, x='Pclass', y='Survived', hue='Sex', ci=None)\n人工神经网络\nhttps://keras.io\n机器学习\n特征工程\n特征工程到底是什么？\n案例分析：共享单车需求特征工程（feature engineering）\n\n数据和特征决定了机器学习的上限，而一个好的模型只是逼近那个上限而已\n我们的目标是尽可能得从原始数据上获取有用的信息，一些原始数据本身往往不能直接作为模型的变量。\n特征工程是利用数据领域的相关知识来创建能够使机器学习算法达到最佳性能的特征的过程。\n\n日期型变量的处理\n以datetime为例子，这个特征里包含了日期和时间点两个重要信息。我们还可以进一步从日期中导出其所对应的月份和星期数。\n#租车人数是由哪些因素决定的？\n#导入数据分析包\nimport numpy as np\nimport pandas as pd\n\n#导入绘图工具包\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#导入日期时间变量处理相关的工具包\nimport calendar\nfrom datetime import datetime\n\n# 读取数据\nBikeData = pd.read_csv('data/bike.csv')\n\n\n#####\n#了解数据大小\n#查看前几行/最后几行数据\n#查看数据类型与缺失值\n####\n# 第一步：查看数据大小\n\nprint(BikeData.shape)\n\n# 第二步：查看前10行数据\nprint(BikeData.head(10))\n\n\n# 第三步：查看数据类型与缺失值\n# 大部分变量为整数型，温度和风速为浮点型变量\n# datetime类型为object，我们将在下面进一步进行处理\n# 没有缺失值！\nprint(BikeData.info())\n\n\n####\n#日期型变量的处理\n####\n\n# 取datetime中的第一个元素为例，其数据类型为字符串，所以我们可以使用split方法将字符串拆开\n# 日期+时间戳是一个非常常见的数据形式\nex = BikeData.datetime[1]\nprint(ex)\n\nprint(type(ex))\n\n# 使用split方法将字符串拆开\nex.split()\n\n# 获取日期数据\nex.split()[0]\n\n# 首先获得日期，定义一个函数使用split方法将日期+时间戳拆分为日期和\ndef get_date(x):\n    return(x.split()[0])\n\n# 使用pandas中的apply方法，对datatime使用函数get_date\nBikeData['date'] = BikeData.datetime.apply(get_date)\n\nprint(BikeData.head())\n\n# 生成租车时间(24小时）\n# 为了取小时数，我们需要进一步拆分\nprint(ex.split()[1])\n#\":\"是分隔符\nprint(ex.split()[1].split(\":\")[0])\n\n# 将上面的内容定义为get_hour的函数，然后使用apply到datatime这个特征上\ndef get_hour(x):\n    return (x.split()[1].split(\":\")[0])\n# 使用apply方法，获取整列数据的时间\nBikeData[\"hour\"] = BikeData.datetime.apply(get_hour)\n\nprint(BikeData.head())\n\n####\n# 生成日期对应的星期数\n####\n# 首先引入calendar中的day_name，列举了周一到周日\nprint(calendar.day_name[:])\n\n#获取字符串形式的日期\ndateString = ex.split()[0]\n\n# 使用datatime中的strptime函数将字符串转换为日期时间类型\n# 注意这里的datatime是一个包不是我们dataframe里的变量名\n# 这里我们使用\"%Y-%m-%d\"来指定输入日期的格式是按照年月日排序，有时候可能会有月日年的排序形式\nprint(dateString)\ndateDT = datetime.strptime(dateString,\"%Y-%m-%d\")\nprint(dateDT)\nprint(type(dateDT))\n\n# 然后使用weekday方法取出日期对应的星期数\n# 是0-6的整数，星期一对应0， 星期日对应6\nweek_day = dateDT.weekday()\n\nprint(week_day)\n# 将星期数映射到其对应的名字上\nprint(calendar.day_name[week_day])\n\n\n# 现在将上述的过程融合在一起变成一个获取星期的函数\ndef get_weekday(dateString):\n    week_day = datetime.strptime(dateString,\"%Y-%m-%d\").weekday()\n    return (calendar.day_name[week_day])\n\n# 使用apply方法，获取date整列数据的星期\nBikeData[\"weekday\"] = BikeData.date.apply(get_weekday)\n\nprint(BikeData.head())\n\n\n####\n# 生成日期对应的月份\n####\n\n# 模仿上面的过程，我们可以提取日期对应的月份\n# 注意：这里month是一个attribute不是一个函数，所以不用括号\n\ndef get_month(dateString):\n    return (datetime.strptime(dateString,\"%Y-%m-%d\").month)\n# 使用apply方法，获取date整列数据的月份\nBikeData[\"month\"] = BikeData.date.apply(get_month)\nprint(BikeData.head())\n\n####\n#数据可视化举例\n####\n\n#绘制租车人数的箱线图， 以及人数随时间（24小时）变化的箱线图\n# 设置画布大小\nfig = plt.figure(figsize=(18,5))\n\n# 添加第一个子图\n# 租车人数的箱线图\nax1 = fig.add_subplot(121)\nsns.boxplot(data=BikeData,y=\"count\")\nax1.set(ylabel=\"Count\",title=\"Box Plot On Count\")\n\n\n# 添加第二个子图\n# 租车人数和时间的箱线图\n# 商业洞察：租车人数由时间是如何变化的?\nax2 = fig.add_subplot(122)\nsns.boxplot(data=BikeData,y=\"count\",x=\"hour\")\nax2.set(xlabel=\"Hour\",ylabel=\"Count\",title=\"Box Plot On Count Across Hours\")\nplt.show()\n\n机器学习\n\n机器学习（Machine Learning）是人工智能的分支，其目标是通过算法从现有的数据中建立模型（学习）来解决问题。\n机器学习是一门交叉学科，涉及概率统计（probability and statistics），优化（optimization），和计算机编程（computer programming）等等。\n用途极为广泛：从预测信用卡违约风险，癌症病人五年生存概率到汽车无人驾驶，都有着机器学习的身影。\n备受重视：人们在决策分析的时候越来越多得用定量方法（quantitative approach）来衡量一个决策的优劣。\n\n监督学习：\n\n监督学习（Supervised Learning）：从给定的训练数据集中学习出一个函数，当新的数据到来时，可以根据这个函数预测结果。监督学习的训练集（training data）要求是包括输入和输出，也可以说是特征和目标。\n监督学习中又可进一步分为两大类主要问题：预测与分类。房价预测是一个典型的预测问题，房价作为目标是一个连续型变量。信用卡违约预测是一个典型的分类问题，是否违约作为一个目标是一个分类变量。\n\n无监督学习\n\n无监督学习（Unsupervised Learning）：训练集没有人为标注的结果。我们从输入数据本身探索规律。\n无监督学习的例子包括图片聚类分析，文章主题分类，基因序列分析，和高纬数据（high dimensional data) 降维等等。\n\n案例分析：波士顿地区房价注意波士顿房价数据是scikit-learn中的Toy datasets  可通过函数datasets.load_boston()直接加载\n学习资源\n机器学习教程 及 笔记https://www.datacamp.com/http://matplotlib.org/2.1.0/g...https://www.kesci.com/https://keras.io\n竞赛\nhttps://www.kaggle.com/天池大数据竞赛和Kaggle、DataCastle的比较，哪个比较好？天池新人实战赛\n参考\nThe Python Tutorialpython写入csv文件的几种方法总结常见安装第三方库问题慕课网 Python在数据科学中的应用慕课网 Python数据分析-基础技术篇《利用python进行数据分析》DataLearningTeam/PythonDataVisualization使用 NumPy 进行科学计算使用Python进行描述性统计Documentation of scikit-learn 0.19.1Seaborn tutorial特征工程\n\n\n                ", "mainLikeNum": ["0 "], "mainBookmarkNum": "3"}