{"title": "crawl facebook user basic infomation and photos - magicfolks ", "index": "scrapy,python", "content": "前言\n自从之前爬取twitter后公司要求对fancebook进行爬取，瞬间心中有一万只×××。毕竟这些社交网络的站点反爬机制做的很不错。但既然上面安排下来只能硬着头皮上了。通过抓包，发现登陆m.facebook.com站点psot的数据相比facebook.com要简单,所有就写了一套利用scrapy爬取facebook的爬虫。\n模拟登陆\n\nfrom scrapy import Spider\nfrom scrapy.http import Request, FormRequest\n\n\nclass FacebookLogin(Spider):\n    download_delay = 0.5\n\n    usr = \"××××\" # your username/email/phone number\n    pwd = \"××××\" #account password\n\n    def start_requests(self):\n        return [Request(\"https://m.facebook.com/\", callback=self.parse)]\n\n    def parse(self, response):\n        return FormRequest.from_response(response,\n                                            formdata={\n                                                'email': self.usr,\n                                                'pass': self.pwd\n                                            }, callback=self.remember_browser)\n\n    def remember_browser(self, response):\n        # if re.search(r'(checkpoint)', response.url):\n            # Use 'save_device' instead of 'dont_save' to save device\n        return FormRequest.from_response(response,\n                                                formdata={'name_action_selected': 'dont_save'},\n                                                callback=self.after_login)\n\n    def after_login(self, response):\n        pass\n\n注：为了保险起见可以在seething文件中添加一个手机端的USER-AGENT\n爬取用户基本信息\n# -*- coding: UTF-8 -*-\nimport re\nfrom urlparse import urljoin\n\nfrom scrapy import Item, Field\nfrom scrapy.http import Request\nfrom scrapy.selector import Selector\n\nfrom facebook_login import FacebookLogin\n\n\nclass FacebookItems(Item):\n    id = Field()\n    url = Field()\n    name = Field()\n    work = Field()\n    education = Field()\n    family = Field()\n    skills = Field()\n    address = Field()\n    contact_info = Field()\n    basic_info = Field()\n    bio = Field()\n    quote = Field()\n    nicknames = Field()\n    relationship = Field()\n    image_urls = Field()\n\nclass FacebookProfile(FacebookLogin):\n    download_delay = 2\n    name = \"fb\"\n    links = None\n    start_ids = [\n        \"plok74122\", \"bear.black.12\",\"tabaco.wang\",\"chaolin.chang.q\",\"ahsien.liu\",\"kaiwen.cheng.100\",\"liang.kevin.92\",\"bingheng.tsai.9\",\"psppupu\",\n                  'cscgbakery',\"hc.shiao.l\",\"asusisbad\",\"benjamin\",\"franklin\",\n        # 'RobertScoble'\n    ]\n                  # \"https://m.facebook.com/tabaco.wang?v=info\",'https://m.facebook.com/RobertScoble?v=info']\n\n    def after_login(self, response):\n        for id in self.start_ids:\n            url = \"https://m.facebook.com/%s?v=info\" %id\n            yield Request(url, callback=self.parse_profile,meta={\"id\":id})\n\n    def parse_profile(self, response):\n        item = FacebookItems()\n\n        item['id'] = response.meta['id']\n        item['url'] = response.url\n        item[\"name\"] = \"\".join(response.css('#root strong *::text').extract())\n\n        item[\"work\"] = self.parse_info_has_image(response, response.css('#work'))\n        item[\"education\"] = self.parse_info_has_image(response, response.css('#education'))\n        item[\"family\"] = self.parse_info_has_image(response, response.css('#family'))\n\n        item[\"address\"] = self.parse_info_has_table(response.css('#living'))\n        item[\"contact_info\"] = self.parse_info_has_table(response.css('#contact-info'))\n        item[\"basic_info\"] = self.parse_info_has_table(response.css('#basic-info'))\n        item[\"nicknames\"] = self.parse_info_has_table(response.css('#nicknames'))\n\n        item[\"skills\"] = self.parse_info_text_only(response.css('#skills'))\n        item[\"bio\"] = self.parse_info_text_only(response.css('#bio'))\n        item[\"quote\"] = self.parse_info_text_only(response.css('#quote'))\n        item[\"relationship\"] = self.parse_info_text_only(response.css('#relationship'))\n\n        yield item\n\n\n    def parse_info_has_image(self, response, css_path):\n        info_list = []\n        for div in css_path.xpath('div/div[2]/div'):\n            url = urljoin(response.url, \"\".join(div.css('div > a::attr(href)').extract()))\n            title = \"\".join(div.css('div').xpath('span | h3').xpath('a/text()').extract())\n            info = \"\\n\".join(div.css('div').xpath('span | h3').xpath('text()').extract())\n            if url and title and info:\n                info_list.append({\"url\": url, \"title\": title, \"info\": info})\n        return info_list\n\n    def parse_info_has_table(self, css_path):\n        info_dict = {}\n        for div in css_path.xpath('div/div[2]/div'):\n            key = \"\".join(div.css('td:first-child div').xpath('span | span/span[1]').xpath('text()').extract())\n            value = \"\".join(div.css('td:last-child').xpath('div//text()').extract()).strip()\n            if key and value:\n                if key in info_dict:\n                    info_dict[key] += \", %s\" % value\n                else:\n                    info_dict[key] = value\n        return info_dict\n\n    def parse_info_text_only(self, css_path):\n        text = css_path.xpath('div/div[2]//text()').extract()\n        text = [t.strip() for t in text]\n        text = [t for t in text if re.search('\\w+', t) and t != \"Edit\"]\n        return \"\\n\".join(text)\n\n爬取用户的所有图片\n虽然图片在https://m.facebook.com/%s?v=info中会有显示，但是真正的图片链接却需要几次请求之后才能拿到，本作在spider中尽量少的操作原则故将抓取图片也单独写成了一个爬虫，如下：\n# -*- coding: UTF-8 -*-\nfrom scrapy.spider import CrawlSpider,Rule,Spider\nfrom scrapy.linkextractor import LinkExtractor\nfrom facebook_login import FacebookLogin\nfrom scrapy.http import Request\nfrom scrapy.selector import Selector\nfrom scrapy import Item, Field\nimport re,hashlib\nimport sys\nreload(sys)\nsys.setdefaultencoding('utf-8')\n\nclass FacebookPhotoItems(Item):\n    url = Field()\n    id = Field()\n    photo_links = Field()\n    md5 = Field()\nclass CrawlPhoto(FacebookLogin):\n    name = 'fbphoto'\n    timelint_photo = None\n    id = None\n    links = []\n    start_ids = [\n        \"plok74122\", \"bear.black.12\", \"tabaco.wang\", \"chaolin.chang.q\",\n        # \"ashien.liu\",\n        \"liang.kevin.92\",\"qia.chen\",\n        \"bingheng.tsai.9\", \"psppupu\",\n        'cscgbakery', \"hc.shiao.l\", \"asusisbad\", \"benjamin\", \"franklin\",\n        # 'RobertScoble'\n    ]\n\n    def after_login(self, response):\n        for url in self.start_ids:\n            yield Request('https://m.facebook.com/%s/photos'%url,callback=self.parse_item,meta={\"id\":url})\n        # yield Request('https://m.facebook.com/%s/photos'%self.id,callback=self.parse_item)\n    def parse_item(self,response):\n        # print response.body\n        urls = response.xpath('//span').extract()\n        next_page = None\n        try:\n            next_page = response.xpath('//div[@class=\\'co\\']/a/@href').extract()[0].strip()\n        except:\n            pass\n        # urls = response.xpath('//div[@data-sigil=\\'marea\\']').extract()\n        for i in urls:\n            # if i.find(u'时间线照片')!=-1:\n            try:\n                self.timeline_photo = Selector(text=i).xpath('//span/a/@href').extract()[0]\n                if self.timeline_photo is not None:\n                    yield Request('https://m.facebook.com/%s'%self.timeline_photo,callback=self.parse_photos,meta=response.meta)\n            except:\n                continue\n        if next_page:\n            print '-----------------------next image page -----------------------------------------'\n            yield Request('https://m.facebook.com/%s'%next_page,callback=self.parse_item,meta=response.meta)\n    def parse_photos(self,response):\n        urls = response.xpath(\"//a[@class=\\'bw bx\\']/@href\").extract()\n        # urls = response.xpath(\"//a[@class=\\'_39pi _4i6j\\']/@href\").extract()\n        for i in urls:\n            yield Request('https://m.facebook.com/%s'%i,callback=self.process_photo_url,meta=response.meta)\n        if len(urls) == 12:\n            next_page = response.xpath('//div[@id=\\'m_more_item\\']/a/@href').extract()[0]\n            yield Request('https://m.facebook.com/%s'%next_page,callback=self.parse_photos,meta=response.meta)\n    def process_photo_url(self,response):\n        # photo_url = response.xpath('//i[@class=\\'img img\\']').extract()\n        item = FacebookPhotoItems()\n        item['url'] = response.url\n        item['id'] = response.meta['id']\n        photo_url = response.xpath('//div[@style=\\'text-align:center;\\']/img/@src').extract()[0]\n        item['photo_links'] = photo_url\n        item['md5'] = self.getstr_md5(item['photo_links'])+\".jpg\"\n        yield item\n\n    def wirtefile(self,str):\n        with open('temp2.html','w') as file:\n            file.write(str)\n            file.write('\\n')\n\n    def getstr_md5(self, input):\n        if input is None:\n            input = ''\n        md = hashlib.md5()\n        md.update(input)\n        return md.hexdigest()\n\n因为我的python水平也是半路出家，所有还没有找到一个好的办法将图片链接的抓取集成到抓取基本信息的那个爬虫上，如果有大神知道还请指点一二。下载图片没有使用scrapy的imagePipline,而是使用的wget命令,原因就是上面所说，python水平太菜。。。下面是自己写的一个下载图片的pipline:\nclass MyOwenImageDownload(object):\n    def process_item(self, item,spider):\n        if len(item) >6:\n            pass\n        else:\n            file = \"image/\"+item['id']\n            if os.path.exists(file):\n                pass\n            else:\n                os.makedirs(file)\n            cmd = 'wget \\'%s\\' -O %s -P %s --timeout=10 -q'%(item['photo_links'],file+\"/\"+item['md5'],file)\n            os.system(cmd)\n        return item\n结语\n至此，整个爬虫基本的结构已经写完。。。源码地址\nIn the end, we will remember not the words of our enemies but the silence of our friends\n\n                ", "mainLikeNum": ["1 "], "mainBookmarkNum": "1"}