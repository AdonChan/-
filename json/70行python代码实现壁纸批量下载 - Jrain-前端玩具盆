{"title": "70行python代码实现壁纸批量下载 - Jrain-前端玩具盆 ", "index": "python爬虫,python3.x,python", "content": "项目地址：https://github.com/jrainlau/w...\n前言\n好久没有写文章了，因为最近都在适应新的岗位，以及利用闲暇时间学习python。这篇文章是最近的一个python学习阶段性总结，开发了一个爬虫批量下载某壁纸网站的高清壁纸。\n注意：本文所属项目仅用于python学习，严禁作为其他用途使用！\n初始化项目\n项目使用了virtualenv来创建一个虚拟环境，避免污染全局。使用pip3直接下载即可：\npip3 install virtualenv\n然后在合适的地方新建一个wallpaper-downloader目录，使用virtualenv创建名为venv的虚拟环境：\nvirtualenv venv\n\n. venv/bin/activate\n接下来创建依赖目录：\necho bs4 lxml requests > requirements.txt\n最后yun下载安装依赖即可：\npip3 install -r requirements.txt\n分析爬虫工作步骤\n为了简单起见，我们直接进入分类为“aero”的壁纸列表页：http://wallpaperswide.com/aer...。\n\n可以看到，这一页里面一共有10张可供下载的壁纸。但是由于这里显示的都是缩略图，作为壁纸来说清晰度是远远不够的，所以我们需要进入壁纸详情页，去找到高清的下载链接。从第一张壁纸点进去，可以看到一个新的页面：\n\n因为我机器是Retina屏幕，所以我打算直接下载体积最大的那个以保证高清（红圈所示体积）。\n了解了具体的步骤以后，就是通过开发者工具找到对应的dom节点，提取相应的url即可，这个过程就不再展开了，读者自行尝试即可，下面进入编码部分。\n访问页面\n新建一个download.py文件，然后引入两个库：\nfrom bs4 import BeautifulSoup\nimport requests\n接下来，编写一个专门用于访问url，然后返回页面html的函数：\ndef visit_page(url):\n    headers = {\n      'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.108 Safari/537.36'\n    }\n    r = requests.get(url, headers = headers)\n    r.encoding = 'utf-8'\n    return BeautifulSoup(r.text, 'lxml')\n为了防止被网站反爬机制击中，所以我们需要通过在header添加UA把爬虫伪装成正常的浏览器，然后指定utf-8编码，最后返回字符串格式的html。\n提取链接\n在获取了页面的html以后，就需要提取这个页面壁纸列表所对应的url了：\ndef get_paper_link(page):\n    links = page.select('#content > div > ul > li > div > div a')\n    return [link.get('href') for link in links]\n这个函数会把列表页所有壁纸详情的url给提取出来。\n下载壁纸\n有了详情页的地址以后，我们就可以进去挑选合适的size了。在对页面的dom结构分析后可以知道，每一个size都对应着一个链接：\n\n所以第一步，就是把这些size对应的链接提取出来：\nwallpaper_source = visit_page(link)\nwallpaper_size_links = wallpaper_source.select('#wallpaper-resolutions > a')\nsize_list = [{\n    'size': eval(link.get_text().replace('x', '*')),\n    'name': link.get('href').replace('/download/', ''),\n    'url': link.get('href')\n} for link in wallpaper_size_links]\nsize_list就是这些链接的一个集合。为了方便接下来选出最高清（体积最大）的壁纸，在size中我使用了eval方法，直接把这里的5120x3200给计算出来，作为size的值。\n获取了所有的集合之后，就可以使用max()方法选出最高清的一项出来了：\nbiggest_one = max(size_list, key = lambda item: item['size'])\n这个biggest_one当中的url就是对应size的下载链接，接下来只需要通过requests库把链接的资源下载下来即可：\nresult = requests.get(PAGE_DOMAIN + biggest_one['url'])\n\nif result.status_code == 200:\n    open('wallpapers/' + biggest_one['name'], 'wb').write(result.content)\n注意，首先你需要在根目录下创建一个wallpapers目录，否则运行时会报错。\n整理一下，完整的download_wallpaper函数长这样：\ndef download_wallpaper(link):\n    wallpaper_source = visit_page(PAGE_DOMAIN + link)\n    wallpaper_size_links = wallpaper_source.select('#wallpaper-resolutions > a')\n    size_list = [{\n        'size': eval(link.get_text().replace('x', '*')),\n        'name': link.get('href').replace('/download/', ''),\n        'url': link.get('href')\n    } for link in wallpaper_size_links]\n\n    biggest_one = max(size_list, key = lambda item: item['size'])\n    print('Downloading the ' + str(index + 1) + '/' + str(total) + ' wallpaper: ' + biggest_one['name'])\n    result = requests.get(PAGE_DOMAIN + biggest_one['url'])\n\n    if result.status_code == 200:\n        open('wallpapers/' + biggest_one['name'], 'wb').write(result.content)\n批量运行\n上述的步骤仅仅能够下载第一个壁纸列表页的第一张壁纸。如果我们想下载多个列表页的全部壁纸，我们就需要循环调用这些方法。首先我们定义几个常量：\nimport sys\n\nif len(sys.argv) != 4:\n    print('3 arguments were required but only find ' + str(len(sys.argv) - 1) + '!')\n    exit()\n\ncategory = sys.argv[1]\n\ntry:\n    page_start = [int(sys.argv[2])]\n    page_end = int(sys.argv[3])\nexcept:\n    print('The second and third arguments must be a number but not a string!')\n    exit()\n这里通过获取命令行参数，指定了三个常量category, page_start和page_end，分别对应着壁纸分类，起始页页码，终止页页码。\n为了方便起见，再定义两个url相关的常量：\nPAGE_DOMAIN = 'http://wallpaperswide.com'\nPAGE_URL = 'http://wallpaperswide.com/' + category + '-desktop-wallpapers/page/'\n接下来就可以愉快地进行批量操作了，在此之前我们来定义一个start()启动函数：\ndef start():\n    if page_start[0] <= page_end:\n        print('Preparing to download the ' + str(page_start[0])  + ' page of all the \"' + category + '\" wallpapers...')\n        PAGE_SOURCE = visit_page(PAGE_URL + str(page_start[0]))\n        WALLPAPER_LINKS = get_paper_link(PAGE_SOURCE)\n        page_start[0] = page_start[0] + 1\n\n        for index, link in enumerate(WALLPAPER_LINKS):\n            download_wallpaper(link, index, len(WALLPAPER_LINKS), start)\n然后把之前的download_wallpaper函数再改写一下：\ndef download_wallpaper(link, index, total, callback):\n    wallpaper_source = visit_page(PAGE_DOMAIN + link)\n    wallpaper_size_links = wallpaper_source.select('#wallpaper-resolutions > a')\n    size_list = [{\n        'size': eval(link.get_text().replace('x', '*')),\n        'name': link.get('href').replace('/download/', ''),\n        'url': link.get('href')\n    } for link in wallpaper_size_links]\n\n    biggest_one = max(size_list, key = lambda item: item['size'])\n    print('Downloading the ' + str(index + 1) + '/' + str(total) + ' wallpaper: ' + biggest_one['name'])\n    result = requests.get(PAGE_DOMAIN + biggest_one['url'])\n\n    if result.status_code == 200:\n        open('wallpapers/' + biggest_one['name'], 'wb').write(result.content)\n\n    if index + 1 == total:\n        print('Download completed!\\n\\n')\n        callback()\n最后指定一下启动规则：\nif __name__ == '__main__':\n     start()\n\n运行项目\n在命令行输入如下代码开始测试：\npython3 download.py aero 1 2\n然后可以看到下列输出：\n\n拿charles抓一下包，可以看到脚本正在平稳地运行中：\n\n此时，下载脚本已经开发完毕，终于不用担心壁纸荒啦！\n\n                ", "mainLikeNum": ["15 "], "mainBookmarkNum": "45"}