{"title": "Python爬虫之多线程下载豆瓣Top250电影图片 - 个人文章 ", "index": "网页爬虫,python", "content": "爬虫项目介绍\n  本次爬虫项目将爬取豆瓣Top250电影的图片，其网址为：https://movie.douban.com/top250， 具体页面如下图所示：\n\n  本次爬虫项目将分别不使用多线程和使用多线程来完成，通过两者的对比，显示出多线程在爬虫项目中的巨大优势。本文所使用的多线程用到了concurrent.futures模块，该模块是Python中最广为使用的并发库，它可以非常方便地将任务并行化。在concurrent.futures模块中，共有两种并发模块，分别如下：\n\n多线程模式：ThreadPoolExecutor，适合 IO密集型任务；\n多进程模式：ProcessPoolExecutor，适合计算密集型任务。\n\n具体的关于该模块的介绍可以参考其官方网址：https://docs.python.org/3/lib... 。  本次爬虫项目将会用到concurrent.futures模块中的ThreadPoolExecutor类，多线程豆瓣Top250电影图片。下面将会给出本次爬虫项目分别不使用多线程和使用多线程的对比，以此来展示多线程在爬虫中的巨大优势。\n不使用多线程\n  首先，我们不使用多线程来下载豆瓣Top250电影图片，其完整的Python代码如下：\nimport time\nimport requests\nimport urllib.request\nfrom bs4 import BeautifulSoup\n\n# 该函数用于下载图片\n# 传入函数： 网页的网址url\ndef download_picture(url):\n\n    # 获取网页的源代码\n    r = requests.get(url)\n    # 利用BeautifulSoup将获取到的文本解析成HTML\n    soup = BeautifulSoup(r.text, \"lxml\")\n    # 获取网页中的电影图片\n    content = soup.find('div', class_='article')\n    images = content.find_all('img')\n    # 获取电影图片的名称和下载地址\n    picture_name_list = [image['alt'] for image in images]\n    picture_link_list = [image['src'] for image in images]\n\n    # 利用urllib.request..urlretrieve正式下载图片\n    for picture_name, picture_link in zip(picture_name_list, picture_link_list):\n        urllib.request.urlretrieve(picture_link, 'E://douban/%s.jpg' % picture_name)\n\n\ndef main():\n\n    # 全部10个网页\n    start_urls = [\"https://movie.douban.com/top250\"]\n    for i in range(1, 10):\n        start_urls.append(\"https://movie.douban.com/top250?start=%d&filter=\" % (25 * i))\n\n    # 统计该爬虫的消耗时间\n    t1 = time.time()\n    print('*' * 50)\n\n    for url in start_urls:\n        download_picture(url)\n    t2 = time.time()\n\n    print('不使用多线程，总共耗时：%s'%(t2-t1))\n    print('*' * 50)\n\nmain()\n其输出结果如下：\n**************************************************\n不使用多线程，总共耗时：79.93260931968689\n**************************************************\n去E盘中的douban文件夹查看，如下图：\n\n  我们可以看到，在不使用多线程的情况下，这个爬虫总共耗时约80s，完成了豆瓣Top250电影图片的下载。\n使用多线程\n  接下来，我们使用多线程来下载豆瓣Top250电影图片，其完整的Python代码如下：\nimport time\nimport requests\nimport urllib.request\nfrom bs4 import BeautifulSoup\nfrom concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETED\n\n# 该函数用于下载图片\n# 传入函数： 网页的网址url\ndef download_picture(url):\n\n    # 获取网页的源代码\n    r = requests.get(url)\n    # 利用BeautifulSoup将获取到的文本解析成HTML\n    soup = BeautifulSoup(r.text, \"lxml\")\n    # 获取网页中的电影图片\n    content = soup.find('div', class_='article')\n    images = content.find_all('img')\n    # 获取电影图片的名称和下载地址\n    picture_name_list = [image['alt'] for image in images]\n    picture_link_list = [image['src'] for image in images]\n\n    # 利用urllib.request..urlretrieve正式下载图片\n    for picture_name, picture_link in zip(picture_name_list, picture_link_list):\n        urllib.request.urlretrieve(picture_link, 'E://douban/%s.jpg' % picture_name)\n\n\ndef main():\n\n    # 全部10个网页\n    start_urls = [\"https://movie.douban.com/top250\"]\n    for i in range(1, 10):\n        start_urls.append(\"https://movie.douban.com/top250?start=%d&filter=\" % (25 * i))\n\n    # 统计该爬虫的消耗时间\n    print('*' * 50)\n    t3 = time.time()\n\n    # 利用并发下载电影图片\n    executor = ThreadPoolExecutor(max_workers=10)  # 可以自己调整max_workers,即线程的个数\n    # submit()的参数： 第一个为函数， 之后为该函数的传入参数，允许有多个\n    future_tasks = [executor.submit(download_picture, url) for url in start_urls]\n    # 等待所有的线程完成，才进入后续的执行\n    wait(future_tasks, return_when=ALL_COMPLETED)\n\n    t4 = time.time()\n    print('使用多线程，总共耗时：%s' % (t4 - t3))\n    print('*' * 50)\n\nmain()\n其输出结果如下：\n**************************************************\n使用多线程，总共耗时：9.361606121063232\n**************************************************\n再去E盘中的douban文件夹查看，发现同样也下载了250张电影图片。\n总结\n  通过上述两个爬虫程序的对比，我们不难发现，同样是下载豆瓣Top250电影，10个网页中的图片，在没有使用多线程的情况下，总共耗时约80s，而在使用多线程（10个线程）的情况下，总共耗时约9.5秒，效率整整提高了约8倍。这样的效率提升在爬虫中无疑是令人兴奋的。  希望读者在看了本篇博客后，也能尝试着在自己的爬虫中使用多线程，说不定会有意外的惊喜哦~~因为，大名鼎鼎的Python爬虫框架Scrapy，也是使用多线程来提升爬虫速度的哦！\n注意：本人现已开通两个微信公众号： 因为Python（微信号为：python_math）以及轻松学会Python爬虫（微信号为：easy_web_scrape）， 欢迎大家关注哦~~\n\n                ", "mainLikeNum": ["2 "], "mainBookmarkNum": "1"}